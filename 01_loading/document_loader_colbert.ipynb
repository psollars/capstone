{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6182a442-8f6e-402c-97cf-95994715a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e54baef-efd1-4560-a6a6-54254442cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"../embeddings\"\n",
    "colbert_path = \"./../colbertv2.0/\"\n",
    "index_root = \"./../colbert_index/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87e71ee-35ec-495d-94fe-1ee5cde3b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in previously processed documents - syllabi and advising\n",
    "with open(f\"{persist_directory}/documents.pickle\", \"rb\") as handle:\n",
    "    documents = pickle.load(handle)\n",
    "\n",
    "with open(f\"{persist_directory}/transcripts.pickle\", \"rb\") as handle:\n",
    "    transcripts = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8fe5369-794a-4885-90a8-215512f32019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove one document from transcripts\n",
    "transcripts = [\n",
    "    t\n",
    "    for t in transcripts\n",
    "    if t.metadata[\"source\"]\n",
    "    != \"01_client-projects-and-data-webinar-from-the-engaged-learning-office.en.txt\"\n",
    "]\n",
    "\n",
    "# Split out documents to separate lists of document text and metadata\n",
    "doc_list = [doc.page_content for doc in documents]\n",
    "metadata_list = [doc.metadata for doc in documents]\n",
    "\n",
    "trans_list = [doc.page_content for doc in transcripts]\n",
    "trans_metadata_list = [doc.metadata for doc in transcripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6416a8-9ae7-4f9a-82f3-7db6e39a1aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:09:26] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnewman/miniconda3/envs/rag/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create new model from downloaded base model available on Hugging Face (https://huggingface.co/colbert-ir/colbertv2.0)\n",
    "# This does _not_ recognize the Apple Silicon GPU at this time\n",
    "RAG = RAGPretrainedModel.from_pretrained(colbert_path, index_root=index_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b165f18-cd8b-495e-8d6a-225a4a45c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Apr 09, 18:09:32] #> Note: Output directory /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/documents already exists\n",
      "\n",
      "\n",
      "[Apr 09, 18:09:32] #> Will delete 1 files already at /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/documents in 20 seconds...\n",
      "[Apr 09, 18:09:52] [0] \t\t #> Encoding 835 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]/Users/arnewman/miniconda3/envs/rag/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 27/27 [01:56<00:00,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:11:49] [0] \t\t avg_doclen_est = 103.75569152832031 \t len(local_sample) = 835\n",
      "[Apr 09, 18:11:49] [0] \t\t Creating 4,096 partitions.\n",
      "[Apr 09, 18:11:49] [0] \t\t *Estimated* 86,636 embeddings.\n",
      "[Apr 09, 18:11:49] [0] \t\t #> Saving the indexing plan to /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/documents/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used 20 iterations (11.6584s) to cluster 82305 items into 4096 clusters\n",
      "[0.031, 0.03, 0.029, 0.026, 0.027, 0.029, 0.029, 0.027, 0.028, 0.027, 0.028, 0.029, 0.03, 0.028, 0.029, 0.03, 0.026, 0.028, 0.026, 0.029, 0.028, 0.03, 0.029, 0.029, 0.028, 0.028, 0.032, 0.029, 0.029, 0.031, 0.032, 0.032, 0.032, 0.028, 0.027, 0.026, 0.03, 0.029, 0.028, 0.034, 0.03, 0.03, 0.028, 0.029, 0.03, 0.028, 0.028, 0.032, 0.031, 0.026, 0.026, 0.028, 0.031, 0.029, 0.028, 0.03, 0.031, 0.03, 0.034, 0.028, 0.029, 0.03, 0.03, 0.029, 0.033, 0.031, 0.03, 0.029, 0.029, 0.029, 0.03, 0.027, 0.03, 0.03, 0.029, 0.029, 0.03, 0.029, 0.03, 0.033, 0.032, 0.03, 0.029, 0.031, 0.029, 0.029, 0.028, 0.029, 0.028, 0.033, 0.029, 0.03, 0.029, 0.032, 0.029, 0.028, 0.033, 0.027, 0.03, 0.029, 0.03, 0.03, 0.028, 0.029, 0.029, 0.026, 0.028, 0.028, 0.027, 0.027, 0.03, 0.03, 0.03, 0.027, 0.031, 0.027, 0.032, 0.03, 0.03, 0.031, 0.029, 0.03, 0.028, 0.031, 0.028, 0.031, 0.029, 0.027]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:12:01] [0] \t\t #> Encoding 835 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▋                                          | 1/27 [00:04<02:03,  4.76s/it]\u001b[A\n",
      "  7%|███▎                                        | 2/27 [00:09<01:56,  4.67s/it]\u001b[A\n",
      " 11%|████▉                                       | 3/27 [00:14<01:52,  4.67s/it]\u001b[A\n",
      " 15%|██████▌                                     | 4/27 [00:18<01:47,  4.66s/it]\u001b[A\n",
      " 19%|████████▏                                   | 5/27 [00:23<01:42,  4.65s/it]\u001b[A\n",
      " 22%|█████████▊                                  | 6/27 [00:27<01:37,  4.64s/it]\u001b[A\n",
      " 26%|███████████▍                                | 7/27 [00:32<01:32,  4.63s/it]\u001b[A\n",
      " 30%|█████████████                               | 8/27 [00:37<01:28,  4.63s/it]\u001b[A\n",
      " 33%|██████████████▋                             | 9/27 [00:41<01:23,  4.64s/it]\u001b[A\n",
      " 37%|███████████████▉                           | 10/27 [00:46<01:18,  4.64s/it]\u001b[A\n",
      " 41%|█████████████████▌                         | 11/27 [00:51<01:14,  4.63s/it]\u001b[A\n",
      " 44%|███████████████████                        | 12/27 [00:55<01:09,  4.64s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 13/27 [01:00<01:04,  4.64s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 14/27 [01:05<01:00,  4.64s/it]\u001b[A\n",
      " 56%|███████████████████████▉                   | 15/27 [01:09<00:55,  4.63s/it]\u001b[A\n",
      " 59%|█████████████████████████▍                 | 16/27 [01:14<00:50,  4.62s/it]\u001b[A\n",
      " 63%|███████████████████████████                | 17/27 [01:18<00:46,  4.62s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 18/27 [01:23<00:41,  4.63s/it]\u001b[A\n",
      " 70%|██████████████████████████████▎            | 19/27 [01:28<00:36,  4.62s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 20/27 [01:32<00:32,  4.62s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 21/27 [01:37<00:27,  4.62s/it]\u001b[A\n",
      " 81%|███████████████████████████████████        | 22/27 [01:41<00:23,  4.62s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 23/27 [01:46<00:18,  4.63s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 24/27 [01:51<00:13,  4.64s/it]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 25/27 [01:55<00:09,  4.64s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▍ | 26/27 [02:00<00:04,  4.65s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 27/27 [02:01<00:00,  4.48s/it]\u001b[A\n",
      "1it [02:01, 121.69s/it]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1586.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:14:02] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Apr 09, 18:14:02] #> Building the emb2pid mapping..\n",
      "[Apr 09, 18:14:02] len(emb2pid) = 86636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████| 4096/4096 [00:00<00:00, 142629.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:14:02] #> Saved optimized IVF to /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/documents/ivf.pid.pt\n",
      "Done indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/documents'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new index. Documents as they stand are too long, even though they have been chunked.\n",
    "# According to the documentation, 512 is about the maximum useful length, so the documents are split agian.\n",
    "RAG.index(\n",
    "    collection=doc_list,\n",
    "    document_metadatas=metadata_list,\n",
    "    index_name=\"documents\",\n",
    "    max_document_length=512,\n",
    "    split_documents=True,\n",
    "    use_faiss=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb4c820-c03f-4307-8434-7a81ce0e323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New index_name received! Updating current index_name (documents) to transcripts\n",
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Apr 09, 18:15:23] #> Note: Output directory /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/transcripts already exists\n",
      "\n",
      "\n",
      "[Apr 09, 18:15:23] #> Will delete 11 files already at /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/transcripts in 20 seconds...\n",
      "[Apr 09, 18:15:44] [0] \t\t #> Encoding 7253 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [03:51<00:00,  4.63s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:39<00:00,  4.39s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:53<00:00,  4.67s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:53<00:00,  4.68s/it]\n",
      "100%|███████████████████████████████████████████| 27/27 [02:04<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:33:09] [0] \t\t avg_doclen_est = 238.19992065429688 \t len(local_sample) = 7,253\n",
      "[Apr 09, 18:33:10] [0] \t\t Creating 16,384 partitions.\n",
      "[Apr 09, 18:33:10] [0] \t\t *Estimated* 1,727,664 embeddings.\n",
      "[Apr 09, 18:33:10] [0] \t\t #> Saving the indexing plan to /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/transcripts/plan.json ..\n",
      "used 20 iterations (9.75s) to cluster 1677664 items into 16384 clusters\n",
      "[0.037, 0.038, 0.037, 0.034, 0.034, 0.039, 0.037, 0.034, 0.036, 0.036, 0.037, 0.036, 0.037, 0.039, 0.037, 0.041, 0.034, 0.037, 0.036, 0.035, 0.037, 0.039, 0.035, 0.037, 0.035, 0.036, 0.038, 0.038, 0.038, 0.038, 0.037, 0.041, 0.039, 0.035, 0.037, 0.033, 0.04, 0.036, 0.037, 0.042, 0.037, 0.038, 0.037, 0.038, 0.038, 0.035, 0.036, 0.042, 0.04, 0.038, 0.036, 0.036, 0.042, 0.038, 0.036, 0.036, 0.041, 0.04, 0.047, 0.036, 0.037, 0.041, 0.038, 0.039, 0.04, 0.039, 0.04, 0.038, 0.035, 0.036, 0.04, 0.034, 0.037, 0.04, 0.038, 0.039, 0.039, 0.039, 0.04, 0.043, 0.042, 0.037, 0.037, 0.039, 0.035, 0.037, 0.036, 0.036, 0.036, 0.041, 0.038, 0.04, 0.036, 0.042, 0.037, 0.036, 0.04, 0.036, 0.038, 0.036, 0.036, 0.04, 0.037, 0.038, 0.04, 0.036, 0.035, 0.036, 0.037, 0.038, 0.038, 0.038, 0.037, 0.037, 0.038, 0.037, 0.042, 0.038, 0.036, 0.039, 0.036, 0.039, 0.038, 0.038, 0.035, 0.042, 0.038, 0.038]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:33:22] [0] \t\t #> Encoding 7253 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:04<03:42,  4.54s/it]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:09<03:36,  4.51s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:13<03:33,  4.54s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:18<03:29,  4.56s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:22<03:25,  4.58s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:27<03:22,  4.60s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:32<03:18,  4.61s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:36<03:13,  4.62s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:41<03:09,  4.63s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:46<03:05,  4.64s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:50<03:00,  4.63s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:55<02:56,  4.64s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:59<02:51,  4.65s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:04<02:47,  4.65s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:09<02:43,  4.67s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:14<02:39,  4.69s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:18<02:34,  4.68s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:23<02:29,  4.66s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:27<02:24,  4.65s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:32<02:19,  4.64s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:37<02:14,  4.64s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:41<02:09,  4.63s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:46<02:04,  4.63s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:51<02:00,  4.63s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [01:55<01:55,  4.63s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:00<01:51,  4.63s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:04<01:46,  4.63s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:09<01:41,  4.63s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:14<01:37,  4.63s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:18<01:32,  4.64s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:23<01:28,  4.65s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:28<01:23,  4.66s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:32<01:19,  4.65s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:37<01:14,  4.66s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:42<01:09,  4.66s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:46<01:05,  4.66s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [02:51<01:00,  4.66s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [02:56<00:55,  4.65s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:00<00:51,  4.64s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:05<00:46,  4.63s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:10<00:41,  4.63s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:14<00:37,  4.63s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:19<00:32,  4.63s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:23<00:27,  4.63s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:28<00:23,  4.63s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:33<00:18,  4.63s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:37<00:13,  4.62s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:42<00:09,  4.63s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:47<00:04,  4.64s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [03:51<00:00,  4.64s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:04<03:48,  4.65s/it]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:09<03:41,  4.62s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:13<03:37,  4.62s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:18<03:32,  4.63s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:23<03:27,  4.61s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:27<03:23,  4.62s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:32<03:19,  4.63s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:37<03:14,  4.64s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:41<03:10,  4.64s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:46<03:05,  4.64s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:51<03:01,  4.65s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:55<02:56,  4.65s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [01:00<02:51,  4.64s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:04<02:46,  4.63s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:09<02:42,  4.64s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:14<02:37,  4.63s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:18<02:32,  4.62s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:23<02:27,  4.61s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:27<02:23,  4.62s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:32<02:18,  4.61s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:37<02:14,  4.63s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:41<02:09,  4.64s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:46<02:05,  4.65s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:51<02:01,  4.66s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [01:55<01:56,  4.66s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:00<01:52,  4.68s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:05<01:47,  4.68s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:10<01:42,  4.68s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:14<01:38,  4.67s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:19<01:33,  4.68s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:24<01:29,  4.71s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:28<01:24,  4.70s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:33<01:19,  4.70s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:38<01:15,  4.69s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:42<01:10,  4.69s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:47<01:05,  4.71s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [02:52<01:01,  4.71s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [02:56<00:56,  4.69s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:01<00:51,  4.70s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:06<00:46,  4.69s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:11<00:43,  4.82s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:16<00:38,  4.80s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:20<00:33,  4.78s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:25<00:28,  4.81s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:30<00:24,  4.88s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:35<00:19,  4.86s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:40<00:14,  4.83s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:45<00:09,  4.80s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:49<00:04,  4.78s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [03:54<00:00,  4.69s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:04<03:50,  4.71s/it]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:09<03:50,  4.81s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:14<03:43,  4.75s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:18<03:37,  4.72s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:23<03:31,  4.71s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:28<03:26,  4.70s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:32<03:21,  4.69s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:37<03:17,  4.70s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:42<03:12,  4.69s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:47<03:07,  4.70s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:51<03:04,  4.73s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:56<03:00,  4.74s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [01:01<02:56,  4.76s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:06<02:50,  4.75s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:10<02:45,  4.73s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:15<02:40,  4.71s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:20<02:34,  4.70s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:24<02:30,  4.69s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:29<02:25,  4.69s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:34<02:20,  4.68s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:38<02:15,  4.68s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:43<02:12,  4.74s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:48<02:07,  4.73s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:53<02:02,  4.73s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [01:57<01:58,  4.73s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:02<01:52,  4.71s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:07<01:48,  4.70s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:11<01:43,  4.70s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:16<01:38,  4.71s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:21<01:34,  4.72s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:26<01:29,  4.71s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:30<01:24,  4.69s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:35<01:19,  4.69s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:40<01:14,  4.69s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:44<01:10,  4.69s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:49<01:05,  4.69s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [02:54<01:01,  4.71s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [02:58<00:56,  4.71s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:03<00:51,  4.69s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:08<00:46,  4.68s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:12<00:42,  4.68s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:17<00:37,  4.67s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:22<00:32,  4.67s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:26<00:27,  4.66s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:31<00:23,  4.66s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:36<00:18,  4.66s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:40<00:13,  4.66s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:45<00:09,  4.65s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:50<00:04,  4.65s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [03:54<00:00,  4.70s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:04<03:48,  4.67s/it]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:09<03:43,  4.66s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:13<03:39,  4.66s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:18<03:34,  4.67s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:23<03:30,  4.67s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:27<03:25,  4.66s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:32<03:20,  4.66s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:37<03:15,  4.65s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:41<03:10,  4.66s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:46<03:06,  4.66s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:51<03:01,  4.67s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:55<02:57,  4.67s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [01:00<02:52,  4.66s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:05<02:48,  4.68s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:10<02:43,  4.68s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:14<02:39,  4.68s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:19<02:34,  4.67s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:24<02:29,  4.67s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:28<02:24,  4.67s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:33<02:20,  4.67s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:38<02:15,  4.67s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:42<02:10,  4.66s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:47<02:05,  4.66s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:51<02:01,  4.66s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [01:56<01:56,  4.66s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:01<01:52,  4.67s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:05<01:47,  4.66s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:10<01:42,  4.66s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:15<01:37,  4.66s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:19<01:33,  4.66s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:24<01:28,  4.67s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:29<01:23,  4.66s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:33<01:19,  4.66s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:38<01:14,  4.66s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:43<01:09,  4.66s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:47<01:05,  4.66s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [02:52<01:00,  4.66s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [02:57<00:55,  4.66s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:01<00:51,  4.66s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:06<00:46,  4.66s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:11<00:41,  4.66s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:15<00:37,  4.66s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:20<00:32,  4.66s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:25<00:27,  4.66s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:29<00:23,  4.65s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:34<00:18,  4.66s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:39<00:13,  4.66s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:43<00:09,  4.66s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:48<00:04,  4.67s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [03:53<00:00,  4.66s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▋                                          | 1/27 [00:04<02:02,  4.71s/it]\u001b[A\n",
      "  7%|███▎                                        | 2/27 [00:09<01:56,  4.67s/it]\u001b[A\n",
      " 11%|████▉                                       | 3/27 [00:14<01:51,  4.67s/it]\u001b[A\n",
      " 15%|██████▌                                     | 4/27 [00:18<01:47,  4.66s/it]\u001b[A\n",
      " 19%|████████▏                                   | 5/27 [00:23<01:42,  4.67s/it]\u001b[A\n",
      " 22%|█████████▊                                  | 6/27 [00:28<01:38,  4.68s/it]\u001b[A\n",
      " 26%|███████████▍                                | 7/27 [00:32<01:33,  4.68s/it]\u001b[A\n",
      " 30%|█████████████                               | 8/27 [00:37<01:28,  4.67s/it]\u001b[A\n",
      " 33%|██████████████▋                             | 9/27 [00:42<01:23,  4.67s/it]\u001b[A\n",
      " 37%|███████████████▉                           | 10/27 [00:46<01:19,  4.67s/it]\u001b[A\n",
      " 41%|█████████████████▌                         | 11/27 [00:51<01:14,  4.67s/it]\u001b[A\n",
      " 44%|███████████████████                        | 12/27 [00:56<01:09,  4.67s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 13/27 [01:00<01:05,  4.67s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 14/27 [01:05<01:00,  4.66s/it]\u001b[A\n",
      " 56%|███████████████████████▉                   | 15/27 [01:10<00:55,  4.66s/it]\u001b[A\n",
      " 59%|█████████████████████████▍                 | 16/27 [01:14<00:51,  4.66s/it]\u001b[A\n",
      " 63%|███████████████████████████                | 17/27 [01:19<00:46,  4.67s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 18/27 [01:24<00:42,  4.68s/it]\u001b[A\n",
      " 70%|██████████████████████████████▎            | 19/27 [01:28<00:37,  4.68s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 20/27 [01:33<00:32,  4.68s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 21/27 [01:38<00:28,  4.68s/it]\u001b[A\n",
      " 81%|███████████████████████████████████        | 22/27 [01:42<00:23,  4.69s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 23/27 [01:47<00:18,  4.70s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 24/27 [01:52<00:14,  4.70s/it]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 25/27 [01:56<00:09,  4.70s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▍ | 26/27 [02:01<00:04,  4.69s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 27/27 [02:04<00:00,  4.62s/it]\u001b[A\n",
      "1it [18:30, 1110.21s/it]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 487.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:51:53] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Apr 09, 18:51:53] #> Building the emb2pid mapping..\n",
      "[Apr 09, 18:51:53] len(emb2pid) = 1727664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████| 16384/16384 [00:00<00:00, 141447.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:51:53] #> Saved optimized IVF to /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/transcripts/ivf.pid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/transcripts'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index for transcripts\n",
    "RAG.index(\n",
    "    collection=trans_list,\n",
    "    document_metadatas=trans_metadata_list,\n",
    "    index_name=\"transcripts\",\n",
    "    max_document_length=512,\n",
    "    split_documents=True,\n",
    "    use_faiss=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8842e126-efd9-41a0-b7c5-9314f09db3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index transcripts for the first time... This may take a few seconds\n",
      "[Apr 09, 18:52:13] #> Loading codec...\n",
      "[Apr 09, 18:52:13] #> Loading IVF...\n",
      "[Apr 09, 18:52:13] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Apr 09, 18:52:14] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 2192.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:52:14] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 30.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:52:14] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:52:14] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Which class involves time series analysis?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([ 101,    1, 2029, 2465, 7336, 2051, 2186, 4106, 1029,  102,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnewman/miniconda3/envs/rag/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': \"In today's lecture, we're going to\\nbe looking at time series and date functionality in pandas. Manipulating dates and\\ntimes is quite flexible in pandas and thus allows us to conduct more\\nanalysis such as time series analysis, which we're going to talk about soon. Actually, pandas was originally created\\nby Wes McKinney to handle date and time data when he worked as\\na consultant for hedge funds. So it's quite robust in this matter. Let's bring in pandas and numpy as usual. All right,\\npandas has four main time related classes. Timestamp, DatetimeIndex,\\nPeriod, and PeriodIndex.\",\n",
       "  'score': 23.03000831604004,\n",
       "  'rank': 1,\n",
       "  'document_id': 'afdfefe6-105f-4a59-8341-c55d5dd4c9b3',\n",
       "  'passage_id': 2142,\n",
       "  'document_metadata': {'source': '08_date-time-functionality.en.txt',\n",
       "   'course_number': 'SIADS 505',\n",
       "   'course_title': 'Data Manipulation',\n",
       "   'start_index': 0}},\n",
       " {'content': 'In fact, this is usually\\nwhat we collect in reality. We take the measurements. We cannot take the\\nmeasurements infinitely. We have to take the measurements every once in awhile and that gives us the discrete\\nlist of our weights. From this discrete\\nrepresentation, we can actually derive\\nthe function of t. We can derive the\\ncontinuous representation. This is known as regression\\nor generalization. Through regression\\nor generalization, we can actually transform\\nthe discrete list of time stamped values into\\nthe continuous function. That can be generalized\\nto any other time points whether or not you have actually taken\\nthe measurements. You can see that time\\nseries is powerful because that allows you to transit between continuous and\\ndiscrete representations. In fact, in time series analysis, we routinely transform the data from and to different\\nrepresentations. Some of them are continuous and some of them are discrete. We will revisit\\nthis figure later. But you can see that through\\ndifferent transformations, such as Discrete\\nFourier Transformation or wavelet analysis, you can actually find different representations for\\nthe same time series data. Some of them are continuous, like the regular\\nFourier transformation, and some of them are discrete. You can even transform time\\nseries into a sequence, which we will discuss\\nlater in this class.',\n",
       "  'score': 21.187427520751953,\n",
       "  'rank': 2,\n",
       "  'document_id': '6f9b872d-934a-4cec-87d6-d92f1e47f4df',\n",
       "  'passage_id': 197,\n",
       "  'document_metadata': {'source': '02_discrete-vs-continuous-data-representation.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 9880}},\n",
       " {'content': \"Remember that if x causal y, then does not mean that y\\ncausal x. [inaudible] , how can we do this? Well, I'm sure that when you are taking the\\ncausal inference class, you'll learn not\\nso far techniques of figuring out the causality\\nbetween two variables. But in particular,\\nin time-series, Granger causality\\nis usually used. The basic idea of Granger causality is\\nactually quite simple. You look at two\\naligned time series, and you try to see, whether, you can actually use\\none to predict the other, conditional on the one itself.\",\n",
       "  'score': 20.30162239074707,\n",
       "  'rank': 3,\n",
       "  'document_id': '88beeba0-5691-4250-a6fe-f96ae133f3e2',\n",
       "  'passage_id': 184,\n",
       "  'document_metadata': {'source': '02_granger-causality.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 2269}},\n",
       " {'content': \"We're making recordings. When recording this video, the computer is actually\\ncollecting my sound, and that's also represented\\nas time series. In this case, they usually use frequencies to record\\nthis time series data. We will talk about\\nthat next week. Time series data\\nare not just used to record natural measures\\nof data attributes. They can also be used to\\nrecord more complicated, even data science results. You can see that in this example, we're also measuring some\\nnumerical attributes over time, over different weeks. This numerical\\nattribute is not just like the count or\\nthe temperature. It is actually a result of a\\ndata mining analysis itself. In fact, this is\\nmeasuring how likely is that different students will drop and book class over time. This numerical value, we're plotting over time, is\\nactually correlation. If the user dropped one course, how likely they are going\\nto drop another course in a similar category or in\\na different category. You can see that time\\nseries data can be used to measure real attributes of data as well as complicated\\ndata science results.\",\n",
       "  'score': 20.003429412841797,\n",
       "  'rank': 4,\n",
       "  'document_id': 'deff940d-55ce-4df8-82a2-495ef3e073cb',\n",
       "  'passage_id': 303,\n",
       "  'document_metadata': {'source': '01_time-series-data.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 5103}},\n",
       " {'content': \"For example, you can discover which timestamps have values that are so much different\\nfrom their neighbors, or which values now the\\nactually observed are so much different from the predicted value\\nusing time series model. Don't take that all\\nanomalies are bad. Sometimes anomalies are good too. For instance, in this paper, people are looking at the\\nspikes of block mentions of certain books and Amazon\\nsubstrings of certain books. They found that there's\\nextra two day gap that the block mentions\\nof a certain book, predicts the sales of\\nthe book on Amazon. So in this case, you can\\nsee that a spike which is the surprising peak of the time series is\\nessentially an anomaly. This time the anomaly or the outlier is\\ngood. It has value. Of course, based on all these lower-level\\ndata science outputs, data mining outputs, we can do classification on time series. For instance, if we gather the sensor data,\\nthe accelerometers, sensors of running, walking, and jogging over time we\\ncan make predictions. We can classify any\\ngiven user activity into walking or into running. This is basically how Apple transfers steps\\nwhere using the House app. We could also do clustering\\nof time-series data. In the previous example, we know what the classes are. We know that people\\ncould be running, could be jogging,\\ncould be walking. But, in this example,\\nin clustering, we don't know what the\\nclasses should be. Instead, we can\\nlisten from the data. We can just learn the categories that you know can summarize\\nmodel time series the best and that will review many different clusters or many different aggregated\\npatterns of times of state. You can see that no\\nmatter what we are talking about\\nclassification, clustering, they can literally be built upon the lower labeled data mining\\noutputs such as patterns, such as similarities, and\\nsuch as time series models. What about classification? Clustering will be introduced\\nin Machine Learning. I strongly recommend you to use what you have\\nlearned in this course to extract patterns and similarities to build fancy\\nMachine Learning models. With so many strong data\\nmining functionalities of time source data, you can actually apply\\ntime series data analysis to almost every domain. In finance, of course,\\nwe make predictions.\",\n",
       "  'score': 19.8687744140625,\n",
       "  'rank': 5,\n",
       "  'document_id': '021882aa-5317-4f36-8810-2acb30779f07',\n",
       "  'passage_id': 234,\n",
       "  'document_metadata': {'source': '03_tasks-of-time-series-data-mining.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 7102}},\n",
       " {'content': \"What is that? The [inaudible] , HMMs are much more\\npowerful than that. In speech recognition,\\nsignal processing, or time series analysis. You can also find\\napplications of HMMs. Essentially, once your\\ndata has a sequence, can be composed as the\\nsequence of innovations. If the Markov assumption holds. If there are hidden states\\nthat you're interested in, you can use HMM. In behavioral\\nanalysis, in finance, sports, business, transportations,\\nor human education. If you are observing a\\nsequence of user behaviors and you're interested in some hidden states of the sequences, you can apply HMM. I have to tell you that, although HMM has\\nbeen very powerful, especially 10 years\\nago, nowadays, they are more oftenly replaced\\nby deep-learning models, especially recurrent\\nneural networks. Although, this is the case, HMMs are still widely used as reliable based states and building blocks for more\\ncomplex techniques. For instance, they are\\nvery recent work that have been combining HMMs into\\ndeep learning models, and the results are actually better than just applying\\ndeep learning models. This is just foreshadowing of what you will learn in\\na deep learning class. Well, you may ask, I have\\nintroduced so many models, HMMs, Markov chain,\\nUnigram Language Models. Even with Markov chain, always Hidden Markov Models, there are many variations. You can have three\\nstates, five states. You can add the input state, add a start state and end state, and you can do many other things. The question is, which\\nmodel is better?\",\n",
       "  'score': 19.157146453857422,\n",
       "  'rank': 6,\n",
       "  'document_id': '4bd34164-51c0-4489-abf9-639813928c88',\n",
       "  'passage_id': 25,\n",
       "  'document_metadata': {'source': '02_hidden-markov-model-part-2.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 6419}},\n",
       " {'content': \"If you can predict the\\nmarket, you'll become rich. In health care, you gather the ECG data and many other\\ntypes of clinical data. You use time series\\nanalysis to either make predictions or just\\ndo classification. You do the diagnosis which is essentially a\\nclassification task if the patient has the\\nparticular disease so that he can provide\\ntimely treatments. In Business Intelligence, you try to predict the supply and demand, and you can also predict the success or failure\\nof the product. You do that through analyzing\\nmultiple time series. In sports analytics, you gather the behavioral data of the athletes and you\\nmake predictions, you estimate the risk of injury, and then you also want to make predictions of your opponents. There are many\\nother applications. Don't forget that you\\ndon't just need to use time series to represent a\\nraw measurements of data. You can also use time series to record the more complicated\\noutputs of the mind. So through mining many\\ndifferent types of data, you have the output such as\\nthe estimated probability. You can represent the estimated\\nprobability over time. You can represent the\\nestimated correlation over data over time. You can actually use\\ntime series analysis to do the second level\\nscience research. In Engineering, the\\nentire field of signal processing is built upon time series analysis\\nas the basis. There are many\\nother applications. I'm sure that you have\\nencountered times series data in your domain, whatever your domain is. Then after learning the\\ntechniques in this course, you can try to apply the time series techniques\\nto your domain. To summarize what we have\\ntalked about so far, I wanted you to understand how the time series is different from a sequence and a vector. Basically, is different\\nfrom a sequence because it's module is majoring\\nnumerical attributes, and it records actual timestamps instead of just the orders. It's different from the\\nvector because there are only ways infinite\\nnumber of dimensions. I hope you understand the difference and\\nconnection between discrete representation and\\ncontinuous representation. That is exactly why we need time series as the spatial\\ndata representation. I hope you remember the\\nbasic data mining tasks of time series analysis and know that they had been applied\\nto almost every domain.\",\n",
       "  'score': 19.10548210144043,\n",
       "  'rank': 7,\n",
       "  'document_id': 'a221bd53-5c65-4f25-8fd7-1b5404ad9d98',\n",
       "  'passage_id': 235,\n",
       "  'document_metadata': {'source': '03_tasks-of-time-series-data-mining.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 11260}},\n",
       " {'content': \"But also the process becomes slower and\\nthen just problem of overfitting. So besides regression based methods, there's a huge family of methods\\ncalled smoothing based methods. And this smoothing-based methods also\\nknown as noise reduction methods. The basic idea is that, if you have a real\\nworld time series, its usually messy. And you want to reduce the noise\\nin this time series, so that the shape becomes\\nmore obvious to you. The particular smoothing based methods, including binning, moving averages and\\nsome of the autoregressive methods. And we will introduce some later in this\\nclass but now let's just look at binning.\",\n",
       "  'score': 18.898386001586914,\n",
       "  'rank': 8,\n",
       "  'document_id': 'dc4f784f-e4d6-44b1-913f-4d520488d27b',\n",
       "  'passage_id': 244,\n",
       "  'document_metadata': {'source': '01_time-series-patterns-trends.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 8970}},\n",
       " {'content': \"Remember that time\\nseries could be represented as the continuous\\nfunction ft, mapped. Given any timestamp t, you can obtain the output x_t. But this function's\\nusually unknown to us because what we\\nusually clicked from the wild are the discrete timestamps\\nand their values. Usually, to model time series, we're actually learning\\nthis function. Now, once we learn this function, we can use this function to make predictions about timestamps\\nthat we haven't gathered. This is also known as generating\\nthe time series data. From the discrete, the\\nsampled time series data, we can also try to infer what is actually\\nin the function. What the function\\nshould look like, and this is known as the inference. We learn a function\\nas the model of a time series and we make use of the function to make predictions. Of course, making predictions\\nis very important. Many people who are interested in time series is because they\\nwant to make forecasting. In the Google stock market\\nexample we have seen, you can see that we're actually plotting the time\\nseries into the future. The plotting is not rumors, it's actually based on\\nwhat we observed so far, and this is known as\\ntime series forecasting. Another example of time\\nseries forecasting is, of course, weather forecasting, which, unfortunately, we\\nare not doing very well. So you can say that if we\\nknow the temperature or the other parameters\\nin the past 10 days, we can actually make a prediction of the temperature tomorrow, and the day after\\ntomorrow, so and so forth. In all these examples, what we have observed on the timestamped values up to now, the whole history of\\ntimestamp values, either a single dimensional\\nor multidimensional. We're trying to predict the values and the\\nfuture time point. We reintroduce time\\nseries forecasting in the third week of the class. With time-series modeling\\nand forecasting, you can also do\\nanomaly detection. For example, you can discover which timestamps have values that are so much different\\nfrom their neighbors, or which values now the\\nactually observed are so much different from the predicted value\\nusing time series model. Don't take that all\\nanomalies are bad. Sometimes anomalies are good too.\",\n",
       "  'score': 18.70126724243164,\n",
       "  'rank': 9,\n",
       "  'document_id': '021882aa-5317-4f36-8810-2acb30779f07',\n",
       "  'passage_id': 233,\n",
       "  'document_metadata': {'source': '03_tasks-of-time-series-data-mining.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 7102}},\n",
       " {'content': \"This case, we can see that we have naturally multi-dimensional\\nnumerical values. Each dimension corresponds to the measurements in a\\ndifferent location. In this case, we can obtain multi-dimensional\\ntime series and they are naturally aligned\\nby timestamps. Another application of time\\nseries is sports analytics. Actually you can find lots of time series data in\\nsports analytics. The athletes wear devices. These sensors can actually measure the different types of sickness and tracks\\nthem over time. You can actually wear\\nmultiple sensors when you are actually\\ndoing the sports. The sensor sickness give us multiple time series and they are naturally\\naligned by time. Another example, in healthcare, you can see that through\\nthose health devices, we can collect data, such as the ECG data or others. They're also taking the\\nform of time series data. Of course, multiple\\naligned times. Yet another example, soundtracks.\",\n",
       "  'score': 18.68090057373047,\n",
       "  'rank': 10,\n",
       "  'document_id': '2975aa2f-a726-477e-b6cb-7fe11e765a92',\n",
       "  'passage_id': 302,\n",
       "  'document_metadata': {'source': '01_time-series-data.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 4182}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This takes 30+ seconds to start up the first time, but runs faster after that\n",
    "RAG.search(query=\"Which class involves time series analysis?\")  # documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39b9e17-5695-44c7-a5b0-7d15844b9653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"Let's start by looking at a very important and widely used linear dimensionality\\nreduction technique called principal component\\nanalysis or PCA. There are a couple of ways\\nto describe how PCA works. An intuitive, more geometric way and then there's\\na linear algebra way. What we're going\\nto do is to start, we're going to look\\nat the geometric way, the visually intuitive\\nway and then later, we'll look at the\\nlinear algebra behind PCA as part of understanding a powerful general\\ndimensionality reduction method called singular value\\ndecomposition or SVD, which is very closely\\nconnected to PCA. Intuitively what PCA does, it takes your\\noriginal data points. Here I have a very simple\\ndataset with two features. It's a two-dimensional\\ndataset and imagine each instance is denoted by a point here in the\\nscatterplot and intuitively, what PCA does geometrically to these original data points\\nis it finds a rotation of the points so that the\\ndimensions are statistically uncorrelated and\\nafter it does that, each data point can be\\ndescribed by new coordinates relative to this new frame of reference in these\\nuncorrelated dimensions. Once we find these new\\nrotated dimensions, the nature of PCA is such that we can drop all but\\nthe most informative coordinates that capture\\nmost of the variation in the original dataset and\\nwe still end up with a good approximation in the least square sense\\nto our original dataset. Looking at this\\nsynthetic example, we have two features\\nthat are somewhat highly correlated and when we apply PCA. PCA will find the direction in the data has highest variance. What that means is, if we think\\nabout PCA trying to find, there are different ways\\nthat you can project this data onto a line. Imagine there are all\\nthese different ways.\",\n",
       "  'score': 25.309688568115234,\n",
       "  'rank': 1,\n",
       "  'document_id': 'd10aecf1-1fe4-4ebc-80f7-a4cedc83e925',\n",
       "  'passage_id': 997,\n",
       "  'document_metadata': {'source': '03_intro-to-dimensionality-reduction-and-principal-components-analysis.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 14870}},\n",
       " {'content': \"To perform PCA, our\\nfirst step is to import the PCA class from\\nSKlearn decomposition. Our next step is to\\nmake sure the data is properly prepared before\\nwe use PCA on it. To prepare your data for PCA, it's usually a good idea to\\nfirst transform the dataset. Each features range of values has zero mean and unit variance. We can do this here using\\nthe fit and transform methods of the standard\\nscaler class as shown here. By default, in scikit-learn, PCA will automatically do\\ncolumn centering of your data, but it won't do the\\nvariance standardization. We recommend this\\nstandard scalar step.\",\n",
       "  'score': 23.016883850097656,\n",
       "  'rank': 2,\n",
       "  'document_id': 'a1568ca9-087f-4d5a-b784-1001148c09b9',\n",
       "  'passage_id': 1008,\n",
       "  'document_metadata': {'source': '03_intro-to-dimensionality-reduction-and-principal-components-analysis.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 31284}},\n",
       " {'content': \"The new data will have fewer\\ndimensions from the old data. So to summarize PCA, we know that principal components\\nare very special patterns of matrix data. And in fact, they're orthogonal\\neigenvectors of the covariance matrix. They are very useful because\\nthey can define new orthogonal coordinate system of your matrix. And based on this orthogonal\\ncoordinate system, many of the vector similarity or\\ndistance matrix are much more trustable. Through PCA,\\nyou can transform the original vector data into low dimensional and\\northogonal vector space. And this is why PCA has been a very\\nuseful step of feature engineering in many machine learning tasks. Is this it? In reality, shouldn't we always use PCA? And in fact,\\nPCA also have some of its limitations. Why? Remember that PCA relies heavily\\non eigenvalues and eigenvectors. And we know that eigenvalues and eigenvectors only apply\\nto square matrices. Also PCA relies on eigendecomposition. And we know that stable eigendecomposition\\ndoes not apply to all square matrices. In fact, it requires that\\nyour matrix is symmetric and sometimes positive semi-definite. And that is why we apply PCA\\nto the covariance matrix XTX, because we know that the covariance\\nmatrix is symmetric and it is also positive semi-definite. But in reality, we know that the original data matrices\\nwe deal with are almost never square.\",\n",
       "  'score': 22.659481048583984,\n",
       "  'rank': 3,\n",
       "  'document_id': 'd1885996-2706-4a68-af36-9ffecefeb06a',\n",
       "  'passage_id': 2616,\n",
       "  'document_metadata': {'source': '04_3-3-4-principal-component-analysis.en.txt',\n",
       "   'course_number': 'SIADS 532',\n",
       "   'course_title': 'Data Mining I',\n",
       "   'start_index': 5971}},\n",
       " {'content': \"There is a package called PCA in Python\\nthat you can install and use if you like. It includes a biplot plotting routine\\nwhich I've shown here on the right, very easy to install and use,\\nalthough I haven't used it myself, some aspects of it are a little bit\\nmore difficult to sort of customize. What I would recommend is just using\\nthe notebook code that we provided, it's kind of instructive just to\\ngo through it to see how it works. And then you feel free to use that code to\\ncreate your own biplots for your datasets.\",\n",
       "  'score': 22.510330200195312,\n",
       "  'rank': 4,\n",
       "  'document_id': '9ff3288b-afed-4288-9132-e8100172ae35',\n",
       "  'passage_id': 786,\n",
       "  'document_metadata': {'source': '05_pca-biplots-and-how-to-use-them.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 19336}},\n",
       " {'content': 'PCA constrains the counterpart\\nto W to be orthonormal. And the rows of H to be\\northogonal to each other. And the math constrains at W and\\nH to be positive as we said. And in order to construct a face for\\nexample, the parts combine additively\\nto form the whole.',\n",
       "  'score': 22.35651969909668,\n",
       "  'rank': 5,\n",
       "  'document_id': '22f090a0-8063-407f-bba0-8c0712c8eee0',\n",
       "  'passage_id': 940,\n",
       "  'document_metadata': {'source': '07_topic-models-2-non-negative-matrix-factorization-nmf.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 9734}},\n",
       " {'content': 'You declare an object of the PCA class,\\nright. You tell the the class how many\\ncomponents you want to keep, and you fit your data to the class. Then you can achieve the transformed data.',\n",
       "  'score': 22.03375816345215,\n",
       "  'rank': 6,\n",
       "  'document_id': 'db04234f-961f-4ec4-a69c-571ad5a00d89',\n",
       "  'passage_id': 2615,\n",
       "  'document_metadata': {'source': '04_3-3-4-principal-component-analysis.en.txt',\n",
       "   'course_number': 'SIADS 532',\n",
       "   'course_title': 'Data Mining I',\n",
       "   'start_index': 5784}},\n",
       " {'content': \"But here's a really\\nimportant reminder. If you are using PCA\\nto find new features, to avoid data leakage, always apply it after you\\ndo the train test split. This applies to any type of normalization, scaling,\\ntopic modeling, or other preprocessing that\\nmakes use of properties, or statistics across\\nthe entire dataset. You always split first\\nand then process later. We'll revisit this rule multiple times in the future applications. After scaling the data, we create the PCA object. We specify that we want to retain just the first two\\nprincipal components to reduce the dimensionality\\nto just two columns. Then we call the fit method, using the normalized data. This will set up PCA so that it learns the right\\nrotation at the dataset. We can then apply this\\nproperly prepared PCA object to project all the points in our input dataset to this\\nnew two-dimensional space. You see that if we\\ntake the shape of the array that's\\nreturned from PCA, it's transformed our\\noriginal dataset that had 30 columns\\ncorresponding to the 30 features into a new array that has\\njust two columns, essentially expressing each original data\\npoint in terms of two new features that\\nrepresent the position of the data point in this new\\ntwo-dimensional PCA space. We can then just create\\na scatterplot using these two new features to see how the data forms clusters. That's what's being shown here. In this example, we've\\nused the dataset that also has labels for\\nsupervised learning, namely, the malignant\\nand benign labels on cancer cells that were\\nprovided by human experts. This helps us see how well PCA serves to find\\nclusters in the data. Here's the result of plotting all the 30 featured data samples, but using the two new\\nfeatures computed with PCA. The first feature is the\\nfirst principle component, along the x axis.\",\n",
       "  'score': 21.949609756469727,\n",
       "  'rank': 7,\n",
       "  'document_id': '925046c6-9789-4faf-a613-1e12f19fd729',\n",
       "  'passage_id': 1010,\n",
       "  'document_metadata': {'source': '03_intro-to-dimensionality-reduction-and-principal-components-analysis.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 36201}},\n",
       " {'content': \"So principal components\\nare very interesting, very special patterns in your matrix data. That they define orthogonal\\ndirections that capture most of the variance in your data. Usually there are multiple\\nprinciple components of a matrix. The first principal component would\\nexplain the most variance of your data. And then the second principal component\\nwould explain the most of the rest of the variances in your data. And the third one will explain most\\nof the rest, and so on and so forth. And the method discovers those\\nprincipal components is known as principal component analysis or PCA. Most of you may have heard\\nthat from many contexts, because this is such the popular\\ndata science technique. The function of principal component\\nanalysis is essentially to find the eigenvalues and\\neigenvectors of the covariance matrix. We will talk about what that is later. The eigenvectors with\\nthe largest eigenvalues of this covariance matrix will\\ncorrespond to the directions, the dimensions that have the strongest\\ncorrelation in your data set. So the first eigenvector that\\ncorresponds to the largest eigenvalue is the most important\\ndirection in your data set. The second eigenvector defines the second\\nimportant direction in your data set, so so on and so forth. So, how does PCA work? First we normalize the matrix. Suppose your original data matrix\\nX has n rows and p dimensions. We subtract the mean of every\\ncolumn from this matrix x. And then we compute\\nthe so-called covariance matrix. Let's just name that A. The covariance matrix of your\\ndata matrix is essentially the product of the transpose of your\\nmatrix X and original matrix X. We call that XTX. XT is the transpose of your data matrix X. So A is the covariance matrix. If X has n rows and\\np dimensions, and p columns, then X transpose X will have p rows and\\np columns. So A is the p by p matrix. And what's even nicer about A is\\nthat A is a symmetric matrix. Does that ring a bell? Because A is a symmetric matrix we can\\nalways compute the eigendecomposition of A as we discussed earlier. And after the eigendecomposition, we can rewrite A as the product\\nof three matrices, the U matrix, a Lambda matrix, and\\nthe inverse of the U matrix.\",\n",
       "  'score': 21.911962509155273,\n",
       "  'rank': 8,\n",
       "  'document_id': 'f5727233-d3af-42f2-9d94-79c60ac36bdb',\n",
       "  'passage_id': 2611,\n",
       "  'document_metadata': {'source': '04_3-3-4-principal-component-analysis.en.txt',\n",
       "   'course_number': 'SIADS 532',\n",
       "   'course_title': 'Data Mining I',\n",
       "   'start_index': 0}},\n",
       " {'content': 'You need to spend\\nan extra effort on interpreting what you really\\ndiscover from SVD and PCA. So to summarize this lecture, I hope you understand\\nthat eigenvectors and principal components are very useful patterns\\nin matrix data, and this also includes\\nthe singular vectors. I want you to understand\\nhow PCA transforms the matrix data into orthogonal and low\\ndimensional vectors. Relatively, I want\\nyou to understand how SVD can achieve similar results, and how SVD is performed on the original data\\nmatrix instead of a square covariance matrix.',\n",
       "  'score': 21.827136993408203,\n",
       "  'rank': 9,\n",
       "  'document_id': '3d4d1883-3e78-4c92-a116-fdc9c7868676',\n",
       "  'passage_id': 2488,\n",
       "  'document_metadata': {'source': '08_3-3-7-svd-in-practice.en.txt',\n",
       "   'course_number': 'SIADS 532',\n",
       "   'course_title': 'Data Mining I',\n",
       "   'start_index': 2369}},\n",
       " {'content': \"We've lost some information about the original second coordinate but depending on the application, that may be okay. We've certainly learned\\nsomething about the data by applying PCA, we found this direction\\nof greatest variation. That first direction of greatest variants that PCA finds is called the first\\nprincipal component. That's the long direction\\nof this cloud of points. PCA will proceed iteratively. It will first find in the\\ndata the direction of greatest variance\\nand it will find this first principle component. Then it'll continue, it'll\\nlook for the direction at right angles that maximally captures the remaining variance. In two-dimensions,\\nthere's only one possible such direction at a right angle to the\\nfirst principle component but in higher dimensions\\nthere are infinitely many. In higher dimensions PCA would find that second\\nprincipal component, the next greatest variance. Now, just to note that there could be two\\nequivalent solutions for principal component based on either a positive\\nor negative sign, you could have a vector that\\nwas oriented this way or you could have a vector that\\nwas oriented the other way. Which of those two\\ndirections is produced by PCA can be arbitrary. In any case, one result of applying PCA is that we now know the best one-dimensional\\napproximation to the original\\ntwo-dimensional data. We can take any data point that used two features before in x and y and approximate it\\nusing just one feature, namely its location when projected onto the first\\nprincipal component. Let's take a more\\ndetailed look at the linear algebra\\nformulation of what PCA does. We assume our datasets is in a matrix X with the instances or individuals in n\\nrows and p columns. Where each column represents\\none of the p observed variables that we often call\\nfeatures or attributes. Remember, our goal is to reduce the dimensionality of X by\\nreplacing the p features in the columns of X with\\na new smaller set of q features that capture most of the interesting\\nvariation in X. We're going to compute\\nthose q columns with the original p columns. Also we're going to focus on linear dimensionality\\nreduction.\",\n",
       "  'score': 21.798871994018555,\n",
       "  'rank': 10,\n",
       "  'document_id': 'ae07cb10-2f65-4b3a-8092-dbee342b1dbe',\n",
       "  'passage_id': 999,\n",
       "  'document_metadata': {'source': '03_intro-to-dimensionality-reduction-and-principal-components-analysis.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 18703}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG.search(query=\"How does PCA work?\")  # transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b92c09b6-78fd-4a33-b5f7-c21a09ae8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ragatouille let's you create a LangChain retriever from the indexed model\n",
    "retriever = RAG.as_langchain_retriever(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e23487c-d41e-405d-b067-3740840b7d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnewman/miniconda3/envs/rag/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"This approach is\\npopularly known as a bag-of-words approach in natural language\\nprocessing literature. The reason it is called\\nbag-of-words is because it just gives about if and how\\nmany times a word occurs. It doesn't care\\nabout the position or order of the word\\nin the sentence. Bag-of-words based language\\nmodeling approaches have been the mainstay of language modeling\\nfor a long time, and give comparative\\nperformance on several natural language\\nprocessing tasks were ordering information\\nis not very important. However, they can\\nperform poorly on tasks where ordering\\ninformation is important.\", metadata={'source': '01_sequence-modeling.en.txt', 'course_number': 'SIADS 642', 'course_title': 'Deep Learning I', 'start_index': 2446}),\n",
       " Document(page_content='', metadata={'source': '05_university-of-michigans-primary-data-center.en.txt', 'course_number': 'SIADS 673', 'course_title': 'Cloud Computing', 'start_index': 0}),\n",
       " Document(page_content=\"So I'll give you an example\\nthat comes to mind. You need to find labels for images. You are trying to find\\ninstances of a backpack or you're trying to do facial detection. And so your thought might be okay, well,\\ngive me 1000 images of people's faces or 10,000 peoples images of people's faces,\\nlabel them. And we'll build a model to do that, and yes, you can do that, but\\nwhy when you have the cloud. When we go into Amazon's\\nmachine learning tab, there's a lot of different\\nservices within here. And we're going to go through\\na lot of these together. Some of these are fun services. Some of these are just really cool. For example, deep racer,\\ndeep racer is the coolest thing, the coolest service from\\nAmazon that I've seen. You get to race a car with machine\\nlearning and you actually, where is it? You actually buy this car? This is a thing they sell on Amazon,\\nlike the actual Amazon.com. You buy this car, it's a little kit,\\nyou put together, it interfaces with Amazon's cloud. And you can build a model around\\nracing on this and it's Amazon. So they're saying,\\nhey you're charged for the storage, that's going to be S3 training and\\nevaluation. That's going to be compute. And this is $3.50 an hour and\\nmodel storage is 23 cents, I'm sorry,\\ntwo cents per gigabyte per month. And you can actually go and buy this and\\ncompete in Amazon services. There's not really like a business\\nvalue specifically to this tool set. That's not entirely true.\", metadata={'source': '06_machine-learning-in-the-cloud.en.txt', 'course_number': 'SIADS 673', 'course_title': 'Cloud Computing', 'start_index': 942}),\n",
       " Document(page_content='Cool.', metadata={'source': '04_using-inkscape-to-trace-and-modify-visualizations.en.txt', 'course_number': 'SIADS 523', 'course_title': 'Communicating Data Science Results', 'start_index': 13476}),\n",
       " Document(page_content='Okay?', metadata={'source': '07_data-types-in-postgresql.en.txt', 'course_number': 'SIADS 511', 'course_title': 'SQL and Databases', 'start_index': 8083})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is a backpack?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf100f-bec6-4d20-a948-ab99a8548e7f",
   "metadata": {},
   "source": [
    "Next step is to add this to the RAG pipeline and check its performance..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
