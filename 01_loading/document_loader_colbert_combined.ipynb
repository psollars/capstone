{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6182a442-8f6e-402c-97cf-95994715a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "import textwrap\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e54baef-efd1-4560-a6a6-54254442cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"../embeddings\"\n",
    "colbert_path = \"/Volumes/ARN_T7/RAG/colbertv2.0/\"\n",
    "index_root = \"/Volumes/ARN_T7/RAG/colbert_index/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87e71ee-35ec-495d-94fe-1ee5cde3b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in previously processed documents - syllabi and advising\n",
    "with open(f\"{persist_directory}/documents.pickle\", \"rb\") as handle:\n",
    "    documents = pickle.load(handle)\n",
    "\n",
    "with open(f\"{persist_directory}/transcripts.pickle\", \"rb\") as handle:\n",
    "    transcripts = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fe5369-794a-4885-90a8-215512f32019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove one document from transcripts\n",
    "transcripts = [t for t in transcripts \n",
    "               if t.metadata['source']!='01_client-projects-and-data-webinar-from-the-engaged-learning-office.en.txt']\n",
    "\n",
    "# Split out documents to separate lists of document text and metadata\n",
    "doc_list = [doc.page_content for doc in documents]\n",
    "metadata_list = [doc.metadata for doc in documents]\n",
    "\n",
    "trans_list = [doc.page_content for doc in transcripts]\n",
    "trans_metadata_list = [doc.metadata for doc in transcripts]\n",
    "\n",
    "combined_doc_list = doc_list + trans_list\n",
    "combined_metadata_list = metadata_list + trans_metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6416a8-9ae7-4f9a-82f3-7db6e39a1aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 18:53:28] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnewman/miniconda3/envs/rag/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create new model from downloaded base model available on Hugging Face (https://huggingface.co/colbert-ir/colbertv2.0)\n",
    "# This does _not_ recognize the Apple Silicon GPU at this time\n",
    "RAG = RAGPretrainedModel.from_pretrained(colbert_path, index_root = index_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b165f18-cd8b-495e-8d6a-225a4a45c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Apr 09, 18:53:36] #> Note: Output directory /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/combined already exists\n",
      "\n",
      "\n",
      "[Apr 09, 18:53:36] #> Will delete 1 files already at /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/combined in 20 seconds...\n",
      "[Apr 09, 18:53:56] [0] \t\t #> Encoding 8088 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]/Users/arnewman/miniconda3/envs/rag/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 50/50 [03:55<00:00,  4.71s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:54<00:00,  4.68s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:53<00:00,  4.68s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:53<00:00,  4.66s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:52<00:00,  4.65s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:12<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 19:13:41] [0] \t\t avg_doclen_est = 224.31997680664062 \t len(local_sample) = 8,088\n",
      "[Apr 09, 19:13:42] [0] \t\t Creating 16,384 partitions.\n",
      "[Apr 09, 19:13:42] [0] \t\t *Estimated* 1,814,299 embeddings.\n",
      "[Apr 09, 19:13:42] [0] \t\t #> Saving the indexing plan to /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/combined/plan.json ..\n",
      "used 20 iterations (10.1077s) to cluster 1764300 items into 16384 clusters\n",
      "[0.038, 0.038, 0.037, 0.035, 0.035, 0.04, 0.037, 0.035, 0.036, 0.036, 0.037, 0.037, 0.037, 0.039, 0.037, 0.042, 0.035, 0.037, 0.036, 0.035, 0.037, 0.039, 0.035, 0.038, 0.035, 0.037, 0.038, 0.038, 0.039, 0.038, 0.038, 0.041, 0.04, 0.035, 0.037, 0.033, 0.04, 0.036, 0.037, 0.043, 0.038, 0.038, 0.038, 0.038, 0.039, 0.035, 0.036, 0.042, 0.04, 0.038, 0.036, 0.036, 0.042, 0.038, 0.036, 0.037, 0.041, 0.041, 0.047, 0.036, 0.037, 0.041, 0.038, 0.039, 0.04, 0.039, 0.04, 0.038, 0.036, 0.036, 0.04, 0.034, 0.037, 0.04, 0.039, 0.039, 0.04, 0.039, 0.04, 0.043, 0.042, 0.037, 0.037, 0.039, 0.035, 0.037, 0.036, 0.036, 0.037, 0.041, 0.038, 0.04, 0.036, 0.042, 0.036, 0.037, 0.041, 0.036, 0.038, 0.037, 0.036, 0.04, 0.037, 0.038, 0.04, 0.036, 0.035, 0.036, 0.037, 0.039, 0.038, 0.038, 0.038, 0.037, 0.039, 0.037, 0.043, 0.039, 0.036, 0.039, 0.036, 0.039, 0.038, 0.038, 0.035, 0.042, 0.039, 0.039]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 19:13:55] [0] \t\t #> Encoding 8088 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:04<03:31,  4.32s/it]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:08<03:27,  4.32s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:12<03:23,  4.33s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:17<03:19,  4.33s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:21<03:15,  4.35s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:26<03:11,  4.36s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:30<03:07,  4.36s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:34<03:03,  4.37s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:39<02:59,  4.37s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:43<02:54,  4.37s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:47<02:50,  4.38s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:52<02:46,  4.39s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:56<02:41,  4.38s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:01<02:37,  4.37s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:05<02:32,  4.36s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:09<02:28,  4.37s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:14<02:24,  4.38s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:18<02:20,  4.39s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:23<02:16,  4.39s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:27<02:11,  4.40s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:31<02:07,  4.40s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:36<02:03,  4.40s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:40<01:58,  4.40s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:45<01:54,  4.40s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [01:49<01:49,  4.40s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [01:53<01:45,  4.41s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [01:58<01:41,  4.42s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:02<01:37,  4.44s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:07<01:33,  4.44s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:11<01:28,  4.43s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:16<01:24,  4.43s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:20<01:19,  4.42s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:24<01:15,  4.42s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:29<01:10,  4.43s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:33<01:06,  4.44s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:38<01:02,  4.44s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [02:42<00:57,  4.45s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [02:47<00:53,  4.45s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [02:51<00:48,  4.45s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [02:56<00:44,  4.45s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:00<00:40,  4.47s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:05<00:35,  4.48s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:09<00:31,  4.49s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:14<00:27,  4.51s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:19<00:23,  4.61s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:25<00:20,  5.24s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:30<00:15,  5.08s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:35<00:09,  4.94s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:39<00:04,  4.82s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [03:44<00:00,  4.48s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:04<03:48,  4.66s/it]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:09<03:43,  4.65s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:13<03:38,  4.65s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:18<03:34,  4.66s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:23<03:28,  4.63s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:29<03:42,  5.05s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:35<03:51,  5.38s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:39<03:36,  5.15s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:44<03:24,  4.99s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:49<03:15,  4.89s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:53<03:07,  4.81s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:58<03:00,  4.75s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [01:03<02:54,  4.73s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:07<02:48,  4.69s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:12<02:43,  4.67s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:16<02:38,  4.67s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:23<02:53,  5.26s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:28<02:42,  5.08s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:32<02:33,  4.96s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:37<02:25,  4.86s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:42<02:19,  4.80s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:46<02:13,  4.76s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:51<02:08,  4.75s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:56<02:02,  4.72s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [02:00<01:57,  4.70s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:05<01:52,  4.68s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:10<01:47,  4.67s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:14<01:42,  4.66s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:19<01:37,  4.66s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:24<01:33,  4.66s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:28<01:28,  4.66s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:33<01:23,  4.65s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:38<01:19,  4.65s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:42<01:14,  4.66s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:47<01:10,  4.67s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:53<01:13,  5.24s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [02:58<01:06,  5.08s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [03:03<00:59,  4.96s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:08<00:53,  4.87s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:12<00:48,  4.81s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:17<00:42,  4.77s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:22<00:37,  4.73s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:26<00:32,  4.70s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:31<00:28,  4.68s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:35<00:23,  4.66s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:40<00:18,  4.65s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:48<00:16,  5.64s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:53<00:10,  5.34s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:57<00:05,  5.13s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [04:02<00:00,  4.85s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:04<03:48,  4.66s/it]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:09<03:43,  4.66s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:13<03:39,  4.67s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:18<03:34,  4.66s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:23<03:29,  4.65s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:27<03:24,  4.64s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:34<03:40,  5.13s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:38<03:29,  4.99s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:44<03:35,  5.27s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:49<03:23,  5.09s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:53<03:13,  4.97s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:58<03:05,  4.88s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [01:03<02:58,  4.81s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:07<02:51,  4.76s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:12<02:45,  4.73s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:17<02:39,  4.69s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:25<03:07,  5.69s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:29<02:51,  5.37s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:34<02:39,  5.15s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:39<02:30,  5.01s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:43<02:22,  4.91s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:48<02:15,  4.83s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:53<02:09,  4.78s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:57<02:03,  4.74s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [02:02<01:57,  4.71s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:07<01:52,  4.69s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:13<01:56,  5.08s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:17<01:48,  4.95s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:23<01:49,  5.23s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:28<01:41,  5.05s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:32<01:33,  4.94s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:37<01:27,  4.86s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:42<01:21,  4.80s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:46<01:16,  4.76s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:51<01:11,  4.73s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:56<01:05,  4.71s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [03:04<01:13,  5.69s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [03:08<01:04,  5.39s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:13<00:56,  5.18s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:18<00:50,  5.03s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:23<00:45,  5.11s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:28<00:40,  5.06s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:33<00:35,  5.03s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:38<00:30,  5.08s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:43<00:25,  5.04s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:48<00:20,  5.00s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:53<00:14,  4.91s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:57<00:09,  4.86s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [04:02<00:04,  4.82s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [04:07<00:00,  4.95s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:04<03:59,  4.88s/it]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:09<03:48,  4.77s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:14<03:43,  4.75s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:18<03:36,  4.72s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:23<03:30,  4.67s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:28<03:25,  4.67s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:32<03:20,  4.66s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:37<03:15,  4.66s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:42<03:11,  4.66s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:46<03:06,  4.65s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:51<03:01,  4.65s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:56<02:56,  4.63s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [01:00<02:51,  4.64s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:05<02:46,  4.63s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:09<02:42,  4.64s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:14<02:37,  4.64s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:19<02:32,  4.62s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:23<02:28,  4.63s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:28<02:23,  4.64s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:33<02:19,  4.64s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:37<02:14,  4.64s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:42<02:10,  4.64s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:47<02:05,  4.64s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:51<02:00,  4.64s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [01:56<01:56,  4.65s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:01<01:52,  4.67s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:05<01:47,  4.66s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:10<01:43,  4.70s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:15<01:38,  4.71s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:20<01:35,  4.80s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:25<01:33,  4.94s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:30<01:31,  5.09s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:35<01:25,  5.02s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:40<01:19,  4.97s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:45<01:15,  5.03s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:50<01:10,  5.02s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [02:55<01:05,  5.02s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [03:00<00:59,  5.00s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:05<00:54,  4.99s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:10<00:49,  4.99s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:15<00:44,  4.97s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:20<00:39,  4.98s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:25<00:34,  4.97s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:30<00:29,  4.96s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:35<00:24,  4.95s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:40<00:19,  4.95s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:45<00:14,  4.97s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:50<00:09,  4.97s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:55<00:04,  4.93s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [04:00<00:00,  4.80s/it]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:04<04:02,  4.94s/it]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:09<03:55,  4.90s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:14<03:50,  4.89s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:19<03:45,  4.90s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:24<03:39,  4.88s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:29<03:36,  4.93s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:34<03:31,  4.93s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:39<03:26,  4.92s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:44<03:20,  4.90s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:49<03:16,  4.90s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:53<03:11,  4.90s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:58<03:06,  4.90s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [01:03<03:00,  4.88s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:08<02:55,  4.86s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:13<02:50,  4.86s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:18<02:45,  4.86s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:23<02:40,  4.86s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:27<02:35,  4.86s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:32<02:30,  4.86s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:37<02:25,  4.86s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:42<02:21,  4.87s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:47<02:16,  4.88s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:52<02:11,  4.89s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:57<02:06,  4.86s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [02:02<02:01,  4.86s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:06<01:56,  4.87s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:11<01:52,  4.88s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:16<01:47,  4.89s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:21<01:42,  4.88s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:26<01:37,  4.88s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:31<01:32,  4.88s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:36<01:28,  4.89s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:41<01:23,  4.90s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:46<01:18,  4.92s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:51<01:13,  4.93s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:55<01:08,  4.92s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [03:01<01:04,  4.96s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [03:05<00:59,  4.94s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:10<00:54,  4.94s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:16<00:49,  4.99s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:20<00:44,  4.98s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:25<00:39,  4.97s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:30<00:34,  4.99s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:35<00:29,  4.97s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:40<00:24,  4.95s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:45<00:19,  4.93s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:50<00:14,  4.92s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:55<00:09,  4.92s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [04:00<00:04,  4.91s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [04:05<00:00,  4.91s/it]\u001b[A\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███████████████                              | 1/3 [00:04<00:09,  4.83s/it]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:09<00:04,  4.78s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:13<00:00,  4.43s/it]\u001b[A\n",
      "1it [21:08, 1268.92s/it]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 594.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 19:35:04] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Apr 09, 19:35:04] #> Building the emb2pid mapping..\n",
      "[Apr 09, 19:35:04] len(emb2pid) = 1814300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████| 16384/16384 [00:00<00:00, 117942.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 19:35:04] #> Saved optimized IVF to /Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/combined/ivf.pid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Volumes/ARN_T7/RAG/colbert_index/colbert/indexes/combined'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new index. Documents as they stand are too long, even though they have been chunked.\n",
    "# According to the documentation, 512 is about the maximum useful length, so the documents are split agian.\n",
    "RAG.index(\n",
    "    collection = combined_doc_list,\n",
    "    document_metadatas = combined_metadata_list,\n",
    "    index_name = \"combined\",\n",
    "    max_document_length = 512,\n",
    "    split_documents = True,\n",
    "    use_faiss = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8842e126-efd9-41a0-b7c5-9314f09db3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index combined for the first time... This may take a few seconds\n",
      "[Apr 09, 19:36:42] #> Loading codec...\n",
      "[Apr 09, 19:36:42] #> Loading IVF...\n",
      "[Apr 09, 19:36:42] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Apr 09, 19:36:42] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1786.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 19:36:42] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 28.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 19:36:42] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 09, 19:36:42] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Which class involves time series analysis?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([ 101,    1, 2029, 2465, 7336, 2051, 2186, 4106, 1029,  102,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnewman/miniconda3/envs/rag/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': \"In today's lecture, we're going to\\nbe looking at time series and date functionality in pandas. Manipulating dates and\\ntimes is quite flexible in pandas and thus allows us to conduct more\\nanalysis such as time series analysis, which we're going to talk about soon. Actually, pandas was originally created\\nby Wes McKinney to handle date and time data when he worked as\\na consultant for hedge funds. So it's quite robust in this matter. Let's bring in pandas and numpy as usual. All right,\\npandas has four main time related classes. Timestamp, DatetimeIndex,\\nPeriod, and PeriodIndex.\",\n",
       "  'score': 23.05040740966797,\n",
       "  'rank': 1,\n",
       "  'document_id': '0cc544ac-8a28-4aad-9f6a-34123f90166c',\n",
       "  'passage_id': 2977,\n",
       "  'document_metadata': {'source': '08_date-time-functionality.en.txt',\n",
       "   'course_number': 'SIADS 505',\n",
       "   'course_title': 'Data Manipulation',\n",
       "   'start_index': 0}},\n",
       " {'content': 'In fact, this is usually\\nwhat we collect in reality. We take the measurements. We cannot take the\\nmeasurements infinitely. We have to take the measurements every once in awhile and that gives us the discrete\\nlist of our weights. From this discrete\\nrepresentation, we can actually derive\\nthe function of t. We can derive the\\ncontinuous representation. This is known as regression\\nor generalization. Through regression\\nor generalization, we can actually transform\\nthe discrete list of time stamped values into\\nthe continuous function. That can be generalized\\nto any other time points whether or not you have actually taken\\nthe measurements. You can see that time\\nseries is powerful because that allows you to transit between continuous and\\ndiscrete representations. In fact, in time series analysis, we routinely transform the data from and to different\\nrepresentations. Some of them are continuous and some of them are discrete. We will revisit\\nthis figure later. But you can see that through\\ndifferent transformations, such as Discrete\\nFourier Transformation or wavelet analysis, you can actually find different representations for\\nthe same time series data. Some of them are continuous, like the regular\\nFourier transformation, and some of them are discrete. You can even transform time\\nseries into a sequence, which we will discuss\\nlater in this class.',\n",
       "  'score': 21.230436325073242,\n",
       "  'rank': 2,\n",
       "  'document_id': '0c539371-80ec-4abc-9bbd-4854b1061d81',\n",
       "  'passage_id': 1032,\n",
       "  'document_metadata': {'source': '02_discrete-vs-continuous-data-representation.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 9880}},\n",
       " {'content': \"We're making recordings. When recording this video, the computer is actually\\ncollecting my sound, and that's also represented\\nas time series. In this case, they usually use frequencies to record\\nthis time series data. We will talk about\\nthat next week. Time series data\\nare not just used to record natural measures\\nof data attributes. They can also be used to\\nrecord more complicated, even data science results. You can see that in this example, we're also measuring some\\nnumerical attributes over time, over different weeks. This numerical\\nattribute is not just like the count or\\nthe temperature. It is actually a result of a\\ndata mining analysis itself. In fact, this is\\nmeasuring how likely is that different students will drop and book class over time. This numerical value, we're plotting over time, is\\nactually correlation. If the user dropped one course, how likely they are going\\nto drop another course in a similar category or in\\na different category. You can see that time\\nseries data can be used to measure real attributes of data as well as complicated\\ndata science results.\",\n",
       "  'score': 20.203397750854492,\n",
       "  'rank': 3,\n",
       "  'document_id': '01b0ea83-1605-4bdd-aba5-c1a656877218',\n",
       "  'passage_id': 1138,\n",
       "  'document_metadata': {'source': '01_time-series-data.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 5103}},\n",
       " {'content': \"Remember that if x causal y, then does not mean that y\\ncausal x. [inaudible] , how can we do this? Well, I'm sure that when you are taking the\\ncausal inference class, you'll learn not\\nso far techniques of figuring out the causality\\nbetween two variables. But in particular,\\nin time-series, Granger causality\\nis usually used. The basic idea of Granger causality is\\nactually quite simple. You look at two\\naligned time series, and you try to see, whether, you can actually use\\none to predict the other, conditional on the one itself.\",\n",
       "  'score': 20.199522018432617,\n",
       "  'rank': 4,\n",
       "  'document_id': '5eceb157-7a6b-40f5-a9b8-20df3d587e7e',\n",
       "  'passage_id': 1019,\n",
       "  'document_metadata': {'source': '02_granger-causality.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 2269}},\n",
       " {'content': \"For example, you can discover which timestamps have values that are so much different\\nfrom their neighbors, or which values now the\\nactually observed are so much different from the predicted value\\nusing time series model. Don't take that all\\nanomalies are bad. Sometimes anomalies are good too. For instance, in this paper, people are looking at the\\nspikes of block mentions of certain books and Amazon\\nsubstrings of certain books. They found that there's\\nextra two day gap that the block mentions\\nof a certain book, predicts the sales of\\nthe book on Amazon. So in this case, you can\\nsee that a spike which is the surprising peak of the time series is\\nessentially an anomaly. This time the anomaly or the outlier is\\ngood. It has value. Of course, based on all these lower-level\\ndata science outputs, data mining outputs, we can do classification on time series. For instance, if we gather the sensor data,\\nthe accelerometers, sensors of running, walking, and jogging over time we\\ncan make predictions. We can classify any\\ngiven user activity into walking or into running. This is basically how Apple transfers steps\\nwhere using the House app. We could also do clustering\\nof time-series data. In the previous example, we know what the classes are. We know that people\\ncould be running, could be jogging,\\ncould be walking. But, in this example,\\nin clustering, we don't know what the\\nclasses should be. Instead, we can\\nlisten from the data. We can just learn the categories that you know can summarize\\nmodel time series the best and that will review many different clusters or many different aggregated\\npatterns of times of state. You can see that no\\nmatter what we are talking about\\nclassification, clustering, they can literally be built upon the lower labeled data mining\\noutputs such as patterns, such as similarities, and\\nsuch as time series models. What about classification? Clustering will be introduced\\nin Machine Learning. I strongly recommend you to use what you have\\nlearned in this course to extract patterns and similarities to build fancy\\nMachine Learning models. With so many strong data\\nmining functionalities of time source data, you can actually apply\\ntime series data analysis to almost every domain. In finance, of course,\\nwe make predictions.\",\n",
       "  'score': 20.010271072387695,\n",
       "  'rank': 5,\n",
       "  'document_id': 'dbdb2d0d-3fd1-47fb-94b5-79b21566df03',\n",
       "  'passage_id': 1069,\n",
       "  'document_metadata': {'source': '03_tasks-of-time-series-data-mining.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 7102}},\n",
       " {'content': 'Data Mining II (SIADS 632), Syllabus SIADS 632: Data Mining II Course Overview And Prerequisites: This course extends Data Mining I and introduces additional data representations and tasks involved in mining real world data, with a particular focus on sequence modeling, time series analysis, and mining data streams. It introduces how to extract patterns, compute similarities/distances of data, and make predictions under these data representations.',\n",
       "  'score': 19.510324478149414,\n",
       "  'rank': 6,\n",
       "  'document_id': '0c4167ca-1ea7-45bf-98ec-e424960ea1c2',\n",
       "  'passage_id': 237,\n",
       "  'document_metadata': {'source': '632_2022-10.md',\n",
       "   'heading': 'Syllabus SIADS 632: Data Mining II Course Overview And Prerequisites',\n",
       "   'section': '1',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'course_date': 'October 2022',\n",
       "   'document': 'https://www.si.umich.edu/sites/default/files/632%20_0.pdf'}},\n",
       " {'content': \"If you can predict the\\nmarket, you'll become rich. In health care, you gather the ECG data and many other\\ntypes of clinical data. You use time series\\nanalysis to either make predictions or just\\ndo classification. You do the diagnosis which is essentially a\\nclassification task if the patient has the\\nparticular disease so that he can provide\\ntimely treatments. In Business Intelligence, you try to predict the supply and demand, and you can also predict the success or failure\\nof the product. You do that through analyzing\\nmultiple time series. In sports analytics, you gather the behavioral data of the athletes and you\\nmake predictions, you estimate the risk of injury, and then you also want to make predictions of your opponents. There are many\\nother applications. Don't forget that you\\ndon't just need to use time series to represent a\\nraw measurements of data. You can also use time series to record the more complicated\\noutputs of the mind. So through mining many\\ndifferent types of data, you have the output such as\\nthe estimated probability. You can represent the estimated\\nprobability over time. You can represent the\\nestimated correlation over data over time. You can actually use\\ntime series analysis to do the second level\\nscience research. In Engineering, the\\nentire field of signal processing is built upon time series analysis\\nas the basis. There are many\\nother applications. I'm sure that you have\\nencountered times series data in your domain, whatever your domain is. Then after learning the\\ntechniques in this course, you can try to apply the time series techniques\\nto your domain. To summarize what we have\\ntalked about so far, I wanted you to understand how the time series is different from a sequence and a vector. Basically, is different\\nfrom a sequence because it's module is majoring\\nnumerical attributes, and it records actual timestamps instead of just the orders. It's different from the\\nvector because there are only ways infinite\\nnumber of dimensions. I hope you understand the difference and\\nconnection between discrete representation and\\ncontinuous representation. That is exactly why we need time series as the spatial\\ndata representation. I hope you remember the\\nbasic data mining tasks of time series analysis and know that they had been applied\\nto almost every domain.\",\n",
       "  'score': 19.298023223876953,\n",
       "  'rank': 7,\n",
       "  'document_id': '55fa6d5e-a35d-4866-a671-81a4646d74c1',\n",
       "  'passage_id': 1070,\n",
       "  'document_metadata': {'source': '03_tasks-of-time-series-data-mining.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 11260}},\n",
       " {'content': \"What is that? The [inaudible] , HMMs are much more\\npowerful than that. In speech recognition,\\nsignal processing, or time series analysis. You can also find\\napplications of HMMs. Essentially, once your\\ndata has a sequence, can be composed as the\\nsequence of innovations. If the Markov assumption holds. If there are hidden states\\nthat you're interested in, you can use HMM. In behavioral\\nanalysis, in finance, sports, business, transportations,\\nor human education. If you are observing a\\nsequence of user behaviors and you're interested in some hidden states of the sequences, you can apply HMM. I have to tell you that, although HMM has\\nbeen very powerful, especially 10 years\\nago, nowadays, they are more oftenly replaced\\nby deep-learning models, especially recurrent\\nneural networks. Although, this is the case, HMMs are still widely used as reliable based states and building blocks for more\\ncomplex techniques. For instance, they are\\nvery recent work that have been combining HMMs into\\ndeep learning models, and the results are actually better than just applying\\ndeep learning models. This is just foreshadowing of what you will learn in\\na deep learning class. Well, you may ask, I have\\nintroduced so many models, HMMs, Markov chain,\\nUnigram Language Models. Even with Markov chain, always Hidden Markov Models, there are many variations. You can have three\\nstates, five states. You can add the input state, add a start state and end state, and you can do many other things. The question is, which\\nmodel is better?\",\n",
       "  'score': 18.968225479125977,\n",
       "  'rank': 8,\n",
       "  'document_id': 'e627ea84-e15f-44a7-b865-2ba071d064d2',\n",
       "  'passage_id': 860,\n",
       "  'document_metadata': {'source': '02_hidden-markov-model-part-2.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 6419}},\n",
       " {'content': \"But also the process becomes slower and\\nthen just problem of overfitting. So besides regression based methods, there's a huge family of methods\\ncalled smoothing based methods. And this smoothing-based methods also\\nknown as noise reduction methods. The basic idea is that, if you have a real\\nworld time series, its usually messy. And you want to reduce the noise\\nin this time series, so that the shape becomes\\nmore obvious to you. The particular smoothing based methods, including binning, moving averages and\\nsome of the autoregressive methods. And we will introduce some later in this\\nclass but now let's just look at binning.\",\n",
       "  'score': 18.91429328918457,\n",
       "  'rank': 9,\n",
       "  'document_id': 'f085c4af-3e2b-462c-a832-d37518aed867',\n",
       "  'passage_id': 1079,\n",
       "  'document_metadata': {'source': '01_time-series-patterns-trends.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 8970}},\n",
       " {'content': \"This case, we can see that we have naturally multi-dimensional\\nnumerical values. Each dimension corresponds to the measurements in a\\ndifferent location. In this case, we can obtain multi-dimensional\\ntime series and they are naturally aligned\\nby timestamps. Another application of time\\nseries is sports analytics. Actually you can find lots of time series data in\\nsports analytics. The athletes wear devices. These sensors can actually measure the different types of sickness and tracks\\nthem over time. You can actually wear\\nmultiple sensors when you are actually\\ndoing the sports. The sensor sickness give us multiple time series and they are naturally\\naligned by time. Another example, in healthcare, you can see that through\\nthose health devices, we can collect data, such as the ECG data or others. They're also taking the\\nform of time series data. Of course, multiple\\naligned times. Yet another example, soundtracks.\",\n",
       "  'score': 18.667510986328125,\n",
       "  'rank': 10,\n",
       "  'document_id': 'edce7d51-2ade-4b5b-96a7-b99bd5b534b6',\n",
       "  'passage_id': 1137,\n",
       "  'document_metadata': {'source': '01_time-series-data.en.txt',\n",
       "   'course_number': 'SIADS 632',\n",
       "   'course_title': 'Data Mining II',\n",
       "   'start_index': 4182}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This takes 30+ seconds to start up the first time, but runs faster after that\n",
    "RAG.search(query=\"Which class involves time series analysis?\") # documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e39b9e17-5695-44c7-a5b0-7d15844b9653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"Let's start by looking at a very important and widely used linear dimensionality\\nreduction technique called principal component\\nanalysis or PCA. There are a couple of ways\\nto describe how PCA works. An intuitive, more geometric way and then there's\\na linear algebra way. What we're going\\nto do is to start, we're going to look\\nat the geometric way, the visually intuitive\\nway and then later, we'll look at the\\nlinear algebra behind PCA as part of understanding a powerful general\\ndimensionality reduction method called singular value\\ndecomposition or SVD, which is very closely\\nconnected to PCA. Intuitively what PCA does, it takes your\\noriginal data points. Here I have a very simple\\ndataset with two features. It's a two-dimensional\\ndataset and imagine each instance is denoted by a point here in the\\nscatterplot and intuitively, what PCA does geometrically to these original data points\\nis it finds a rotation of the points so that the\\ndimensions are statistically uncorrelated and\\nafter it does that, each data point can be\\ndescribed by new coordinates relative to this new frame of reference in these\\nuncorrelated dimensions. Once we find these new\\nrotated dimensions, the nature of PCA is such that we can drop all but\\nthe most informative coordinates that capture\\nmost of the variation in the original dataset and\\nwe still end up with a good approximation in the least square sense\\nto our original dataset. Looking at this\\nsynthetic example, we have two features\\nthat are somewhat highly correlated and when we apply PCA. PCA will find the direction in the data has highest variance. What that means is, if we think\\nabout PCA trying to find, there are different ways\\nthat you can project this data onto a line. Imagine there are all\\nthese different ways.\",\n",
       "  'score': 25.38526725769043,\n",
       "  'rank': 1,\n",
       "  'document_id': '1b40a2a6-4b2f-4dcc-aa3f-93f7b8438d08',\n",
       "  'passage_id': 1832,\n",
       "  'document_metadata': {'source': '03_intro-to-dimensionality-reduction-and-principal-components-analysis.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 14870}},\n",
       " {'content': \"To perform PCA, our\\nfirst step is to import the PCA class from\\nSKlearn decomposition. Our next step is to\\nmake sure the data is properly prepared before\\nwe use PCA on it. To prepare your data for PCA, it's usually a good idea to\\nfirst transform the dataset. Each features range of values has zero mean and unit variance. We can do this here using\\nthe fit and transform methods of the standard\\nscaler class as shown here. By default, in scikit-learn, PCA will automatically do\\ncolumn centering of your data, but it won't do the\\nvariance standardization. We recommend this\\nstandard scalar step.\",\n",
       "  'score': 23.240402221679688,\n",
       "  'rank': 2,\n",
       "  'document_id': 'c7060be4-d65f-4cfe-9b2f-9b9659006b70',\n",
       "  'passage_id': 1843,\n",
       "  'document_metadata': {'source': '03_intro-to-dimensionality-reduction-and-principal-components-analysis.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 31284}},\n",
       " {'content': \"The new data will have fewer\\ndimensions from the old data. So to summarize PCA, we know that principal components\\nare very special patterns of matrix data. And in fact, they're orthogonal\\neigenvectors of the covariance matrix. They are very useful because\\nthey can define new orthogonal coordinate system of your matrix. And based on this orthogonal\\ncoordinate system, many of the vector similarity or\\ndistance matrix are much more trustable. Through PCA,\\nyou can transform the original vector data into low dimensional and\\northogonal vector space. And this is why PCA has been a very\\nuseful step of feature engineering in many machine learning tasks. Is this it? In reality, shouldn't we always use PCA? And in fact,\\nPCA also have some of its limitations. Why? Remember that PCA relies heavily\\non eigenvalues and eigenvectors. And we know that eigenvalues and eigenvectors only apply\\nto square matrices. Also PCA relies on eigendecomposition. And we know that stable eigendecomposition\\ndoes not apply to all square matrices. In fact, it requires that\\nyour matrix is symmetric and sometimes positive semi-definite. And that is why we apply PCA\\nto the covariance matrix XTX, because we know that the covariance\\nmatrix is symmetric and it is also positive semi-definite. But in reality, we know that the original data matrices\\nwe deal with are almost never square.\",\n",
       "  'score': 22.703372955322266,\n",
       "  'rank': 3,\n",
       "  'document_id': '72db8b26-4f30-4c43-ae59-e7f39e78ffe1',\n",
       "  'passage_id': 3451,\n",
       "  'document_metadata': {'source': '04_3-3-4-principal-component-analysis.en.txt',\n",
       "   'course_number': 'SIADS 532',\n",
       "   'course_title': 'Data Mining I',\n",
       "   'start_index': 5971}},\n",
       " {'content': \"There is a package called PCA in Python\\nthat you can install and use if you like. It includes a biplot plotting routine\\nwhich I've shown here on the right, very easy to install and use,\\nalthough I haven't used it myself, some aspects of it are a little bit\\nmore difficult to sort of customize. What I would recommend is just using\\nthe notebook code that we provided, it's kind of instructive just to\\ngo through it to see how it works. And then you feel free to use that code to\\ncreate your own biplots for your datasets.\",\n",
       "  'score': 22.491031646728516,\n",
       "  'rank': 4,\n",
       "  'document_id': '0b019eb6-6549-4fa3-86a1-ca1ed0afdf21',\n",
       "  'passage_id': 1621,\n",
       "  'document_metadata': {'source': '05_pca-biplots-and-how-to-use-them.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 19336}},\n",
       " {'content': 'PCA constrains the counterpart\\nto W to be orthonormal. And the rows of H to be\\northogonal to each other. And the math constrains at W and\\nH to be positive as we said. And in order to construct a face for\\nexample, the parts combine additively\\nto form the whole.',\n",
       "  'score': 22.4458065032959,\n",
       "  'rank': 5,\n",
       "  'document_id': 'd3db8341-d8ee-465c-b5cd-c64de8964127',\n",
       "  'passage_id': 1775,\n",
       "  'document_metadata': {'source': '07_topic-models-2-non-negative-matrix-factorization-nmf.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 9734}},\n",
       " {'content': \"But here's a really\\nimportant reminder. If you are using PCA\\nto find new features, to avoid data leakage, always apply it after you\\ndo the train test split. This applies to any type of normalization, scaling,\\ntopic modeling, or other preprocessing that\\nmakes use of properties, or statistics across\\nthe entire dataset. You always split first\\nand then process later. We'll revisit this rule multiple times in the future applications. After scaling the data, we create the PCA object. We specify that we want to retain just the first two\\nprincipal components to reduce the dimensionality\\nto just two columns. Then we call the fit method, using the normalized data. This will set up PCA so that it learns the right\\nrotation at the dataset. We can then apply this\\nproperly prepared PCA object to project all the points in our input dataset to this\\nnew two-dimensional space. You see that if we\\ntake the shape of the array that's\\nreturned from PCA, it's transformed our\\noriginal dataset that had 30 columns\\ncorresponding to the 30 features into a new array that has\\njust two columns, essentially expressing each original data\\npoint in terms of two new features that\\nrepresent the position of the data point in this new\\ntwo-dimensional PCA space. We can then just create\\na scatterplot using these two new features to see how the data forms clusters. That's what's being shown here. In this example, we've\\nused the dataset that also has labels for\\nsupervised learning, namely, the malignant\\nand benign labels on cancer cells that were\\nprovided by human experts. This helps us see how well PCA serves to find\\nclusters in the data. Here's the result of plotting all the 30 featured data samples, but using the two new\\nfeatures computed with PCA. The first feature is the\\nfirst principle component, along the x axis.\",\n",
       "  'score': 22.096254348754883,\n",
       "  'rank': 6,\n",
       "  'document_id': '36e35df1-eb00-4701-bd84-e5faa9acca11',\n",
       "  'passage_id': 1845,\n",
       "  'document_metadata': {'source': '03_intro-to-dimensionality-reduction-and-principal-components-analysis.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 36201}},\n",
       " {'content': \"So principal components\\nare very interesting, very special patterns in your matrix data. That they define orthogonal\\ndirections that capture most of the variance in your data. Usually there are multiple\\nprinciple components of a matrix. The first principal component would\\nexplain the most variance of your data. And then the second principal component\\nwould explain the most of the rest of the variances in your data. And the third one will explain most\\nof the rest, and so on and so forth. And the method discovers those\\nprincipal components is known as principal component analysis or PCA. Most of you may have heard\\nthat from many contexts, because this is such the popular\\ndata science technique. The function of principal component\\nanalysis is essentially to find the eigenvalues and\\neigenvectors of the covariance matrix. We will talk about what that is later. The eigenvectors with\\nthe largest eigenvalues of this covariance matrix will\\ncorrespond to the directions, the dimensions that have the strongest\\ncorrelation in your data set. So the first eigenvector that\\ncorresponds to the largest eigenvalue is the most important\\ndirection in your data set. The second eigenvector defines the second\\nimportant direction in your data set, so so on and so forth. So, how does PCA work? First we normalize the matrix. Suppose your original data matrix\\nX has n rows and p dimensions. We subtract the mean of every\\ncolumn from this matrix x. And then we compute\\nthe so-called covariance matrix. Let's just name that A. The covariance matrix of your\\ndata matrix is essentially the product of the transpose of your\\nmatrix X and original matrix X. We call that XTX. XT is the transpose of your data matrix X. So A is the covariance matrix. If X has n rows and\\np dimensions, and p columns, then X transpose X will have p rows and\\np columns. So A is the p by p matrix. And what's even nicer about A is\\nthat A is a symmetric matrix. Does that ring a bell? Because A is a symmetric matrix we can\\nalways compute the eigendecomposition of A as we discussed earlier. And after the eigendecomposition, we can rewrite A as the product\\nof three matrices, the U matrix, a Lambda matrix, and\\nthe inverse of the U matrix.\",\n",
       "  'score': 21.94761085510254,\n",
       "  'rank': 7,\n",
       "  'document_id': '018a8fa3-ae5f-4213-8e5b-5c1b9bfe11f2',\n",
       "  'passage_id': 3446,\n",
       "  'document_metadata': {'source': '04_3-3-4-principal-component-analysis.en.txt',\n",
       "   'course_number': 'SIADS 532',\n",
       "   'course_title': 'Data Mining I',\n",
       "   'start_index': 0}},\n",
       " {'content': 'You need to spend\\nan extra effort on interpreting what you really\\ndiscover from SVD and PCA. So to summarize this lecture, I hope you understand\\nthat eigenvectors and principal components are very useful patterns\\nin matrix data, and this also includes\\nthe singular vectors. I want you to understand\\nhow PCA transforms the matrix data into orthogonal and low\\ndimensional vectors. Relatively, I want\\nyou to understand how SVD can achieve similar results, and how SVD is performed on the original data\\nmatrix instead of a square covariance matrix.',\n",
       "  'score': 21.850704193115234,\n",
       "  'rank': 8,\n",
       "  'document_id': 'f2f248fe-45fa-4c98-8a01-5e5612eaa9b5',\n",
       "  'passage_id': 3323,\n",
       "  'document_metadata': {'source': '08_3-3-7-svd-in-practice.en.txt',\n",
       "   'course_number': 'SIADS 532',\n",
       "   'course_title': 'Data Mining I',\n",
       "   'start_index': 2369}},\n",
       " {'content': 'You declare an object of the PCA class,\\nright. You tell the the class how many\\ncomponents you want to keep, and you fit your data to the class. Then you can achieve the transformed data.',\n",
       "  'score': 21.822832107543945,\n",
       "  'rank': 9,\n",
       "  'document_id': '41b3b7b3-5dfc-40ad-b3f7-9f25a1a5008c',\n",
       "  'passage_id': 3450,\n",
       "  'document_metadata': {'source': '04_3-3-4-principal-component-analysis.en.txt',\n",
       "   'course_number': 'SIADS 532',\n",
       "   'course_title': 'Data Mining I',\n",
       "   'start_index': 5784}},\n",
       " {'content': \"You can take a look at\\nthe paper for details, we've included the paper\\nin the course materials. Finally, here's some results for two different image\\nprocessing algorithms, as shown in the original principal component\\npursuit paper. In that paper the author\\nis focused on task, they had a main object with\\npredictable structure, like furniture in rooms or faces, and then some form\\nof sparse noise. For the background\\ndetection task, the sparse noise was the\\nmoving image of a person, that was not part of the\\ndesired background image. For the face imaging task, earlier work in image\\nprocessing had shown, that fairly simple objects\\nthat were lit from a distance, can be modeled by a\\nlow-rank lighting model, with nine dimensions. Thus, the face images here, could be considered the\\nsum of the low-rank model, plus pixels that are\\nlarge magnitude, spatially sparse outliers, in the form of very dark shadows and very bright\\nshiny reflections. As you can see, the\\nrobust PCA algorithm is highly effective, at removing these artifacts, in order to clean up\\nthe original image. The face imaging task, is a nice example of how an unsupervised preprocessing\\nstep, like Robust PCA, can be used to remove unwanted\\nartifacts in the data, and thus potentially\\nboost prediction accuracy of a supervised learning methods, such as an image classifier. Hopefully, robust PCA in some\\nform will become a part of Scikit-learns unsupervised\\nlearning libraries in the near future. Final variation of PCA we will cover is called kernel PCA. Unlike traditional\\nPCA, which uses a linear transformation\\nof the input variables, kernel PCA can perform a non-linear transformation\\non the input data, thanks to the application of\\na nonlinear kernel function. The details of how kernel PCA works internally\\nare a bit complex, but the main idea is\\nexactly the same one that you may be familiar with from kernelized support\\nvector machines, where we introduced the use\\nof a kernel function to map the original dataset to a higher-dimensional space\\nwith a nonlinear mapping. In that high dimensional space, the points became easier to\\nanalyze with a linear method. Ironically, by\\ntemporarily increasingly the dimensionality of the data with a nonlinear kernel mapping, kernel PCA is able to eventually reduce its dimensionality\\nmuch more effectively.\",\n",
       "  'score': 21.786365509033203,\n",
       "  'rank': 10,\n",
       "  'document_id': 'b163a7a7-97be-4ca1-aece-17f242c86ed0',\n",
       "  'passage_id': 1710,\n",
       "  'document_metadata': {'source': '06_pca-variants-sparse-rotated-robust-and-kernel-pca.en.txt',\n",
       "   'course_number': 'SIADS 543',\n",
       "   'course_title': 'Unsupervised Learning',\n",
       "   'start_index': 10113}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG.search(query=\"How does PCA work?\") # transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92c09b6-78fd-4a33-b5f7-c21a09ae8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ragatouille let's you create a LangChain retriever from the indexed model\n",
    "retriever = RAG.as_langchain_retriever(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e23487c-d41e-405d-b067-3740840b7d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnewman/miniconda3/envs/rag/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Class Registration > Q: What is a Backpack?: A: The Backpack is a feature available on [Wolverine Access](https://wolverineaccess.umich.edu/) that works much like the \"shopping carts\" you have seen on many retail websites. With the Backpack you can prepare for your upcoming registration appointment by filling it with classes you want to take. When it is time to register, you will select one or more classes from your Backpack to register for it. NOTE: Placing a class in your Backpack does not enroll you in that class. You must register for a class to become enrolled in it. It is important to note that receiving an override does not enroll you in the course, you still must register through [Wolverine Access](https://wolverineaccess.umich.edu/) to claim the seat that has been opened for you.', metadata={'source': 'advising_guide.md', 'heading': 'Class Registration > Q: What is a Backpack?', 'section': '21', 'course_number': 'n/a', 'course_title': 'n/a', 'course_date': 'n/a', 'document': 'https://docs.google.com/document/d/1A3zdTF0AYQY_zzD2-OlpSHeDxnWqFVEhXl446SyT_pA/edit'}),\n",
       " Document(page_content=\"This approach is\\npopularly known as a bag-of-words approach in natural language\\nprocessing literature. The reason it is called\\nbag-of-words is because it just gives about if and how\\nmany times a word occurs. It doesn't care\\nabout the position or order of the word\\nin the sentence. Bag-of-words based language\\nmodeling approaches have been the mainstay of language modeling\\nfor a long time, and give comparative\\nperformance on several natural language\\nprocessing tasks were ordering information\\nis not very important. However, they can\\nperform poorly on tasks where ordering\\ninformation is important.\", metadata={'source': '01_sequence-modeling.en.txt', 'course_number': 'SIADS 642', 'course_title': 'Deep Learning I', 'start_index': 2446}),\n",
       " Document(page_content='Cool.', metadata={'source': '04_using-inkscape-to-trace-and-modify-visualizations.en.txt', 'course_number': 'SIADS 523', 'course_title': 'Communicating Data Science Results', 'start_index': 13476}),\n",
       " Document(page_content='', metadata={'source': '05_university-of-michigans-primary-data-center.en.txt', 'course_number': 'SIADS 673', 'course_title': 'Cloud Computing', 'start_index': 0}),\n",
       " Document(page_content=\"So I'll give you an example\\nthat comes to mind. You need to find labels for images. You are trying to find\\ninstances of a backpack or you're trying to do facial detection. And so your thought might be okay, well,\\ngive me 1000 images of people's faces or 10,000 peoples images of people's faces,\\nlabel them. And we'll build a model to do that, and yes, you can do that, but\\nwhy when you have the cloud. When we go into Amazon's\\nmachine learning tab, there's a lot of different\\nservices within here. And we're going to go through\\na lot of these together. Some of these are fun services. Some of these are just really cool. For example, deep racer,\\ndeep racer is the coolest thing, the coolest service from\\nAmazon that I've seen. You get to race a car with machine\\nlearning and you actually, where is it? You actually buy this car? This is a thing they sell on Amazon,\\nlike the actual Amazon.com. You buy this car, it's a little kit,\\nyou put together, it interfaces with Amazon's cloud. And you can build a model around\\nracing on this and it's Amazon. So they're saying,\\nhey you're charged for the storage, that's going to be S3 training and\\nevaluation. That's going to be compute. And this is $3.50 an hour and\\nmodel storage is 23 cents, I'm sorry,\\ntwo cents per gigabyte per month. And you can actually go and buy this and\\ncompete in Amazon services. There's not really like a business\\nvalue specifically to this tool set. That's not entirely true.\", metadata={'source': '06_machine-learning-in-the-cloud.en.txt', 'course_number': 'SIADS 673', 'course_title': 'Cloud Computing', 'start_index': 942})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is a backpack?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecdf100f-bec6-4d20-a948-ab99a8548e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step is to add this to the RAG pipeline and check its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca3988-f590-4950-9f02-3368fccc9324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
