{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d50fa-aff8-4da5-a04f-ccf1741f2e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import (\n",
    "    RetrievalQA,\n",
    "    ConversationalRetrievalChain,\n",
    "    RetrievalQAWithSourcesChain,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import pandas as pd\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from semantic_router import Route, RouteLayer\n",
    "from semantic_router.encoders import HuggingFaceEncoder\n",
    "\n",
    "import gradio as gr\n",
    "from json2html import Json2Html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca920e-4920-46df-8a0f-9314293dfe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some helper variables\n",
    "\n",
    "model = \"mistral-7b-instruct-v0.2\"\n",
    "model_path = \"/Users/arnewman/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "\n",
    "rootdir = \"..\"\n",
    "persist_directory = \"./../embeddings\"\n",
    "index_root = rootdir + \"/../colbert_index/\"\n",
    "colbert_path = rootdir + \"/../colbertv2.0/\"\n",
    "index_path = rootdir + \"/../colbert_index/colbert/indexes/documents/\"\n",
    "transcript_path = rootdir + \"/../colbert_index/colbert/indexes/transcripts/\"\n",
    "combined_path = rootdir + \"/../colbert_index/colbert/indexes/combined/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762a64b-75b6-4d0d-a58b-2772ab803e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Colbert model for documents and transcripts\n",
    "RAG = RAGPretrainedModel.from_index(index_path = combined_path)\n",
    "\n",
    "# Get metadata and convert to DataFrame\n",
    "df_rag = pd.read_json(combined_path+'docid_metadata_map.json').T.reset_index()\n",
    "\n",
    "# Helper to identify relevant documents for retrievers\n",
    "def filter_pids(df, search_term):\n",
    "    return list(df['index'][df.course_number==search_term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c81e82-cc39-4dd6-9ccc-c484faf755c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MADS course numbers and titles\n",
    "courses = {\n",
    "    \"501\": \"Being a Data Scientist\",\n",
    "    \"502\": \"Math Methods I\",\n",
    "    \"503\": \"Data Science Ethics\",\n",
    "    \"505\": \"Data Manipulation\",\n",
    "    \"511\": \"SQL and Databases\",\n",
    "    \"515\": \"Efficient Data Processing\",\n",
    "    \"516\": \"Big Data: Scalable Data Processing\",\n",
    "    \"521\": \"Visual Exploration of Data\",\n",
    "    \"522\": \"Information Visualization I\",\n",
    "    \"523\": \"Communicating Data Science Results\",\n",
    "    \"524\": \"Presenting Uncertainty\",\n",
    "    \"532\": \"Data Mining I\",\n",
    "    \"542\": \"Supervised Learning\",\n",
    "    \"543\": \"Unsupervised Learning\",\n",
    "    \"571\": \"Business SQL\",  # No syllabus for this one :(\n",
    "    \"593\": \"Milestone I\",\n",
    "    \"601\": \"Qualitative Inquiry for Data Scientists\",\n",
    "    \"602\": \"Math Methods II\",\n",
    "    \"611\": \"Database Architecture & Technology\",\n",
    "    \"622\": \"Information Visualization II\",\n",
    "    \"630\": \"Causal Inference\",\n",
    "    \"631\": \"Experiment Design and Analysis\",\n",
    "    \"632\": \"Data Mining II\",\n",
    "    \"642\": \"Deep Learning I\",\n",
    "    \"643\": \"Machine Learning Pipelines\",\n",
    "    \"644\": \"Reinforcement Learning Algorithms\",\n",
    "    \"652\": \"Network Analysis\",\n",
    "    \"655\": \"Applied Natural Language Processing\",\n",
    "    \"673\": \"Cloud Computing\",\n",
    "    \"680\": \"Learning Analytics and Educational Data Science\",\n",
    "    \"681\": \"Health Analytics\",\n",
    "    \"682\": \"Social Media Analytics\",\n",
    "    \"685\": \"Search and Recommender Systems\",\n",
    "    \"687\": \"Introduction to Sports Analytics\",\n",
    "    \"688\": \"Data Science for Social Good\",\n",
    "    \"696\": \"Milestone II\",\n",
    "    \"699\": \"Capstone\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9972be7-c2b7-428f-a75c-9585e2598bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build semantic routes for each class\n",
    "routes = []\n",
    "for num, name in zip(courses.keys(), courses.values()):\n",
    "    route_name = \"SIADS \" + str(num)\n",
    "    route_utterances = [route_name.lower(), route_name.lower().replace(\" \",\"\"),\n",
    "                        name.lower(), name.lower() + \" class\", name.lower() + \" course\",\n",
    "                       \"who teaches \" + route_name.lower(), \"who teaches \" + name.lower()]\n",
    "    routes.append(Route(name=route_name, utterances=route_utterances))\n",
    "\n",
    "# Select local encoder and build route layer\n",
    "encoder = HuggingFaceEncoder(str=\"/Users/arnewman/.cache/huggingface/hub/models--sentence-transformers--UAE-Large-V1/\", device=\"mps\")\n",
    "rl = RouteLayer(encoder=encoder, routes=routes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e01927-a956-4a28-b6d0-7a5785658019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use evaluation questions to test accuracy\n",
    "# From: https://github.com/aurelio-labs/semantic-router/blob/main/docs/06-threshold-optimization.ipynb\n",
    "\n",
    "test_data = [\n",
    "    (\"Which class involves time series analysis?\", \"SIADS 632\"),\n",
    "    (\"Who teaches the SQL and Databases class?\", \"SIADS 511\"),\n",
    "    (\"What are the prerequisites for Data Science for Social Good?\", \"SIADS 688\"),\n",
    "    (\"When are the office hours for the Math Methods course?\", \"SIADS 502\"),\n",
    "    (\"Are there any weekly readings for Milestone II?\", \"SIADS 699\"),\n",
    "    (\"What are the outcomes of Qualitative Inquiry?\", \"SIADS 601\"),\n",
    "    (\"What textbook is required for SIADS 505?\", \"SIADS 505\"),\n",
    "    (\"What textbook is required for Data Manipulation?\", \"SIADS 505\"),\n",
    "    (\"Which week of unsupervised learning covers DBSCAN?\", \"SIADS 543\"),\n",
    "    (\"How many credits are required to complete the MADS program?\", None),\n",
    "    (\"How long do students have to complete the MADS program start to finish?\", None),\n",
    "    (\"How many points is the comprehensive oral exam worth in SIADS 593?\", \"SIADS 593\"),\n",
    "    (\"What is the penalty for late submission in SIADS 630?\", \"SIADS 630\"),\n",
    "    (\"How do I get accommodations for a class?\", None),\n",
    "    (\"What is a backpack?\", None),\n",
    "    (\"When is the latest I can drop a course?\", None),\n",
    "    (\"How do I get an override to take a class?\", None),\n",
    "    (\"How do I take a leave of absence from the MADS program?\", None),\n",
    "    (\"What are the prerequisites for Search and Recommender Systems?\", \"SIADS 685\")\n",
    "]\n",
    "\n",
    "# unpack the test data\n",
    "X, y = zip(*test_data)\n",
    "\n",
    "# evaluate using the default thresholds\n",
    "accuracy = rl.evaluate(X=X, y=y)\n",
    "print(f\"Original Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Check defaults\n",
    "# route_thresholds = rl.get_thresholds()\n",
    "# print(\"Default route thresholds:\", route_thresholds)\n",
    "\n",
    "# Call the fit method\n",
    "rl.fit(X=X, y=y)\n",
    "\n",
    "# Updated thresholds\n",
    "# route_thresholds = rl.get_thresholds()\n",
    "# print(\"Updated route thresholds:\", route_thresholds)\n",
    "\n",
    "# evaluate using the new thresholds\n",
    "accuracy = rl.evaluate(X=X, y=y)\n",
    "print(f\"Revised Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c787d69-574d-489a-9ae8-a8f0e065b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model - two options\n",
    "\n",
    "# This option assumes that we are running an OpenAI-compatible\n",
    "# endpoint, in this case on the local host. We can do this with\n",
    "# llama-cpp-python. This has the advantage of simulating the use\n",
    "# of a remote LLM.\n",
    "llm_open = OpenAI(openai_api_base = \"http://localhost:7999/v1\",\n",
    "                  model = \"mistral-7b-instruct-v0.2\",\n",
    "                  openai_api_key = \"hello\",\n",
    "                  temperature = 0.1,\n",
    "                  top_p = 1,\n",
    "                  max_tokens = 1024,\n",
    "                  callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "                 )\n",
    "\n",
    "# This option also uses llama-cpp-python but starts up the model\n",
    "# within the notebook. This does not require spinning up a separate\n",
    "# endpoint, although the notebook will necessarily use more memory.\n",
    "# llm_open = LlamaCpp(model_path=model_path,\n",
    "#                     n_ctx=32768,\n",
    "#                     n_gpu_layers=-1,\n",
    "#                     temperature=0.1,\n",
    "#                     top_p=1,\n",
    "#                     top_k=40,\n",
    "#                     repeat_penalty=1.1,\n",
    "#                     max_tokens=1024,\n",
    "#                     callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "#                     #stream=True,\n",
    "#                    )\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages=True,\n",
    "                                  output_key = \"result\")\n",
    "\n",
    "\n",
    "# Set prompt template\n",
    "template = '''\n",
    "Use only the following pieces of context to answer the question at the end. \n",
    "Keep your answers concise and do not provide additional explanations or interpretations. \n",
    "If the answer cannot be deduced from the context, just say that you don't know the answer, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a99cd-1db9-4353-8efc-cf7e494fbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant documents based on query and create retrievers\n",
    "# If relevant documents cannot be identified, build retrievers on all documents\n",
    "# Separate retrievers from the documents index and the transcripts index\n",
    "\n",
    "def get_chain(query, k=5):\n",
    "    r = rl(query)\n",
    "\n",
    "    if r.name:\n",
    "        doc_list = filter_pids(df_rag, r.name)\n",
    "        if len(doc_list) > 0:\n",
    "            retriever = RAG.as_langchain_retriever(doc_ids=doc_list, k=k) # Set k if desired\n",
    "        else:\n",
    "            retriever = RAG.as_langchain_retriever(k=k)\n",
    "        print(f\"Selected route: {r.name}\")\n",
    "    else:\n",
    "        retriever = RAG.as_langchain_retriever(k=k)\n",
    "        print(f\"Selected route: None. Using all documents.\")\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm_open,\n",
    "        chain_type=\"stuff\",\n",
    "        memory=memory,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        verbose=True,\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": PromptTemplate(\n",
    "                template=template,\n",
    "                input_variables=[\"context\", \"question\"])},\n",
    "    )\n",
    "\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdae311-dc26-4993-a5cf-7c72b1fc9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = Json2Html()\n",
    "\n",
    "# Grab the metadata for the first n relevant documents\n",
    "# and convert to an HTML table for display.\n",
    "def create_source_html(response, n = 5):\n",
    "    source_doc_list = []\n",
    "    num_docs = len(response[\"source_documents\"])\n",
    "    len_res = n if n <= num_docs else num_docs\n",
    "    for doc in response[\"source_documents\"][:len_res]:\n",
    "        doc_json = {}\n",
    "        doc_json['metadata'] = doc.metadata\n",
    "        doc_json['page_content'] = doc.page_content\n",
    "        source_doc_list.append(doc_json)\n",
    "    return j.convert(source_doc_list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed8b6e-c338-4493-b387-e5c778a6751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Glass()) as demo:\n",
    "    gr.Markdown(\n",
    "        f\"\"\"\n",
    "\n",
    "        # MADS-RAG: A Helpful Chatbot for Master's of Applied Data Science\n",
    "        # Students at the University of Michigan\n",
    "\n",
    "        ## Current model running is {model}.\n",
    "\n",
    "        Source data includes class syllabi, the MADS Student Handbook,\n",
    "        MADS Advising FAQs, and transcripts of class videos. Some source\n",
    "        data may be out of date.\n",
    "\n",
    "        Please note that answers provided by this chatbot protoype may not\n",
    "        be correct and should be verified through other means.\n",
    "        \"\"\"\n",
    "    )\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    with gr.Accordion(\"Source Documents\", open=False):\n",
    "        html = gr.HTML(label=\"Source Documents\", show_label=True)\n",
    "    clear = gr.ClearButton(value=\"Clear\", components=[msg,chatbot,html])\n",
    "    \n",
    "    def user(user_message, chat_history):\n",
    "        qa_chain = get_chain(user_message)\n",
    "        response = qa_chain(user_message)\n",
    "        chat_history.append((user_message, response[\"result\"]))\n",
    "        res_html = create_source_html(response, n=3)\n",
    "        yield gr.update(value=\"\"), chat_history, res_html\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot, html], queue=False)\n",
    "\n",
    "demo.launch(debug=True, share=True, auth=(\"madsuser\", \"winter2024\"),\n",
    "            server_name=\"0.0.0.0\", server_port=5678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84339208-d2eb-441b-8296-79ffebf3dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display results\n",
    "# def process_llm_response(llm_response):\n",
    "#     print(f\"\\nQuestion: {llm_response['query']}\")\n",
    "#     print(f\"\\nAnswer: {llm_response['result']}\")\n",
    "#     print(\"\\n\\nSources:\")\n",
    "#     for i, source in enumerate(llm_response[\"source_documents\"]):\n",
    "#         m = source.metadata\n",
    "#         try:\n",
    "#             print(f\"{i + 1}. {m['course_title']} ({m['course_number']}): {m['heading']}\")\n",
    "#         except:\n",
    "#             print(f\"{i + 1}. {m['course_title']} ({m['course_number']}): {m['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4467a7f-13f0-4135-9dfb-d0e87ce8ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_response = qa_chain(query)\n",
    "# process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
