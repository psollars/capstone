{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01886c15-019c-40bc-9b79-eef3f837353e",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Semantic Routing\n",
    "\n",
    "This notebook allows the user to run the complete Retrieval-Augmented Generation Pipeline for MADS-RAG. The pipeline can include a customized semantic router, if desired. The user can interact with the pipeline with a Gradio-based user interface.\n",
    "\n",
    "Dependencies include:\n",
    "- langchain (including langchain_community, langchain_core, and langchain_openai)\n",
    "- pandas\n",
    "- ragatouille\n",
    "- semantic_router\n",
    "- gradio\n",
    "- json2html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cbe7d-39ae-4f49-bd3d-bb64ca29b553",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d50fa-aff8-4da5-a04f-ccf1741f2e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain components\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Use ragatouille to work with ColBERT in a LangChain pipeline\n",
    "# Pandas is for ease of working with metadata\n",
    "from ragatouille import RAGPretrainedModel\n",
    "import pandas as pd\n",
    "\n",
    "# Semantic Router components\n",
    "from semantic_router import Route, RouteLayer\n",
    "from semantic_router.encoders import HuggingFaceEncoder\n",
    "\n",
    "# Gradio for UI with json2html to create HTML for source document metadata\n",
    "import gradio as gr\n",
    "from json2html import Json2Html\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e31f3-cb08-4411-be8f-87a5dfe1e8ae",
   "metadata": {},
   "source": [
    "## Helper Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca920e-4920-46df-8a0f-9314293dfe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"..\"\n",
    "\n",
    "model = \"mistral-7b-instruct-v0.2\"\n",
    "model_path = rootdir + \"/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "\n",
    "\n",
    "embeddings_path = rootdir + \"/embeddings/\"\n",
    "index_root = rootdir + \"/../colbert_index/\"\n",
    "colbert_path = rootdir + \"/../colbertv2.0/\"\n",
    "combined_path = rootdir + \"/../colbert_index/colbert/indexes/combined/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a5b0e-8a37-4e53-8d30-ac5b3dce49fe",
   "metadata": {},
   "source": [
    "## Start ColBERT and Create Metadata DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762a64b-75b6-4d0d-a58b-2772ab803e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ColBERT model for documents and transcripts\n",
    "RAG = RAGPretrainedModel.from_index(index_path = combined_path)\n",
    "\n",
    "# Get metadata and convert to DataFrame\n",
    "df_rag = pd.read_json(combined_path+'docid_metadata_map.json').T.reset_index()\n",
    "\n",
    "# Helper to identify relevant documents for retrievers\n",
    "def filter_pids(df, search_term):\n",
    "    '''Returns IDs of documents with search_term in metadata'''\n",
    "    return list(df['index'][df.course_number==search_term])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b500dd9-0a9a-4102-a9ef-ee1f724624a2",
   "metadata": {},
   "source": [
    "## Identify MADS course numbers and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c81e82-cc39-4dd6-9ccc-c484faf755c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = {\n",
    "    \"501\": \"Being a Data Scientist\",\n",
    "    \"502\": \"Math Methods I\",\n",
    "    \"503\": \"Data Science Ethics\",\n",
    "    \"505\": \"Data Manipulation\",\n",
    "    \"511\": \"SQL and Databases\",\n",
    "    \"515\": \"Efficient Data Processing\",\n",
    "    \"516\": \"Big Data: Scalable Data Processing\",\n",
    "    \"521\": \"Visual Exploration of Data\",\n",
    "    \"522\": \"Information Visualization I\",\n",
    "    \"523\": \"Communicating Data Science Results\",\n",
    "    \"524\": \"Presenting Uncertainty\",\n",
    "    \"532\": \"Data Mining I\",\n",
    "    \"542\": \"Supervised Learning\",\n",
    "    \"543\": \"Unsupervised Learning\",\n",
    "    \"571\": \"Business SQL\",  # No syllabus for this one :(\n",
    "    \"593\": \"Milestone I\",\n",
    "    \"601\": \"Qualitative Inquiry for Data Scientists\",\n",
    "    \"602\": \"Math Methods II\",\n",
    "    \"611\": \"Database Architecture & Technology\",\n",
    "    \"622\": \"Information Visualization II\",\n",
    "    \"630\": \"Causal Inference\",\n",
    "    \"631\": \"Experiment Design and Analysis\",\n",
    "    \"632\": \"Data Mining II\",\n",
    "    \"642\": \"Deep Learning I\",\n",
    "    \"643\": \"Machine Learning Pipelines\",\n",
    "    \"644\": \"Reinforcement Learning Algorithms\",\n",
    "    \"652\": \"Network Analysis\",\n",
    "    \"655\": \"Applied Natural Language Processing\",\n",
    "    \"673\": \"Cloud Computing\",\n",
    "    \"680\": \"Learning Analytics and Educational Data Science\",\n",
    "    \"681\": \"Health Analytics\",\n",
    "    \"682\": \"Social Media Analytics\",\n",
    "    \"685\": \"Search and Recommender Systems\",\n",
    "    \"687\": \"Introduction to Sports Analytics\",\n",
    "    \"688\": \"Data Science for Social Good\",\n",
    "    \"696\": \"Milestone II\",\n",
    "    \"699\": \"Capstone\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad530acd-e98c-4c72-88c2-f4d70447f2e8",
   "metadata": {},
   "source": [
    "## Option to Build Semantic Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369975ed-3612-4b0b-bcb0-53b2536b3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build semantic routes for each class\n",
    "# Route utterances are very simple - name of class, number of class, who teaches (name/number of class)\n",
    "def build_semantic_router():\n",
    "    routes = []\n",
    "    for num, name in zip(courses.keys(), courses.values()):\n",
    "        route_name = \"SIADS \" + str(num)\n",
    "        route_utterances = [route_name.lower(), route_name.lower().replace(\" \",\"\"),\n",
    "                            name.lower(), name.lower() + \" class\", name.lower() + \" course\",\n",
    "                           \"who teaches \" + route_name.lower(), \"who teaches \" + name.lower()]\n",
    "        routes.append(Route(name=route_name, utterances=route_utterances))\n",
    "    \n",
    "    # Select local encoder and build route layer\n",
    "    # This is an encoder that happens to score well on the MTEB leaderboard (https://huggingface.co/spaces/mteb/leaderboard)\n",
    "    # Device here is set to MPS for Apple Silicon - change to CUDA for an NVIDIA GPU or CPU if GPU unavailable\n",
    "    encoder = HuggingFaceEncoder(str=embeddings_path + \"models--sentence-transformers--UAE-Large-V1/\", device=\"mps\")\n",
    "    rl = RouteLayer(encoder=encoder, routes=routes)  \n",
    "\n",
    "    # Use evaluation questions to test accuracy\n",
    "    # From: https://github.com/aurelio-labs/semantic-router/blob/main/docs/06-threshold-optimization.ipynb\n",
    "    \n",
    "    test_data = [\n",
    "        (\"Which class involves time series analysis?\", \"SIADS 632\"),\n",
    "        (\"Who teaches the SQL and Databases class?\", \"SIADS 511\"),\n",
    "        (\"What are the prerequisites for Data Science for Social Good?\", \"SIADS 688\"),\n",
    "        (\"When are the office hours for the Math Methods course?\", \"SIADS 502\"),\n",
    "        (\"Are there any weekly readings for Milestone II?\", \"SIADS 699\"),\n",
    "        (\"What are the outcomes of Qualitative Inquiry?\", \"SIADS 601\"),\n",
    "        (\"What textbook is required for SIADS 505?\", \"SIADS 505\"),\n",
    "        (\"What textbook is required for Data Manipulation?\", \"SIADS 505\"),\n",
    "        (\"Which week of unsupervised learning covers DBSCAN?\", \"SIADS 543\"),\n",
    "        (\"How many credits are required to complete the MADS program?\", None),\n",
    "        (\"How long do students have to complete the MADS program start to finish?\", None),\n",
    "        (\"How many points is the comprehensive oral exam worth in SIADS 593?\", \"SIADS 593\"),\n",
    "        (\"What is the penalty for late submission in SIADS 630?\", \"SIADS 630\"),\n",
    "        (\"How do I get accommodations for a class?\", None),\n",
    "        (\"What is a backpack?\", None),\n",
    "        (\"When is the latest I can drop a course?\", None),\n",
    "        (\"How do I get an override to take a class?\", None),\n",
    "        (\"How do I take a leave of absence from the MADS program?\", None),\n",
    "        (\"What are the prerequisites for Search and Recommender Systems?\", \"SIADS 685\")\n",
    "    ]\n",
    "\n",
    "    # unpack the test data\n",
    "    X, y = zip(*test_data)\n",
    "    \n",
    "    # evaluate using the default thresholds\n",
    "    accuracy = rl.evaluate(X=X, y=y)\n",
    "    print(f\"Original Accuracy: {accuracy*100:.2f}%\")\n",
    "       \n",
    "    # Call the fit method\n",
    "    rl.fit(X=X, y=y)\n",
    "\n",
    "    # evaluate using the new thresholds\n",
    "    accuracy = rl.evaluate(X=X, y=y)\n",
    "    print(f\"Revised Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    return rl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c38e1-7f82-47b9-a4c0-531f8cc8b76d",
   "metadata": {},
   "source": [
    "## Load Model - Two Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c787d69-574d-489a-9ae8-a8f0e065b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This option assumes that we are running an OpenAI-compatible\n",
    "# endpoint, in this case on the local host. We can do this with\n",
    "# llama-cpp-python. This has the advantage of simulating the use\n",
    "# of a remote LLM.\n",
    "llm = OpenAI(openai_api_base = \"http://localhost:7999/v1\",\n",
    "              model = \"mistral-7b-instruct-v0.2\",\n",
    "              openai_api_key = \"hello\",\n",
    "              temperature = 0.1,\n",
    "              top_p = 1,\n",
    "              max_tokens = 1024,\n",
    "              callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "             )\n",
    "\n",
    "# This option also uses llama-cpp-python but starts up the model\n",
    "# within the notebook. This does not require spinning up a separate\n",
    "# endpoint, although the notebook will necessarily use more memory.\n",
    "# llm = LlamaCpp(model_path=model_path,\n",
    "#                 n_ctx=32768,\n",
    "#                 n_gpu_layers=-1,\n",
    "#                 temperature=0.1,\n",
    "#                 top_p=1,\n",
    "#                 top_k=40,\n",
    "#                 repeat_penalty=1.1,\n",
    "#                 max_tokens=1024,\n",
    "#                 callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "#                 #stream=True,\n",
    "#                )\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages=True,\n",
    "                                  output_key = \"result\")\n",
    "\n",
    "\n",
    "# Set prompt template\n",
    "template = '''\n",
    "Use only the following pieces of context to answer the question at the end. \n",
    "Keep your answers concise and do not provide additional explanations or interpretations. \n",
    "If the answer cannot be deduced from the context, just say that you don't know the answer, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9125f-b926-4494-9536-fac0969b423e",
   "metadata": {},
   "source": [
    "## Create Question Answering Chain with Option to Include Semantic Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a99cd-1db9-4353-8efc-cf7e494fbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant documents based on query and create retriever\n",
    "# If relevant documents cannot be identified, build retriever on all documents\n",
    "# Then, integrate retriever into a chain\n",
    "def get_chain(query, use_router=True, router=None, k=5):\n",
    "    '''Creates question answering chain with retriever.\n",
    "       If use_router is True, the retriever is built based on \n",
    "       the routing choice of the router object.'''\n",
    "    print(f\"Query: {query}\")\n",
    "    if use_router and router != None:\n",
    "        r = router(query)    \n",
    "        if r.name:\n",
    "            doc_list = filter_pids(df_rag, r.name)\n",
    "            if len(doc_list) > 0:\n",
    "                retriever = RAG.as_langchain_retriever(doc_ids=doc_list, k=k) # Set k if desired\n",
    "            else:\n",
    "                retriever = RAG.as_langchain_retriever(k=k)\n",
    "        else:\n",
    "            retriever = RAG.as_langchain_retriever(k=k)\n",
    "    else:\n",
    "        retriever = RAG.as_langchain_retriever(k=k)\n",
    "        \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        memory=memory,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        verbose=False,\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": PromptTemplate(\n",
    "                template=template,\n",
    "                input_variables=[\"context\", \"question\"])},\n",
    "    )\n",
    "\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac1cb4-5bcf-4972-b289-2b89ebf84be7",
   "metadata": {},
   "source": [
    "## Function to Output Answers to Test Questions Without Using the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d066bf1-f51d-4b4a-b248-79f7daf4a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(outfile=\"sample_results.pkl\", use_router=True, k=5):\n",
    "    '''Produces pickle file (to outfile) with answers to test questions.'''\n",
    "    qlist = [\n",
    "        \"What is Elle O'Brien's role in the MADS program?\",\n",
    "        \"According to your information, who is Laura Stagnaro?\",\n",
    "        \"What can you tell me about Kevyn Collins-Thompson?\",\n",
    "        \"Which MADS class involves time series analysis?\", \n",
    "        \"Who teaches the SQL and Databases class?\", \n",
    "        \"What are the prerequisites for Data Science for Social Good?\", \n",
    "        \"What is a backpack for MADS students?\",\n",
    "        \"When is the latest I can drop a course?\",\n",
    "        \"How do I get an override to take a class?\", \n",
    "        \"How do I take a leave of absence from the MADS program?\",\n",
    "    ]\n",
    "    \n",
    "    alist = []\n",
    "    rl = build_semantic_router()\n",
    "    for q in qlist:\n",
    "        start = datetime.now()\n",
    "        if use_router:\n",
    "            qa_chain = get_chain(q, use_router=True, router=rl, k=k)\n",
    "        else:\n",
    "            qa_chain = get_chain(q, use_router=False, router=None, k=k)\n",
    "        llm_response = qa_chain(q)\n",
    "        end = datetime.now()\n",
    "        alist.append(llm_response)\n",
    "        #print(f\"Question: {q}\")\n",
    "        print(f\"Answer: {llm_response['result']}\")\n",
    "        print(f\"Processing Time: {end-start}\")\n",
    "    \n",
    "    with open(outfile, \"wb\") as f:\n",
    "        pickle.dump(alist, f)\n",
    "\n",
    "# get_samples(\"sample_results_norouter_5.pkl\", use_router=False, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2ebc2-5d08-42fd-9579-c1889379b2b1",
   "metadata": {},
   "source": [
    "## Function to convert JSON into HTML table as part of UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdae311-dc26-4993-a5cf-7c72b1fc9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = Json2Html()\n",
    "\n",
    "def create_source_html(response, n = 5):\n",
    "    '''Takes metadata from first n relevant documents\n",
    "       and converts to an HTML table for display.'''\n",
    "    source_doc_list = []\n",
    "    num_docs = len(response[\"source_documents\"])\n",
    "    len_res = n if n <= num_docs else num_docs\n",
    "    for doc in response[\"source_documents\"][:len_res]:\n",
    "        doc_json = {}\n",
    "        doc_json['metadata'] = doc.metadata\n",
    "        doc_json['page_content'] = doc.page_content\n",
    "        source_doc_list.append(doc_json)\n",
    "    return j.convert(source_doc_list)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d5f11-25d1-4858-99cd-4425f0443391",
   "metadata": {},
   "source": [
    "## Gradio-based UI\n",
    "Use username \"madsuser\" and password \"winter2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed8b6e-c338-4493-b387-e5c778a6751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Glass(primary_hue = \"blue\", secondary_hue = \"yellow\",\n",
    "                                     neutral_hue = \"indigo\", text_size=\"md\")) as demo:\n",
    "    gr.Markdown(\n",
    "        f\"\"\"\n",
    "\n",
    "        # MADS-RAG: A Helpful Chatbot for Master's of Applied Data Science\n",
    "        # Students at the University of Michigan\n",
    "\n",
    "        ## Current model running is {model}.\n",
    "\n",
    "        Source data includes class syllabi, the MADS Student Handbook,\n",
    "        MADS Advising FAQs, and transcripts of class videos. Some source\n",
    "        data may be out of date.\n",
    "\n",
    "        Please note that answers provided by this chatbot prototype may not\n",
    "        be correct and should be verified through other means.\n",
    "        \"\"\"\n",
    "    )\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    with gr.Accordion(\"Source Documents\", open=False):\n",
    "        html = gr.HTML(label=\"Source Documents\", show_label=True)\n",
    "    clear = gr.ClearButton(value=\"Clear\", components=[msg,chatbot,html])\n",
    "    \n",
    "    def user(user_message, chat_history):\n",
    "        qa_chain = get_chain(user_message)\n",
    "        response = qa_chain(user_message)\n",
    "        chat_history.append((user_message, response[\"result\"]))\n",
    "        res_html = create_source_html(response, n=3)\n",
    "        yield gr.update(value=\"\"), chat_history, res_html\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot, html], queue=False)\n",
    "\n",
    "demo.launch(debug=False, share=True, auth=(\"madsuser\", \"winter2024\"),\n",
    "            server_name=\"0.0.0.0\", server_port=5678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e0016-1016-4766-8bb9-e812717b07ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
