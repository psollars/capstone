So now we're actually going to
look at a real experiment where the instrument was designed
before we run the experiment, anticipating one-sided noncompliance. And we use this experiment as an example
to show the experiment design and the IV regression analysis. Okay, so
the second example is called recommending teams promotes pro-social lending
evidence from online microfinance. This is an experiment designed and
conducted by a group of researchers from the University of Michigan, the National
University of Singapore, and kiva.org. So you should have access to this
article in our homework assignment, the data is also coming from
this particular data set. So this is a problem faced by
microfinance institutions. 3 billion people in
the world subsist on $2.5 per day adjusting for
purchasing power parity. So in lots of developing
countries micro and small enterprises collectively
are the largest employers. They provide jobs, they provide a means of
living for many people, but their growth is often stifled by a lack of access
to credit or other financial services. So to resolve this problem, microfinance programs provide small
loans and other financial services. The Internet, in particular
online peer-to-peer microlending provides another method to
pull the resources from ordinary people to help
resolve the poverty problem. So the first online peer-to-peer
microfinance lending programs started as kiva.org
by two Stanford graduates. So the idea of Kiva is that
anybody can join Kiva online and lend as little as $25 to create
opportunity for people around the world. An important part about this lending
practices unlike the Prosper or Lending Club is that this loan
earn zero interest for the lender. So over the years, about 1.3 billion
loans were funded through Kiva and the loan help 3.2 million borrowers primarily in developing countries,
in more than 80 countries. And the loans were provided by 1.8 million
lenders across other 200 countries. So this is a way to leverage
the willingness to help of ordinary people to help the small entrepreneurs
in developing countries. So this is the landing page of Kiva,
what you see here, at any given time there are about
2,000 active requests from borrowers. Kiva a faces a challenge, however. So what you see on the graph
is the number of loans and the number of lenders on horizontal
axis making this many loans. So what you can see from this
graph is that very few lenders made many loans and
many lenders made very few loans. Actually one-third of the users
on Kiva had never made a loan. So the Kiva's problem at the time
was how do you get people engaged? How do we increase lenders participation? When Kiva first started it
caught a lot of press, so people came make a loan then
they forget about Kiva. So their solution which is a homegrown
solution is team competition. So Kiva in August 2008, started this organization called,
Lending Teams. So anybody can set up a team and
anyone can join any open team. And lots of these teams, so this is the leaderboard around
the time of the experiment, what you can see is, the top teams
tend to a group identity based team. So let's zoom in, the top team had
always been the Kiva Atheists and the second largest team
is the Kiva Christians. So these are teams based on
people's religious identity and they have a fair amount of
competition among the teams. So a natural question to ask
it's also very simple question, which is,
does joining a team increase lending? What you see here is the raw data between 2006 to 2012, so
that's six years worth of data. On the vertical axis is the number
of loans per person per month. So on the horizontal axis,
you have the month. So the black dots are the people,
the average number of loans of those who never
joined a lending team. Whereas the light triangles on
top are those who joined a team. So if you look at the end on the right
hand side of the graph, you would say, well, it looks like people who belong
to a team, the little triangles, are more active than those who
don't belong to any teams. So there seemed to be a gap
between the triangle and the dots. The only trouble is if
you have this logic is to look at the vertical
line starting August 2008. So this vertical line is
when team started, but if you look towards the left
of the vertical line, you will see that the triangles
lie above the dots, the black dots even before
teams even existed. So this gives us some clue about
selection potentially selection bias, which is joining teams is
an endogenous decision, probably. Those who are more active before teams even existed are more
likely to be in teams. So this is very much like
the participation decision in the job training program that we talked about. So our hypothesis is, lenders will be more likely to join
is we make good recommendations. At the time of the experiment, 82% of
the Kiva users do not belong to any team. So let's go back to this diagram, if you do simple empirical analysis with
OLS, you will see that the team dummy, the fact that someone belongs
to a team is hugely important. So the question is, how important is it? We know that because joining
a team is endogenous, just simply running OLS probably give
you a biased result, biased estimate. So this experiment is about
generate an instrument and estimate the the effect of joining teams. How do we do that? So we decided to use recommender systems. So we look at recommendations based on
three algorithms, location similarity, or long history similarity,
or leaderboard positions. So location similarity is when people live
A lender live close to a team of lenders. Loan history similarity basically means
that they've made similar loans in the past. Leaderboard position captures status,
which is, the higher ranked teams have higher
status in the in the Kiva community. And so
we decided to design a field experiment. So Kiva imposed a set of
selection criteria, which is, the people should not have joined any
team, that they have the location information in their profile, so
we can use it in our computation. And the last two is that
they allow marketing emails, they set their page as public and
they have to have made at least two loans in the six months
before the intervention. So this actually means that our sample is more active than
the population in general. So we ended up with 69,845 users. So the experiment design is
a three by two factorial design. So on one dimension, one factor varies
the algorithm, how do we recommend teams? We recommend teams based
on location similarity or loan history similarity or
leaderboard position. The other dimension is whether
we actually explain how we come up with the recommendations. So one factor is with explanation,
the other value is no explanation when you control for
the recommendation algorithm. What I want to point out
is the control conditions. So we have two control conditions,
one is the no-contact condition, so this is a random sample that we don't
even contact or don't even touch, okay. There's another condition,
which is team exists condition. So this is a subset of our sample
who receive an email without recommendation, so
this is like our placebo effect. So this is essentially
a method to anticipate that we will have one-sided non-compliance. In other words, some people we intend
to treat might not open their email. So what does the placebo look like? So this is the team exists email. So every email contains
these four paragraphs. So the first two paragraphs is always,
since you're such an awesome Kiva lender, we wanted to let you know about a fun
feature of the Kiva experience, Kiva Lending Team. So if you click on the hyperlink,
it will take you to the team's page. And then the second paragraph
explains what teams are. The last two paragraphs are check out some
of the thousands of lending teams to find the right one for you. So again, if you click,
it will take you to the team's page, and the last paragraph says thank you. And so what does a treatment group
look like, a treatment email? So you will see that in this particular
case the first two paragraphs are exactly the same as the first
two paragraphs in the placebo email. And now in the middle,
what's cut off is the last two paragraphs, which are also the same as
those in the placebo email, but the middle part is the treatment,
that's the ingredient. So we say based on your
past lending people, who have made similar loans enjoy
being a part of these teams. So these are the three
top teams based on the on the algorithm for
lending history similarity. What about location similarity? If you are in one of the locations
similarity condition, we say, other lenders who live near you
enjoy being a part of these teams. What about lending history similarities? So this is what's in this email exactly. What if it's by popularity, social status? We say some of the most popular teams are,
and we list the top three teams. So what we see here is a summary of
the eight experimental conditions, so the six treatments and
two control conditions, in other words, the no-contact and
the placebo conditions. So the intervention is sent out by emails,
so Kiva help desk send out the emails,
we'll look at how many emails were sent in each of
the treatments and placebo conditions. How many emails were actually opened,
how many people joined teams, and the last column is that they
actually joined a recommended team. So if you look at the first column,
each experimental condition with the exception of the no-contact
condition has about 8,000 lenders. So we sent out 8,000 emails in each
of these experimental conditions. This is called our intent to treat,
in other words, we want to treat them. And the second column is the number of
people who actually opened their emails. So about a third of the people
opened their emails, most people actually did not, so this is, again, one-sided non-compliance. So the people who opened their emails,
we assume that they actually read it. These are the treated group. So when you conduct a field experiment,
you always want to differentiate between your intent to treat and
the treated group. Among the treated group, the fourth column
is the number of people who joined teams. And the last column is the number of
people who joined recommended teams. In the first two rows of the last
column it says n/a, not apply, because we don't email those
in the no-contact condition. Even though we emailed those in
the team exists condition, the placebo, they don't receive recommendations. So let's take a look at
a graphical illustration of this participation decision. So the left panel is for all lenders, so
that's our intent to treat, and the right part is lenders who actually opened their
emails, so this is the treated group. So the red bar is when they
join the recommended teams and the green bar is they join other teams. Again, for the team exist condition,
this is our placebo. We did not recommend any team, so
everybody who joined teams in that condition joined other teams by
definition, so you only see a green bar. So that's the first result,
and the statistical analysis basically confirms our impression
from just eyeballing the data. So the treatment effect on joining
teams is that every treatment, except team exists, did significantly
better than the control condition. Location with explanation, let's go back, which is on the right hand side,
the second bar, the bar that goes out the most,
has the largest effect. Among those who opened teams
there are two treatments that did significantly
better than the placebo. Remember, we want to actually compare
the treatments with the placebo and not with the control. The location with explanation is the best. History with explanation is also
significant when you run the regression, but if you correct for
multiple hypothesis testing, only one effect is significant that
survives the multiple testing correction, which is location with explanation. In other words, when you recommend teams
from similar geographic locations, people are more likely to join that team. And you tell people that's why
you recommend those teams. So this is our two stage least square
regression, IV regression, okay? So this is the main
application of our IV approach. In fact, in this case we used
both dif-in-dif and IV, okay. So the main question is,
do people who join teams lend more? So we wanted to estimate the effect
of team membership on lending amount. So the dif-in-dif is created
by looking at the window after intervention versus a comparable
window before the intervention. So we did three different specifications. It's the 1-day window,
which is column 2, the 7-day window, column 3, and the 30-day window, column 4. So in the first stage, the first column,
which says IV first stage, we use our instrument, which is what
we constructed in the experiment. It's exogenous, so it should not be
correlated with the error term, so that's email, okay, whether you
received an email from us or not. We look at this, and
we see that email is a significant predictor on the likelihood
of someone joining a team. So the three stars means it's
significant at the 1% level. In other words, when you receive an email, that increases your likelihood of joining
a team by half a percentage point. So that's the first stage. And we also, so remember,
we want to test for inclusion restriction, which is the instrument
that you created actually correlates with the endogenous variable,
joining teams. So this is called
the inclusion restriction. How do you do that,
you use the F-statistics. It turns out in this case it's highly
significant, the F-statistics is 23.55, which means that it's a good instrument,
it's a strong instrument. In the second stage, we look at the effect
of joining teams on lending amounts, actually on lending difference. So we could look at the 1-day effect,
the 7-day effect and the 30-day effect. So essentially what you interpret is, in column 2, the effect of joining teams on those who joined is about $300, $298.56. So that's the effect,
increases lending by about $300. What about column 3? In column 3,
you will see that joining a team has an effect of about 55.9 or
56 per day for 7 days. If you multiply it by 7, it's about $400. So the effect for a week,
the 1-week effect, is an increase of $400. It turns out that over 30 days,
if you look at the 30-day window, it's no longer significant, okay. So now we'll look at the second stage. We know that the instrument has to
satisfy the exclusion restriction, which is that the email, the instrument
by itself, does not increase lending. How do we know that? It turns out that we have
a companion experiment, where we go to the forum,
the Kiva Team Forum, and send out messages,
which is summarized by Kiva as an email. It turns out that email by itself
does not increase lending, but for the second stage you just have to argue or
find supporting evidence to show that your instrument satisfies
the exclusion restriction. So this is a visualization of
the effective team membership on lending amount. So the 1-day window's about $300,
that's the first red bar, and the 7-day window is about $400. What you see on the right-hand
side is a teeny little green bar, and what is that, that's $25. That's the median lender's
lifetime contribution. So the effect or at least the short-term effect of
team joining is quite significant. Now it's large,
economically large as well. So now let's go back to the nuts and
bolts about IV. So what we haven't talked about
is that you can use multiple instrumental variables at the same time. So what we show before,
in our two examples, each has only one. The second part about IV is that
IV actually does not give you the average treatment effect. The IV method identified
the average gains to the persons induced to change their behavior
by change of the instrument. So this is only for
the subset of compliers, okay. So in other words,
the effect estimated is called the local average treatment effect, and
is often shortened as LATE. And you have to be careful not to
extrapolate it to the whole population. The third part, the third point is, you
must account for the estimation error in the first stage when computing
the standard errors in the second stage. So in this particular example,
in the second stage, we're actually regressing the outcome, which is the difference in
lending amounts on p hat, where p hat is what we
obtained from the first stage. These numbers are estimated from
a statistical program called Stata, which automatically
combined the two stages and gave you the right standard error. And if you're doing this on
another statistical package, you have to make sure, and
you can check manually, that the standard errors are estimated
correctly in the second stage. How good are IV regressions? The bottom line is, IV is only as good
as the instruments on which it is based. So you have to check the two criteria, one is the inclusion restriction,
about the first stage. So weak instrument just
don't work as well. So this is a criterion that you can
check objectively by using the F-test. So the second criteria is
exclusion restriction, which says that the instrument only affect the outcome variable through
its effect on participation. It's difficult to test this empirically,
you have to argue or cite other empirical evidence. The exception is a case when you have
more than one plausible instrument, you can see if they both
give you the same answers. And you can do that by using
the Sargan test or the Wu-Hausman test. This is very rare in
the sense that it's not so easy to find one good instrument,
let alone multiple good instruments. So now let's summarize the application. So basically what we show in using
this field experiment is that team recommendation emails significantly
increase the likelihood that a lender joins a team,
compared to the control condition. And location with explanation
has the largest effect. For those who joined teams, the average
lending amounts increased by about $400 in the 7-day window, so
it's a fairly large effect. And this is a study that
support group membership as an effective mechanism for
promoting pro-social behavior. The method used,
there are two lessons, one is, the researchers anticipated that there
will be one-sided non-compliance, that some people will simply
not open their emails. How do we correct for that? We constructed a placebo condition,
where lenders receive emails, but the email does not have
recommendations or the treatments. The second part on
the methodology front is to build an instrument ex ante into
the experiment design, and the instrument is
the recommendation email. So this is a lot easier than looking for
instrument after you conduct the study. So for for experiments one way to
think about it is encouragement or randoms receiving of emails,
information interventions typically are good candidates for
a good instrument. So there's a lot more about instrumental
variables, I encourage you to read a lot, because it is a very powerful method for
causal inference, and to practice a lot by designing experiments
and check if they're good instruments. Okay, so that concludes our
fourth lecture, thank you.