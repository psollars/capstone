Now we're going to go through the in
the field constraints and the type of randomization you can implement in
the field, and in the lab as well. So you will have many
opportunities to randomize and you get to choose
the level of randomization. It turns out that different
levels of randomization implies different statistical power and
analysis method. And you have to choose which aspect
of the program to randomize. So let's first talk about
the opportunities to randomize. What can be randomized? There are lots of different
things that can be randomized, depending on what you want to learn. There are three basic elements of
a program that can be randomized. The first one is access. So experimenters can choose which
people are offered access to a program. The second aspect is timing. We can choose when people
are offered access. And so
we'll use the Uber tipping experiment, which is a real large-scale experiment
that they ran in the field before they introduced the tipping feature. And we'll also talk about what's
called an encouragement design, which is experimenters can choose which
people are given encouragement to participate in a program. And well, in this case we'll talk
about a postcard intervention by the University of
Michigan Undergraduate Admissions Office. Let's take a look at some of
the examples of what can be randomized. Let's see that an NGO wants
to tackle obesity, but they're not sure what
the program should look like. They can randomize on different
aspects of the program design. They can offer multiple programs and
randomize people into different programs. Sometimes you have new services coming up. Let's say an insurance company wants to
introduce a new insurance product for farmers and you want to evaluate how the new insurance
product compared to the old ones. You can then randomize a subsample of
the farmers to be exposed to the new insurance programs. Sometimes you get to have new people. So for example, Oregon has money to add
more people to its Medicaid program and that would enable them to study the effect
of having Medicaid health outcomes. And you can infer causally what's
the effect of access to Medicaid program if you design
a randomized field experiment. Sometimes you get to have new locations. So for instance a microcredit company
might want to expand to a new location. There that offers opportunities
to randomize as well. Another opportunity to randomize is
when there are over subscription. So let's say more families sign up for education vouchers than
the government can fund. In that case you can randomly
select a subset of the families who signed up to receive
the education vouchers. And the other subsets, who are randomized
out, would serve as your control. Very exciting. Sometimes there are under subscription. So for instance, lottery for conscription during the Vietnam War can
be analyzed as a natural experiment. You might have communities taking
turns hosting an event, and if the order is randomized that enables
you to evaluate the effect of the program. In some cases when there
are admission cut-offs that offers an opportunity to randomize. For instance,
scholarships has merit cutoff and the ability to randomize admissions
just below and just above the cutoff. So instead of having a one hard
cutoff you can have a range, and that enables you to look at the effect
of receiving merit Scholarships. You can have admissions in phases, and that phased in design also offers you
opportunities to evaluate causal effects. Now we'll be talking about choosing
the level of randomization. So the level of randomization
can be at the individual level. Or sometimes when you have,
let's say, school lunch, suppose your budget is not
sufficient to cover all students. You can randomly select, let's say,
half of the students to receive free school lunch, and that's randomization
at the individual level. Or you can have a cluster. So for instance,
instead of at the individual level, you randomize at the class level. Some of the classes could be randomized
into receiving an extra teacher in the classroom, and some are randomized
without the extra teacher. That enables you to evaluate the effect
of having an additional teacher in the classroom. Sometimes you might want to
randomize at the school level. Why would you want to do that? It turns out that there might be
spillovers when you randomize at the individual level. Let's go back to the school lunch program. Let's say you randomize half of
the students into receiving free school lunch and the other half are randomized
into the control condition. During lunch time you cannot prevent
kids from sharing their free lunch with their friends, which means that
your data is actually contaminated. Some of your children in the control
condition also receive treatments because their friends share
school lunch with them. And sometimes for political or
logistic reasons, you have to randomize at a higher level,
for instance, at the school level. Here are some considerations for
the level of randomization. You might be worried about
the unit of measurements. Usually larger clusters will
reduce your statistical power. You might be worried about feasibility and
fairness. So for instance, if one class receives
additional teachers, another one doesn't, parents my feel that that's unfair, that
their children might be disadvantaged. In that case you might want to
randomize at the school level. Sometimes you might
worry about spillovers, just as we talked about in
the school lunch example. Treated children might want to share
their school lunch with the control group children. In that case you don't have a clear
separation of the treated and the control group. You might have attrition or
compliance issues, and as we will develop later that there
are implications for statistical power. When you randomize at a lower level,
such as an individual level, you typically have higher statistical power compared to,
let's say, at the school level. And sometimes you have to
cluster on the ground. And the next aspect that we're going
to talk about is to decide which aspect of the program to randomize. We will talk about five very practical
research designs of randomization. So we mentioned three different aspects,
access, timing, and encouragement. Access, which is who are offered
access to a program. We'll talk about two different designs. One is called the treatment lottery. The second one is called
the treatment lottery around cutoff. We'll then talk about two designs for
timing. One is called phase in,
The other one is called rotation. And the last one, the fifth research design that we'll
mention, is the encouragement design, which is which people are given
encouragement to participate. So the first research design we'll
talk about is the treatment lottery. So when do you want to
run a treatment lottery? That's when there are limited access and
over-subscription. And you also want to measure
the effect in the long run. So for instance, Kenya wants to assign
extra teachers to some of the schools. There are only 120 extra teachers,
but 210 schools. And so, you want to,
in this case, run a lottery. And the blue dots are the schools
which are randomized into receiving the treatment,
which is the extra teachers. Whereas the comparison group or
the control group, sometimes we use this interchangeably, are randomized into having the same
number of teachers as before. The second type of research design is
called treatment lottery around cutoff. So for instance, for merit scholarships,
you might have a cutoff score, and everyone above the score
is accepted into the program, whereas everyone below is rejected. Sometimes you also have centralized
college admissions using standardized tests, which also has similar features. So the question is if you want to
evaluate what is the effect of the merit scholarship on student outcomes, you won't be able to answer that
using naturally occurring data. Because people who
are accepted have systematic different characteristics
than people who are rejected. And these are confounders. However, as a researcher,
if this is feasible, you can instead generate a lottery zone,
okay? So among the people who are rejected,
for instance, based on the score,
you can set a lottery zone. So everybody inside the lottery
zone can be randomized into being accepted or being rejected. That means you can have a larger
lottery zone or a smaller lottery zone. And you can have a lottery zone
among the accepted students as well. This will enable you to study
the effect of the merit scholarship on student outcomes by comparing
those in the lottery zone. These are people who are roughly
similar in terms of their scores, but some of them are randomly assigned
to receiving the scholarship, whereas others are randomized into
not receiving the scholarship. So the comparison of this group
will enable you to say something about the effects of the merit
scholarship on student outcomes. The third research design is
called a phase-in design. When can you run this
type of research design? That's when everyone is given
access eventually, but not at once. So the average program impact can be
evaluated during different periods. However, if everyone knows
that the treatment is coming, the anticipation might
dilute your effects. So the time to effect should be
shorter than the treatment period. So here's an example of a phase-in design. So in year one,
randomly selected block A is treated. So you can compare the outcomes
of those in BC with A. That will give you the one year effect. In the second year,
then you phase block B into the treatment. Then, comparing C with A will
give you the effect after the program has been run for two years. Where the comparison between C and
B will give you the effect of one year. And then, in year three,
everyone receives treatment. So that type of phase-in design
enables you to study what's the effect of receiving treatment for
one year? What's the effect of receiving
treatments for two years, and so on. And it's still fair, broadly speaking because eventually
everybody receives treatment. There's a recent implementation
of the phase-in design at the ride-sharing platform Uber. So the subsequent content is
from a working paper called Evaluating Market Outcomes in a Nationwide
Experiment on Tipping, Evidence from Uber. The slides is from Bharat Chandar. So some of you might recall that when
ride-sharing first became implemented, the two dominant ride-sharing platforms,
Uber and Lyft, have somewhat different features. On Lyft, if you download the Lyft app,
it allows the rider to tip the driver. Whereas, initially, on the Uber app, you don't even have
the feature to tip the driver. In summer 2017, Uber decided to enable the riders to tip the drivers. However, they decided instead of
just adding it to every driver and every rider's app,
they decided to use a phase-in design. And so, they were interested in
whether tipping affects labor supply, and whether it affects demand. Does it affect driver earnings and
the quality of service? So this experiment was fairly large scale. It was implemented across 209 operating cities in the US and Canada in July 2017. But they rolled out this
feature in two phases. On July 6, about half of the cities
on Uber were given the option to tip, and the feature was fully
rolled out on July 17th. In other words, the half of the cities
that were given access later can serve as the control group for
the cities that were given access earlier. And the key feature here is
the cities were randomized into receiving the tipping
feature earlier verses later. And they used what's called
a randomized block design. So cities were matched into pairs,
and in each pair one of them were randomized into early,
the other one was randomized into later. So altogether, there are 815,000
drivers across 209 cities. Among those cities, 110 cities were randomized into
receiving the treatments earlier. We'll come back to this experiment
when we talk about block design and cluster randomization because
this is an interesting field experiment that uses both features. The remaining cities were randomly
assigned to treatment or control. So there are altogether 105 control
cities and 104 treated cities. When you finish your randomization, what you should do is conduct
a randomization check, or balanceness check,
which says the idea of randomization is to make sure that you generate two samples,
a treatment sample and a control sample,
which are statistically indistinguishable. And can you check that? Yes, you can actually check it. So in this Uber experiment the researchers
checked whether the randomization worked. So did randomization work? So they presented a table, essentially
describing the degree of similarity between the treatment and the control
groups in terms of the covariance. You can list the means,
or other moments, for each covariate, and look at whether
the covariates are balanced or not. And if they're not, that can also
help foreshadow whether the covariate adjustment is likely to change
the estimated average treatment effects. So we can present it in
many different ways. This is looking at the time series of mean
minutes worked among the control cities, so the control cities are in red and
the treatment cities are in blue. The data is from June to July,
and you can observe that they pretty much follow each
other day by day there. From eyeballing they look pretty similar,
but you should really look at whether
they are similar statistically. So here are two tables which check for
randomization, both at the individual level and
at the city level. So at the individual level,
which is the upper table, the researchers present t-test for
equality of means across individuals in the period before
the tipping feature is launched. So we can look at whether
they are significant or not. So for instance,
in terms of the number of minutes worked the average of
control group drivers worked 653.5 minutes, whereas the average treated drivers worked on average 682.6 minutes. And if you look at the T statistics,
they're not significantly different. So the p-value is 0.66, and
you can check other features. The bottom line is if you
look at the p-value for equalities of means test,
none of the features is significant, which says that the randomization
worked at the individual level. The bottom table looks at whether
randomization worked at the city-level, whether the cities are balanced. You can also look at equalities of means
tests by looking at t-test statistics. And in this case two of the measures,
minutes worked and probability of working,
are not significantly different between the control cities
versus the treated cities. But the minute worked conditional
on working is different, the average treated city
actually has higher number of minutes worked conditional on working. Even though they were
careful in the randomization, I suppose some of the features
might still be different. And so when you run your
experiments after randomization, that's the first thing you should do and
that's what you should report, which is to look at a set of balance
tests or randomization check. So you might be wondering, what is the effect of this large-scale
field experiment using a facing design? It turns out that none of the metrics
they used to measure demand, supply, hourly earnings, is significant. So it can happen,
it has a large experiment, but they also used clustered random
assignment, which is a unit of randomization is at the city level
rather than the individual level. What we might want to think about,
why do they randomize at the city level? One speculation is that they might
be worried about spillover effects. So suppose I'm an Uber driver, and
my neighbor's also Uber driver. I'm treated, so I look at my app and
say, I can receive tips. When I talk to my neighbor,
my neighbors say, is that right? Mine doesn't have this feature. So you might have spillover effects,
and other issues from the spillover. So it is a cleaner experiment
design to randomize at city level, however, it does reduce the statistical
power of the experiment. So as a result, the confidence intervals
in their estimates are very large, which leads to sort of none
of the metrics actually are different across the treatment and
the control group. Now we're going to discuss a fourth
design option, which is rotation. So when would you use a rotation design? This is when you have limited
resources that everyone needs and you're not expected to have
an increase in these resources, and the effects only happens
during the time of the experiment. So let's take a look and
see how the rotation design works. So this example is from a remedial
education program in India, so we have a limited number of tutors,
so in year 1, grade 3. So you have two schools,
school A and School B, in year 1 among third grade
students school A receives tutor, where school B does not receive tutors. In this case, you can look at
the difference between school A and school B to look at the effects of
receiving extra tutors on student outcomes among third graders. And with the fourth graders we can rotate, we can have treat school B and
not school A. In year 2 we can now, for
instance in school A we can treat grade 4 and
withdraw the tutors from grade 3, whereas in school B now in 2,
grade 3 students get treated and the fourth graders do
not get extra tutors. So you will have multiple control groups
within the same grade across schools, and within schools across grades
to look at the effect of tutoring. And if you compare school
A second year outcome with school B second year outcome
among fourth graders, you can look at the impact of tutoring for
two years. And the last one that we're going to
discuss is called the encouragement design, this is when usually
the program is open to everyone but their under-subscribed. So in that case, you can encourage
some students, or some participants, for instance the retirement savings
program at American University, but not everyone takes advantage of that. In that case, you can offer subsidies to
people who attend an information fair, and that serves as an encouragement for
participation. And the subsidies can be offered
to a random subset of people, that gives you a way to analyze
the causal effect of information, intervention, and
subsequent retirement savings outcome. I'm going to use
an experiment conducted by Professor Sue Dynarski at
the University of Michigan. So Professor Dynarski worked with
the Undergraduate Admissions Office to essentially use targeted encouragement
to, she combines encouragement to apply to the University of Michigan
with the promise of free tuition. The experimental design is implemented
through sending postcards to some high income students. So the empirical observation is
that high-achieving low income students typically do not even apply for
select colleges lots of times, because they're not aware of
the generous financial aid packages. So the researchers target these students
by sending postcards to these students, and they encourage students to apply
with the promise of free tuition. What they find is that students
who receive postcards, those who receive postcards
are under the encouragement, they're more than twice as likely to
apply to the University of Michigan, and once accepted they're more
than twice as likely to enroll. So if you look at those students who
enroll, they would have attended a less selective college, a community
college, or not attending college at all. How do we know that? We look at those who
are in the control group, because there are a group of
students in the control group who are statistically equivalent in
every dimension that we can observe. Let me summarize what
we have just covered. So there are many different
ways to randomize, as a researcher you get to choose what
aspect of a program to randomize, when to randomize, and
how to do the randomization. There are two different
ways of randomization, one is called Simple, the other one
is called complete randomization. And we went through five common
designs of randomization.