Hi everyone. So this week, we're going to talk about
different choices of randomization. We will also discuss a very well-known
game called the trust game. And in general, we're going to
talk about how one might use these lab experiments to measure
certain traits in people. And whether they have
predictive power in the field. So this is the outline
of this week's lecture. So we will first define some notations and framework to talk about causal
inference and experimentation. We'll talk about different
methods of randomization and we'll go to the very concrete topic
of measuring trust in the lab. And use an example of lab in the field. We're going to start with causal
inference and experimentation. So this is a little bit dense, but
we'll introduce some notation first. So these notations will be consistently
use throughout the rest of the lectures. When you have experimental subjects,
we're are going to index a subject with i. And the subscript I refers
to subjects 1 through N so the treatment variable is going to be di. So the variable di means,
whether the ith subject is treated. In a binary treatment situation, so you
have a treatment and a control condition, di equals 1 means that the ith
subject receives the treatment. Whereas di equals 0, means that the ith
subject does not receive treatment. It is assumed that di is observed for
every subject. In practice, you have to define
what it means to be treated and measure whether each subject is treated. From now on, I we'll discuss this
potential outcomes framework and why randomization is important. So regardless of which treatment a subject
receives, the subject is set to have a potential response in the event that
the treatment is or is not received. So potential outcomes
may be written as Yi(d), where the argument d
indexes the treatment. So these potential outcomes are fixed
attributes of each subject. And represent the outcome that
would be observed hypothetically, if that subject were treated or untreated. In the binary treatment world, Y1(1) means the potential outcome
if the ith subject were treated. Whereas Yi(0) measures the potential
outcome if the subject is not treated. So here's an example from the textbook. Let's say we have seven villages and this is based on a real
policy implemented in India. And the policy is to assign one-third of the villages randomly
into a treatment where a woman is appointed as
the village council head. And so now, we have seven villages and the
outcome, what we're interested in measure, is the share of local budgets
allocated to water sanitation. So the first column is Yi(0), which is the
budget share if the village head is male. Whereas, the second column, Yi(1), is the budget share if
the village head is female. So the treatment effect, remember here,
treatment means, you are randomized into the experimental condition where
the village head has to be a female. So the treatment effect is going
to be the difference between the treated units and the untreated units. So in this case, is the potential
outcome between Yi(1) and Yi(0). So in this hypothetical scenario,
the last column, Tau of i, gives us the treatment effect for
each of the seven villages. We can also, at the bottom line,
take the average. So let's define the subject
level treatment effect. So in this case, the subject-level is actually
the village level of treatment effect. So it is defined using the notation Tau i. So Tau i for a given subject,
in this case, a given village, is defined as Tau i
equals Yi(1) minus Yi(0). Okay, so the average treatment
effect is the difference between the average value of
all the Yi(1)s for subjects and all the values of Yi(0)s for
all the subjects. In most of the applications, the average
treatment effect is the parameter that a researcher wants to
estimate through experimentation. So the average treatment effect is simply, let me explain the notation
mu Yi(1) minus mu Yi(0) and each of the mus is simply the average
of the potential outcomes. So you sum up all the subjects potential
outcome when they're treated from 1 to N. And take the average divided by N and do
the same thing for the control condition. In the end, it's just going to be the average of
the subject-level treatment effect. So it's 1 over N times the sum of Tau is. At this point, the definition of
the average treatment effect or subject-level treatment effects makes
no reference to random assignment. We, at this point, assume that we can observe
the potential outcome for each subject. Now, we're going back to our village
council experiment and we look at the treatment effect at the subject-level
or at this case, the little village level. And then we're simply going
to take the expected value of a randomly selected value of Tau i. So we'll looking at all potential,
all the year treatment effects, there are only several levels,
they're minus 5. One out of seven villages has
minus 5 as the treatment effect. Two with zeros, which means that
there's no effect at the village level. One has a treatment effect of 5,
two has a treatment effect of 10 and one has a treatment effect of 15,
so the average of that is 5. So there is a fundamental problem
of causal inference, which is, we typically observe only one
potential outcome and not both. So in the Indian village example, a village is either randomized
to receive the treatment or not. And those who receive the treatment
will have a female village head. And in that case, we don't observe
what the outcome would have been had there been a male
village council head. So this problem is sometimes, more
fundamental in some problems than others. So for instance, if we want to assign
subjects to the question of, on a scale of 1 to 10, where 10 is the highest, tell
me how much you like vanilla ice cream. Other subjects could be assigned the same
question, but substituting chocolate. In that design, we observe either Yi of
vanilla or Yi of chocolate, but not both. So the question is,
you could ask subjects both questions, but in that case,
we might be concerned about order effects. So the observed outcome, the corresponding
concept is the observed outcome. What's the connection between the observed
outcome and the potential outcome world. And is given by this equation, Yi equals di times Yi(1)
plus 1 minus di times Yi(0). So in real experiments or
in real natural experiments, you are often either treated or
not treated. For a given subject,
we typically do not observe both. So for instance,
if the causal inference problem that you're trying to address is
that smoking cause lung cancer. In this case, the treatment is smoking. For ethical reasons, we actually cannot randomly assign
people to smoking versus non-smoking. But let's carry out with this
hypothetical experiment, suppose you can. Then the treated group
would be the smokers, whereas the control group
are the non-smokers. But typically, you cannot both
be a smoker and a non-smoker. So this table actually gives us what
we typically observe, which is for village 1, village 1 is
randomized into the treatment. So you only observe the budget
share allocated to water sanitation when the village
has a female village head. Whereas village 2 is in
the control condition. So you only observe Yi(0)s and
that holds true for every village. In this case,
you can still take the average of Yi(0), which is 16 and
the average of Yi(1), which is 22.5. And that gives you the average of
the treatment effect, which is 6.5. As you can see from the previous table
that we have this missing data problem, which is at each experimental unit,
in this case, is the village. You observe either the treated outcome or
the control outcome. So you have this missing data problem and the solution to this problem
is random assignment. So we would like to have the treatment and control groups to have
the same expected outcomes. In the actual experiment that was run
in India, the one-third of the local villages are randomly assigned to have
women to head the local village councils. So the question is whether the random
assignment solves the missing data problem. What we're now going through is
the expectation of the sample mean under random sampling. So under random sampling, the expectation
of the sample average can be shown to be equal to the population average. So let's say that out of N observations or
N cases, you're going to take a random
sample of m subjects. And so how many different
ways are there to assign it, this is a standard combinatorial problem. So for each of the m subjects, we record
the value of Yi, the observed outcome. So for the sample,
we can take the expected outcome, which is the average of the expected
outcome for each of these units. And in that case, we'll get mu of Y,
which is the expected value of Yi. So this feature of random sampling is one
of the basic principles of experimental research. Prior to the intervention,
there's no reason to expect that the treatment mean is higher or
lower than the control group mean. And this, of course, can also be checked
after you conduct the random assignment. Now, let's formally
define random assignment. Random assignment refers to a procedure
that allocates treatments with known probabilities that are greater
than zero and less than one. The most basic form of random assignment
allocate treatments such that every subject has the same
probability of being treated. So let's say you have N subjects and m is the number of subjects who
are assigned to the treatment group. Assume that both N and m are integers and m lies strictly in between 0 and N. Now, we can talk about two different
forms of random assignment, one is called simple random assignment. That refers to a procedure whereby
each subject is allocated to the treatment group with
probability of m over N. Whereas complete random assignment refers to a procedure that allocates
exactly m unis to treatment. The number of possible allocations
of subjects to treatment or control conditions under
complete random assignment is N factorial over m factorial
times N minus m factorial. So each time we're going
to talk about estimation techniques with experimental data. So the first estimator that we're
going to talk about is called the difference-in-means estimator or
sometimes called D-I-M or DIM under complete random assignment. Suppose di were randomly assigned such
that m subjects were selected for treatment and the remaining N
minus m to the control condition. So this is equivalent to sorting
the observations randomly and call the first m subject
the treatment group. We want to show that
the expected difference-in-means estimator is equal to
the average treatment effect. So this is a proof that the expected DIM,
difference-in-means estimator, is equal to the average treatment effect. So if you're interested,
you can walk through each line. We'll basically take expectations
of the treatment mean minus the expectations of the control mean. And we show that this can be
manipulated such that this is exactly the average treatment effect. Under random assignment, the
difference-in-means estimator is unbiased. And DIM is the best you can do if you only
observe outcomes after the treatment. So sometimes you want to implement
a treatment, but for some reason, you cannot have access to data
before your intervention. And what you observe is
after random assignment and after you implement your treatments, you get to see the difference between
the treatment and the control group. And so DIM,
the difference-in-means estimator, is how you estimate
the average treatment effects. Sometimes you have better observations. So for instance you have access
to how everybody's doing before the intervention and
also after the intervention. And so in the next couple of lectures, we'll talk about when you have more
observations you can use what's called the difference-in-difference estimator and
that's coming up. So but if the only thing you observe
is outcomes after treatment, this is the best you can do. So far, we have talked about potential
outcomes versus realized outcomes. We defined the average treatment
effects and random assignment. So we talked about two very
simple form of random assignment, which is simple random assignment and
complete random assignment. In the analysis, we talked about
the difference-in-means estimator.