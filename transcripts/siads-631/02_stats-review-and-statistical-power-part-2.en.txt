Now we're going to look at
the factors affecting the power. So the first one is
the size of the effect. So what is, let's say, the mean of
the two different distributions? The larger the effect size,
the larger your power will be. And the second factor is the standard
deviation of the characteristics, in other words, how noisy your data is. The noisier your data becomes,
the lower your power will be. And the third factor is your sample size, so a larger sample size is good,
is a good thing. So other things being equal, a bigger sample size will
increase your statistical power. And the last one is
the significance level desired. If you have lower significance level,
meaning lower error rate, type 1, type 2 error rate,
then the lower your power will be. So that's the factors
affecting your power. So now we're going to go through a set
of graphical illustrations to give us some intuition about why these
factors affect the statistical power, and then we're going
to derive it formally. So the first one is larger
effect size increases power. What is the effect size? The effect size is, in these two distributions you have a null
distribution, which is on the upper panel, and an alternative distribution
on the lower panel. So a larger effect size means that
the difference between the mean of the two distributions is larger. So when it's larger,
the overlap is smaller, which means that you will
have more statistical power. So if the alternative is move
further away from the null, then that increases your power. So that's the first factor, the second
factor is about the standard deviation. So the standard deviation, remember,
measures the noise in the data. So the bigger the standard deviation,
the lower your power will be, so standard deviation decreases power. And so here are again our null and
alternative distributions. And noisier data means that
the distributions are fatter, and if they become fatter,
other things being equal, there will be more overlapping regions. It makes it easier for
you to make inference errors, so that's why it decreases power. The third one is sample size, so a larger sample size would increase
your power, why is that true? So again,
let's start with these two distributions. When the sample size becomes larger, that means your distribution
becomes more squeezed together. When they're more squeezed together, the
overlapping in the tails will be smaller, which means that that
increases your power, again, that reduces the likelihood that
you make an inference error. And the last one is significance level, we say that higher significance
level decreases power. So from the same graphics,
we can see that the rejection region, which is the alpha, the error rate
alpha depicts the region on your right. And if that becomes higher,
more significant, which means that's shifting towards the
right, then there will be more overlap, the tails will overlap more with
a distribution of the treatment. That increases the likelihood
that you make an inference error. So there is this same trade-off
between type 1 and type 2 errors, so these are the factors. Now we're going to derive
these in a more rigorous way. So the derivations comes from the List,
Sadoff and Wagner 2011 paper. We will go through the first
two parts in the paper. One is the simplest case,
0/1 treatment, 0 means the control and 1 means the treatment. So you have two experimental conditions,
a control condition and a treatment condition. And it's also the very simple
example where the outcome variables are equally
noisy in the treatment and the control condition, so
that's the our simplest case. So now we're going to derive some of
the principles for sample size allocation. We're going to look at
the simplest case and start with the case where
the treatment is denoted by T = 1, so T for treatment, and
the control is set to T = 0. Then the outcome variable X0, X naught,
so think of the outcome variable, if we continue our education example,
this is the test score, the test score of the students
in the control condition. So it follows a normal distribution with
mean mu naught and variance sigma squared. And then X1 is the test scores
of the treatment condition, so let's say the treatment
has a smaller class size. And that also follows
the normal distribution with mean mu1 and
variance sigma 1 squared. So from now on, the zeros almost
always denote the control condition, whereas the subscript one
denotes the treatment condition. So the minimum detectable effect is
the difference in these two means, so it's mu1- mu0. And we use the Greek letter delta for
the difference between the control and the treatment, so delta is
the Greek letter equivalence for d. So the null hypothesis is
that there's no effect. So reducing the class size doesn't
really change the learning outcome. So the null is going to be mu0 = mu1, whereas the alternative hypothesis
is that there is an effect size. So the treatment condition,
when you reduce the class size, you produce a difference in
the average test score, so mu1- mu0 = delta, which is positive. So then we will need the difference
in sample means X1- X0, so that's what we use to test the hypotheses,
to satisfy two conditions. So these are the statistical requirements. So the first one is significance level,
that's the probability that you make a Type I error, and so we use
the Greek letter alpha for Type I error. And then the second one is the power,
so you as the experimenter will need to commit ahead of time
how large the power should be. And so we use 1- the probability of Type
II error, so that's going to be 1- beta. So these two conditions will give
us two equations to help us solve the sample size. So these are the two equations. What you have here is, the first
equation gives you T of alpha over 2. So that's the probability
of a Type I error. So we call it the T statistics. So we're going to have X1- X0 divided
by basically the noise in the data, which is sigma 0 squared /
n0 + sigma 1 squared / n1. And we take the square root of that. And then what we do now is to multiply
both sides with the denominator. So that gives us X1
minus X naught equals T of alpha over 2 times
this square root symbol. So that's the first condition. The second condition is that
the power is set to 1 minus beta. So we could set beta equals 0.2 or
0.1, depending on a host of factors, your budget, and how how much error
you're willing to to tolerate. So that will give us another equation,
which is X 1 minus X naught minus Delta. Delta is the minimum detectable effect. So remember Delta stands for, basically,
the difference between the treatment and the control condition, and
that equals minus t of beta. So this again gives us the condition. So again, we multiply both sides of
the equation with the denominator. So that gives us X 1 minus
X naught equals Delta minus t times the square root of
essentially the standard error. So what you have is that,
if you look at equation 1 and equation 2,
the cleaned up one on the right hand side, you recognize that they both
have X 1 minus X naught, so which means that we can reduce
these two equations into one. So the right hand side
of the equation 1 can equal the right hand side of equation 2. Okay, so that's what we're going to do. So we solve equations 1 and 2 by equating the right hand side of equations 1 and 2. And remember, this is the simple
case where we assume equal variance, which means that treatment and the control conditions will generate
data that has the same amount of noise. So Sigma 1 square equals Sigma 2 square. That will give you the optimal solution,
which is when there's equal variance, you want to set the sample size
in the treatment condition and the control condition to be the same,
which is n star. Okay, we use the term n * to stand for
that. So that equals two times
in the parenthesis. You have t of alpha over 2 plus t of beta squared times the Sigma Delta
ratio squared. Okay. So let's interpret this formula. So what does this formula tell us? Well the first thing it tells us is that
if the treatment and control conditions will generate the same variance,
then you want to split your sample. Basically, allocate the same number
of users to your treatment and the same number of users
to your control condition. The second part is that if you
desire a significance level and power, which is your Alpha and
Beta increases, that increases the sample size. And then the next factor,
let's look at Sigma squared. So Sigma is the noise in the data. If Sigma goes up,
which is if the data you generate is going to be very noisy,
you have to have a larger sample size. So the sample size is going
to increase proportionally with the variance of
the outcome variables. How about the minimum detectable size,
the Delta? So the difference, remember Delta is the
difference between the treatment mean and the control mean. If the anticipated difference
is going to be large, you don't have to have
as many observations. So the sample size would decrease
proportionally with the square of the minimum detectable size. So let's wrap this up. Okay.
So the sample size depends on several factors. It depends on the ratio of the effect
size to standard deviation. So that's what we're going to say over and
over again is the sigma Delta ratio, and it's going to also depend
on the significance level, the Alpha and the beta,
that you set ahead of time. The other part to notice is what if
you don't have any pilot data when you design your experiment and
you want to calculate the sample size? You don't have any data to work from,
then how do you set the sample size? You simply need to set
the sigma-delta ratios. In other words, the effect size can
be expressed in standard deviations, that will solve your problem. So let's look at several examples of that. So if you use Alpha equals 5%,
and power 80%, and you want to detect a 1 standard deviation
change using the standard approach, you know, or the assumptions are
satisfied, that means that we're going to need Sigma equals Delta, or
the sigma Delta ratio is going to be 1. So we plug it back into our formula. We're going to get n equals
2 times 1.96 plus 0.84, that's the T statistics for
Alpha equals 5%, and power beta equals 20%. So that number squared times the sigma
Delta ratio, which is 1 squared. That means you're going to get 15.68 and
we cannot have any fraction of subjects participating in any
experiment, so that's going to be 16. In that case, you only need 16
subjects in your control condition and 16 in your treatment condition. What if you want to be able to detect
a half standard deviation change? So in this case the sigma Delta
ratio is going to be 2, right? So Sigma over half of Sigma, and that means you're going to have to
multiply your standard sample size by a factor of 4, which means now you
need 64 observations per cell. So we can keep looking
at the rule of thumb. What if we only tolerate
the Beta equals 0.5? So from 0.2 to 0.05, so not 0.5, 0.05, then your sample size will
have to be 1.65 times n. What if type 1 error is now reduced
to 1% and the power is 80%? Then you basically need
1/2 times the sample size. And the last one, let's say Alpha
equals 1% and beta equals 5%, now you need, essentially,
227% of your original sample size. Okay, so that's how we derive rules of
thumb when we decide how many subjects to assign to the treatment condition
versus the control condition. So now we're going to move on to
something a little more complicated, which is now your treatment and
control condition have different variance. So the data is not going to be
equally noisy as we just covered. Now it's going to be more complicated, but we use the same principle in
deriving our sample size. So we go back to equations 1 and 2, so
I'm not going to derive every step. You can refer to the original
paper if you're interested. Now, the total number of
subjects that you will need, n *, is going to Is going to be again
a function of the t-statistics for the Type I error alpha over 2 and
T of theta. Again, if the effect size is in
the denominator, delta is there, and you see sigma1-squared and
sigma0-squared. But there are two new notations,
which is the pi-nought-star and pi1-star, and they're just a shorthand notation for
the ratio of the sigmas. So the bottom line of this is that
you can still solve analytically for the optimal sample size. But in most of the cases we're
just going to plug it into our statistical software and let
the software do the calculation for us. And so, in most cases you should be
able to have either some old data which enables you to compute the empirical
effect size and variance pilot data. Or you can think about
the sigma-delta ratio and that will help you compute
the sample size, okay? Now, we're going to look at more factors. What if you have a budget constraint? So in all previous analysis, we assume
that you have no budget constraint, so you can basically use as
many subjects as you like. However, if the cost of
collecting data from a subject in the control condition is
different from the cost of collecting data from the treatment,
what are you going to do? So we can do some derivations
just as what we did before. So we know the delta, right? That's the difference between
the treatment mean and the control mean. So that's the minimum
detectable effect size. So we have that derivation already,
and we also know how much it costs us as the experimenter or
the analyst to collect one unit of observation from a user
in the control condition. So we denote the marginal
cost to be c-nought. And the marginal cost for a subject from
the treatment condition is going to be c1. And your total budget, you're given
a budget, let's say, of $20,000. So you cannot break the budget constraint. The s dot t dot is subject to
the budget constraint that your entire spending on the control subject and
the entire spending on the treatment subject should
add up to your budget, okay? What are you going to do in this case? We can derive this,
you can do this probably after class. What you can do is to solve n-nought as
a function of n1 from the constraint, and plug it into the objective function. Then, you have essentially one equation. You take the first order condition,
that will give you one equation and one unknown. And you should be able to derive the
optimal proportion between the number of subjects in the treatment condition and the number of subjects in
the control condition. That's the last line,
which is m1-star over n-nought-star. What is that? That's going to be the square
root of c-nought over c1 times sigma-y over sigma-nought. So let's look at the right-hand
side of this equation. So that says that sigma is the standard
deviation of the data in the treatment or the the control condition. So sigma1 says that's
the standard deviation, how noisy the data is in the treatment. And sigma0 is the standard
deviation in the control condition. So this tells us that if the treatment
is going to have a large variance, it's going to be more noisy. Then, we want to put more subjects
in the treatment condition. And so, the ratio of allocation is going to be
proportional to sigma1 over sigma-nought. So that's one condition. What about the condition underneath,
the square root of c-nought over c1? That says that if the treatment
condition is very expensive, if c1 large, then this entire
ratio is going to be small. Then, we're going to scale
back the number of subjects we put into the treatment condition. But not proportional to the marginal cost. It's proportional to the square
root of the marginal cost. So this is all very intuitive. So this exercise essentially says
that this is a very flexible analytical framework. Depending on your objective in
the experiment and depending on your constraint, you can change it and
solve for the optimal sample size.