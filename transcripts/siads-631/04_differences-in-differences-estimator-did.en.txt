So the last module we're
going to talk about is the differences-in-differences estimation. When you have experimental data, I'm going to use a field experiment
that we conducted a movie allowance. So this paper was published in
the American Economic Review. It's a relatively small data set, and this is going to be part of our
homework assignment as well. So I'm going to look at applications
of block random assignment in this experiment, and
also introduce a second estimator. Remember in the first two lectures,
we talked about the difference-in-means estimator, and this is a more precise
estimator, when you have good data, is called the difference-in-differences
estimator. Sometimes it's called diff-in-diff,
or DID. So the basic idea of
the differences-in-differences, or the diff-in-diff estimator,
is that you would have observations both before your intervention
and after your intervention, so this can be used for experimental data and
also for naturally occurring data. So now we're going to first
talk about some notation. Suppose Y I represent the post-test score. So again, this is the education setting,
and XI represents the pretest score. Okay. So for each student, if you have both
the post-test score and the pretest score, suppose in the middle, you introduce
some education intervention by reducing the class size, or adding an additional
teachers into the classroom. So with difference-in-means, what we're
going to do is to only look at Y I, right, the post intervention test
score for the treatment group. For DI=1, and the average value
of y I4 the control group for DI=0, then the difference-in-means
is going to be, you know, the difference between the average of
the treatment and the difference, and the average of the control group
that gives you the treatment effect. Now if we also know the pre-intervention
test score, the outcome variable x i, what we can do is to
construct a first difference, you know, by talking about
differences-in-differences. We naturally will be talking
about two differences. So for difference-in-means estimator, we only look at what happens after between
the treatment and the control group. So that is one level of difference. For diff-in-diff, we're going to talk
about two levels of differences. So the first level is for
each individual student, I look at the post intervention test score
YI and the pre-intervention test score XI. i take the difference,
which is y I minus x i. I do this for
every student in the treatment group. Then I also do this for
every student in the control group. So for each student,
I have one difference. Then I take the second difference, which is the differences between
the treatment and the control group. So that's how I get two
levels of difference. The first thing that we're going to prove,
which is not required, is to show that is unbiased when
the treatments are randomly assigned. So recall that in the first lecture, we talked about the unbiasedness of
the difference-in-means estimator, and so we're going to evaluate
the difference-in-differences estimator. I showed that it's just as good,
in terms of unbiasedness. So how does the proof go? So the proof is, we're looking at
the expected average treatment effect. So for the difference-in-differences
estimator, we're looking at the expected difference for
subjects in the treatment group. So that's y I- XI conditional
on Di equals one. So that's conditional on being, subjects
being assigned to the treatment group, minus the expected difference y I
minus X1 conditional on Di equals 0. So these are subjects
in the control group. One thing we know about the expectations
operator is when there are summations or subtractions, we can
just break them apart. So then that first line becomes,
you know, the expected y I for those in the treatment group minus the expected
XI for those in the treatment group. And then we break apart the second
term minus the expected y I for the control group subjects, plus the expected XI for
the control group subjects. So, what does that give us? We know that we had random assignment,
right? Random assignment guarantees that
the pretreatment characteristics should be identical, so
the subjects they expected, xi for the subjects in the control group should
be the same as the expected xi for subjects in the treatment group. So the second and the fourth
term cancel out with each other, that just by virtue of random assignment. So we're left with the first and the third
term, which is the expected outcome for those in the treatment group
minus the expected outcome for those in the control group. That's exactly the difference-in-means
estimator, right. So by that, we proved that
the this diff-in-diff estimator is unbiased, so we're good. You know, this is,
this is a good property. How about a comparison with
something that we used before, which is the difference-in-means
estimator. We also call it DIM. So we just show that both
generate unbiased estimates. So on that front, on that dimension,
they're comparable. How about the Precision. The reason we like
difference-in-differences estimator, the diff-in-diff, is because that reduces the variance in
the treated and the untreated outcome. So it's more precise. They're both unbiased but diff-in-diff is more precise than
difference-in-means estimator. Intuitively, what that does is,
because afterwards, when you do the, you know, for the same subject in
the control of the treatment condition, you do post-intervention-
before intervention, that takes away
the individual fix effects. So that reduces, that's one of
the reasons why it reduces the variance in the treated and untreated outcomes. So let's take a look at
the simulation results. So the top graph is the density of
a difference-in-means estimator. So this is just through simulations, and
the bottom one is essentially the same observations, but we first
difference once within individual, then we compare the treatment and
the control condition. So that's the second one is
the diff-in-diff outcome. What you see is that they have the same
mean, you know, they're aligned, but the bottom density function
is much less spread out, which means that the standard deviation
around the mean is a lot smaller. It is more precise, or it is less noisy. Okay, Okay, so that's the advantage. So as an experimenter,
when should you use definitive? The answer is always, if possible. So you should always take advantage
of the opportunities to gather background information before
you implement an intervention. So you could do a lot of things. You could, in the ride-sharing context, since we just talked about Uber,
we can look at the driver's history. Let's say two months before or the entire
lifetime history in terms of how many hours they've driven, how often they
drive for Uber, and other demographics. So you collect
the pre-intervention information. Then, after you implement
your intervention, you get your outcome variables and
covariates. So collecting that pre-intervention
information is always helpful, and you should always do it,
if at all possible. So sometimes you can use, in the education context you can have
students test scores before you change, you randomize some classrooms
into the treatment condition. Again, so
that will help you with the accuracy or the precision of the estimation
after your experiments. So now we're going to move to a field
experiment on social comparison and contributions to online community, is a field experiment that
was conducted on MovieLens. So I'm going to use this to illustrate
several design features and analysis. It's also the data set is part
of your homework assignment. So this part also provides some background
about the data and the variables. The feature that we're going to talk about
in the random assignment is block random assignment. We're going to look at how
the researchers use blocking here. And the analysis strategy is definitive. This also gives you some idea
about the role of theory in experiment design and analysis. So I'm going to actually do a fairly
detailed analysis of how we designed and analyze this particular
online field experiment. So MovieLens is one of the earlier
online movie recommender sites. It's hosted at the University of
Minnesota, and it is non-commercial. So this is one feature that says MovieLens
helping you find the right movies, making you think that it's the algorithm
that helps you find the right movie. It's actually people, the users input that
help each other find the right movies. It is a free and
personalized non-commercial sight site. Around the time that we conducted
the experiment it was a fairly active and successful online community. It had had a hundred thousand users and 13 million ratings of about 9,000 movies. The main activities that people do
on the site is to rate movies and receive movie recommendations. So the technology that they use is called
collaborative filtering technology. But even for a successful online community
like MovieLens, they had problems with, for instance, 22% of their movies in
their database had fewer than 40 ratings. So few that the software cannot
make accurate predictions. So this is an illustration
from John Riddle, the k-nearest neighbor
collaborative filtering technology. So this helps motivate
why we would like people to contribute more ratings
in such an online community. So suppose you have a target user and
you would like to recommend movies for this person, but you don't know whether this person
would actually like a particular movie. What do you do? You have a community of users, so
think of these as MovieLens users. And you can compute based on their past
rating history how similar they are. So for each pair of users you
can compute the similarity. And using some matrix you
can define a neighbor. Let's say the k-nearest neighbor, okay? So let's say in terms of ratings,
of pass ratings, this target user is most
similar to users two and three. Then when you decide what movie to
recommend to the target user you can just basically take the weighted sum of
these users two and three's ratings and recommend movies roughly what they
like to the target user, okay? So this says that movie ratings
is both a private good and a public good, because it helps you and
it also helps others. So this is when we wanted to run
an experiment to look at whether social information increases
people's rating activities, we had to first decide on our sample. So MovieLens had 100,000 users, the x equals 113 refers to
the fact that the average number of movie ratings among these
100,000 users is a 113. So we imposed a number of criteria,
which is they have to be active in the past year,
they have to have at least 30 ratings. And so the recommendation
would be somewhat accurate. And they have to have given
us permission to email them. And that reduces the number
of eligible users for the experimental to 5,488. Now, the number of ratings
now is going to be 311, which is significantly different from
the universe of users in MovieLens. Among these we emailed 1,900, and about 400 consented to participate. So our experiment design had three stages, the first one is the pre-experiment
survey, remember the definitive. So we collected their behavioral
data before our experiment, that will help us with
the definitive analysis. And we also collected survey data. We asked them time it took them to
search for and rate ten movies, their willingness to pay for
a list of top ten movies, and so on. Then the intervention is implemented
as experimental newsletter. So we randomize the users into
the rating information treatment, the net benefit treatment,
which we are now going to talk about, and the control condition, okay? At that time the newsletter
intervention was new, so we did not have any prior about
what the information would do to the movie rating,
to the variance of the movie ratings. Therefore we assigned the number of
users equally to the control and the treatment condition. So these newsletters are personalized. So these implement the treatment. And the third stage is after their
experiment we implemented a, we send out a post experiment survey. So this is the timeline of the experiment. The survey response rate was 78%,
which is pretty high, it was incentivized. So for people who completed the survey
a random number will be drawn to receive Amazon gift cards. So now let's talk about the intervention. So for the control condition, the people in the control condition also
received a newsletter, and that contains some information about the percentage of
movies they've rated that are comedies. The reason for sending out a newsletter
is to control for the placebo effect. The rating information treatment
is essentially telling each user the median number
of ratings by similar users. And we divide the users into below
medium median and above meeting group. Basically, the bottom one third one, the
middle one third, and the top one third. In stage two,
which is the intervention stage, each newsletter contains five shortcuts
for them to do different things. These are designed to basically
incorporate both the retained information condition and
the net benefit condition that we're not talking about for
this short introduction. And so there's rating, you can rate
popular movies, you can rate rare movies, update the database, which is a costly
activity, but it helps other users. You can invite a buddy or
just visit the MovieLens homepage. So this is what the newsletter
looked like for the treatment,
the rating information treatment. Every newsletter start
with the same paragraph. Ever wonder how many movies you've
rated compared to other users like you? And it says you've rated 287 movies. So this is just for
this particular user, Max. Compared with other users who joined
MovieLens around the same time as you, you've rated more movies than the median,
and the median number of ratings is 100. So notice that it's personalized, right? So the 287 movies was the actual
number of movies that Max rated, and the median information is also true. So there's no deception
in this experiment, everything that we told them was true. Then we say if you would like to rate
more movies, here are some options. You can rate popular movies,
you can rate rare movies, or you can try new features, and
at the very bottom, which is cut out, we have the you know, or
just visit MovieLens. Okay, so
that's the the treatment condition. So for the rating information treatment, what's useful is the sort of,
you can rate more movies. The try out new features was designed for
the other treatment. How about the control condition? So if you're in the control condition and we say here are some statistics
about your ratings behavior for one popular movie genre, about 38.6% of
the movies you've rated are comedies. Your average rating for this genre is 3.5. And then it's followed
by the same five links. The top four links
are randomized in order. So the ideal control should be
just a newsletter which says, hi here are some links, without
the information about the comedies, but you know every field
experiment is a compromise. So when we talk about the administrators
of MovieLens, and no you cannot send out a newsletter, which just says, hi,
you have to give them some information, so we decided to input some personal
information without the social aspect. So it's in a way not idealized control,
but it can be controlled. You know, for instance, you might be
worried about the anchoring effect, but that can be, you know, of these particular
numbers, but you can control for that at the end in the analysis. What does rating popular movies look like? So if you're going to the rating
popular movies screen, you will find list of movies that lots
of people have seen, there are popular. So it's less costly to rate. How about rare movies? These are movies where, by definition,
in their database have very few ratings. So it's costly to actually
scroll through a lot of them. Okay updating the database. So now I'm going to talk about one design
feature, which is block random assignment, that we just introduced on
the the MovieLens experiment. So, what did we block on? We decided to block on MovieLens age. That's the number of months that
someone has joined MovieLens, because if someone has joined for
a long time, they're more likely to have rated a lot of movies, than someone
who haven't joined for a long time. So we blocked on the,
you know, we defined new, mid, old based on their MovieLens age. So the new users joined MovieLens for
a couple of months, whereas mid-age users have joined
MovieLens for more than a year, on average, and the old users
have joined for several years. So that will help us with reducing
the standard errors in the estimate of the average treatment effect. I'm going to talk about
the theory a little bit because the theory sometimes tells
you what to look for. And it helps you generate hypotheses. So we model MovieLens,
the users utility function, how much satisfaction they
get out of MovieLens, as U of I, which is a function
of the ratings you put in and the sum of ratings of everybody else. So this is how like a public good for
the community and the information from the median, okay? So that's your private benefit
derived from these activities minus, essentially, the second term,
GI, and what's in the parentheses are the the difference in how
much you deviate from the median. So in a way, you know deviating from
the median gives you this utility, so that incorporates
conformity into the model. So the first proposition says that you
know after you take the first order conditions, you look for the optimal
solution and do comparative statics. It predicts that below median users will
rate more movies than the median users. The second one says above meeting users
will rate less than the median user, and conformity to the median says that
the movie ratings post-intervention. So in the month after they
receive the newsletter, the distance from an arbitrary
user to the median is going to be smaller than
before the intervention. So this actually should tell us something, which is there's
a conformity to the median. So, in theory, theory also should
tell us that a more efficient design would be to allocate more
users to the control group, because we expect the distribution
of movie ratings to shrink to the median post-intervention
because of conformity. So this is what we see, which is
the below median users actually, so the blue bar is the month before And the red bar is the month after
each experimental condition. So on the leftmost side,
these are the pair of average movie ratings below median. So the average went from 4.2 to 26.4. So that's a 530% increase. It turns out that median users, about median, also increase their ratings. For the above median, the power users,
their ratings actually decrease. So the blue bar shrunk,
basically it came down, right? The red is a lot lower than the blue,
whereas the control actually also went down a bit, which is why we
actually need to have the control. Why did the control also came down? It's because now they're new features
such as updating the movie database that the control users were drawn to actually
for the above meeting users as well. So here's how we could do, a very simple way to do the difference
in differences estimation. So we basically, if you look at this
table, which is taken from the paper, the first three lines
are the treatment group. I will call it rating
information group and the bottom three rows
are the control conditions. So for each condition,
for each person, in fact, we're going to construct the first
difference, which is delta x, right. So for each user, I,
in experimental condition E, we construct the after minus before, so
the month after minus the month before. And we do this for below medium,
median and above median group, for both the treatment and
the control group. So what you see here is, the overall and stratified by the new, mid,
old by movie land's age. And then you can do the second
level of difference, which is the difference between
the treatment and the control condition. And there are three results,
which is the difference, the change in movie ratings is significantly larger for
the below median group. So these are the laggers,
or who were laggers, okay. They're also significantly larger for
the median group, which is not predicted by theory. So number one is predicted by theory and
they are about the same for the above median group. So even though you see on this graph,
there seem to be a much larger drop from the above median group
compared to the control group. It turns out that the difference
is not statistically significant. So this is an example of definitive. So for each user,
you know both their before and after intervention, okay,
the monthly ratings. Then you can construct
the first difference and then do the treatment
control difference analysis. So to summarize,
we find that social comparison, this type of social information,
significantly influence behavior, especially for
those who were previously below median. These are the laggards. It turns out that they increase
their ratings by 530%. For the above median users,
their ratings decreased by 62%, so it's considered a fairly
successful experiment. However, the main message from
a design perspective is first of all, collect information before your
intervention and use that, for instance, for blocking. And also use that information for the analysis in the form
of definitive analysis.