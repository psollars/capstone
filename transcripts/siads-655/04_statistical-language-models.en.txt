In this segment, we're going to try to talk about how
to learn a language model from text. So a language model is going to
be a particular statistical model that's going to allow us to estimate
the probability of an arbitrary sequence of words. We've already seen one example of
this using a unigram language model. That was the dice that we could roll
where each face represents a word, and that let's us estimate the probability
of an arbitrary sequence by simply multiplying the probabilities
of these words. After we saw that this is a bad model to
begin with because it doesn't depend on context. So how do we add context to estimate
the probability of a particular sequence? This is one of the core
challenge of language models. Typically, will think of something like
a higher order n-gram language model. The conditions on the previous
sequence of words. So for example, in our unigram language
model, to estimate the probability. We multiply the probabilities
of the individual words, so the probability of set is independent
of anything that comes before it. In a bigram language model,
this is a higher order model. We condition the probability of
set based on the previous word. So the probability of word I, given the word that came before
it at position I minus 1. And again, in a trigram language model,
we condition on the previous two words. Together these can help us estimate
the probability of sequence, and I'll show you how this works in practice. The higher order language models
require adding start tokens, so if you may have noticed, what do we
have this condition on previous tokens. But for the first token there's
nothing before it, so for the probability of the cat meows, we need
to add some additional Start tab token. So here let's take a look at the example
of the bigram language model. To get the probability of the cat meows,
we have to multiply the probability of the, given the Start token times
the probability of cat given the. Times the probability of meow
giving cat times the probability of the stop token given meow. To initially estimate these
probability distributions, we can actually do something
quite simple in counting. So let me give you an example of how
exactly we might get these probabilities. Let's start with the unigram model. Say to estimate the probability
of the sequence, we multiply the probabilities of the words
times the probability of the stop token. We'll begin by estimating
the maximum likelihood estimate, which is what's
the likelihood of the data? What's the probability distribution
that's most likely given the data we saw? To get this, the probability of the word
is simply, how many times that word occurs in our data divided by the total
number of tokens in our data set? For the bigram model, again we are now
going to condition the probability of a word given the previous word. And we finally multiplied
by the probability, this stop giving the final
word in the sequence. To get these conditional probabilities, we count how likely are the two word
sequences that we saw, say the cat divided by the count of how many times
the first word in that sequence occurred. Say for a trigram model, similarly, we're going to condition
on two words of context. And we would count how many times
a particular three word sequence occurred and divide by how many times
the particular two word sequence occurred independent of the third word. For these higher order
language models were going to make a simplifying assumption. Essentially, these models make
a Markov assumption about how much context we need to
estimate the full probability. Let me give you an example
of why this is useful. Let's say for
example that we have a five word sequence. To fully estimate this, we have to
multiply the probability of the first word times the probability that second word. Given the first word times the probability
of the third word given the first and second words and so on. This is using the chain
rule of probabilities. If we use a first order Markov assumption,
this makes the assumption that we only need one item of context to fully
estimate that full conditional sequence. So the probability of word I,
given all the prior words is equivalent to the word I,
given just the one that came before. Similarly, for a second order Markov
assumption, we make the assumption that we can fully estimate the entire sequence
just using two words of context. This may sound incorrect, however it's
often very necessary in practice due to the fact that we have data sparsity and the fact that longer sequences require
more data to be stored in memory. And for computational feasibility, we have
to make some sort of Markovian assumption. We once again can try to
generate text from these as well. So to do so, we have to once at the start, we'll have to condition
on the start tokens. Let's say that we have a trigram
model to generate two context words. So here, we have START2 and START1. Conditioning on this context,
we might say sample a distribution, example from its distribution. And so, each particular pair of
words in context defines a different probability distribution for
what's likely to come after. Here, you can think
about these as different dice that we might roll
given the prior to context. So here I might roll sample the, given
START1 and the word that I just sampled. Given the, and by sample the next from
the next distribution and I'll sample cat. Given the next thing that I've
generated from the previous role, I can sample the, cat, meows, and so on. Together this can be used to generate
context from these higher order models to give you an example of what kind
of generation looks like here. Here's an example of a unigram model that
was trained on Shakespearean text using no context, essentially just conditioned
on the probability of the word alone. You can see that has almost
no linguistic structure. If you sample from a bigram model that
conditions on one piece of context, you can see that this
makes much more sense. In our case, we actually see
the characters show up at the start of the line, reflecting the fact that each
line begins with a new person saying something in the Shakespearean text. Even though they aren't necessarily
grammatical or coherent, they still look a lot more like
English than, say the unigram model. At the trigram level,
conditioning on two prior context words. We start to see much more coherent text,
say Coriolanus, no, I'll not kill thy soul, sounds almost
reasonable as a moment of dialogue. Even from a 4gram model,
now we start to see things that look quite coherent that are still
randomly generated. This suggests that maybe we want
to extend to 5gram models or 10gram models, so
why not extend to even more context? To begin with,
we have to do longer n-gram models. We have to have even more data, and
we need this more data to estimate the conditional probability
of longer sequence. Keeping this around is a huge
computational requirement. That's said, even with more context,
we still suffer from long range dependencies and trying to
generate correct in grammatical text. So for example, say that I have the slide
that I just uploaded to Coursera for the MLB course has typos. If I change the word slide to slides,
I would need to change the verb at the end of this sentence to have,
to reflect the subject-object agreement. However, I would need
a much longer sequence and n-gram sequence to do the language
model to correctly generate this. I could arbitrarily make this sequence
even longer, suggesting that even 20 or potentially 30 n-gram models
would not be sufficient for guaranteeing that I could correctly
do subject-object agreement. That said, n-gram models are still
useful in effective at practice for estimating these probabilities. We'll talk about some of
the applications in a few segments.