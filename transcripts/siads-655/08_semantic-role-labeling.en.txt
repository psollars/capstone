In the previous segment, we talked about
roles, has particular arguments for predicate might be asking how to
actually label these roles in text. So for example, the student wrote
the program in Python using PyCharm. And we say that wrote is the predicate. We'd like to label which
things are the arguments and what are the types of those arguments. Say that student is the agent in
the sentence, or that program is a result. One common way to do that is to treat
it as a supervised learning task. In this segment, I'll show you an example using one
definition of roles from Prop Bank. Propping itself is an existing set of
annotations on top of the Penn Treebank which was originally designed for parsing doing constituency parsing,
later dependency parsing. Here we think about each verb
is having its own rolls. These will somewhat loosely aligned in
the sense that the roles are numbered. An argument zero typically refers
to something like a proto agent, something that's the volitional cause. Movers things that have sentients
argument one will be the pro to patient. So the thing that is affected or
changed and potentially remain stationary. In the case we may have other types
of arguments, so the benefactive or the starting point or the end state, and these may vary across
different types of predicates. Let's look at one example for
a sentence for increase. Here we can have argument zero, an argument one which correspond to
the Proto agent and the product patient. Increase has several other arguments,
such as the amount of increase or the starting point prior to increase or
the ending point after increase. As you can see,
these are specific to the verb increase. Here is the example label for the student. Increased their grade 5%. We can see that the student as a phrase
is argument 0, not just student. Similarly argument one is
there grade not just great. If you look at a different paraphrase
at this, the assignment for grade increased by 5%, we can see that
argument one is present in both and. This hopefully helps you give a sense of
how these frames and their arguments can be used to compare annalynne sentences
that have very different syntactic or surface structures. Similarly, in the third example, the grade
was increased from 90 by 5 points. We can see that argument two
can take different forms, and that argument one still is present. To label these one of the most
common algorithms goes as follows. Well, first constituency
parse the sentence and I'll show you an example of that,
and then for each of the predicates will basically try to identify whether
that predicate is a frame or not. Let's take a look at our example. The student increased their grade 5%. Here we would constituency,
parse the sentence. If you remember from the parsing
segment in a constituency parse, we have intermediate nodes. These are the constituents and each of these forms that complete
phrase within the sentence. They're recursive. So for example, increase their grade
is the verb phrase on the right, but underneath it we have the noun phrase
their grade and another noun phrase 5%. We can analyze each of these recursively
in the next step, so here for example, we're looking at the noun phrase
which has been tagged with the fact that is the subject
of the sentence. Will try to examine is these students
a particular argument relative to the predicate? Will feature eyes this. These features may look quite similar
to what you would do for using a mem or something for part of speech tagging. Here we can look at what's the head word
of the phrase what's the part of speech, whether whether its position
is within the sentence, the first word of the constituent itself. Will convert these into a set of
features that we can try to use for machine learning prediction. And then we'll try to
predict from these sets of features whether this is any of the
particular arguments for this predicate. In this case, the verb increase. Here we just use a supervised setup using
the label data from Prop Bank to train on the constituent data
that's in the data set. Semantic role labeling itself
is quite challenging and often there are too many predicates
to classify all of them. In the example I showed you before,
we are only a few constituents, but for many sentences the parse
tree becomes quite large and this requires run the classifier many,
many times. Often one of the most successful
techniques involves pruning these, and there are many heuristics to prune. That said, there are many also
many different types of arguments. One of the benefits of Prop Bank is that
argument Zero an argument one are often the same in terms of their nature,
which allows you to train across different types of verbs to recognize
what is argument zero. The Proto agent, an argument one, the
probation across different types of verbs. There are similar schemes that,
like Proto rolls from Dowdy, and they have a huge advantage in the sense
that you can leverage them across many different types of verbs in context. That's it, they are often different
types of arguments that you would like to recognize. Say, how much did the amount increase for
increased verb, so it's helpful to have more training data. That's a training. Data is growing, but it's often quite limited by the fact that
this is a challenging annotation task. We will see the prop ink annotations
are available in many languages, so this is again it's not
an English specific task. One more promising techniques of late is
again using neural network approaches. These are still limited, however, by
the amount of data and your ability to use semantic role, labeling and practice will
depend on how much data you have the how many instances you have for each of
the arguments and rolls and predicates.