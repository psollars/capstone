In this segment we'll talk about a common technology that you probably already use but aren't aware of, language models. Has this ever happened to you, where you're typing on your
phone and you maybe type in a word and your
phone connects it to a word that you don't
think you wanted to type. This happens to many
folks and this is an example of a language
model in action. Another good example
of a language model that you might run into, is thinking about autos
suggestions on search engines. Here, the model has learned something about what comes after, how do I convert to. Many of these are
useful, and this too is an example of a language
model in action. Let's look at a
different example. The thinker sat on the mat. The thinker sat on the cat, or the thinker sat on the idea. Well, all three seem possible. We'd probably say that the
third one is more likely, and we just swapped out
that child for the thinker. In this case, the language
model is going to try to estimate which of
these is more likely. Specifically, we'll say
that a language model describes the probability of
a particular word sequence, so that we could say
something like "the probability of the
thinker sat on the mat" as a phrase
is around 0.00021. We can do this for
the second phrase and for the third phrase. Ideally, the language
models knows something about the
structure of language and the frequency with
which words appear together to estimate
this probability. For language models, we're going to try to learn them
directly from text. We're going to try to take
this unstructured text, the mess of text of any kind, and try to learn it's
structure representation. As a result, it's going to be sensitive to which text it's learned from to estimate the probabilities of
what's more likely. It's going to be sensitive
to how much text in terms of what the kinds of
phrases it's likely to see. There are many
different ways to learn a language model and to
estimate the probability. In this segment we'll
talk about a few. One easy way to think
about a language model is what's known as a
unigram language model. If you remember,
unigram means one word. Here we can think about a
language model as a dice, where each face is a particular
word, that may come up. We can think about this as a probability distribution
over the space of words, where here each word is
equally likely to come up. However, we know that
most words don't have the same probability
of appearing. In fact, the word
"the" is probably much more likely to
occur than dog or cat. What we can do is reshape the
probability distribution. Here I'm going to visualize
it by the size of the word and make it
a language model. It is a slightly
different probability where "the" is more likely. This dice is a simple example of a unigram language model. We can use this language
model to do estimates. I should point out a
few different features of a language model itself. One, we're going to add all the probabilities
together so they sum to one. This distribution has to be a probability distribution for the language model to work. Second, we're going to add a
separate token for stop that says how likely is it the phrase ends at
this particular word. We'll take a look at
example of this next, but essentially it
means that if you think about the unigram model, if we're sampling from it, when do we stop sampling? Finally, we'll also
call the set of words in our model,
it's vocabulary. This will include the stop token. Let's think about how we estimate the probability of a sequence. Let's say that we
wanted to estimate the probability of
"the cat meow." For unigram language model. To estimate the probability
of any particular sequence, we simply multiply
the probabilities of the words individually. To estimate the probability of the phrase, "the cat meow", we take the probability
of "the" times the probability of "cat" times
the probability of "meow", times the probability
of the stop token, that the phrases
ended at that point. Using this simple estimate, we can now rank phrases for which is more
likely than others. I should again point out, that language probabilities depend
on which data we learned. So if we wanted to say, what's the probability
of that phrase, "I like cats" or "house cats are domestic species of
small carnivorous mammals" or "your
cat looks perfect." The probability of
each of these will depend on how much text, say an arbitrary selection
of texts we found on the web versus just
social media text, or say, children's books. A language model can be sensitive to each one
of these and can be tuned by training on
data that you think is relevant to your
particular application. We can also generate text
from a language model by sampling from its
probability distribution. Here, we could essentially
roll the dice according to the different
probability and record which word comes up on head. This might give us
a phrase like "the dog bark" and "the stop token". It could also just give us
"the stop token" or "cat, cat, cat, stop token." Each of these have
different probabilities, but are still outcomes of sampling from a particular
language model. In this case, the
unigram language model. You might also recognize
that this is probably not a great model to
start from for language. In fact, it doesn't work well in practice and for a
particular generation. For example, the
probability of "the cat" is equal to the probability of "the" times the
probability of "cat". However, the probability
of "cat the", is the same probability because we don't
have any context. It's essentially
the probability of "cat" times the
probability "the". In more advanced
language models try to address the challenge
of adding more context. We'll talk about these in
some of the next segments.