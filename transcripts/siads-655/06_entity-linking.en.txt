Let's talk about a
relative problem towards disambiguation. Notice entity linking. Let's start with an example. Say a friend comes
to you and says, "Michael Jordan was amazing
in his latest movie." Who do you think of when
you see Michael Jordan? It could be say, Michael
B. Jordan the actor. We could be talking about say, the Black Panther the movie. It could also be Michael Jordan, and maybe your friend has not seen anything since Space Jam. It could even be
Michael I. Jordan, the famous Machine
Learning Specialist. In fact, people often use their
middle initial to do this disambiguation forests
to signal that you are Michael B. Jordan and not
another Michael Jordan. Entity linking is
the task of giving some entity and text,
say Michael Jordan, identifying which
of the references in some ontology
refers to that entity, say that Michael B.
Jordan is the actor. In practice, we
often use Wikipedia as an ontology because People of Note often appear in Wikipedia, though
not exclusively. There are other ontologies
that are useful, say, for linking to medical or
pharmacological entities, or say music ontology for
linking about rock stars. Entity linking is
quite similar to word-sense
disambiguation as a task in the sense that
there's ambiguity as to which thing a particular
phrase refers to. Entity linking, however, is quite different in
the sense that we often don't have an
established ontology. Wikipedia's appearance,
has created that ontology to some degree. Yet there're often many
domains in which we have entities that are ambiguous
that we need to resolve. Let's think about entity
linking as a task. Typically it will
try to break into entity linking into two phases. The first of this is what's
known as mentioned detection. In this case, we
will try to look at the text and figure out which parts of it actually
referred to that entity. You can think of
co-reference resolution that we've talked
about in Week 3, as one particularly idea, trying to figure out what
are the different ways in which this entities
being expressed. However, there are
some folks will try to do both at the same time because coreference resolution
maybe influenced itself by the entities
that are being linked. The second step then says, let's identify how to disambiguate given
all the mentions. In this case, we look at jointly at the information that's present across these
different mentions to do some resolution task, and break the ambiguity. This mentioned
disambiguation step is often dependent on the
underlying knowledge base. It will depend on whether
you want to use, say, Wikipedia or Freebase or
some other knowledge source. Whereas, mentioned detection can be typically universally applied. There are many different
approaches for doing the disambiguation and they often follow to be very
similar from those used for word-sense
disambiguation, particularly long the
supervised domain. To start with one example, let's look at a paper
from Mihalcea and Csomai, trying to do entity
linking with Wikipedia, system known as Wikify. Here, they'll start
with raw hypertext that has URL's, and
try to decompose it. Using this, they'll
identify candidates, and try to identify which of these reference refer
to the same entity. From that step they'll
identify sense definitions, and use it less
Gleick definition to identify which
reference referred to the particular entity
in the knowledge base. They can use also
information from Wikipedia, say, as to which
pages get more views. There's going to be a rough prior on what's the likely person, given that many people who have Wikipedia entries are not necessarily that often
referred to in text. By doing some voting combination, they can then do entity linking, here referred to as
word-sense disambiguation, which highlights the close relationship between
the two task. For entity linking in practice, we often need to do it
to reason about texts. For example, if
you wanted to ask, when did Michael
Jordan got his PhD, it would help to know
which Michael Jordan that is in order to answer
that correctly. Most NLP libraries do not
have entity linking built in, and the fact that we often had dedicated entity
linking libraries, often for specific ontologies, say Wikipedia or
BabelNet or Freebase, or for other types of domains
that are not just people. To give you one example of
entity linking in practice, we can look at BabelNet
that we talked about briefly for word-sense
disambiguation. Here you can see I took a class on machine learning
with Michael Jordan, and I've shown you the
disambiguation step at the bottom, we can see that class, and machine learning
are linked together. However, that sense of Michael Jordan is
probably not correct. In fact, we have to say
Michael I. Jordan in order to get the correct
sense of entity linking, this somewhat, it expresses the fragility of
these typical models. In fact, in practice, entity linking is
quite difficult, and it has to rely
on different types of robustness checks
tick it a correct. But that said, it can be a very useful information
extraction technique for trying to get
more information from particular sentence.