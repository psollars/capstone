In this segment,
we'll take a look at another way to learn
a language model. Specifically, we'll
think about how to learn a classifier that acts
as a language model. Let's think a bit
about what language models and classifiers are. On one level, a language
model is a set of parameters describing
the language sequences, the probability of a
particular sequence. A classifier can be thought
of parameters describing features and it's a
mapping to classes, language models take context as input whereas classifiers
take features in input,. Language models predict
the probability of the next award. And the classifier predicts
the probability of classes. We could actually try to
create a situation where language model is represented
as a classifier by, say, using something like multiclass logistic regression
and treating it as a set of a language
model that predicts the next word based on
features of the context. Let me give you an example
how this would work with logistic regression
as a language model, works through the equation
of logistic regression, we can talk about the
features in a second. Here, is the left-hand side of the logistic regression equation. We're going to try to
estimate the probability of a specific next word y, being next to given
some sort of features. In this case, the
input context x. Beta will be the set of learned parameters that we're going to get from the model. This isn`t the example
of the right-hand side. In this case, we're
going to try to multiply the specific input contexts
by some set of features. This bottom half is a
normalizing factor. As rule of thumb, if you ever see an equation where
there's something being calculated on the top
and there's a sum of something that looks like the
top inside the summation, this is typically trying to normalize it to be a
probability distribution. In fact, this is what's known as the softmax function that's used throughout machine learning. In this case, we're
going to try to understand how to learn
these Beta values to let us estimate
the probability of the next word in the given
some sort of context. Let's take a look at what
this means in practice. Here's an example of
logistic regression as a language model. And we're going to look at
a bigram language model. Here we'll have one context word. This is our feature vector x that a logistic regression
will be trained on. In fact, this value
here is the feature for that which preceding word
is present in the context. In this case, the
proceeding word is dog. What we'd like to do is learn
a set of weights that given this context will help us
predict the output space. We'll have a global bias
that helps us learn what's the models bias towards
certain words overall. And we`ll learn a set
of Beta coefficients. These help our weights, we`ll learn one set of features
for each output space. Again, mapping the feature space, we can see what's
the beta value for the output given this
feature vector x. When we multiply the feature
vector by the Beta vector, we can actually get what's
the raw value on the output, We get this Beta vector
for dog, for cat, for barks, for all
the probabilities, for all the words
in our vocabulary. To say, let's estimate what's the probability of dog and barks. We would take the Beta vector for barks and multiply the two. Classification based
language models allow for a really rich representation for estimating this probability. We can actually add knowledge
not just from the context, but from other sources as well, say the document or the whole context beyond just
the preceding two words. For example, we can look
for higher-order N-grams, not just the previous two words. We can look for things
like spelling or capitalisation or
word elongation. Each of these may help
us predict what's the next word that's most likely. We can even look for
document features such as, which URL was this
text accessed from, or what's the topic,
or who was the author. One of the downsides to
using these features is that we have a potentially much
larger feature space, which could lead to over-fitting or slower training times. However, because we still get a probability distribution on the output for things
like logistic regression, this is still a useful
way for encoding lots of knowledge that can make
sense in some applications.