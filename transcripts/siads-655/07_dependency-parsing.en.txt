Let's look at one formalism for
parsing in natural language processing, Dependency Parsing. What is Dependency Parsing? Well, here we're going to define
a parsing tree or the syntax, by how individual words
relate to each other. These relationships be asymmetric,
so it'll be one head and one tail. And the relationships are from
a small set of typed categories. They'll be no intermediate nodes or
elements in our parse tree. And, let's take a look at one example. The cat ate the cookie. Here we can think about how
the words relate to each other, and draw these arrows, say between
ate as a verb and cat as a noun. And we could label these with
particular grammatical relationships, here from the Dependency Parsing
formalism. We could say that, the forms the dependent
or determiner relationship with cat. And cat forms a noun subject
relationship with ate. We could think about the direct object of
the verb, and the determiner again, for cookie. These dependency parsing relationships,
form direct relationships, which we'll call the head and
the dependent. So, which word is the head and
which is the dependent? Here, the dependent will be the word
that is pointed to by the arrow, and the head will be the word that
the arrow originates from. There are many different conflicting
frameworks for thinking about heads and dependence. In general,
I would say that the head is obligatory, where dependent words are optional. And you may be able to
colloquially think about this is, the head words are more
important in a sentence. So a cat is likely more
important than the. However, this is a [LAUGH] long
standing issue in linguistics for a precise definition. But in general,
most definitions do agree with each other. These dependency parsing relationships,
will end up forming a tree. So, particularly it will form a directed
graph consisting of vertices and arcs. And there'll be a single root
vertex that has no incoming arcs. In many cases,
this would be the verb of the sentence. Every vertex will have one incoming arc,
except root. So for example,
the subject of the sentence, will not have multiple
arcs coming into it. And there'll be always a unique
path from the root, say the verb, to every other vertex in the tree. There'll be an acyclic
constraint on the graph. There are many different
Dependency Parsing Algorithms, some of which are covered
in your textbook. And they often make simplifying
assumptions as to how these words relate to each other, and the structure of the
tree itself that make it possible to parse efficiently in practice. The current state of the art is
using neural network parsers, which may sound slow but
actually can be quite fast in practice. And there are quite a few variations
in how arcs are labeled and where they attach. This often has to do with ambiguity. And, it may seem obvious that well,
the subject connects to the verb, but let's take a look at a few in practice. Let's look at these variations and
how the parse trees are connected. Here, I'll show you two examples. This example comes from
Stanford's CoreNLP parser, which is one of the most common parsers. Here we can see, I like cats and dogs. The verb like connects to the first cat,
and and dogs the conjunction, it connected back to cat. An alternative formulation
comes from the SpaCy library, which is another fast parsing library. Here you can see that for
the sentence, I like cats and dogs, we again have cats
connected to the verb. However, the conjunction shows up to
be connected to cats rather than dogs. In this case, there's an ambiguity into
where this and should be connected to. Let's look at another example
of this type of variation. Here again, showing you
an example from Stanford CoreNLP, I believe in our football team. We can think about where that, belief
in our football team should connect to. It's that easiest to recognize this
difference when contrasting with SpaCy's representation. Here, believe directly connects
to the preposition in. In contrast, the Stanford case,
believe connects to team. Which of these formalisms,
is correct in practice? Depends on what you're
trying to do with this. And in practice,
many of these actually can be used for downstream test if you
know what's going on. So, it's actually quite useful to
check how your parser works, and what types of parse trees it will give
you for sentences that you care about. Both of these representations
can be useful, and no one is particularly correct. In fact, it only depends on what you
want to do with these representations. Dependencies often offer a really
useful representation for trying to reason about language. Again, taking a common sentence that you
might see, Michigan beat Ohio on Saturday, we could ask who did what to whom? If you think about this from
a constituency parsing framework, you'll realize that there are many
different intermediate nodes. And while this may be useful for
say checking the grammatical structure of the sentence, it makes it difficult to
assess who is actually doing what to whom, as we have to traverse this larger graph. However, if you think about this from
a Dependency Parsing point of view, because if there are clear grammatical
relationships that identify the noun subject and the direct object of the verb. Thinking about just this sentence, we
can think about trying to extract binary relationships that form tuples, such as
the down subject or the direct object. And that these tuples form a triple
relationship which forms the basis for many information extraction systems, and the creation of knowledge bases by mining
these tuples from large volumes of text. In fact, for one example,
from Katherine Keith et al, looking at civilians killed by police, we can look at different types of subjects
and objects that occur for verbs. Here looking at the verb receive. We're looking at this, we can actually
identify some of the counts for things. For example, police receive calls,
receive reports, dispatchers receive call,
dispatchers receive tips. All these together point to a strong
thematic relationship that can be uncovered from this type
of information extraction. We can even learn embedding from
this type of syntactic context. So Levy and
Goldberg in the ACL 2014 paper, showed that we can actually think about
dependency tuples as redefining what's in that context window that we looked
at before, for word to veck. Here, thinking about
the subject Michigan beat, we can think about beat as occurring
in the context for Michigan. Or similarly,
as Ohio occurring in the context for beat. Regardless of how far away they
actually appear within the sentence, provided that they have this
grammatical relationships. We can learn embeddings that capture
these types of relationships that end up reflecting more functional roles. So for example, this figure I'm
showing you, whether we use a plus or minus five bag of words window, plus or
minus two bag of words window, or dependency relationships. For that given target word,
you can see that, often the dependency relationships
capture more functional phrases. So for example, in that second row for
Hogwarts, we can see that the bag of words five relationship learn from
traditional word to veck, relates Hogwarts to Dumbledore,
hallows half-blood, mouthful in stape. These are definitely thematically related,
but not actually reflective of
the meaning of Hogwarts. Whereas in the dependency relationship,
the most similar words to Hogwarts are Sunnyvale, column words,
collards, grim, dill minnifield. These reflect more city names, that are likely more reflective
of the meaning of the practice. I should also point out that, Dependency
Parsing is more than just for English. This is a unique and
universal syntactic phenomena, that has been generalized in
many different languages. In fact, there's been a conserved effort
from the natural language processing field of trying to build a universal
set of dependencies. Here, we can think about
what's known as a Treebank. A collection of parse trees reflect
the syntactic structure of a particular language, that we could use
machine learning to train on. This universal dependencies have many
different languages, over 200 currently. And this actually helps us
creating multilingual parsers, that we can use in practice
on arbitrary languages. But also in cross lingual learning,
where we say we learned on English, and translate on Tagalog. And we could actually try to learn
something about the structure of language from a type of logical perspective
by comparing how different sentences in different languages result in
similar different types of parse trees.