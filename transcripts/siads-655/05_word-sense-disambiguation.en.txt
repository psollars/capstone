We've talked a little bit about word senses and you
might be wondering, how do we figure out
which word's sense is present in a given context? This is known as word
sense disambiguation. For the task word
sense disambiguation, we're going to try to identify
which sense is present. So for that earlier
example from Stan Core, we'd want to know that say, recognize a sense four or
recognize a nonsense one. There are typically
two approaches to word sense disambiguation, which is also commonly
abbreviated as WSD. The first is what's known
as an All Words Model. In this case we'll try to identify the senses
of all words jointly. This can essentially be done with respect to some sense
inventory like WordNet, but it could be done with
other sense inventories. The idea being, if we tried
to do all words together, that maybe knowing one word sense can help us decide
another other word sense. However, this is a
challenging task so other approaches have adopted a single-word model where they typically take a particular
word like ''bank'' and try to label just that word looking
at the rest of the context. We'll look at both
approaches today. For word sense disambiguation, I should point out that it's
quite a difficult task. So there are many, many
different types of approaches. There are some that use the
knowledge-based directly, say the WordNet senses are
glosses and try to use these to help identify the different meanings
in a particular context. The benefit of this is
that often it does not involve any type of
Machine Learning and instead relies on the
knowledge-based capturing enough information to
do the disambiguation. Other approaches,
however, have used supervised methods
that essentially adopted traditional
machine learning approach that we've used so far. There's a third type of approach that uses
semi-supervised methods. That given that there's only
a little bit amount of sense labeled data we'll
try to bootstrap from that to do better. We'll talk briefly about some of these approaches as well. Let's look at knowledge
based methods. These are often some of
the earliest methods and servers effective
baselines for comparison. The idea is thample, the hypernyms of words or
the glosses or definitions. We can use this type
of information to try to disambiguate
any given context. So knowledge-based
methods will try to compare whatever
information is in a given context with reference to this information from
the knowledge-base. One famous example of this is what's known as
the Lesk algorithm. So say that we looked at the
sentence: the student got a good tip for a job opening
from the recruiting fair. Lesk will essentially
look for which gloss and definition has the most overlap with the particular context. Here, if we scroll down, we could see that the third definition has an indication of a
potential opportunity. He got a tip from
the stock market, a good lead for a job
and in this case, we notice that job appears in both our context and this
definition in WordNet. So we will relate this
heavily and choose the third sense as the most
likely sense for tip here. In fact, the simplified Lesk Algorithm is
essentially this. You can read about
it in your book, where for each sense of the word, we try to compute
the overlap between the sense signature
and the context. Like if the overlap is greater
than the previous overlap, we assign it to that sense. You might realize, however, that there are some types of contexts and don't
have any overlap. For example, if the ''student got a good tip for the opening from the recruiting fair'' does not include
the word ''job'', then we can't match it. So one approach that you
may be thinking about already is rather than
measuring word overlap, maybe we could look
at cosine similarity between word vectors. Say, the types of word vectors
you looked at in week 2, and to take the sense definition and treat it as an
average word vector. Here we would choose
the sense that has the highest cosine similarity with the context
average word vector. Valerio Basile
proposes back in 2014, and it works quite well
as a generalization of Lesk using distributional
information. There are even all-words knowledge-based methods
that use graphs. Here, we can think about trying
to get all the senses in a particular context and project them onto
the WordNet graph. Here I've shown you an example of the WordNet graph
and you can see that sensors are distributed in
a hierarchical fashion, from entity somewhere
in the middle, all the way out to the
very specific things. The idea being, if
we have a context, we can take all of its senses and highlight which ones are
potentially present. We can then run
personalized PageRank to identify which senses
are important, essentially, which nodes are
most central to the rest, and choose the sense that
has the highest PageRank. This is a quick example, but one that hopefully
helps you think about the different types
of information that's encoded in these
knowledge-bases and sense inventories for supervised word sense
disambiguation, we'll typically treat it again as a supervised
learning problem and we'll try to use the
off-the-shelf techniques that before we talked about. We can think about using HMMs or MEMMs that we've talked about. For all-word methods, we may use a conditional random field, which we briefly mentioned, but which tries to jointly
optimize the output labels. Modern approaches will often use deep learning to try
to do this as well. Here all of them rely
on training data, which is often in short supply. For example, SemCor again
only has 2,000 tokens, and not all the senses of order even covered within
these 2,000 tokens. In fact, many words have none
of their senses present in SemCor due to the fact that it's selected from a particular
type of American English. I will say many approaches
will often try to procedurally generate training data by taking advantage of the
knowledge base itself. There are many other approaches
that build upon this. But to mention one recent
one for Pasini and Navigli. For features, we can think
about using the standard off the shelf features
that we thought about safe for part
of speech tagging. We can think about words that appear in positions
before and after. Syntactic modifiers say whether this word is modified by
a particular adjective. We can even use features like the language model predictions
of the target context. This can maybe give us a sense of what other types of words are unlikely that may influence our decision on what
sense is present. We can also try to
generalize these using distributional
vectors for say, the words of a document or
even the context itself. There are many different
approaches that have been tried and you can probably guess them pretty quickly by working through them. Semi-supervised word
sense disambiguation applies on a slightly
different idea. Here we have an initial set of seed examples that
have been labeled. These could be sentences say, from WordNet or some that
have a specific n-gram. Say "riverbank", which
typically always, in that co-location or bigram, refers to one particular sense. If we collect these altogether, we can use a larger set
of unlabeled context. Where we first train a
supervised classifier on whenever labeled instances we have and then look at what our predictions are
for the unlabeled context. If we take the high
confidence predictions and treat those as labeled examples, we can then repeat step
two over and over, trying to convert, identify
additional examples. We can take a look at this as an example from David Yarowsky from a favorite famous
paper from 1995. Looking at the word plant. Here, we have a visual depiction of a bunch of
unlabeled examples as question marks and we
have two examples of plant: plant life, and
manufacturing plant. But these have all
been labeled, say, from some co-location or bigram. If we treat these as
supervised classifiers and we then run it, we can see that we
discover new instances, say words like employee
or automotive that may help us learn different features that
we can use to generalize. We can keep repeating
this to try to get better and more robust features to allow us to learn
the different meanings, by using the small
amount of data and then treating unlabeled
data that we've classified as ground
truth in the future. This is the basic idea for doing semi-supervised word
sense disambiguation. There are few other simplifying heuristics that people have tried to make this problem
a little less hard. One idea is to think
about that the fact that there's often only one
sense per discourse. Gale famously noted that if a word appears multiple
times in a document, it's usually with the same
meaning or the same sense. For example, articles
about restaurant tips typically also
aren't talking about mountain peaks or
hints that were given. If we keep seeing
many different uses, we could either
pool those together or we could try to do
some prediction, say, labeling all instances
and then taking the majority label or taking the label with
the highest confidence. By relying on this one
sense for discourse, we could even try to combine the context together
to do some type of classification on
the joint context instead of them individually. We can also try to simplify word sense disambiguation
by doing what's known as supersense tagging that
we briefly mentioned on the sequence
classification segment. Here again, is the example, the teens ate superman ice
cream in the quadrangle. Ciarmita and Altun had
proposed that there are only 26 supersenses we might need to label
the different things. Rather than use the 100,000
different senses in WordNet, we could essentially
just label teens as say, a group or superman
has an ice cream. By doing this, we can actually make the problem much simpler, often again to part of speech
tagging and complexity, rather than tabbing
100,000 of labels. Data is often a key limitation. SemCor remains one of
the most common datasets and WordNet remains one of
the most common inventories. However, there are
often new forms of say, heuristically labeled data from the one sec or the
train nomadic group. These often help us generalize to multilingual contexts and to build new classifiers
as a result.