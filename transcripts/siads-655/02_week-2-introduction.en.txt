Welcome back. In week 2, we're going to be looking at
the structure of language. In particular, we're
going to look at what can we learn from large
amounts of text. We're going to look at two
applications that you may already be using that
you don't know yet, language models and
looking at word vectors. In lecture 2, we're
going to look at a common set of methods for NLP. I should point out that many
different techniques for NLP are used throughout and many
different NLP applications. These will build on
classification and prediction. So the techniques that we talk about in week 1 and in week 2, are going to be used throughout. All of these methods
typically use statistics and we'll
start to adopt some of the languages
statistics to try to talk about the inputs and
outputs to our systems. I should point out that
many of the methods that we'll talk about build
upon each other. So some of the applications may be other NLP
applications for doing more different language
understanding or different types of application for helping in
interacting with users. This week will focus on the
word level, specifically. The types of models that
we'll talk about in week 2, have wide applicability in both width in NLP and
for applications. Typically these models will take advantage of having lots of text available say
books, from Tweets, social media, and
any other type of text that has easily
accessible forums. From these unlabeled segments, we can learn a lot about
the structure of language. This week's goal is
to introduce you to different techniques that
take advantage of this data. In particular, language
models and word vectors that are used throughout NLP
for many different tasks. We'll see later, at the end of those segments about
the applications, both within an NLP and how these can be used
directly as well.