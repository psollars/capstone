I'd like to wrap up
week 4 content with a quick aside about a
new NLP technique, interpreted machine learning for natural language processing, which as a practitioner
you may find quite useful. Let's say that for example
you have this sentence, I love it when my
phone battery dies because my charger was
loose the whole time. If you ran let's say through
a blackbox classifier, you might say sarcastic. You'd like to know, why did it choose to say that
this was sarcastic? This can be quite
challenging actually depending on the nature
of the classifier. If it was a logistic
regression classifier, we might be able to
look at the weights but for more advanced
classifiers, how do we actually do that? How do we know if a classifier is making a good prediction? One idea for looking at classifiers prediction is to look at what features
it cares about. It is possible to do this for some like logistic regression, which is one of the things
that we started talking about. Here the Beta vector or it's weights shows you which
features the model is actually waiting
heavily in its decision to decide on a particular class. However, for other more
complicated models, say Random Forest or gradient boosted trees or once you
move to deep learning. This can be more challenging
because often there are many different nonlinear
decisions taking place at once to make that
ultimate classification. For explainable machine learning, this is a new genre machine
learning that tries to get human readable explanations of explaining why a model actually
has made its decision. There are many recent
techniques trying to do this particularly for text, the key ideas for a machine
learning explanation, is one that practitioners like you and me can understand
the explanation. It's not just that neuron
17 has a higher value or the trees and Random Forest, it actually has a branching at the second level that explains something that's
often not useful in practitioners for understanding
why the model did it. We also want explanation that's faithful for with the
model actually does, and something that ideally we can try to compare
against multiple models. The same instance,
we can say that this models explanation makes more sense and we
trust this model more. [inaudible] caveat to this, is that there's actually
lots of discussion for what really makes a
good explanation, and that this is an
open area of discussion that will continue to evolve
over the next few years. The key question here is, who are these
explanations useful for? Do we want explanations that are useful for practitioners or say for end users or even for
machine learning experts? Each of these different end users may have different
purposes with them. You may end up looking for a particular explainable
ML models that match you and what
you want them to do. Let's look at one example for a machine learning system that generates such explanations. This is what's known as LIME, which stands for local interpretable model-agnostic
explanations. Ribeiro at all has
this great paper from 2016 trying to look at this. The key idea is that
we'll start from some classifier that we
don't know anything about. Here, it has some non
linear decision boundary, which I'll show here
in the pink or blue and if a point shows
up somewhere in here, it's an example of an
instance that would be labeled with one of
these two classes, say sarcastic or non sarcastic. We'd like to explain why
the model has chosen to label some particular
point, say some instance, or in our case a text document, tweet as explained why it into labels that
as say sarcastic, which is the pink region here. To do this, they'll create a
local approximation of what the global big model does that ideally is faithful to that
models decision boundary. Some replication of that, although maybe not entirely faithful but one that
is easy to explain. We'll train a simple model that's globally bad but
locally faithful. As you can see here in
the figure on the right, the local line matches the decision boundary
trained to pink and blue, but only really applies near that one point
that we'd like to explain and doesn't
really explain how the model makes decisions
for the rest of the class. However, if that
decision boundary that particular dashed line that
I'm showing is explainable, then we can at least give it explanation for why the model is choosing the blue
versus the pink here. Off late the explainable
machine learning is a very hot topic and there's many new approaches each year, often with lots of
user-friendly libraries. LIME itself has one fast library that you'll actually use
and weak force homework, but also SHAP and ELI5, which is explained
it to me like M5. There are many other
recent models that come out every year and
as a practitioner, you can use these techniques to build trust in
your own models, especially as you move
to more complex models like Random Forest
or deep learning. These types of
explainable ML models can help you understand why it isn't exactly the your models
making some decision. You can use it to test are your features themselves visible. To double-check that the
model's not overfitting to some feature that has
crept into your data, but actually maybe
it's correlated with an outcome variable
but it's not useful. You can use this to
test whether it's looking at the right
things as well, if some end-user wanted to say, why I'm I being
protected this way? You could have some
sort of justification. I encourage you to
take a look at these and to use them in
your own practice. They often have applicability
outside of NLP directly, for NLP they're often quite useful because we can
look at text itself.