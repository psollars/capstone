In the previous segments we talked
about using counts to estimate the word representations. These distributional representations and
meaning. In this segment we'll look at one of the
more recent techniques in the past decade, called word 2vec. Word2vec is really a family of methods for learning dense word
vector representations. It's not actually just one algorithm,
although it's commonly described as such. In fact, we're going to instead move
from counting co-occurrence to actually predicting which words co-occur. In practice word2vec turns distribution
learning into a supervised prediction task. Taking a lot of the unsupervised
messy text data and turning it into something
that is self supervised. Here, we're going to actually try to take
the parameters of the word2vec model and use these as the word representations,
the word vectors. The supervised task itself doesn't matter,
but the representations are very
powerful in practice. We can even start to think about word
2vec as a kind of fuzzy language model. Here we're going to try to train
a classifier on a binary prediction task. Is word one likely to
show up near word two? It's actually, looks sort of like
a language model except a language model. We're trying to predict the next word and
a word2vec model we're actually only trying to predict if
they show up near each other. So, context doesn't matter. But your intuition, if they look the same,
is probably quite close. We actually don't care
about the task itself, whereas we do care about it quite
a bit for language modeling. And in practice,
we're going to take away the task and only care about the parameters that
we learn as a part of this task. These will actually become
our word embeddings. Word2vec is going to be a suite of methods
for trying to learn these word embeddings. There's two distinct modes for
forming the classification task itself. The first, which is known as CBoW or
center bag of words. Try to take the context to
words that appear nearby and predict which target
word was in the center. The second one, which is more
commonly used, is known as skip-gram. From the target word, predict
the context words that appeared nearby. There are many other fancier methods and hyper parameters that allow word2vec
to train itself efficiently. And this is one of the things
that led to its success. One important idea that we'll talk about
particularly today is what's known as negative sampling. Which tries to figure out
how to create examples to make the supervised
classification task work. Let's look at this skip gram algorithm and
try to break down the steps for word2vec to build some intuition. Here, let's look at the example sentence. When the cat landed,
the wooden chair creaked loudly. We'll pick a word in the center or
any arbitrary word and think of this as the target
word in a context. Will then define a context
window around it. Here a plus or minus two word window. So, the wooden and creaked loudly. Both are our target
with our context words. We can think of these as
positive training examples. Where given chair, we would predict the,
we would predict wooden, creaked and loudly. Each of these forms a separate
training instance that ideally, whatever parameters we'll
learn to make this task work. Would end up having chaired predicted
all of these words together. Where do we get the negative samples for
this or the negative instances? Here we can think about
our example as before. We're going to treat the target word and
its neighboring context. Where it's just positive examples that
we'd like to predict when trying to learn the task. Here the X and Ys. We're then going to randomly sample
other words that are lexicon, our whole vocabulary to
get the negative samples. Here we might sample virus,
information, tire, or yell. In these cases, we will try to just learn
parameters where chair would correctly predict wooden, creaked, loudly. But not predict virus,
information, tire or yell. We're going to use a feedforward
neural network to train a classifier. To distinguish between these two cases. And in this practice, we'll actually use the weights of the
neural network itself as the word vectors. Let's look at how this neural
network works at a very high level. Given a tuple of a target and
a context word, this could either be a positive example or
a negative example. We're going to try to return
the probability whether the context word is the real context word. How might we do this? You might have some intuition. Well, if we have some sort
of representation for the target in the context word, maybe
we could look at the cosine similarity. If the cosine similarity is high,
well maybe it's a correct context word. If it's low, it's not. One of the problems with it though, is that cosine similarity
isn't a probability. We need to have a probability to estimate
whether the context word is the real context word. So, an idea in practice is to use
what's known as the sigmoid function. Which we can use to the dot
product between two vectors, which is the unnormalized. Cosine similarity and
to feed through the sigmoid, you could think of the sigmoid
as a squashing function. It takes any value and
squashes it to between 0 and 1. Let's look at this in practice. Again, we'll have two dimensions
that represent meaning. We'll have our example
doggy from before an doggo. We then might have another vector for
say payment. The cosine similarity between doggy and doggo has a small inner product whereas
we have a large cosine distance or small cosine similarity between doggo and
payment. If we pass through the sigmoid, we might
get a value close to 0.96, reflecting that they're very similar or a value close
to 0 reflecting their very dissimilar. So we can use the inner product, passing
it to the sigmoid to estimate whether the probability of this context
word is a real context word. Let's take a look at how our
neural network will model this. Here we're going to be given
a couple a target in a context word. We want to output the probability that
the context word actually appeared in the context of the target. As input will receive the tuple, and in this case we want to predict one that
creak did occur in the context to chair. On the input side of the neural network,
I'm going to show you three neurons. These will reflect our three
vocabulary words that we only have. In practice they'll be many more neurons
reflecting all the potential target words. Similarly, on the output side, I'm only
showing you three vocabulary words, chair creak, tire, but in practice this
would expand the entire vocabulary to reflect all the different context words. So if you think about what our input is,
it's a vector reflecting which target word occured, and a y vector
reflecting which output we want. In this case, Creak occured. In the middle of the neural network
will have one hidden layer. This will be fully connected,
reflecting the weights from each of the input words to the hidden layer
will often called the W matrix. You can think of the W matrix as a series
of two dimensional vectors since we have two neurons in our hidden layer. Similarly will have a see matrix
that connects the hidden layer to the output vocabulary. And in this case this
also has 2 dimensions. In practice, the W matrix itself
becomes the vector for chair, so that as we train this network to
predict from chair that creak was the correct output word that
appeared in the context. As these matrix values get updated, we learn a vector that
represents the meaning of chair. So here to walk you through the example,
we activate the neuron for chair, this then activates the neurons in
the middle and our hidden layer, and then these are multiplied again by the C
matrix to predict the output activation. If we multiply these together through
our sigmoid matrix we end up with the probability of 0.82. That yes creak was in the context
of the target word chair. As a process of learning this network,
we would update the rates so that these values will become closer to 1. The output sigmoid would
become closer to 1. And as part of that we would end up
learning a representation for chair. Let's review a high level summary of
the word2vec, skip-gram learning process. Here we're going to use
a shallow one layer neural network to try to train a supervised task. The task revolves predicting whether
a word appeared in the context of the target word. This is like just what we saw before for wether creak was a correct context
word for the target word chair. As a part of learning the supervised task,
we're going to use stochastic gradient descent to update
the weights of the network, iteratively switching
between true positive and true negative examples of words that
appeared in the context of a target. After training,
will use the weights of the word and the W matrix as its embedding, and
this is its distributional representation. Why does this even work? It does seem somewhat like magic that
this should even work in practice. Let's think about a few examples here. Here I centered them on 2 words, chair and
table showing the context around them. We can see that there are many different
words shared in the context between the two. We have wood in here, but there are other
words that seem similar like moved. In practice, as we learn
the representations for both chair and table, the vectors for chair and
table have to become similar in order to predict the shared words
that appear in both of their contexts. In practice,
by learning this as a prediction task, we forced the model to learn similar
representations to match the words that appear in similar types of context. Word2vec in practice is actually a quite
complicated suite of software, and there are many hyperparameters
not covered here. They actually multiple
software implementations. There's actually the original
software which was written in C, which is very fast and
can be run from your command line. There's also a robust Python
implementation in the Gensim package. Finally, there are other GPU-based
implementations if you want to use deep learning which can significantly
speed up your approach. However, there are many pre-trained
embeddings that you can download yourself and these are trained over massive
corpora with billions of words. One of the reasons that word2VEC has
been successful in practice is because this work has been done for you and you can simply download the embeddings
to use them in your applications. In practice the word2vec
embeddings actually make for better representations of meaning,
then count-based vectors which have been shown in many downstream tasks
that will talk about next.