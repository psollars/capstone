In the past few segments, we talked about how to
train language models, but we haven't yet talked about how to evaluate their quality. In this segment we'll
think about how to evaluate language models. How do we actually know if we've learned a good model of language? Well, to evaluate language
models we'll follow actually a similar setup that we'd use for
machine learning. We'll use test development
and train sets to evaluate quality. There are several different
types of evaluations; one of which is an intrinsic test of a language model to see how well it captures the probability of seeing particular sequences. There's also more common extrinsic tests
of language models, but see how they
impact the performance of downstream tasks. Let's take a look
of both of these. First, when trying to train
our language model we'll use the standard machine learning
setup where we're going to train on typically 80
percent of the data. We'll fit the language
model on this data, and then we'll use
the development data, a 10 percent held-out
sample to help figure out which hyperparameters
we should choose. For example, when
you want to choose how many words will be in
our vocabulary, the set V, or which times of lined up
parameters we should use for interprating between
language models or even how many n-grams
we should use? The development and
data can help us decide which of these to use by measuring its impact on each of the two evaluation tests
we'll describe next. Finally, we'll use the dev
and test data to see how good the language model
is for identifying the probabilities of
unseen sequences. Potentially, these
sequences include words that we
haven't seen before, so these are critical for testing how well our model
generalizes to new data. For an extrinsic test of data, we use what is known
as perplexity. Perplexity you can
think of as a way of measuring how surprised
is the model when it sees unseen sequences ones that it hasn't seen before
in the training data. Ideally, the model should be unsurprised by these and give them a regular probability
that would be expected. In practice, perplexity is
the inverse probability of the test data normalized by the test probability length. You could see within
the square root, we'll use the
probability estimates from the language
models directly. The way that the probability is calculated depends
on its formulation. For example, if we use a
bigram then we would estimate the probability by conditioning it just on the previous word. Perplexity can only be compared between models tested
on the same data set. A perplexity score
of say six could be good on some data sets
and terrible on another's. We would only use
perplexity for deciding between two models when
testing on the same data set. Typically, we'll only use perplexity for
model selection and itself is not really
a good metric for thinking about how
good a model is, only how much it captures the probability estimates
of the training data. A more common evaluation is to look at extrinsic evaluations. Here we'll try to embed
the language model or two language
models to see whether they benefit some
downstream task. As an example, we could
think about spell checking. Here on the right, I've
shown you an example of a spell checking where it says "This is an example
of spell checking. However, we'd like to estimate whether the
language model could be used for correctly correcting the
spelling of checking. We'd like to know if we use two different language models, which of them is better
able to just say predict the right
word in that place? We could use this as an
extrinsic test to evaluate, but there are many
other possibilities. For example, in
machine translation and search query suggestions, things that we may run
into on a daily basis. We could even think
about language models in speech recognition as to whether they can correctly assign the
probabilities of words.