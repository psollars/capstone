Hello and welcome to
our lecture on how to use pandas and SQL together. Really the first thing
I want to talk about is why even use SQL and pandas. Now, I want to say up front
that I'm not a pandas expert. My main research inquiry is building online educational
technology systems, and you've been
actually experiencing some of the systems
that I've built. I hope that you've
liked them so far. So my goal in life is more about writing large databases that
go really fast, and you've probably seen
that theme throughout. And so some of you know a lot
more pandas than I do, and so you rightfully
would say, "Well, just I've been using
pandas and it's great. I just, why not use it?" And there's a lot of things
in pandas that are very database-like, as
we're going to see. Sometimes your database
just doesn't fit. You have a 16-Gig computer, or an 8-Gig computer, or perhaps you're
using cloud resources, and it's expensive to
have a 32-Gig computer. And if the data is in
a database you can do your pandas on a
1-Gig computer perhaps. Other times, perhaps this is
is even more common, is that your data is live. It's happening. It's
happening right now. It's a log, set of log files, or a time series database, it's the last 24 hours and
nothing else is in it, and you have no other choice. And so one of the things about pandas is pandas primarily stores
its data in memory. But depending on the
size of your data, pandas use of memory is either really a
tremendous advantage, and it's faster than a
database in most situations, or it becomes a disadvantage. And so we can think of
this as there's some range of easy to impossible, and somewhere between easy
and impossible is difficult. And so you can think of anything under a 100 Meg is really easy, and 20 Gig is going to be
pretty much impossible. And so some in between there is an area that's kind of just difficult. Is it hard, it's difficult. And there are techniques that you
can use inside of pandas for what some people call medium-sized data.
There's small data, there's medium-sized data,
and there's large data. And that's data that almost fits or is pretty
big but it fits, and pandas is pretty smart. It thinks like a main
memory database, but it takes advantage of things that you can do in
a memory database like links and in
regular databases, there was a theory
that we would have network style where you'd
go to one place on disk, then it would point you
to another place on disk, and you'd follow
chains of blocks. But that just turned out
not to be such a good idea, because with good indexes
that tend to live in memory, we can bounce right
to the disk in two to maybe three disk hits. Whereas a network disk, you might get link, link, link, link, and you
don't know how long those links are going to be. In memory, it turns out
those links are super fast, and as long as it's
all in memory, just bouncing through the
memory is really fast. One thing you can do
is as your data gets closer and closer to too
large for your memory, is you can add a bit of a schema to the
columns in pandas. Probably the best
one is category, where you might have
a series of states. And so you've got this string that has no more than 52 possible values, that is a category type. It's vertical replication, it is exactly the same as normalizing
data and pulling it out and putting it into
a small side table. And then pointing to
that with a number, and I'm going to guess that for situations where categories, the total number of different values and
categories is small, categories is probably even
better than a database. And so category is a wonderful way
to squeeze your data down. datetime is really valuable. Databases do the same thing. The idea is that
you might have a 4, 5, 6, 7, 8, 9, 10 a 10-character string
representing a date. Well, if you squeeze
that down to a number, which is the number
of seconds since 1970 or something else, kind of the datetime, and it's the same information
as long as it's like a date, you can save a lot of space. You can make your integer small if you know in advance that you have less than 2 billion is the range within
those integers. And if you're doing
floating point numbers and you're willing to tolerate
seven digits of accuracy, then you can say, "I don't
really need double precision, which is 64 bits, I can use float32." Now, there's a couple
of blog posts here, and I recommend that you
take a look at them. There's one blog post. This only gets you so far, right? if you have a 16-Gig laptop, and your 16-Gig
laptop is almost working or running
a little slower than you'd like,
but it's working, well, these kinds of almost data compression make more efficient use of the
memory that you already have. And then if your data
gets a little bigger, then you've used these techniques and there's not another set of techniques to squeeze like some larger medium stuff
down to medium-medium stuff, and so there are times
that you can do this. The other thing that you can do is use disk on your computer. And there is a built-in mechanism inside of pandas called HDFStore, and it basically allows
you to take chunks or segments of a DataFrame
and move them in and out. And as you start to think about that, that's not so bad if you're
just scanning a lot of data, but if you're sort of like
correlating between things, then you kind of have to page
all the things in. So you've got one thing
and you want to look up its friends, you might have to. And so I think this is a bad idea. Or you may work in a place that
does this all the time, and if you do find
someone using it, I'm sure they thought very
carefully how to do it. It kind of has the advantage that it's working on your local computer and you don't have to
install a whole database, which itself is a bit of work. And so I personally wouldn't lean toward this because with SQL so easy and so close, and so scalable,
SQL is very nice. So I'm not going to show you how to do HDFStore
because you really have to build clever
algorithms to manually pull stuff in and out at the time that you need it, and you have to turn things
that were otherwise real simple, to do this with a DataFrame you have to say, "Oh, grab the pieces and
bring them in and do it, and then somehow merge
those pieces together." So in compare and contrast that pandas with
databases in general, the one thing that makes
me love databases, and I hope makes
you love databases, is the fact that everything
about data architecture is not about squeezing
it into the memory, squeezing all your
data into the memory. We optimize based on the fact that data is going
to be far larger than memory, far larger than memory. And to the extent that we don't
even need the memory, technically, we really build algorithms and databases
and structures and techniques and databases assuming literally everything is on disk, including indexes. Meaning if you want to look
something up by a key, you hit the index. That
costs you one disk hit. You might have hit
the index twice, but then you go
right to the data, and that's three disk hits. Oh and by the way, if you
can cache the index, it's only one disk hit. And so databases don't think of memory as where
the data lives, databases think of
memory as a place to cache the on-disk stuff
for commonly used things, and it's very good at that. And so it's very good at keeping the right things
in cache and using it. So you can have a 20-terabyte database on
a really small computer. It might hit disk
a little bit more, and then if you make
it a little bigger, it'll hit disk less. But all the algorithms
are designed for disk, and I think we talked about that. In normalization, we
know how to do it, we are naturally compressing it by the concept of categories. We call them lookup
tables in databases. There's a wide range of
wonderful index technologies, hashing, B-trees,
inverted indexes, etc. And things like
filtering and aggregating and sorting quickly, that's what databases
are designed to do. And that's not to say that pandas can't do it if your
data is small, but you really don't
want to write a out a memory sort yourself, whereas a database just does
that without even trying. It's a beautiful thing. And so sometimes in the real world you're just handed an SQL
connection to your data, right? It might be too
large to download. It might be a read replica
of a production system. You say like your job is to sit and stare at
this read replica, figure out activity, come up
with algorithms to analyze activity in this read replica
of our production system. You might even be
pointed at a data lake like Amazon Redshift
which we've talked about, which is based on Postgres and gives you an SQL interface. Or you might have something that you're checking
every morning, which is like a
time window database, where you just have
the last 24 hours of some real high-volume data, but like after 24 hours it's gone, and there's no way that you
can put it on your laptop. And often this database has
been set up by somebody else, it's been optimized, there's structure, it's
got normalization, it's carefully put together, and you must like wander in
and explore its shape and then come up with strategies to pull stuff out of it and
then makes sense of it. And that's part of the
reason I've spent so much time in this class of like look at the table,
look at the schema, do a couple JOINs, type it by hand, explore it. The exploratory SQL, when
you're just sitting at a command line, is a
great way to learn how to wander around
and come up with a strategy to do data mining
on an existing SQL database. Another reason that's
really cool compared to, say, the HDFStore in pandas is that you have so
many SQL clients. We've played a lot with the command line psql,
the desktop application pgAdmin, which I haven't used but I've shown you the
PHP web application. Those are just all clients. They just, "here's a database, it talks SQL, here's
a bunch of clients." And then pandas then is
just another client. So you can actually
run a pandas thing, and while it's
running, you can be sitting in a terminal window, and you can be looking at what's going on in your database, right? Or planning your next move, and not have to just explore
it in pandas all the time. And so that's one of
the advantages of a database is that
you can talk to it. It's a multi-user
system and you can talk to it from many different points. So up next, I'm going to
talk a little bit about the actual technique of
connecting SQL and pandas. [MUSIC]