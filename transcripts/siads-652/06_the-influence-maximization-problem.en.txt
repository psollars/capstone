Hello, today we're going to be talking
about the influence maximization problem. And what this problem is about is
given a network in a diffusion model, can we maximize the number of infected
nodes at the end of the diffusion process? So why would we ever want to maximize
the number of infected nodes in a diffusion process? Well, there are actually many applications
on which we want to do exactly that. For example, in marketing business may
provide a few free samples of a product to a few people in hopes that they
will tell their friends and their friends will tell their friends. And the word about the product will
kind of spread through the network. And in this case,
what business is one is exactly for the set of people who are infected
with the knowledge or the appreciation for
the product to be maximized. Another example is in political campaigns. Political campaigns may recruit a few
volunteers to help the campaign and they might choose these
volunteers strategically, so that the word about the candidate
spreads really widely. So how can businesses and political
campaigns and people who want to do this determine who to give the free products
to or how to choose the volunteers. During the network, how do I pick the
nodes that I want to sort of activate so that the size of the set that is
eventually infected is very, very large? That's what the influence
maximization problem is about. Let's talk about the problems
definition in a more formal way. So first of all, imagine that we're
trying to do this in the context of the independent cascade model. So you have an arbitrary network with
the corresponding weights on the edges, something like this. And we're going to let a knot or
a zero be the set of signals. So these are the nodes that are infected
in the beginning without them getting the infection from someone
else into network. So you can think of these as the people
who were handed out the product or the people who were chosen as
volunteers for the campaign. And what we're going to do is we're
going to let sigma of A0 be the expected number of nodes that are infected
after the cascade ends. So you can think of this sigma as
a function ,that takes in a set and that set is the set of initial adopters
of the product or the set of seed nodes. And then it outputs the expected
number of nodes that are infected when the cascade ends. And one thing to keep in mind is that here
we are talking about the expected number of nodes, not the actual number of nodes, because the independent cascade
model is not deterministic, right? So even if we fix the set of seed nodes,
if we run the cascade multiple times we may get a different number
of infected nodes each time. And so sigma cannot tell us exactly
the number of nodes that will be infected, because that number will vary, but instead we'll talk about the expected
number of nodes that will be infected. So the influence maximization problem
says giving a budget of keynotes, find a set A0 of seed notes of size k,
such that sigma of A0 is maximized. So our goal is to maximize
this set function sigma, which takes in the set of seed nodes and outputs the expected number of infected
nodes and we have a budget of k. So our said that we choose A0
must be of size at most k. We cannot get anything larger than that. One problem with this influence
maximization problem is that we can show it's NP hard, at least for
the independent cascade model. So we can't expect to solve it exactly. It would be too computationally
expensive to do so. The good news is that there are
approximation techniques that we can use for the problem. So while we cannot expect to find
the exact A0 that maximizes sigma, we can expect to find an A0 that
gets close to the optimal one. And the main goal for this video is to
show you what that approximation algorithm would look like and why the sigma
function in the influence maximization problem actually can be approximated
by this particular algorithm. The first thing we need to do is we
need to talk about a set of properties that sigma must meet in order
to be a good candidate for the type of approximation algorithm that
we have in mind to solve the problem. And the key properties that
we need are the following. So we're going to claim that the sigma
function is non-negative, but it's monotonic and that it's a modular for
the independent cascade model. Now, two of these properties
are pretty straightforward and one will take a little bit more work. So let's start with the easy ones. First one is non negativity, we have
to have that sigma is non negative. But this is pretty easy since sigma maps
a set of seed nodes to the expected number of infected nodes, and
we cannot have negative infected nodes. So it is very clear that whatever that
number is, it's not going to be negative. So non negativity is pretty easy. Monotonicity says that if you have two
sets A1 and A2 where A1 is a subset of A2, then you must have that sigma of A1 is
less than or equal than sigma of A2. And this is also pretty straightforward, if we think about what
sigma represents here. Sigma represents the number of
infected notes that you'll have at the end after you use the input
set as a set of seed nodes. So what this says is that if you
take a large set of seed nodes, let's say A2, and
you check how many nodes get infected, and now you take a subset
of that larger set A2. So you take potentially fewer seed
nodes then the number of infected nodes is only going to get smaller. Or it may stay the same, but
it's not going to get larger. So it's going to stay the same or
get smaller, but not get larger. And so this value here is going to be
at least as large as this value here. And that is also, pretty straightforward
just from the definition of sigma in the context of
the independent cascade model. If you take a larger set of seed notes, you get potentially a larger
cascade at the end. The one that will take more
work is sub modularity. So first, let me tell you what
sub modularity even means. So a non negative monotone
set function f is said to be sub modular if for
all elements v in all pairs of sets S, T such that S is a subset of T we
have the following inequality holds. Now this inequality might seem
a little confusing or scary, but let's try to break it down so
we can explain what it's saying. And what the saying is that if we
look at the increase in value of f to when the input is this
set S plus some element v and we compare that to the increase in
the value of f, from when the input is the said T, to when the input
is the said T plus the element v. The increase is larger when we add
the element to the smaller set S, which is a subset of T, right? In other words,
adding the element v to the superset T provides us You lower gain than
adding it to the subset S. Let's look at a specific example
to make things a bit clearer. So let's imagine that we have
subset T that looks like this. It has all these letters in it. And now we have a set as
that is a subset of that. So the set S just has the letters V and
F, and we have some element h
outside of both sets. What the sub modularity property
says is that If we look at the gained utility from adding
an element to set S, and we're thinking of f here as measuring
some type of utility from this set. Again, this function, f, is monotonic. So it always gets larger in value
as we Increase the input sale. So how does it increase as
we add this h to the set df? So here comparing what is the value
of f when is just d,f that is the input to what id the value of f
when the input is d,f plus now we added h? And we are going to compare this
gain to the case where we're adding the element to the super set T,
to the larger set T. So here we have the value of the function
when the input is the entire set T and here now we've added h
to that larger set T. And we're in what the property
says is that the gain is larger when you add h to
the smaller set than when you add into the larger set that it that
contains that smaller set S. In other words, we can think of F as
sort of having diminishing returns. So if you look at the return that you
get from adding a new element when the set is smaller. That's larger than when
the said got larger, and now you're adding that element
you get diminishing returns for adding that element to the larger set t. So what that's what the sub
modularity property means Now we have to show that our sigma
function is indeed sub modular. The problem here is that, if you remember
the Sigma function is this expected value. And the reason why is
an expected value again is that, if we look at the independent
cascade model, it has all of these probabilities
attached to the edges. And so each time we run it,
we get a different number. And so what we're going to do is we're
first going to consider just one instance of the independent cascade model. And consider a sigma function for
that one instance. And then we're going to generalize
it to the expected value. And furthermore, we're going to think of that one instance
of the independent cascade model. In a different but equivalent way than what we described
when we first introduced that model. And the key difference is going to
be that rather than deciding whether an infected node will infect its neighbor
whenever the node gets infected. We're going to in advance decide
whether any of the edges in the network will actually
transmit the infection. In other words, when we first
introduced the model we said okay. As soon as the node gets infected,
we're going to look at the neighbors and we're going to look at the edges that
are connected to that infected node. And we're going to flip a coin with
whatever probability that it has, and we're going to determine whether
the infection is transmitted or not. What we're going to do now is
we're not going to wait until we need to know whether a particular
edge is going to transmit the infection. We're going to in advance, pre determine whether all of the edges are
going to transmit the infection or not. And the argument is that
we actually do not lose or change anything by making
this decision in advance. We, making the decision in advance
doesn't change the model in any way. So if we're looking at
the network like this, we're going to flip all
the coins in advance. And some of these edges are going to
be dead or alive and the probabilities are going to be following whatever
the weight of the edge is. And so since we've already
used this probabilities, and we've already decided which
way they're going to go. Then we can count probabilities. And now, all we need to know is which
edges are dead and which edges are alive. We will call the specific outcome
of live and dead edges X. And again, one thing to note is that,
if I rerun this process, if I reflip all of the coins, I would get
a different set of that are no live edges. So it's important to keep in mind that
this particular outcome is just one of many possible outcomes. And we're going to call
this particular outcome x. And now given a set of seed nodes a knot,
we're going to let sigma sub x of A not be the number of nodes that will
be infected under the specific outcome x. In this case we are talking about the
expected number of nodes because the set of infected nodes after I have all ready
decided on which edges are dead and which edges are alive. It's not going to change from simulation
to simulation if I don't change my set of seed nodes. Now note that the sigma sub x is
simply going to be the size of the set of nodes that can reach any node in
a knot through a path of live edges. If I want to know whether a notice
infected under this particular outcome x. All I need to check is Whether
there is a live path or a path of life edges that takes me from
any node in a nod to that particular note. Let's look at an example. So let's say that the set of
seed nodes is just the node D. So D is the infected
node in the beginning. Now we need to assign which
other nodes get infected and what is the value of sigma x? Well, in the way that we
explained the model before, what we would do is we would
look at the neighbors of D. Let's say in this case C and G. And we would decide whether they get
infected and that would be based on the probabilities attach to the edges
that connect D to C and D to G. But in this case, we've already pre determined that
these two edges are actually alive so they do transmit the, the C's and
so C and G will get infected. And then we'll kind of
continue that process. Right so C and G get infected. And the next step would be to ask, well,
this G transmit the infection to F or not. And in this case it does because we've
determined that this H here is alive. So F get infected, whereas for example,
see here who also got infected, we would be asking,
does it transmit the disease to be an age. And we would be doing it based
on the probabilities before but now we've pre determined that
actually these two edges are dead. So, B and H do not get infected and
so on, right? And so if I need to know whether, for
example, he gets infected, all I need to check is whether there's a path of live
edges that takes me there from D right. And in this case,
there is we can go D, G, F, E, He doesn't get infected
because this edge is dead. And another thing to note is that there
is this live edge A to B which is live, but we never needed to use it. Because a never got infected so
we never needed to flip this coin But we don't change anything by having flipped
it right we can just ignore it now. So this way of running the model
by pre flipping the coins Doesn't change anything about
the dynamics of the model itself. And the answer here is that
the set of affected notes is c d, e, f g, which is which has size five. So sigma x is five in this case. Okay, so now what we are going
to do is we're going to show that the function sigma x is submodular. So we need to consider two
sets of seed notes A1 and A2 where A1 is a subset of A2 and
what we must show is that for any node v, we have the inequality that comes from
the definition of sub modularity. So, let's see how we can argue
that this inequality holds. When sigma x has the meaning in the
context of the independent cascade model for this particular outcome x, let's first look at the right hand
side of this inequality here. So this difference measures the increase
in the number of nodes that were infected only after adding node V. To the set a two, but not when
a two alone was the set of signals. Right.
So here we have the number of nodes that were infected when a two
was a set of seed nodes. And here we have the set of nodes that
were infected when a to Union v was a set of seed notes. So it's measuring the difference. Between these two cases when a two
was the sort of seed nodes when versus when a two plus the set
B was the surface he knows. Now, the argument is that a node can
only contribute to this difference if it has a life path from V but
not alive path from any node in a two. Right? And that's because if it had a path from
a node and a two, it would be counted in both terms and it would not add
anything to the difference, right. So if there is a live path from any
node in a tube to that part to some hypothetical note then that
hypothetical node would be contributing to sigma both here and here. And so for
it to counter a sigma here but not here, it needs to have a live path from V but not from A to otherwise it
gets counted in both places. So it looks something like this. We have a two here and
we have the node v here. And the argument is that any node
that contributes to the right hand side of this inequality
must have a life path. From you, but does not have
a live path from any node in a to this diagram here doesn't
mean that there is an actual edge directly from VTU, but
there is a path, right. Potentially of many intermediary notes
here, a life path from the, to you. Now, the next argument is that
since a one is a subset of a two then any of such nodes that you
that have a life path from V but not a life path from A to do not have
a life path from any node in a one And we already said that they have
a life path from a node and V. So they have a life path from
a node in A1 union V, right? So A1 is a subset of A2,
so it's right here. And so since we already know
that there is no life path from A2 to V then there must also not be a life
bathroom and you notice A one two V. And we already know that
there is a path from V to U. Now if we look at the left hand side,
then the kinds of nodes that would contribute to this side would be
exactly those that don't have a life path from A one to me but do have
A life path from V to U In other words, any node that contributes
to this difference, on the right also contributes
to this difference on the left. And therefore the difference on
the left must be at least as large as the difference on their right. And therefore we have the inequality for
some modularity and so this function sigma x is submodular. So we have shown that
sigma x is submodular but our goal was to show that sigma
is submodular not sigma x. We kind of cheated a little
bit by fixing the outcome. So we now have to fix that. So we have shown that after choosing
a particular outcome x of live and dead edges, the sigma x is a modular,
that's great. But what we really need to show
is that sigma is sub modular and this one maps to set of seed nodes
to the expected number of infected nodes without fixing the outcome. And so we're not quite there, but
it turns out we're pretty much there because the sigma function
is just a linear combination of the stigma X functions is
the expected value, right? So it's just the sum or
the probability times the sigma X. Functions. And the good news is that a linear
combination of a non negative submodular function is also submodular. So actually, we have done all the hard work to show
that sigma is indeed sub modular. Okay, so now we have shown the three
properties that I said we're going to be important for our sigma function to be
able to be approximated by some algorithm, let me tell you now what
that algorithm will be. And the algorithm will
be a greedy algorithm. And so modularity is the main
ingredient we need it. So let me tell you first
what this algorithm is. So the greedy algorithm says
during my date of Casey notes sequentially construct
a set of seed nodes, a zero by adding nodes one at a time,
such that we always are looking for the node that gives us the largest
increase in the sigma function. So what we do is we first,
the empty set and then we're going to choose the first
node to add to the empty set. And the way we choose that first node is
we ask which node by itself would give us the largest cascade. And the way we're going to figure that
out is we're going to run a bunch of simulations on this network. Where we allow every note to kind of take
a turn in being the seed node, and then we take the one that gives us the largest one
on average, we run multiple simulations for each of the nodes, and we find
the one that gives us the best outcome. And so we add that one and that one's in
the set and the stain is that forever, then it's time to add the second
node into the second node. We asked. Well, what is the best possible
second node to add to the set? And so we try out every single
node to add to that set and ask which one gives us
the biggest gain in. And the number of infected notes, and we
add that one again, we use simulations for this process and we continue this way
until we have constructed a set of keynotes,and this is our approximation for
the best possible set of notes. Of course, this algorithm. Doesn't always work in the sense
that it's not always very close to the optimal answer, right. There might be some other clever ways
of constructing this set of seed nodes. That does not mean and involve just
sort of checking which one works one at a time and constructing
it that way in the greedy way. But the good news is that we have a
theorem that tells us that if we're trying to maximize a function and
that function has certain properties, then the greedy algorithm
actually does a pretty good job. So the theorem says, for a non negative
monotone modular set function f. Let S be the set of size k
obtained from a greedy algorithm. So, as is the degree
approximation to optimize this function f and
let s star be the set that actually maximizes the value of f
over all k element sets. This is S star is the one that we
cannot get because the problem is and be hard, but let's imagine we have it and
give it a name as star. What the theorem says is that if
f has all of these properties, then the value of f,
when you input the approximation set S is going to be larger than or
equal than one minus one over e times. The value of f when we input
the actual optimal set. In other words, the value of f
in the approximation is going to be 63% of the value of f
when you get the optimal set. So it provides a solution with a
performance of at least 63% from optimal. And because we have done all the hard work
of showing that our signal function meets all of these properties. It's non negative is monotone so
modular for the independent cascade model, then we know that our greedy
algorithm provides 63% approximation. In optimizing this signal function,
so it's pretty good. It's not the optimal set of seed nodes,
but it's one that gets us close to optimal,
at least somewhere there. 63%. Okay, that works for
the independent cascade model. But what about other models? So we'll consider the influence
maximization problem and there say the linear threshold model and
recall the linear threshold model is different from the independent
cascade model. Here you have nodes have some thresholds
and the edges have some weights and you have to check whether the neighbors
of a node meet the threshold of that node before the node
gets infected and so on. Right?
So it's different than the independent cascade model. So can we apply a similar approach in
order to do influence maximization here so if we also want to find the set
of seed nodes that maximizes. The number of infected notes at the end,
does the greedy algorithm work well here, we'd have to show that
the sigma function is also so modular non-negative and
monotone and all of that. And the answer is yes,
we can actually show all those things for the Sigma function in
the linear threshold model. Am I going to go through that? But you can see it in this 2003 paper
that actually does a really good job at. Also explaining the entire
influence maximization problem as well as the many of the details
behind all of these ideas. So this is a good paper
that you should read. And in the next video, what we're going to do is we're
going to talk about heuristics. So, as you can see here, one of
the things I kind of ignored is that, the way that we construct a set of
cenotes under the grid algorithm is by running simulations and running
those simulations can be expensive. And so even though the grid
algorithm has this guarantee, it can be itself expensive to run. And so can we use heuristics to
solve the problem in different ways? We're also going to look at cases where we
actually don't have full information or any information about the network. And imagine you wanted to still do this
kind of thing of finding the seed nodes, but you don't have information
about the network. What do you do in that case? That's what the next video will be about. So I'll see you there.