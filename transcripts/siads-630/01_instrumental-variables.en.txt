In this video, I'm going
to introduce you to the method of
instrumental variables. With this method is used to address selection
on unobservables. Now, before we begin, I wanted to briefly
mention the backstory of this method because
it's quite fascinating. Instrumental variables
was invented about a 100 years ago by a
father and son duo, Philip and Sewall Wright. Philip was an economist and his son was a genetic
statisticians, worked on causal models. In 1928, Philip wrote a book on the economics of animal
and vegetable oils. In the appendix of the book, there was a section where instrumental variables was
worked out before a long time, it was unclear whether it was the father or the son
who wrote the appendix. Now, recent analysis of the writing styles revealed that the father was
indeed the author. However, he certainly benefited from the discussions
he had with his son. So let's start now with
a motivating example. Suppose you run a subscription-based
sports journalism app where users can read in-depth sports coverage and
comment on articles. You see that users who refer more friends also
read more articles. So you want to know whether
referring a friend has a causal impact on the level
of engagement with the app. Referrals of good for business as they bring new customers, but referrals may also influence those who
make the referrals. So why might we suspect a causal relationship
between referring a friend and reading
more articles? Well, according to
a recent study, people care more about
products that they have invested time
contributing to. This is what psychologists
call the IKEA effect. So it could be that contributing
to the app's success makes users more likely to stick around and read more articles. So here's the correlation that
led you to speculate that the users who refer more friends
also read more articles. Now, does the
positive relationship imply a causal effect of referrals on engagement
with the app? Maybe not. Users who refer their friends are probably more
committed to the app. So this reverses the
chain of causality. Another potential issue
here is selection bias. So it could be that
users who refer more friends have
more friends due to their open-minded personality, and that is personality
trait is also linked to the likelihood
of reading articles. So the positive
correlation market just picking up the effect of
an unobserved confounder, having a curious or
open-minded personality. As a result, if we
run a regression of the number of articles read on the number of referrals, we will likely overestimate
the causal effect. Now, we could control for this confounding the regression, but the problem is that
we often don't have the full set of confounders
in our dataset. This is especially true when confounders are difficult
to obtain or quantify, such as users personality traits. Moreover, there could be many other unobserved
confounders. I picked just one. So for now, suppose we had data on user's
personality traits. So we may find that
open-mindedness is indeed positively related
to the number of referrals, and that having an
outgoing personality is positively linked with
the number of articles read. Now ideally, we would
run an experiment that manipulates whether
uses refer friends, but the problem is that we cannot force people to refer friends. So what can we do here? This is where IV comes into play. We have a situation where there is unmeasured confounding, that is the treatment is endogenous rather than
randomly assigned. In other words, people choose whether to
get treated or not. So in our example, the treatment is
referring a friend and the possible confounder is having an open-minded
personality. So users who are
more open-minded may be more likely to
refer their friends, but maybe we can find or create a third variable z called the instrument that affects
the treatment variable, but that doesn't have a
direct effect on the outcome. So the variation in the treatment caused
by the instrument would help us identify the causal effect of the
treatment on our outcome. So here's a simple causal graph to illustrate the situation. What do we want to know? We want to know the
causal effect of D on Y. The causal effect of the
treatment on the outcome. Now there are
pre-treatment variables U, that are possible confounders, so we can just look at the
correlation between D and Y. As you can see, U
affects both D and Y. Moreover, you might be
unobserved by us that is, we cannot control for it
in a regression analysis. So what we can do is to look for an instrument Z that affects Y through changes in D. What are the requirements for
the instrument Z? Well, first, we
need the instrument to cause changes
in the treatment. This causal effect is
called the first stage. So we need a strong first stage. Second, we need the instrument
to be randomly assigned, or at least as good
as randomly assigned. In other words, we don't want the instrument to influence
confounders or vice versa. Otherwise, we might
observe a correlation between Z and Y that
doesn't go through D, but indirectly through U. This is called the
independence assumption. Finally, we also want to exclude that Z
directly affects Y. This assumption is called
the exclusion restriction. In other words, we want a single channel through
which Z affects Y, and that channel has
to be D. This is why the exclusion
restriction is sometimes also referred to as the
only true assumption. Now let's revisit the
three assumptions of IV because they are so
central to this approach. So first, we need that the instrument
effects to treatment. As a rule of thumb, if you have a single
instrument we need the t-statistic to be larger
than the square root of 10. If that's the case, we can say that we have
a strong first stage. Second, the independence
assumption says that there are no common causes of the
instrument and the outcome. In other words, we
say that there is no indirect effect of the
instrument on the outcome. Finally, the exclusion
restriction implies that there is no direct effect of
the instrument on the outcome that doesn't
go through the treatment. This assumption can in
general not be tested, so you will have to always do and find ways to justify that
this assumption holds. If you have more instruments
than endogenous treatments, then you can run an
overidentification test to test for the plausibility of
the exclusion restriction. This test is called the
Serban Hanson test. However, this test requires that at least one of
the instrument is exogenous and that
the treatment effect is the same for everyone. So IV requires us to find one or more variables that induce variation in the
treatment variable for reasons unrelated to
the outcome variable. So how can we find these
magical instruments? Well, good instruments are
often challenging to find. Historically, researchers leveraged
randomness created by policies as instruments. For example, Angrist
and Lavy used to so-called Maimonides rule to study the effect of class
size on test scores. In Israeli public schools, class sizes increase one for one with enrollment
onto 40 students, but when there are 41
students enrolled, then there is a sharp drop in class size to an average
of 20.5 students. So this Maimonides rule, which determines class size, is a good instrument for
the student-teacher ratio. Quasi-random variation in
the world such as rainfall, is another good source
for instruments. For example, Madestam and
colleagues used rainfall to study the impact of political protests on public
support for the Tea Party. The rainfall creates
random variation in rally attendance as more people will attend the rally
if the weather is nice. A third option is to leverage a randomized experiments that encourage people to
take up the treatment. So let's see now how
such an experiment with encouragement design plays
out in our referral example. So if the app has a
strong referral program, it could be that you
have been running AB tests to push users
to refer more friends. So suppose you ran a promotion
that offered half of the users a $5 bonus if
they would refer a friend. Some users were encouraged
to refer more friends. Now, why is this a
good instrument? Let's check the
three assumptions. First, the five-dollar
referral bonus has to increase referrals. Well, some users do
not refer friends regardless of whether they would receive money for doing it. Others refer friends
independent of money. But some users are on the fence, receiving five
dollars might just be enough to nudge them
to refer a friend. For those users, the five dollar referral
offer changes their behavior. They refer a friend if they
receive five dollars for it, and otherwise they don't. For them, referring a friend is not a personal choice anymore. Their decision is affected
by an external force. As a result, we can estimate the causal effect of
referrals on the numbers of all articles read for this subset of users which
we call the compliers. If we assume that
the causal effect of referrals on app use is
the same for everybody, then we can get an estimate of the average treatment effect. Second, the five-dollar
referral bonus, our instrument, needs to be randomly assigned or
as good as random. Now, since this
was an experiment, the independence assumption
is met by design. Now, what about the
exclusion restriction? Well, this one is a
little bit more tricky. We basically have to assume that the five-dollar offer had no
direct impact on app use. When might this
assumption be violated? For example, if users perceive
the five-dollar offer as a nice gesture and
feel obligated to reciprocate by spending
more time on the app, we won't be able to test this. For now, let's just assume
that this was not the case. Let's use some data now. We can see here that
users who were offered the five-dollar bonus
for referring a friend, indeed refer more
friends on average. To indicate that there are several observations with
the exact same values, I made the size of the dots proportional to the
number of observations. The control group which wasn't offered the
referral bonus, referred about five
friends on average. In contrast, the treatment group referred 6.9 friends on average. A t-test reveals that we have a strong first stage as
the t value is 3.42, which is higher than
the square root of 10. Now, we typically don't have data on all possible confounders. But for the sake of illustration, suppose that we know the
personality types of our users, which is a potential confounder. Here we see that
our instrument is unrelated to having an
open-minded personality. Now, this makes sense because the treatment
was randomly assigned. Now that we have justified our instrument except for
the exclusion restriction, which in general is not testable, what can we do next? We can start thinking about
estimating causal effects. First, we can look at the correlation between the
instrument and the outcome. The instrument shows a
clear correlation with the outcome because the treatment did have an effect
on the outcome. Assignment to the five-dollar
referral offer is associated with a 4.5 increase in the number
of articles read. Because we have said
that the experiment, our instrument satisfies
the three assumptions, we can interpret this
correlation as a causal effect. This causal effect is
called the reduced form. If we have an
experiment like this, we call this the
intention-to-treat effect. The intention-to-treat effect is the causal effect of being
assigned to the treatment. In other words, it's the effect of being encouraged to
take up the treatment. But this is not necessarily
what we want to know. What we really would
like to know is how the treatment
affects the outcome. The ITT does not take
into account that some users did not comply with
the treatment assignment, so we have imperfect compliance. Some users assigned to the five-dollar referral
treatment don't refer more friends or
they simply didn't read the message about
the referral offer. It can thus be interpreted as a lower bound of the
average treatment effect. But so how can we estimate the causal effect of the
treatment on the outcome? This is going to be the
topic of the next video.