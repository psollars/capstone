In the previous video, we discussed
sharp regression discontinuity designs. We've sharp RD,
the treatment assignment is perfect. Individuals above a specific cutoff take
to treatment, while those below don't. But this kind of deterministic assignment
to the treatment does not always happen. Sometimes there is a discontinuity, but compliance with the treatment
assignment is not perfect. In those situations, we say that we have
a fuzzy regression discontinuity design, which is what we will
talk about in this video. In fuzzy RD designs, treatment
assignment is no longer deterministic function of the running variable. Some units above the cutoff fail to take
treatment despite being assigned to it. And the other units below the cut off take
the treatment despite being assigned to the control condition. There is still a discontinuity, but the jump in the probability does
not go up from zero to one. In other words, we now allow for a smaller jumps in the probability
of receiving the treatment. For example, after applying to
the MADS program, you were required to pass an online test that covers
topics in introductory statistics and Python in order to be
accepted to the program. If your score was above the threshold,
you were admitted to the program. However, you could still decide whether or
not to enroll. On the other hand,
if you score bus below the threshold, you were not admitted to the program. But suppose for the moment that there are
other ways to get into the MADS program even if you fail to
pass the entrance test. So passing the test increases
the probability of enrollment, but it doesn't guarantee
participation in the program. This is a situation where the threshold
increases the probability of receiving the treatment, but it's not like an on
off switch as with sharp RD designs. Fuzzy RD signs are often useful when
the threshold encourage participation in the program, but the threshold
doesn't force people to participate. Does this sound familiar? One way to think about this setup is that
the running variable is an instrument. It affects the value of the treatment and it only affects the outcome through
its effect on the treatment. So it's useful to introduce
a new dummy variable T, which simply indicates whether the running
variable has crossed the cut off point. In sharp RD designs,
we have D equals T for everyone. However, here this is
not the case anymore. Some people are above the cut off,
but don't get the treatment and others are below the threshold, but
none of the less received the treatment. Since fuzzy RD is essentially
an ID estimation strategy, we will make the same assumptions. For example, we will assume monotonicity. That is, that no one is discouraged from taking
the treatment by crossing the threshold. Calculating the average treatment effect
on the re-fuzzy RD design is very similar to calculating the average treatment
effect with instrumental variables. Specifically, it's the ratio of
the reduced form difference in MENA comes around the cutoff to the first
stage difference in mean treatment assignment around to cut off. In other words, since the treatment
doesn't affect arguments, the jump at the cutoff in the outcome
needs to be rescaled by the jump at the cutoff in the probability
of getting the treatment. As with other binary instruments, fuzzy RD designs estimate the local
average treatment effect or late around the cut off, which is the
average treatment effect for compliance. So who are the compilers in this setting? Well, compliance are those who take
the treatment when encouraged to do. So that is, when they're running
variable is above the threshold and they don't take the treatment when they
are not encouraged to take the treatment. That is, when they're running
variable is below the threshold. Just like with IV,
we have a first stage regression and the second stage regression. T is the dummy variable for
crossing the threshold and it acts as the instrument for
the treatment variable D. The second stage regression uses the fate
of values of the from the first stage regression. Now to obtain the correct standard errors, we again need two stage least squares or
two SLS estimation. There are also the same
caveats as with IV. For example, we need to make sure
that we have a strong first stage. A week first stage problem
may occur if that jump in. The probability of getting
the treatment at the cutoff is small. Let's look at an example from
the mastering metrics book. House prices may vary dramatically
from neighborhood to neighborhood depending on which school
district they belong to. This is because many parents who are
looking for a new home are willing to pay a premium to see their children attend
what appear to be better schools. So they're willing to pay more, because
better schools may attract better teacher, better schools may have more resources and the other features that
are beneficial to student achievement. Many also believe that student learn
as much from their peers as from their teachers. Since better schools spring
high ability students together, the peer quality is
higher at those schools. Indeed, regressions that link
student achievement to the average ability of their classmates suggest
a strong association between the performance of students and
achievement of their peers. For example, in the sample of
middle school students in Boston, Josh Angrist and his colleagues estimate
that the peer effect is about 0.25. This means that the one standard deviation
increase in the ability of peers is associated with a 0.25 standard
deviations gain in own achievement. However, this is probably not
an apples to apples comparison. Students educated together tend
to be similar in many ways, such as that they come
from similar families. So control the regressions cannot
account for all these shared influence. So there will be likely selection bias
if you would use controlled regression. Ideally, we would run an experiment that
randomly assigned students to different levels of peer quality. But as you might guess,
this is often not possible. So what can we do? Luckily for us, there is a network of
selective public schools in Boston and New York that offer students the
opportunity to attend public schools with much higher achieving peers. These schools are called exam schools
because students are selected by entry exams with sharp cutoffs. So Angrist and
colleagues exploit this admissions cutoff in the fuzzy regression discontinuity
design, which allows them to study the causal effects of peer
achievement, unknown achievement. Specifically, they identify top
exam schools where the peer quality is much higher
than at other schools. Applicants with exam scores just
above the cutoff tend to end up at the school where the peer
quality is higher. To most applicants who are rejected at the
top school still end up at another exam school. But their tapir quality is much lower,
so there is a discontinuity in peer quality depending on weather top
exam school applicants are just above or below the admissions cutoff, so
this calls for a fuzzy RD design. Note that here, the discontinuity
changes the intensity rather than the probability of the treatment
which is peer quality. But the identification
strategy remains the same. Peer achievement is measured by the math
score of applicants schoolmates on a test it took in fourth grade, that is two years
before they applied to the exam school. These test scores are measured
in standard deviation units. My dose just above and below did
admissions cutoff have very similar characteristics such as family background, the quality of their peers is very
different depending on which side of the cut off they fall,
which is more or less random. The jump in peer quality is about
0.8 standard deviations for math scores,
which is a pretty big difference. It's about the same as the difference
in peer quality between Boston Inner City Schools and
schools in welfare suburbs. While admitted applicants are exposed
to much stronger peer group, we see no similar jumping own achievement. That is, on test, they took one or
two years after taking the entry exam. We could basically Stop here because, as you may remember from
the discussion of Irene. If you can see it in the reduced form,
it ain't there. Note that for those around the cutoff,
we assume that attending a top exam school only causes a dramatic change
in the quality of their peers. But there may be many other features
that change abruptly depending on whether someone falls just above or
below the cutoff. For example, the racial
composition of classmates might be different in the top exam school
compared to other schools. However, since the reduce form is
essentially zero, it doesn't matter whether we consider peer quality or for
example, racial mix as the causal channel. The results will always be the same. There is no effect of whatever
changes between top exam schools and other schools. To complete the analysis,
let's quantify those effects. First we need to estimate
the reduced form. The dependent variable y,
are middle school math scores on test taken after one or
two years of middle school? T is a dummy variable indicating
applicants who qualify for the top exam schools. And X is the running variable
that determines qualification. So this regression produces
an estimate of -0.02 and has a standard error of 0.1. This confirms our visual impression
of the graph from the previous slide. There is no jump in owner achievement for applicants who just barely
passed the entry exam. Next we apply 2SLS to estimate
the causal effect of peer quality on an academic achievement. The first stage is a regression of
average peer quality D bar on a dummy variable T for passing the exam, and
X which is our running variable. That is, the score in the entry
exam center data cutoff average peer quality D bar is our
endogenous treatment variable. The subscript i, reminds us that
student i is not included when calculating the average
achievement of his or her peers. To parameter 5 captures the jumping
nearing peer quality induced by an exam school offer. We have seen this jump in the first stage
figure which suggested that peer quality increases by 0.8 standard deviations for
students above the admissions cut off. The last piece of the 2SLS setup estimates
the causal effect of peer quality on own academic performance, measured as
the average math score in 7th grade. As always, the second stage includes the
same control variables as the first stage. The estimated treatment effect is -0.023,
so close to 0. Since the reduce form is 0, it's not surprising that the 2SLS
estimate is also close to 0. This estimate is also far from
the estimate generated by the controlled regression, which was
0.25 standard deviations. So if we ignore the problem
of selection bias, we would wrongly conclude appears manner. Let's now discuss potential threats for the identification of treatment
effects in RD Designs. By RDs in an experimental desig,
we have some kind of local randomization provided that
the following assumption holds. Individuals have imperfect control
over the running variable. They can influence the running variable,
but whether they fall just below or
above the threshold is essentially random. Now this assumption can be violated if
people manipulate whether they're above or below the threshold such as when students
ask for a regrading of their assignments. In practice, this gaming of
the threshold is more likely to happen if the assignment rule
is known in advance, or individuals are interested in adjusting or
individuals have time to adjust. So what can we do about this? Well, we can look for
signs of manipulation. For example, if some people manipulate
the running variable, we should see the more individuals are showing up
just above or just below the threshold depending on the consequences of
being above or below the threshold. If there is no manipulation, then the density should be
continuous at the cutoff point. This can be tested with
the McCrary density test and alternative approaches to check for
covariant bounds. There should be no discontinuity
in covariance at the cutoff point. However, if a certain type of
people are gaming the threshold, we may observe that the average
characteristics of those just above the threshold, differ from
those just below the threshold. As a fur test, we could also check for this continuity's in the outcome
at arbitrary thresholds. If people move around to be above or
below the threshold, this means that there
are missing at other places. This, in turn may generate discontinuity's
at values of the running variable where you do not want to find anything. Finally, are these all
about visualization? So it's worth spending some time to figure
out how to best present your results? For example, it's common to construct
bins an average the outcome within those bins, rather than
showing every single data point. So what results should you
present when you using RD design? 1st and most importantly,
you should always show how the outcome variable changes as a function of
the running variable, this is a must. Remember, good graphs tell the story. 2nd, if you have a fuzzy RD design, you
also want to show how the probability or the intensity of the treatment
jumps at the cutoff. This is the first stage. Finally, you could also show
the results of your placebo tests. For example,
as discussed in the previous slide, you could show that there is no jumping
covariance at the cut off point. That other outcome variables do not
change meaningfully at the cutoff. And that the outcome of interest does
not jump at artificial thresholds, so this concludes our lecture on
regression discontinuity designs.