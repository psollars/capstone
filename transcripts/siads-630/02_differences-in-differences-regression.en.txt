In this video, we will discuss
how to use regression to estimate causal effects in differences-in-different
settings. In particular, we will focus on panel data with more
than just two periods. Let's go back to our
pricing example. We have an e-commerce company that wants to increase revenue, but the company doesn't
know whether it should raise or lower the price
to achieve its goal. While the company is reluctant
to conduct an experiment, we can make use of a
natural experiment. The company lowered the price in June to October for the US, but not on the Canadian side. Here is a graphical
representation of the different depth design. Previously, we looked
at the situation with just one pre and one
post-treatment period. Now we're extending the analysis to 10 data points for each unit. That is, we have one observation
for each month and unit. We use the control group that is the Canadian side to identify
the time-trended sales. As you can see, sales tend to go down in the
post-treatment period. When we estimate the
treatment effect, we have to account
for this time trend. Otherwise, we would
underestimate the effect size. We also see that sales are generally higher for the US side. This is not a problem if the
parallel assumption holds. That is, if the difference would remain the same in the
post treatment phase, absent to price reduction. But remember, we cannot test the parallel
trends assumption. How can we estimate the treatment effect
in a regression model? The typical
diff-in-diff regression model we estimate is shown here. Y is the outcome
variable, in our example, this is the sales figure
for unit i in month t. TREAT is a dummy which equals to 1 if the unit is in
the treatment group, and POST is a dummy for
the post-treatment period. We also have an interaction
term between TREAT and POST. It's equal to 1 for the treatment group in the
post-treatment period, and its coefficient
is what we're after. There are a number
of advantages of using a regression
framework instead of simply collapsing the data into four data points and manually computing
the treatment effect. First, we can calculate
the standard errors. We need standard
errors if we want to assess statistical
significance. Second, it's easy to include multiple periods as we have here. Finally, we can control for
additional variables which may increase the precision of our estimates of
the treatment effect. In other words, including
control variables, may help us getting
smaller standard errors. Now, how do we interpret
these coefficients? Well, let's just plug in the
values for TREAT and POST. If TREAT and POST both
equal to 0, that is, the observation is from Canada
into pre-treatment period, then the only coefficient
that remains is Alpha. For post-treatment observations
from Canada, that is, if TREAT equals 0, and POST equals 1, then we get Alpha plus Gamma. We have Alpha plus Beta for pre-treatment
observations from the US. That is, when TREAT equals to 1, and POST equals to 0, and finally, we have
all four coefficients, for post-treatment
observations from the US. If we calculate the
difference-in-differences, what remains is the
coefficient term, our estimate of the
treatment effect. Estimating the equation from the previous slide generates
the following estimates. The coefficient on
TREAT suggests that the US [inaudible] side generates higher monthly sales
than the Canadian side. We also find that there is
a general drop in sales for the post-treatment period as indicated by the negative
coefficient of POST. This effect is identified
through the control group. Our coefficient of interest, the interaction term, is
positive and significant. Revenue increased by 19.8 in response to the price change. The results suggest
that the company should lower the price to
increase revenue. But the sample size is
relatively small here. We did the analysis with only 20 observations
from two countries. Now, suppose the company operates in 10 countries
and that it had decided to reduce the price in five out of
these 10 countries. Now we have five
treatment markets and five control markets. Here you can see the treatment
and control markets in the months before and
after the price change. The change in the treatment
group minus the change in the control group provides us with an estimate of
the treatment effect. We again see a jump in sales
after the price reduction. However, it's much
smaller than if we were to compare the
post-treatment period only. Here you can see the value of having baseline measurements. Looking at the control markets, that we see a noticeable
downward trend over time. Now, with many units, we can capture this trend by modifying our diff-in-diff
regression model. The multiunit diff-in-diff
regression model could look something like this. This is a model with
many dummy variables. Here, every country except one gets its own dummy variable, and it is indexed by the
subscript k for the kth country. The kth country dummy equals 1 when an observation
is from country k, meaning that i
equals k. Otherwise, this variable is zero. We call the bidders
country fixed effects as they capture fixed differences
between countries. Remember that we have to omit one country as the
reference group. Now we can do the
same for time by including dummy variables
for each month. This switch on when
observations come from month j, that is, when t equals j. We call this month or
time fixed effects as they capture monthly trends that are common to all countries. Here too, we need to omit one month as the
reference time period. This data structure is called
a country month panel. Now let's have a look at
the estimation results. Our coefficient of interest is to treatXpost interaction effect. This provides us with an estimate of the
treatment effect. The other coefficients, that is, the month and country
fixed effects are not displayed here because
of space constraints. The estimate of the treatment
effect suggests that the price reduction costs
sales to increase by 13.7. It's a relatively
precise estimate of the treatment effect as the associated standard error is small relative to the
value of the coefficient. We can safely reject the null
hypothesis of no effect. One issue with panel data
is that there might be serial correlation in
the dependent variable. For example, if the US side makes a
lot of money in February, it's possible that it also
sells a lot in March. Serial correlation
doesn't necessarily bias our estimate of
the treatment effect, but it can lead to an underestimation of
its true standard error. Meaning that we tend to over reject null hypothesis
of no effect. In a 2004 paper, Bertrand, Duflo, and Mullainathan
highlight the problem of serial correlation in diff
in diff applications. As a side note, Duflo received the
2019 Nobel Prize for using experimental
methods to address poverty. Now going back to our discussion
of serial correlation, most diff in diff
applications use long pre-treatment and post
treatment time series. If the dependent variable
is serially correlated, standard errors are typically downward bias when they are calculated with
conventional methods. Bertrand and
colleagues illustrate this problem by
running placebo tests. They showed that random
intervention are often significant despite that they should produce zero effects. What can be done? Well, Bertrand and colleagues
proposed three solutions. First, we can calculate
clustered standard errors that take the intracluster
correlation into account. While this is the most
popular approach it requires a fairly large
number of groups or clusters. As a rule of thumb, you should have at
least 30 groups. A second option is to block
bootstrap to standard errors. Bootstrapping is a
resampling procedure. It uses data from a
sample to generate the sampling distribution by repeatedly taking a random
sample from the known sample. The resampling is done
with replacement. Now, what the block bootstrap does is that it
tries to replicate the correlation by
resampling blocks of data rather than
individual observations. In our case, a block
will be a country so we would have to sample entire countries
with replacement. The last option is
to simply aggregate the data into one pre and
one post-treatment period. With this approach,
we no longer have a time series that
spans multiple periods. Now if you go back to
the previous slide, you will see that I clustered the standard errors by countries. However, as mentioned before, you should have at
least 30 groups to get unbiased standard errors. In this example, we have only 10 clusters,
the 10 countries. Now let's talk about
possible threats to the validity of the
diff in diff strategy. A common problem is that interventions are not
as good as random. One way this can happen is
when an intervention is implemented based on pre-existing differences
in outcomes. For example, our company might select treatment
markets based on their potential to attract new customers and
sell more products. Another possibility
is that people select the treatment based on some idiosyncratic shock that occurs just before
the intervention. A famous example of this from economics is Ashenfelter's dip, named after a labor
economist at Princeton. It was common to
compare wage gains among participants
and non-participants in job-training programs to evaluate the effect of
training on earnings. The problem is that
participants often experienced a dip in earnings just before the end
to the program, which is presumably why they entered the program
in the first place. Since wages have a
natural tendency to revert to the mean, this leads to an upper bias of the diff and diff estimate
of the training effect. When targeting or selection
into treatment happens, we may no longer be able to justify the common
trends assumption. Going back to our example, we need to be worried
about possible confounders popping up in
particular treatment at our control markets. Indeed, as shown in the
figure on the left, we see that the number of
users who visit the site drops suddenly in treatment markets just before the price change. It could be that there
are new competitors in treatment markets which drive
away uses from our sides. This may explain why the company decided to reduce the
price in those markets. Now since the number
of users who visit the site likely affects revenue, this might bias our results. The figure on the
right suggests that the key assumption of parallel
trends may no longer hold. What can we do if we think the parallel trends
assumptions violated? Well, maybe the parallel
trends assumption holds conditional on covariates. If we have good proxies
for the confounders, we can simply include those time-varying
control variables in the regression model in
a linear additive way. For example, we could take the previous equation
and just add a variable for the
number of users who visit the site in each
country and month. Here's the regression output for the diff-in-diff model with the number of users as
a control variable. As expected, the number of users is positively related to revenue. The more people
visit the web shop, the more items are sold. Each additional user
is associated with revenue increase of roughly 0.05. If we are to see that the
estimate of the price effect is relatively similar
to the regression without the control variable. It is slightly smaller though, compared to when we do not control for the number of users. But this makes sense given that sales would have
likely been higher in the treatment markets before the price change had the
number of users not trapped. In addition to adding controls, we can also allow each country to have
its own time trend. For each country, we
add an interaction term between a linear time trend
and a dummy for the country. This can be estimated from
pre-treatment data and it helps us detect if there really
are varying time trends. While adding unit-specific
linear time trends to the regression model, relaxes the common
trends assumption, we still assume that the trends continue in the
post-treatment period. Models with unit-specific
time trends provide an important check for whether the parallel trends
assumption is problematic. If there are
pre-existing trends that differ between treatment
and control groups, this will cast doubt on the randomness of
the intervention. While the country-specific trends vary to some degree here, the inclusion of those trends has little effect on our
diff-in-diff estimate of the treatment effect. However, the inclusion of unit-specific time trends is computationally and
econometrically very demanding. They may also absorb part
of the treatment effect, especially when the effects
emerge only gradually. This can be seen by looking
at the standard error. The addition of trends increases the standard error of the diff-in-diff
estimate quite a bit. This new standard error is
now almost twice as large. However, our treatment
effect is so large that we can bear
the loss in precision. What else can we do to check for the robustness of our
diff-in-diff results? Well, we can perform a variety of placebo tests if we think the parallel trends
assumption is violated. For example, one thing
we could do is to check that the intervention really does occur
before its effect. We can include leads of the treatment and
see whether there is a sudden change in the outcome already before the start
date of the intervention. If there is an effect before the intervention
takes place, there are good chances
that our estimate of the treatment effect
is biased as well. More generally, when many
years are available, it is useful to check how the treatment effect
evolves over time. This allows us to detect whether there are
anticipatory effects. A second approach is to use alternative control groups that experience the same
unit-specific time trends. For example, suppose we believe that the drop
in sales shortly before the intervention is
due to some event that affected all online
shops in a given country. Then we could compare
our treatment shops with shops from different
companies but that operate in the
same countries. Now the downside is
that we now make even more parallel
trends assumptions. For example, if the
decline in sales was caused by our competitors
lowering their prices, then the parallel
trends assumption is obviously violated. A third approach is to
perform falsification tests. We could replace the
outcome variable by another outcome that is not supposed to be affected
by the intervention. For example, suppose we lower the price only for a
certain class of products, we should therefore
see no change in series for these other products. If we nonetheless find an effect, then it is likely that our original diff-in-diff
estimate is biased as well. Remember, we can never directly test the common
trends assumption. So the best we can do is to perform placebo tests and makes sure we don't observe effects where we don't
expect them to see.