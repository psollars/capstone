In this video, we're
going to talk about the most important statistical
method, regression. Almost every analysis
that you will see uses regression in
some way or another. Regression helps
us to learn about how two variables are related. There are essentially two ways of viewing regression:
descriptive, that is, as a
correlation, and causal. For example, suppose we
observe that students with higher SAT scores earn higher
wages later in their lives. This is simply a correlation between SAT scores and wages. It is not the same as saying that higher SAT scores
cause higher wages. Correlations can be useful
to predict what will happen, but it doesn't tell us
why something happens. Before we learn about
what it takes to interpret regression
coefficients as causal effects, it is useful to first understand what
regression is about. Imagine the following
population relationship between the number of reviews in logs and the average rating of
recipes on a cooking site. The population
relationship involves the entire universe of recipes
and ratings on the site. Remember that we rarely have data on the
entire population, but we're focusing on the
population version of the regression because this is what we are going
to try to estimate. Here, we see a
positive relationship between the number of reviews and the average numbers of stars that recipes received. For now, we're holding back
the question of whether more reviews cause
higher ratings. Our focus is on estimating the relationship between
the two variables. One way to describe
the relationship between the number of reviews and the average numbers
of stars obtained by recipes is the conditional
expectation function. The conditional expectation,
as its name suggests, is the population average conditional on holding
certain variables fixed. The red circle indicates one such conditional expectation. Recall that expectation,
as used in statistics, refers to the population
average of a random variable. The collection of all
those averages is called the conditional
expectation function. The CEF, in blue, joins all those red
circles together. It tells us how the
population mean of a variable Y changes as we change values of the variable X. Note that conditional
expectation functions are functions that we
don't get to observe, so when we try to estimate a CEF, we will have to estimate a function whose
form we don't know. Generally, our goal is to predict how Y varies as a function of X. It turns out that the CEF, the conditional
expectation function, provides a useful tool for this prediction because
of two properties. First, we can always decompose Y into the CEF and an error, u. The error term u is defined as the difference between
Y and the CEF. This implies that the error
is mean independent and therefore that the error is uncorrelated with
any function of X. Intuitively, it means that Y can be decomposed into two parts, a part that is explained by X and a part that is
not related with X. The second property
is that the CEF gives the best predictions for Y
using the information in X, at least in terms of minimizing
the prediction error. Suppose you wanted
to find a function g of X that minimizes
the prediction error, that is the mean squared error, so what function g
should you pick? It turns out that the CEF
is the optimal choice. The CEF provides a good summary of the relationship
between Y and X. How can we model the CF? Well, we can just assume
that the CEF is linear. Of course, the linear
model may not be correct. So why should we choose
a linear function? Linearity makes
things a lot easier. With one covariant, we just need to estimate two parameters. Note that Alpha and Beta are population parameters which
we will try to estimate. We use Greek letters for parameters to distinguish
them from variables. The parameters in
this linear model also have a nice interpretation. Alpha is called the intercept and it is the mean
of Y if X is 0. In our example, suppose some recipes have
only one review. We're using the log of
the number of reviews, so the log of 1 is 0. Thus the intercept Alpha gives us the average numbers of stars for recipes that have one review. The second parameter, Beta, is called the slope, as it gives us the
change in the mean of Y when we increase X by one unit. In our example, Beta
answers the question, how much does the number
of stars change if we change the log of the number
of reviews by one unit. What linear function of X is the best approximation
for predicting Y? In other words, how shall we choose the parameters
Alpha and Beta? Our goal is to minimize
the prediction error, that is the vertical
distance between the variance of Y and
the linear function, so it's a minimization problem. We want to find the Alpha
and Beta that minimizes the average of the
squared distances between Y and the line. The solution to this problem is called the population
regression function. Using some calculus, it can be shown that the
population slope is the covariance between X and Y divided by the variance of X. The covariance measures
the linear dependence between two variables, that is, how values of one variable change if the values of the
other variable change. The population intercept
is the population mean of Y minus Beta times the
population mean of X. This is just a bivariate
case to get the idea, but it works similarly
if we have multiple Xs. A nice feature of
regression is that even if the CEF is non-linear, it provides us the best linear
approximation of that CEF. If the CEF is linear in X, then the linear regression
is the same as the CEF. Here's an example of the population regression
function using our data on the number of reviews and the average stars rating
obtained by recipes. As you can see, the
regression function is only an approximation
of the CEF. But as explained on
the previous slide, it is the best linear
approximation of the CEF. The linearity assumption
means that when we change the variable
X by one unit, it is associated with
the same change in Y, regardless of whether we
fix the variable X at say, 2.5 or 7.5. Remember that the
population regression line is a population concept. It's an unknown function in the population that
we don't observe. How can we get an estimate
of the population function? Well, we simply use
sample data and replace the population expectations
with the sample means. Because the resulting Alpha and Beta are estimates of
the true Alpha and Beta, we use the hat symbol
to indicate this. The estimator defined
by this procedure is known as ordinary least
squares or short, OLS. Here is an example of the
population regression line and its sample counterpart
estimated using OLS. The sample regression is based on a random sample of
100 observations. As you can see, the two
lines are not identical. This is because they are based
on different observations. The line on the
right-hand side is just an estimate of the
line on the left-hand side. However, because I
took a random sample, the two lines are the
same in expectation. In other words, if
I were to repeat the sampling process many times, and each time I would
estimate the sample line, then the average of
all sample lines will correspond to the
true regression line. This is because of the
unbiasedness property we discussed in the video on
statistical inference. Let's try now to get a better intuition for
the OLS estimator. We define a fitted
value as the value we predict for Y given a
particular value of X. The residual u hat is
the difference between the observed Y and the fitted value
generated by our model. Note that the errors
and residuals are two closely related but
easily confused variables. An error is the
difference between the observed value of y and the population
regression line, which we don't get to observe. A residual on the other hand, is the difference between
the observed value of y and the value predicted by
the sample regression model. The residuals you had tell us how well the
line fits the data. Larger residuals means that points are very
far from the line. Residuals close to zero means that the data points are
very close to the line. Suppose we measure the
size of the mistakes, that is the residuals by squaring them and then we add them all up. OLS chooses Alpha hat and Beta hat to minimize the
sum of squared residuals. The squaring eliminates
all negative values of the mistakes so that
everything has a positive value. Now that we know how to estimate a slope of a sample regression, the beta hat, we need to determine its sampling
distribution. This gives us a measure of the precision which we will need. For example, if we
want to test whether the estimate is significantly
different from zero. To derive the sampling
variance of the OLS estimator, it is typical to make one additional assumption,
homoscedasticity. This assumption stipulates
that our population error term u has the same variance given any value of the
explanatory variable x. Since we don't observe the variance of the
population errors, we somehow have to estimate
it with our sample data. We can do this by using the residuals from the
sample regression. But we have to make a small
sample adjustment that is, divide by n minus one
instead of n because otherwise we would slightly
underestimate the variance. With this assumption in place, we can show that
the standard error of the slope estimate in the bi-variate
regression is given by the estimated
standard deviation of the residuals divided by the
square root of the sum of the squared deviations of
the x's from the mean of x. The sampling variance
gets smaller the closer the regression line is
to the data points, and the sampling variance
gets smaller with a larger sample size or
more variation in x. The graph here on
the left illustrates the assumption of
homoscedasticity. It implies that the
variance in the errors is constant across
different levels of x. However, in reality, the constant variance
assumption is often violated. We have heteroscedasticity. As you can see in the graph
on the right-hand side, the variance of the errors is different at different
levels of x. In this particular example, the variance increases
with values of x. A common fix for heteroscedasticity is to
estimate robust standard errors, sometimes also called Huber
wide standard errors. Robust standard errors
are computed with a different estimator called
the sandwich estimator. The formula can be
quite complicated, so we will just let our statistical program
estimate it for us. Robust standard
errors are usually larger than conventional
standard errors, but this is not always the case. So far, we haven't really talked about the fact that I used the natural logarithm
of the number of reviews instead of
the actual numbers. As you can see in the
graph on the left, there are many recipes with just a few reviews and a few recipes with
lots of reviews. In other words, the variable for the number of
reviews is right-skewed. As a result, the relationship
between the number of reviews and the numbers of
stars is probably not linear. Now, can we handle that
with a linear model? Yes, we can. By transforming the variables or including
additional covariates. For example, when we use the natural logarithm of
the number of reviews, the relationship looks
much more linear as shown in the graph
on the right-hand side. Another way to account for
the non-linearity is to model the relationship as a
polynomial instead of a line. For example, we could
add a quadratic term to the regression model to
approximate the non-linearity. Here's a handy chart for interpreting logged variables
in the regression model. As I mentioned on
the previous slide, we can take the log of the
x or y variable or both. However, this changes
the interpretation of the estimates
depending on what you do. For example, suppose we use
the log of the Y variable, but not for the X variable. We have a log level model. If we regress log Y on x, then a 100 times beta can
be roughly interpreted as the percentage increase in y associated with a one
unit increase in x. Now suppose we take the log of both the X and the Y variable. We are in the log-log situation. n this case, beta is roughly the percentage increase in Y associated with a one
percent increase in X. Note that these
approximations work only for small changes of
the logged variables. Now that we have explored
the impact regressions as a way to understand the relationship
between two variables, it's time to estimate
the relationship between recipe ratings and
number of reviews. Suppose we have a sample of
100 observations and regress the average numbers of stars on the natural log of the
number of reviews. Here I've also used the
option robust to estimate robust standard errors in
case of Heteroscedasticity. The results suggests that a one percent increase in the number of reviews is roughly associated if 0.0008 increase in the average number of stars. The associated p-value
is 0.013 or 1.3 percent. We say that the coefficient is significantly different from zero at the five percent level using a double-sided t-test. However, note that the size of the coefficient is
not very large. It is 0.08 percent. This illustrates
that just because the coefficient is
statistically significant, it doesn't mean that
the relationship is meaningful in terms of magnitude. We nonetheless find the
positive relationship between the number of reviews and the
average numbers of stars. Does this now mean that more reviews cause
higher ratings? Probably not. There could be issues with reverse causality
and selection bias. For example, recipe with a high rating
attracts more users, which may result in a
higher number of reviews. This would be an example
of reverse causality. Another issue is that
people choose to rate recipes based on the
recipes unobserved quality. We may also have a problem
with selection bias. When exactly can we interpret regressions
in a causal way? This is what we are going to
discuss in the next video. What did we learn in this video? We often want to understand how two variables are related. For example, we wanted
to know whether there is a positive
relationship between the number of reviews and the average number of
stars obtained by recipes. One way to describe the true relationship is through the conditional
expectation function. How the mean of the
average number of stars changes as a function
of the number of reviews. The problem is, how can we
estimate that function? It turns out that the
regression provides us with the best linear
approximation of the CEF, even if the CEF is non-linear. Regression is one of the most popular
statistical tools to study the relationship between
two or more variables. It's implicitly may well
be its biggest strength. Regression estimates are
relatively easy to compute and interpret and to provide a simple benchmark for
fancier techniques. Moreover, we are used to think about relationships
in terms of average, yet a common argument against the regression approach
it's that it's too simple. After-all, real-world relationships
are often non-linear. However, statistical models are not meant to replicate
the real world, but to provide useful insights. Moreover, regression is much less restrictive than
it might appear. As mentioned
previously, we can just transform the original
variables or add more variables such as a quadratic term to take
nonlinearities into account. The key is to understand
that regression is linear in the coefficients
alpha and beta and so on, but not necessarily
linear in the variables.