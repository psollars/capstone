The instrumental variables
approach helps us identify causal
effects in situations, where the treatment is not
delivered in a random manner. As we saw in the previous video. This can occur, for example, when the treatment is
offered in a random fashion, but people can choose
whether or not to actually take
up the treatment. We then looked at how
we can still estimate causal effects under
the assumption that the effects are
the same for everybody. In this video, we're
going to relax the assumption that the effect is the same for every person. Now, can we still estimate causal effects with the
instrumental variables approach? If we assume that people can have different responses
to the treatment, we need to find out what
IV is actually estimating. With heterogeneous
treatment effects, we also need to understand what additional assumptions are required so that IV can
identify causal effects. The reason why this matters, is that once we allow for heterogeneous
treatment effects, we introduce a
distinction between the internal validity of a study, and its external validity. We already talked about internal and external validity in the video on experiments. Internal validity is the
question of whether our strategy successfully identifies
a causal effect for the population we study. On the other hand,
external validity relates to the
question of whether our study's findings applies
to different populations. Under constant effects, there
is no such tension between external and internal validity because everyone has the
same treatment effect. But under heterogeneous
treatment effects, there is potentially
a large tension. To better understand IV
with heterogeneous effects, let's go back to the
potential outcomes framework. So far, we have been using the term potential outcomes for the outcome variable only. But now, we have a second
potential variable, potential treatment status as opposed to observed
treatment status. D1 is a person's eye treatment
status when z equals one and a lot justly D0 is a person eye's treatment
status when z equals 0. Now, I'm not going to derive the IV estimator on the
heterogeneous effects, because it requires
quite a bit of math. As you will see, the result looks essentially the same as in the constant
effects world. What really changes is the interpretation of
what we're estimating. To simplify the analysis, let's assume we have a binary treatment and
the binary instrument. In this case, the potential
outcomes framework divides the population
into four groups. Always takers is the group of people that always
takes a treatment, regardless of the value
of the instrument. Never takers are
those who never take the treatment regardless of
the value of the instrument. Compliers is the group of
people whose treatment status is affected by the instrument
in the correct direction. In an experiment with
encouragement design, this is the group of people who takes the treatment when
assigned to the treatment, and they don't take the treatment when they are assigned to
the control condition. Finally, the fourth
group are the defiers. Defiers are those whose
treatment status is affected by the instrument
in the wrong direction. In an experimental situation, these are the people who do the opposite of what
they are supposed to do. They don't take up the treatment
when assigned to do so, and they receive the treatment when they're not
supposed to get it. With heterogeneous
treatment effects, we have four key assumptions. The first three
assumptions are the same as in the constant
effects framework. We need the first stage. Our instrument should
be as good as random, and the instrument
cannot have an effect on the outcome once we fix the
value of the treatment. But now we're making an
additional assumption, that there are no defiers
in the population. While the instrument may have
no effect on some people, all of those who are affected are affected in the same way. In other words, the
instrument pushes affected people in
one direction only. It just makes people more
likely to get the treatment. This is why the
assumption is sometimes also called the
monotonicity assumption. Now, in the KIPP example, no defiers means that there is no one who attends
KIPP when losing the lottery and does not attend KIPP when
winning the lottery. The no defiers
assumption gives us more information
than we might think. If we assume there
are no defiers, then anyone with Z equals 0 and D equals 1 must
be an always-taker. Likewise, anyone with Z equals 1 and D equals 0 must
be a never-taker. Because the instrument
Z is randomly assigned, we know that the
share of each type has to be the same
in the two groups. This allows us to calculate the share of each type
in the population, which is crucial for estimating causal effects with IV and
heterogeneous effects. The no defiers assumption
may or may not hold so what might be situation where
assumption is likely valid? One example, our experiments
where the treatment is only available if a person has been assigned
to the treatment. Think about a medical trial
where people receive a pill. They might not take it, but it's impossible
to get the pill if you have been assigned to
the control condition. In this particular case, there are also no always takers because an always
taker is somebody who always gets the treatment regardless of whether he or she was assigned to it or not. Since those who are assigned to the control group have
no access to the pill, no one can be an always taker. Such research designs are called experiments with
one-sided non-compliers. The non-compliers comes only from those who receive the pill, but then decide not to take it. If all four assumptions are satisfied then what
exactly is IV estimating? It turns out that IV estimates the average causal effect
for the compliers. This is the group of people whose treatment status was
changed by the instrument. For example, in the KIPP study, these are children
who are enrolled at KIPP because of the lottery, but who will not have
attended KIPP otherwise. IV estimates the average
treatment effect of attendance on test scores for this particular type of children. This treatment effect
is called the local average treatment
effect or short LATE. How informative is LATE? The average causal effect
for the compliers. We typically want to learn about the average
treatment effect, that is, the average of all
individual causal effects. If the treatment
effect is the same for every person then LATE equals the average
treatment effect. However, this is
typically not the case, the effects need
not to be the same, for example, always
takers and compliers. Suppose an experiment
only encourages women to take up the
treatment, as a result, IV will only identify
the causal effect for women but if the effect
is different for men, we are unable to capture it. But what we usually
want to know is the causal effect for
both men and women. In this example, we assume
that women are the compliers, they take up the treatment
when told to do so. However, in reality, we typically don't know
who the compliers are. They are an unknown
subgroup in our data set. While we can calculate the share of compliers
in our sample, we don't know much about them. Those assigned to the treatment
and take up the treatment could be always
takers or compliers. Those assigned to the control
condition and don't get the treatment could be
never takers or compliers. Since we don't observe
both states of the world, we cannot describe a
particular type to a person. The good news is
that compliers are often in-group we would
like to learn about. For example, in the KIPP study, compliers are children who
would likely attend KIPP if a new school opened and they were offered one of the coveted sits. If the number of charter
schools is regulated by law, then the LATE estimate provides useful information
for the question of whether the laws
should be changed. Second, researchers
and policymakers are often interested in the average treatment
effect on the treated. As mentioned before,
experiments with one-sided non-compliance
such as medical trials, they allow us to estimate
this causal effect. This is because there
are no always-takers and by assumption,
also no defiers. Any person who takes up the treatment has
to be a complier. Finally, there is a 2003
paper by Alberto Abadie showing how we can identify the average characteristics
of the compliers. But this can be useful as
it allows us to examine the extent to which
the complier group differs from the
whole population. Before we turn to an example, I would like to add
one more thing. IV says that we can learn about causal effects by splitting the variation in the
treatment into two parts, an exogenous part and
an endogenous part. There are lots of variables
that could explain why a particular person
is treated or not. One reason is that the person has a particular value
of the instrument. But there may be lots of other reasons unrelated
to the instrument, such as an individual's
personality. IV estimates are based on the exogenous part of the
variation in the treatment. That is, those who change treatment status because
of the instrument. This is the sub-population
of compliers. It's as if we are
working with less data. As a result, IV estimates tend to be more imprecise
compared to other methods. In other words, we will often be under-powered to get statistically
significant results. Let's apply what we
have learned about LATE to an example from the
mastering metrics book. The example is about police response to
domestic violence reports, a topic that might be
sensitive to some of you. If you think you will not feel
comfortable watching this, you can just skip this part. In the early 80s, the mayor and police chief
of Minneapolis launched a ground-breaking experiment to assess the value of
arresting batterers, the Minneapolis domestic
violence experiment. The experiment was motivated
by a debate over whether arrests would deter batterers
to commit further assault. The police was often
reluctant to make arrests for domestic violence
unless the weak thing demanded it or the suspect, it's something that warranted arrest besides the
assault itself. The experiment involved
three conditions that varied how the police are supposed to respond to domestic
violence calls. The first condition was
arresting the suspect. The second condition was
ordering the offender off the premises for
like eight hours. This is called the
separation condition. The third condition
offered some form of advice that might
include mediation. Now, for simplicity, we will pull the softer police responses, that is separation and advice, into one condition and call
it the coddled treatment. The primary outcome in this study was whether
there was another case of domestic assault at
the same address within six months of
the original visit. Note that cases of
life-threatening are severe in trivial excluded. Moreover, both suspects and victims had to be present
upon arrival of the police. The police officers who
participated in the experiment, they had volunteered
to take part, and were therefore
expected to comply with the randomization protocol. However, strict adherence
to the randomization was both unrealistic and
inappropriate at times. In practice, officers
often deviated from the responses called for by the random assignment
of the treatment. The most common deviation from random assignment
was the failure to separate or advice when random assignment
called for this. This table here
shows the deviations between assigned treatment
and actual treatment. The numbers reflect
percentages with the numbers of cases
in parantheses. For example, of the 108 suspects randomly
assigned to receive advice, 19 were arrested and
five were separated. Overall, the compliance rate with the advice treatment
was about 78 percent. We have an experiment with
imperfect compliance. While the experiments
seems broken, IV can fix the problem
of imperfect compliance. Josh Angrist, one of the authors of the
Mastering Metrics books, re-analyzed the experiment
using the IV approach. Analysis of the Minneapolis
domestic violence experiment based on treatment delivered is misleading because
of selection bias. As we just saw on
the previous slide, some suspect that
batterers received a different treatment than the
one they were assigned to. In other words, treatment
delivery was not random. Thus, we cannot simply
compare differences between those who were and
those who were not coddled. What is random is
treatment assignment. This allows us to do two things. First, we can estimate the
intention to treat effect, this is the effect of being
offered the treatment. Second, we can use treatment assignment
as an instrument for treatment delivered
and then we can calculate the local
average treatment effect, that is, the effect
among compliers. Being assigned to
the coddle treatment increased recidivism by
11 percentage points, from roughly 10 percent in the rest condition to 21 percent
in the coddle condition. These numbers are taken from
the Mastering Metrics book. ITT comparisons are useful because they tell us
about the effects of interventions in situations
where compliance rate are expected to be similar to those observed in the study. However, ITT estimates are almost always too small relative to
the actual treatment effect. For example, in this study, police officers arrested
suspects who were violent regardless of whether or not they were assigned to
the coddle treatment. So these violence
suspects are probably more likely to again
commit an assault. If those had been treated
as intended, that is, if they had been coddled, then the difference
between the treatment and control group would
be even larger. Now let's examine what
happened with those who were actually exposed to one
of the coddled treatments. To calculate LATE, that is, the effect among compliers, we need two ingredients. First, we need to estimate
the reduced form, which is what we just did. Second, we need to estimate the first stage to
scale the effect. If everyone had complied
with treatment assignment, the first stage estimate
would have been one. But as we saw previously, not every suspect was coddled who was
supposed to be coddled. So the first stage in
this study is 0.786, about 79 percentage points. In other words, this
means that suspects were 79 percentage points
more likely to be coddled when assigned
to the coddling treatment. The ratio of the reduced form to the first stage gives us the local average
treatment effect, which in this study amounts
to 14.5 Percentage points. It's larger than the
corresponding ITT estimates and it's also much larger than a simple comparison of
means across treatments. So in this example, selection bias leads to a
gross underestimation of the actual causal effect
of coddling on recidivism. How generalizable is
the LATE estimate? After all, LATE only measures the effects
among compliers. It turns out that the experiment basically has one-side
and non-compliant. When randomized to arrest, the police faithfully
arrested with only one exception in 92 cases. So we basically have a no
always-takers situation. All of the treated
units are compliers. This means that LATE
roughly corresponds to the average treatment effect
on the treated or the ATT. In other words, LATE is the average causal effect
of coddling on the coddled. So it gives us information
about what would happen if the coddled batterers
were arrested instead. This analysis suggests
that applying a soft police response to batterers is more problematic
than previously thought. As a result of this experiment, arrest in case of suspected domestic abuse has become mandatory in many states.