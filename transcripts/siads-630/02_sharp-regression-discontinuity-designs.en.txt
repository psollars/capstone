In this video, we're
going to discuss sharp regression
discontinuity designs. In sharp RD designs, the assignment to treatment is a deterministic function of
the running variable and the threshold c. If the running variable is equal
or above the threshold, then people received
the treatment. If on the other hand, the running variable is
below the threshold, people don't get to treatment. The word sharp refers
to the fact that treatment assignment changes
sharply at the cutoff, just like an on off switch. The example from
the previous video was a sharp RD design. If a student earns a
grade of 70 or above, she passes the course. But if the student earns
a grade of 69 or below, she does not pass the course. The idea behind RD designs is that we have local randomization. For people close
to the threshold, it's as good as random, whether they are just above versus just below the threshold. These two groups of people
are comparable in terms of both observably
and an observable. Now, let's connect this idea to the potential outcomes framework. If we assume constant
treatment effects, then in potential outcome terms, we get Y_0, which is the potential outcome in the
control state of the world, equals to Alpha
plus Beta times X. Remember, X is our
running variable. Y_1, the potential
outcome when someone is treated is equal to Y_0 plus Tau. So Tau is the treatment effect. Using the switching equation, we get the following
regression equation. Y, the realized outcome
equals to Alpha plus Beta times X
plus Tau times t, our treatment variable
plus some error E. The sharp RD estimation can be interpreted as the average causal effect of the treatment at the discontinuity. It's a local average
treatment effect. The quantity of interest is
the fact that the threshold. This is to expect the potential
outcome at the cutoff if treated minus the
expected potential at the cutoff, if untreated. The problem is that for
the untreated group, the gray dots, we don't observe their potential outcomes
at the cutoff and beyond. Likewise, for the treated
group here in blue, we don't observe their potential outcomes
below the cutoff. In other words, we have a lack of overlap or common
support as treat, then the control
units cannot have the same values of the
running variable X. What we need to do is to
extrapolate from what we observe in order to compare
treated and control units. Extrapolation, even
at short distances, requires a certain smoothness in the functions we're
extrapolating. Absent to treatment, the expected potential outcomes wouldn't have jumped
at the cutoff, they would have remained smooth function of the
running variable X. The key identifying assumption of RD designs is that the expected
potential outcomes remain continuous or smooth
through the cutoff C. This is called the
continuity assumption. Note that this assumption
is not directly testable because it's based on potential outcomes
that we don't observe. Another way to describe the continuity assumption is to make the following two rules. First, individuals
have imperfect control over assignment
around the cutoff. That is, they cannot
control whether they are just above or just
below the cutoff. For example, you can study harder to do well on the
assignments for this course. But there is always some
randomness left in the outcome. Now, when might this
assumption be violated? Well, if students could write in to me and complain
about the grading, and then I give
them a grade boost. Now if that were the case, then it's not as good
as random anymore. If someone falls left or
right of the cut-off. Second, there are no
confounding discontinuities. This means that being
just above verse, just below the cutoff
should not dramatically influence other features that
are related to the outcome. In our example, this means
that passing the course is the only differentiator
between a grade of 69 and 70. Now, when might this
assumption be violated? Well, for example, if
students who get 70 or above also get reimbursed for their tuition
by their employer, this would generate an income
shock for those students, but not for students
with lower scores. The effect we would estimate would then be a combined effect of passing the class and
getting tuition reimbursed. But what we are after is simply the causal effect of
passing the class. If there is perfect compliance and the continuity
assumption holds, then the change in
treatment status induced by the cutoff can be used to
estimate the treatment effect. But what exactly
are we estimating? We're estimating the average
treatment effect for some narrow or ''local''
range around the cutoff. If there're different
treatment effects for different types
of individuals, then the estimates may not be broadly applicable for
the full population. Whether the estimated
treatment effect applies more broadly, is a question of
external validity. Now, let's look at an example from the Mastering Metrics book, the effect of the minimum legal drinking age on death rates. In 2008, a number of
college presidents and higher education officials signed the Amethyst Initiative, which calls for a reexamination of the minimum
legal drinking age. The minimum legal age
to purchase alcohol in the US is currently at
21 for most states, whereas in many other countries
in the Western world, the age limit is at
18 or even lower. Proponents of this initiative raise concerns about
the current policy, arguing that it encourages
more dangerous drinking among the 18-20 years olds than if it were legal
for them to drink. Opponents claims that the age
21 limit while imperfect, nonetheless reduces youth access to alcohol and therefore
prevents harm. The causal question is
whether legal access to alcohol leads to more
drinking related deaths. Perhaps this is not the case, and the supporters of the
Amethyst Initiative are right. Legal access to alcohol may
discourage binge drinking and promote a culture of
mature alcohol consumption. Note that this is not
the same as asking whether alcohol consumption
increases the risk of death. The question here is about
legal access to alcohol. Here we can take advantage
of the fact that the drinking age turns on suddenly when the
person turns 21. People slightly younger than 21 are subject to the
drinking age law, while those slightly
older than 21 are not. But otherwise, the two groups have very similar
characteristics. If nothing else changes
discretely at age 21, then the discrete increase
in mortality rates at age 21 can be plausibly attributed to
the legal drinking age. We can first check
graphically how mortality rates change
as a function of age. As usual with RD designs, the story can be
told in one picture. The figure here plots death
rates measured as deaths per 100,000 people per
year by month of age, defined as 30 day intervals, and centered around
the 21st birthday. Each dot is the death rate
for a specific month of age. Death rates fluctuate from month to month as
can be seen here, but few rates to the left of the age 21 cutoff are above 95. By contrast, at ages over 21, death rates go up and most
data points remain above 95. We also see a downward trend, meaning that fewer people die as they grow
older at that age. However, if the trend will
continue after age 21, we should see a death rate
of about 92 at the age 23. Instead, the death rate is roughly 99 at that age,
so considerably higher. We can estimate this effect with a local linear regression. Here, local means that we only consider ages between 19 and 23. Our running variable
is age in months. We saw that death rates
clearly change with age for reasons unrelated to the
minimum legal drinking age. For example, it's well-known that deaths from external causes, such as car accidents, homicides, and suicides fall
in the early 20s. This is not a problem as long as these confounders do not change suddenly
around the age 21. Moreover, our regression models control for this linear trend. This is actually why we call this approach regression
discontinuity. RD analysis controls for smooth variation in the outcome generated by the
running variable. The unit of observation
are each groups by months, with months defined
as 30-day intervals. X is our running variable, and it's common to transform the running variable by
re-centering it at the cutoff, C. This doesn't change the interpretation of
the treatment effect, only the interpretation
of the intercept. D is a dummy variable
that equals to one if the age group 21 or older. The fitted values
of this regression produce the red lines in
the graph on the right. Beta captures the
slope of these lines. Tau captures the jump in
death rates at age 21. Should we include
other controls here? We could, to increase precision, but it shouldn't matter. Although treatment isn't
randomly assigned, we know where it comes from. Treatment assignment is only determined by the
running variable. Here is age. This is the sharp RD assumption. Although RD uses regression
to estimate causal effects, RD designs should be seen as a distinct method from
controlled regression. There is no value of the
running variable where we get to observe both treated
and controlled unit. We simply assume
that the trend will continue above the cutoff
absent to treatment. Now, this seems plausible for observations close
to the cut-off, but the further away we
moved from the cut-off, the less plausible
is the assumption. Here, we see the output from the regression model
on the previous slide. The regression
generates an estimate of roughly 7.7 additional deaths per 100,000 people when young people have legal
access to alcohol. The p-value is very small, so the effect is
highly significant. Now, a practical question is whether there are nonlinearities. For example, what if we have a trend that does not
jump at the cut-off, but it's simply non-linear, as shown in this graph? Note that smoothness and
linearity are different things. It's not that there is
a jump at the cutoff that is generated by confounders. The underlying relationship
is simply nonlinear. If that were the case, then our previous
regression model will pick up an effect even though there is no discontinuity in the outcome at the cut-off. This example illustrates
how important it is to visually inspect
your RD design. What can we do to address
the issue of nonlinearity? There are essentially two
strategies to deal with nonlinearities and reduce
the likelihood of mistakes. The first approach is to
directly model the nonlinearity. Nonlinearities in RD
frameworks are typically modeled using
polynomial functions of the running variable. Equation 1 is an
example of a model that includes a quadratic term
of the running variable. Note that we, again, transform
the running variable, X, by re-centering
it at the cut-off. But you have to be
cautious with adding higher-order polynomials
as it may introduce bias. A recent paper by Gelman and Imbens
suggest that we should only use a quadratic term of the running variable and no
higher-order polynomials. Another way to model
nonlinearities is to allow for different slopes on
both sides of the cut-off. This modification generates
models that interact a running variable
with the treatment dummy as shown in equation 2. Of course, we can also combine the two
approaches, that is, adding a quadratic term of the running variable
and interacting the treatment dummy with both the running variable
and its quadratic term. Here, you can see the
combined model and how the results look
in graphical form. When we add a quadratic
term and allow for different trends on
either side of the cut-off, we don't see a big jump anymore. While a simple linear
model suggested that there is a discontinuity
at the cut-off, now we have reasons to doubt that whether this
is truly the case. You should always check if the
main result holds when you allow for different functions on both sides of the cut-off. The second approach to deal with nonlinearities is to focus on observations closer
to the cut-off. In other words, in order to distinguish jumps from
non-linear trends, we restrict our estimation to observations
near the threshold. Observations that are closer to the cutoff are clearly
more similar in ways that we possibly cannot
measure compared to those that are further
away from the cutoff. Let's see what happens if we use a more narrow window
around the cutoff. By the way, the width of the window is referred
to as the bandwidth. Here are the results for a
bandwidth of 60, that is, we ignore all observations
with values of the running variable that are 60 units away from the
cutoff or more. Now, we see that the
gap in the outcome at the threshold
starts to disappear. Note that the smaller the window, the less we need to be
concerned about nonlinearities. The choice of the
simple RD model versus a fancier model with a
quadratic term and interaction terms shouldn't matter much as we zero in on observations
close to the cutoff. If we further reduce
the bandwidth to 20, the depth completely disappears. Thus reducing the bandwidth is an effective strategy to protect us from
misleading trends. But it comes at a cost. If the window is very narrow, then there are only
few observations left. This means that the
resulting estimates are likely to be too
imprecise to be useful. The bandwidth is a tuning
parameter that we set. But as we just discussed, there is a trade-off. A higher bandwidth
means that there's a high risk of bias when
using a linear specification, but the standard errors of
the point estimate will be lower because we have more
data for the estimation. On the other hand, a lower bandwidth reduces
the risk of bias, but it also reduces the
precision of our estimates. While estimating a
local linear regression around the cutoff is
straight forward, what is less clear is knowing how large or small we
should make the window. Luckily, some good
econometricians have developed an algorithm to determine the optimal bandwidth. But the method still requires you to choose certain parameters. In practice, the choice
of the bandwidth, much like the choice
of polynomials, requires a judgment call. The goal should not be to
find the optimal bandwidth, but to show that your results are robust to different
choices of bandwidth. Let's go back now to the
question of whether legal access to alcohol leads to more
binge-drinking and deaths. Our first approach was to
fit a simple linear model that allowed death rates
to jump at age 21 cutoff. Using this model, we estimated
the treatment effect of about 7.7 additional
deaths per 100,000 people, as shown by the red lines. The question now is
whether there is indeed a discontinuity
at the cutoff, or whether our model
is misleading, perhaps because the
relationship is not linear. We can estimate a
fancier model with quadratic terms and allowing for different trends on both
sides of the cutoff. The solid black curves visualize the results
of this fancier model. We can see that the
results are not very sensitive to the details
of our modeling choices. If anything, the model suggests that the jump
in death rates near the age 21 cutoff is even larger than what we
previously estimated. We can also check
for robustness by reducing the bandwidth
of the window that is by including only ages 20-22
instead of ages 19-23. While we get a less
precise estimate of the treatment effect, because we can use
fewer data points, the result still support
the conjecture that the legal access to alcohol
increases death rates. As a final robustness check, we can perform a placebo test. That is, we can look for outcomes where there
shouldn't be any effects. For example, although
alcohol is poisonous, young people rarely die from
alcohol-related diseases. Disease related
causes of death are typically referred to
as internal causes. The number 1 reason for alcohol related deaths among younger people is drunk driving. With this reasoning,
we should see a jump in motor
vehicle fatalities, but little change in death
rates due to internal causes. This graph here supports
the argument that the jump in death rates is
indeed due to drinking. The change in deaths
for internal causes is small around
the age 21 cutoff. By contrast, we see a large jump in deaths due to car accidents
at the threshold. Our analysis suggests
that legal access to alcohol leads to a sustained
increase in death rates. However, keep in mind
that in RD designs, we are estimating a local
average treatment effect. The estimates may not be broadly applicable to other
drinking age limits. It seems likely that
we would observe a similar jump if we were to lower the drinking age to 18. But in the end, there
remains some speculation on whether the effects are representative of
other age groups.