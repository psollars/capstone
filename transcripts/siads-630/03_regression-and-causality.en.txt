In most cases, regression is
used with observational data. This means that without the benefit of
random assignment of the treatment, regression estimates may or
may not have a causal interpretation. In this video, we're going to
discuss when we can use regression, to uncover and quantify causal relations. Most techniques of causal inference rely
on regression in one way or another. So, when can we think of a regression
coefficient as a causal effect? While not necessary, we can always analyze a randomized
experiment with regression. Rather than comparing means across groups, we could also run the regression to get an
estimate of the average treatment effect. The advantage of regression
to analyze experiments, is that we can add other
explanatory variables to increase the precision of our
estimate of the treatment effect. But note that, the other variables
don't allow for causal interpretation. As we will see in future modules, other techniques of causal inference
such as regression discontinuity and differences-in-differences, also
heavily rely on the regression. What we're going to focus on in this
video, is controlled regression. Suppose we observe every single variable
that could jointly affect the treatment and the outcome. In that case, we could just control for
the influence of these variables. However, in practice we rarely observe
every single relevant variable. So when we will use controlled regression, we will have to make a strong assumption
and we will have to come up with good arguments as to why this
assumption is justified. The key assumption about
causality in a regression model, is the zero conditional mean
of the errors assumption. It says that the error, u,
has an expected value of 0 for any value of the independent variable X. But this sounds complicated,
it's just the version of the conditional independence assumption we
discussed in the matching video. But what does this
assumption mean exactly? Well, it means that the average
of all the other stuff that effects Y except X,
is the same at every level of X. So as you can imagine, it's a very strong assumption that
is almost certainly violated. It basically requires,
that there is no reverse causality and no omitted variable bias. Note that the zero conditional
mean assumption is not testable. This is because it's based on
the population regression function, which is something we
don't get to observe. Now let's look at an example of when
the zero conditional mean assumption is likely violated. Suppose we want to estimate the effect
of years of schooling on wages. An important variable that we would
like to control for is ability, but unfortunately we don't
have a good proxy for ability. So, zero conditional mean implies, that the average ability is the same
in the different subpopulations. In other words, the average ability of
a person with an eighth grade education, should be the same as for
people who finished high school but didn't go to college, or for
people with a four year college education. However, because people choose education
partly based on their ability, the zero conditional mean assumption
is likely violated in this case. If we had the proxy for ability,
such as standardized test scores, this would allow us to control for
it in the regression model. But standardized test scores may
not be a good proxy for ability, because of measurement error and
other unobserved aspects of ability, not captured by the test. In addition, there may be many other
confounders that are captured in u, and which we don't observe. So these graphs here illustrate when
the zero conditional mean assumption is violated or not. The red line represents
the population regression line. So in the graph on the left,
we see that there is no correlation between the errors, the,u's,
and the exponentory variable X. So, the errors are roughly 0 on
average for each level of X. On the other hand, in the graph on the right we see
a correlation between the u's and the Xs. The errors are mostly negative,
for negative values of X and positive for positive values of X. So in other words, this means that the
data points in this case don't really fit the regression line at all. So when is the 0 conditional
mean assumption most plausible? Well, when the Xs are randomly
assigned in experimental data, this ensures by design,
that the Xs are unrelated to the Us. So when we have observational data,
that is, when we observe X, instead of assigning those values, it will
be difficult to justify this assumption. And as we have seen in the schooling
example, people may choose their X based on their U,
therefore violating this assumption. So now let's return to our question of
whether attending a private college pays off in terms of
higher future earnings. We are now going to estimate the private
college effect using regression. Our sample here has 14 observations. This is a small sample to estimate
regression parameters, but it's just meant to be an example. The regression model in this context, is an equation linking the treatment
variable to the dependent variable while holding other variables fixed
by including them in the model. With only one control variable,
our model can be written as follows. So Y is the dependent variable in this
case, student i's earnings later in life. P is our treatment variable, a dummy that indicates whether
someone attended private college. A is our control variable,
a discrete variable for students SAT scores, which we will
use as a proxy for their ability. U is the error term that captures
everything that effects Y, except college choice and ability. So the SAT score could affect both P and
Y. The probability that an alumni
attend a private college, and the earnings of the person later in life. So we therefore want to control for
that potential confounder, by adding it to the right
side of our model. Remember, these are unknown population
parameters that we want to estimate now. So how can we interpret
this regression model? The conditional expectation of Y,
given our treatment variable P equals 1,
is alpha plus beta plus gamma times a. If P equals 0, then the conditional expectation of Y is just
alpha plus gamma times a. So these are actually just
two different lines, one for when P equals 0 and one for when P equals 1. The two lines have the same slope's,
gamma, but they differ in the intercept. It's alpha plus beta, if P equals 1,
and it's just alpha if P equals 0. So, the parameter beta tells us
the causal effect of private school on earnings, if we assume
that our treatment is as good as randomly assigned
conditional on SAT score. Thus, we make the strong assumption here, that there are no additional variables
that differ between private and public school alumni's and that jointly
affect college choice and earnings. Now let's estimate this model with
our sample data and using OLS. So we regress earnings on
the dummy for private school and the variable for SAT scores. The results suggest that going to
private college increases annual earnings by roughly 15,
so, $15,000 in average. By the effect is relatively large. It is only marginally significant
with a P value of 8.7%. This has to do, with the large standard
error associated with this coefficient, which is the result of
the small sample size. SAT scores are also positively and
significantly related to future earnings in this model,
they have an associate P value of 0.4%. So here we can see
the results graphically. Our model assumes a linear relationship
between SAT scores and earnings. The slope of that line It's the same for
private and public college students. So what differs is the intercept. It is alpha for public college students. And alpha plus beta for
private college students. So for a given SAT score,
say 1000, private college students are in beta more money
than public college students. Note that in our model we assume that
the private school effect is the same for different levels of SAT scores. So regression is a way to
make other things equal. But equalities generated only for variables included as controls
on the right side of the model. If we don't include enough controls,
or perhaps the right controls, we still have the issue of selection bias. The regression version of
the selection bias generated by inadequate control is
called omitted variable bias. Suppose we run the short version of the
regression of earnings in private school. That is, we run a regression
where we include a dummy for private school, but
we don't control for SAT scores. So if we do that, our estimate of the
treatment effect is 16.5 rather than 15. Thus our estimate of the treatment effect
is larger compared to the previous model. But this is a biased estimate
of the treatment effect because private school alumni
tend to have higher SAT scores. And we know that SAT scores
affect future earnings. In other words, if we don't control for SAT scores, we violate the zero
conditional mean assumption. Let's assume that in our scenario, the true model includes
ability that is SAT scores. We call this the long regression because
the model includes more variables and used a superscript l to remind us that the
parameters are from the long regression. Now let's think about running
a misspecified model that omits A our variable for SAT scores. We call this the short regression
as it has no controls. Notice here that us equals
to gamma l times A plus ul. Intuitively, this is saying that
there is a correlation between P and the misspecified error us due to
the correlation between P and A. So our main question is how the estimate
of beta s from the short regression relates to the estimate of beta l from
the long regression, our true model. So is beta s equal to beta l? And if not, what's the difference? So what is the relationship
between beta s and beta l? It turns out that there
is a simple relation between the two estimates of the beta. Beta s equals, beta l plus gamma l times pi1. We use pi1 to represent the coefficient
on P of a regression of A on P. The last term on the right side is
what we call omitted variable bias or a selection bias in the causal model. If we will know gamma l and pi1,
we could determine the bias. These are the slope estimates
of two regressions. Gamma l is the coefficient
of a regression of y on A. And pi1 is the coefficient
of a regression of A on P. So the omitted variable
bias is the effect of A on Y multiplied by the effect of P on A. Put differently, it's the effect of
the omitted variable on the outcome times the effect of the included
variable on the mid variable. Now, it seems silly to omit A our proxy
for ability if it belongs in the model. But in practice we often have no
choice as we may not observe the A. What we want to know is if our estimate
of the treatment effect is biased because we have to meet potentially
important variables such as ability. Remember that by OLS,
the coefficient pi1 is the sample covariance between P and
A divided by the sample variance of P. But we usually cannot estimate
P1 due to the lack of data. We can simply guess the sign of
this coefficient by thinking about the relationship between P and A. Then we can do the same with gamma l,
that is guess whether the relationship between Y and
A is positive or negative. If we know the signs of
these two relationships, we can determine the sign of
the omitted variable bias. There are essentially two cases
where beta s is unbiased. If gamma l equals 0. That is, if A doesn't appear in the true
model, then beta s is unbiased. The second case is more interesting. If the covariance between P and A is 0, then beta s is unbiased
even if gamma l is not 0. That is, even if ability effects earnings. So let's apply this to our example. Suppose we don't observe
students SAT scores. That is, we have no proxy for
their ability. But we can speculate that private
school alumni's have higher ability, as it's harder to get
into a private school. So the covariance of P and A is positive. It's also plausible to assume that ability
is positively related to earnings. So we may expect gamma l to be positive. Accordingly, omitting ability in
the regression model should lead to an upward bias of the effect
of private school on earnings. And we know this just by guessing
the signs of these two relationships. Since we actually have a proxy for
ability, that is the alumni's SAT scores,
we can check whether we guess correctly. First, we the estimate the effect
of the omitted variable that is the SAT scores in the long regression. Then we estimate the relationship
between the included variable that is private school and
the omitted variable. Multiplying these two coefficients,
we obtain a positive bias of about 1.5. To make sure this is correct, let's
run the long and short regression and compare the coefficients on
the private dummy across models. We first estimate the long regression and get an estimate of the private
school effect of about 15. Next, we run the short regression, which gives us a private
school effect of about 16.5. So the difference between the short and
long regression estimate of the treatment effect is roughly
1.5 just what we obtained before. Remember, we usually can't calculate
this difference because we don't have the omitted
variable in our data set. So how can we be sure that our
set of control variables fully capture selection bias? For example, it could be that our control
variables are incomplete proxies for the true omitted variables. After all SAT scores may be a poor
measure of students' ability. It could also be that we
can't find the proxy at all. Or that we simply don't think of the right
variables that affect selection into the treatment. So what do we do in those situations? A common approach to evaluate
the robustness of regression estimates of causal effects is to explore the sensitivity of treatment effects
to the inclusion of controls. If adding more controls don't change
the coefficient on the treatment variable meaningfully. Then we can argue that
our controls account for most of the relevant selection. In other words, if we see that our
regression results don't change much by adding more observed variables. Then we can assume that the same will
be true for unobserved variables. Now obviously this is not a perfect test,
but in the end we have to work with
the variables that we observe. If you're interested in a more
sophisticated method to assess selection on unobservables, go check
out the recent paper by Emily Oster, published in the Journal of Business and
Economic Statistics. So now we have learned what it takes
to interpret regression coefficients as causal effects.