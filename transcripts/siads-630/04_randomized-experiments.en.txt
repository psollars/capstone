This module is about
randomized experiments. Randomized experiments
are often considered the gold standard in
empirical research. In this video, you will
learn why this is the case. Remember, our goal is to estimate average
treatment effects. Our first approach was to compute the
simple difference in mean outcomes between treated
and untreated individuals. The problem with this approach is that there is selection bias. Because people can choose, for example, whether or not
to get health insurance, simple comparisons of treated
and untreated individual is not other things equal. For example, people expect
large benefits from having insurance may be more
likely to get insurance, and they may be less
healthy to begin with compared to those
without insurance. This means that the average potential
outcome under no insurance, so y_0, is not the same for
the insured and uninsured. Now, suppose we randomly assign individuals to the treatment
or the control condition. If individuals are randomly divided into the treatment
and control group, then the law of large number promises that the two groups
will be similar on average, provided that the samples
are large enough. Mathematically speaking,
this means that treatment assignment
is independent of the potential outcomes. Now, let's look at an example of random assignment
and how it works. Suppose we have only
two people and we randomly assign them to the
treatment and control group. Even though we randomly assigned them to
the two conditions, the two groups will not be comparable because
it's only two people. Here, the purple individual was assigned two the
treatment group, and the green person was
assigned to the control group. When we compare there outcomes, we cannot conclude that
the only thing that differs their the two
is to treatment status. However, when we randomly assign many individuals to the
treatment or control group, then the two groups become
reasonably similar. They're probably not identical as shown here on the slide, but random assignment of a
larger group of people ensures that the mix of individuals being compared is
roughly the same. Random assignment
and random sampling, which is what we talked
about in the previous video, are closely related
but distinct ideas. Both concepts rely on the
law of large numbers. Random sampling relates to the external validity or
generalizability of results. Results are externally
valid if they are unbiased for the
population of interest. Random assignment relates to the internal validity of results. So results are
internally valid if they are unbiased for the
sub-population studied. How can we check
whether two groups are indeed similar after
the randomization? Well, what we can
do is to check for balanced in the characteristics. However, we will never
be able to assess whether the two groups are
similar in all respects. After all, we can only work with the variables that
we can observe. For example, we can check whether age and gender is
balanced across groups, but we may not be able to
check whether the two groups have similar medical
histories or eating habits. For randomization checks,
it is common to use non-parametric tests
as they require fewer assumptions than parametric
tests like the t-test. I typically use the chi-square
test for binary that is 0, 1 variables, and to rank sum tests for
non-binary variables. Now, let's return to
the question of whether health insurance has a positive impact
on people's health. We will consider other
examples in future modules, but for now, let's stick
with that question. In 2008, Oregon initiated a limited expansion of the Medicaid program for
uninsured low-income adults. About 90,000 people signed
up for this program, but the state didn't
have enough money to allow everybody to
participate in the program. To ensure a fair process, the state decided to conduct a lottery by drawing names
from the waiting list. If those selected submit
that the paperwork on time, they were enrolled in
the Oregon health plan. The lottery created a
randomized experiment, that allowed researchers
to study the effects of Medicaid health insurance on
emergency department use, and various other
health outcomes such as cholesterol and
blood pressure. Here's an overview
of the study design. Among the 90,000 people who put their names
on the waiting list, about 15,000 were excluded
because they forgot to submit the paperwork
or were deemed ineligible for various
other reasons. The remaining, about 75,000, were randomly split into
the treatment group, that is Medicaid, and the control group
that is no insurance. About one year later, the researchers came in to ask participants and number of
health related questions. But due to budget constraints, they were not able to
interview everyone. They selected to interview about 30 percent of
the participants. Among those selected
to be interviewed, about 73 percent actually
completed the interview. This example illustrates some of the issues that can happen
when you run an experiment. First, even though people were randomly assigned to the
treatment and control group, the researchers were not able
to get data on everyone. In particular about 30 percent did not complete the
interview on time. Second, although enrollment
in Medicaid was free, not everyone who was eligible
to enroll actually did so, and some people in
the control group somehow managed to
get into the program. Since the decision to enroll
is probably not random, it creates the familiar
selection bias problem, which hinders our ability to
learn about causal effects. As mentioned on the
previous slide, some people did not receive the treatment to which
they were assigned to, so there is a problem
of noncompliance. Now, can we somehow
fixed this problem? Well, there are several ways to deal with the issue
of noncompliance. The first thing we could do, is to redefine the
treatment variable. Instead of measuring the effect of Medicaid on some outcome, we could study the
effect of "giving people the option" of Medicaid. So Noncompliance is not possible anymore if the
treatment is defined that way because
the lottery decided whether or not someone was
offered the treatment. Experiments that offered the
option instead of forcing people to take up the treatment are called encouragement designs. Another approach is to use Instrumental
variable analysis. This would allow us to estimate the causal effect of
receiving Medicaid. But one drawback is that
we can only learn about the causal effects for subgroup
of people to compliers. Now we will talk more about this approach in a future module. By the Oregon Health Insurance
Experiment in not create an ideal zero percent versus 100 percent health insurance
coverage situation. Lottery winners were
26 percentage points more likely to be
covered by Medicaid. Researchers who analyze the
Oregon Health Experiment collected data on various outcomes using
different sources, including in-person interviews, mail surveys, and
administrative data. The sample sizes you will see may not match the
numbers you saw on the previous slides
where I describe the research design in terms
of in-person interviews. The first outcome
variable that we consider is the use of
emergency departments. Many policymakers hope that having health
insurance would shift formally uninsured
patients away from emergency departments towards
less costly services. The researchers obtained
administrative data on emergency department visits, but only for the Portland area. So it's a smaller sample size. So what is the impact
of having the option to receive Medicaid on ER visits? Now, this is easy to determine because of
random assignment, we can just compare averages across treatment
and control groups. Column 3 in this table shows that people in
the control group went about once to the ER
during the observation period. Those who were offered to
enroll in Medicaid went about 0.1 times more often to the
ER as shown in Column 4. Note that Column 4
displays to treatment effect rather than the average
of the treatment group. Those who had the option to
receive health insurance for free were actually more
likely to use the ER. Now, is the difference
significant? Yes, it is. The ratio of the treatment
effect to its standard error, which is reported in parentheses below the
treatment effect, is larger than two. So even without the
statistics program, we know that the difference
is statistically significant. Another outcome that
the researchers looked at is subjective health. This variable is coded as
one if survey participants assessed their health
as being good or better, are zero otherwise. So it's a binary variable. Here we see a positive
treatment effect in the desired direction. In the control group, about 55 percent of survey respondents said that they were satisfied with their health. Those into treatment
group were 3.9 percentage points more likely to say that their health
is good or better. So while the effect
is significant, you can see that by the ratio of the treatment effect
to its standard error, it's again larger than two, the effect size is
relatively modest. In contrast to subjective
measures of health, there's basically
no difference in physical health indicators like cholesterol or blood pressure. The treatment effects for the objective measures
are close to zero, and therefore not
statistically significant. So overall, the health-related
benefits of Medicaid were rather disappointing
for policymakers. But keep in mind
that these estimates only measure intention
to treat defects. These are the effects of being
offered health insurance, not the effects of
health insurance itself. So the observed
effects will likely underestimate the true
health effects of Medicaid. Finally, the researchers also
looked at the effects on financial strain as shown at
the bottom of this table. Lottery winners were
less likely to have large out-of-pocket
medical expenses, that is expenses that are more than 30 percent
of their incomes. They were also less likely
to have medical debt, and because medical debt
usually never gets paid, the financial burden
was also smaller for medical providers or whoever
has to pay for the debt. So the program provided the financial safety net
for which it was designed. To summarize, experiments are often considered
the gold standard for causal inference because they
eliminate selection bias on both observables
and unobservables. Another advantage of
experiments is that the analysis of treatment effects is often straightforward. You can simply compare
averages across groups. So why don't we always
run an experiment? The reason is that experiments
are not without issues. For example, they can be plagued by selective
participation, as we just saw in the
Oregon Health experiment. But more importantly, experiments can be costly
and difficult to implement, and some experiments
are simply unethical. For example, it will be
morally wrong to randomly force teenager to smoke two
packs of cigarettes a day, and then wait 20 years to test whether they
develop lung cancers. So this is clearly
not something we want to do, but in general, experiments are the
preferred method for identifying causal effect.