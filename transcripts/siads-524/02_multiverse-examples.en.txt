Welcome back. In this lecture, we will continue to explore multiverse modeling by looking
at four basic examples, ways we can represent
multiverse modeling. The first example is
the teams approach. In the teams approach, many individuals create
a model based on data and then the totality of
the results are considered. A good example of the teams
approach comes through a paper on the game of football or in America
we call it soccer. A research team
collected data on various performance
metrics in football. Their fundamental
research question was, are football referees more likely to give red cards
to players of color? Now, rather than pursue one approach this
question with a team, the researchers recruited
29 analytical teams to independently
analyze the data. Again, these are people
who are well-experienced, well-seasoned data
scientists, and researchers. Now, again, the traditional
approach here is that the research team would take the data and work
through it analysis, doing some data cleaning, feature engineering,
modeling, and ultimately, they would share the results. The research team took more of a multiverse approach
where they worked with 29 other analytical teams
who all independently analyzed the data and then publish their answer
to the same question. The analytical teams
used a variety of methods to evaluate the problem. They used logistic regression,
Poisson regression, linear models, Bayesian models, all kinds of things. Across the 29 teams, they used a combination of 21 different
covariates to try and evaluate the question of racial bias in
football refereeing. In the end, the
analytical approaches varied widely
across these teams. The effect size estimates, as expressed as an odds ratio, range from less than
one to almost three. With an odds ratio of one, we would say that a player
of color was just as likely as a player not of
color to get a red card. Odds ratio of three
would say a player of color is three
times more likely. Twenty of the 29 teams or 69% of the teams found a statistically
significant relationship. The team's approach
illustrated that even well intended
experienced teams make decisions which
cause wide variation in the research results. A second example to doing multiverse modeling is
the algorithmic approach. In the algorithmic approach, data scientists use
coding to create many more models than one
could manually make by hand. I'm going to start this section by reviewing the results of a single study which examined a pretty
interesting question. Do hurricanes with feminine
sounding names have higher death tolls than hurricanes with masculine
sounding names, which the study
found was the case. This particular study
concluded that hurricanes with feminine sounding names do have higher death tolls than hurricanes with masculine
sounding names. Now, why would this be? Now, the authors argue that residents tend to
dismiss the danger of a hurricane and take fewer precautions when the name of the hurricane is
feminine sounding, a form of really gender bias. This study has been explored by more than one
multiverse researcher. These multiverse researchers
such as Young and others point out several important aspects to the hurricane study. First of all, the study
is exploratory in nature. Second, there's no prior
empirical research on this topic. Third, there's no
established theory connecting a hurricane's
death toll with its name, though, I would
argue that's what the authors are attempting
to do in the study. Fourth, there is substantial
model uncertainty. The number of explanatory
variables seems large, especially relative to
the size of the data, the number of storms. In the original analysis, the research group makes
a series of choices, all of which seem reasonable given their data and
their research question, but do create what
Gelman and others consider a garden of walking
paths or a multiverse. As we already know, multiverse analysis is
a good option to model the large world uncertainty that seems to permeate this
research question. A lot of researchers have
conducted follow up studies, some of those from a
multiverse perspective. Munoz and Young use some additional
individual studies to define the bounds of their
multiverse analysis, which consisted of
over 1,000 models. They captured the
key coefficient for hurricane femininity on deaths and created a kernel density graph
for the results. The original study
which we already saw had a positive sign
for this coefficient. The graph on screen shows
that about 64 percent of the models that Munoz and Young created had a positive sign. However, less than
five percent of the models yield a statistically
significant effect. Which is not necessarily only
input for our evaluation, but it is a point
of consideration. They also mark Young et
al's original estimate in their multiverse
of reasonable models. You can see that
original study estimate is at the far end of the distribution of
possible effects. Munoz and Young conclude that this analysis does
not necessarily mean that the original study produced false positive results. However, given that 95 percent of the studies do not produce a statistically
significant effect, they argue that is not the data supporting the
original study's conclusion, but the model specification,
the scholars chose. Sarma and Kale found
something similar when running a separate
multiverse analysis. They modeled the mean difference in expected deaths based on Hurricane femininity and found many of the values
are close to zero. These approaches to
multiverse analysis , the algorithmic approach, use code to produce hundreds,
thousands, millions, and even billions of potential
models in our multiverse. Rather than run each
model individually, data scientists use code to run many models in their analyses. This is the
algorithmic approach. The third example of a multiverse modeling approach is the specification curve. The specification
curve is a way of visualizing a
multiverse approach. Specification curves
are named after specifications which are
specific model configurations. Each specification represents
a model in our multiverse. Now specification curves can
vary in their constructions, but generally speaking,
you have three elements. First of all, you have
the specifications really the inputs themselves. This can include controls, subsets, really any design
choice in the modeling. Second, you have outcomes. This is the thing that
you're interested in. It's the curved
part of the graph. An outcome might be a key
relationship of interest. For example it might be a predictor coefficient
in a regression example, if I were modeling home
size versus home price, that is how well does home
size predict home price. We might plot the
coefficient for home size as our outcome variable in our
specific specification curve. You also have a third element
and that is coloring. The specifications and
outcomes are colored based on the significance of the particular specification. In this specification,
blue coloring means the results are statistically significant
and positive. The color gray represents any result that is
statistically similar to zero and red means that the results are statistically
significant and negative. The reason why we call it
can vary, but in general, they mark something
important about the outcome. It's also important to note in this particular
specification curve, we have about 400
specifications or models. Specification curves
build vertically. The inputs create
the model output that's plotted on the graph. We don't use specification
curves to understand how a single set of decisions
lead to a specific outcome. After all, this is
multiverse analysis. Rather we use them to
understand overall patterns. For example, you may consider the general
pattern of outcomes, are they mostly
positive effects, neutral effects,
negative effects. This may help us
understand how robust the overall relationship is
given these sets of inputs. You can also use the
specification curve to understand which inputs may
be driving the outcomes. Here, we're highlighting
the y-variables. There seems to be an important
relationship between our choice of y-variable and the outcomes that
we are producing. As I mentioned,
there's many types of specification curves. This is a different rendering
of a specification curve. In this example, we still
have the core components. We have inputs, we have
output and we have coloring. Notice that the output
in this case is AIC, which is a fit statistic that provides the quality
of fit for our model, but also includes a penalty for model complexity.
Very useful. Let's look at a piece
of research that actually uses
specification curves, these ones are birth orders. Now the research
question here is do the personality traits of first borns differ
systematically from those of their
younger siblings? There are a lot of design
decisions you need to consider for this
research question. For example, what do
we do when the number of siblings varies
between families? What exactly does
firstborn mean if someone has a half
sibling or step sibling? What about the age
gaps between siblings? What do we do if the
age gap is pretty close between siblings or if they're
really far apart in age? The different answers to
these questions will lead to different analyses and this is again a multiverse problem. In this case, the researchers
found that about 90% of the specifications didn't
show a positive effect. The outcome they're
focused in on is something called
positive receptivity, that's the tendency
to payback favors. What they found
was that sometimes firstborn showed
positive receptivity, but in other cases it was the youngest in a sibling group. Given these results, it's
difficult to make the case that the overall finding of
birth order effect is robust. In the last example, I call these the classics. Like specification curves, this is more about
how to visually represent multiverse
modeling results. This figure is found in an
article by Steegen et al. It explores how we can use
multiverse analysis to better understand how data processing modeling choices impact results. Each panel on the screen represents a key
social factor in their study and the cells represent model
results as p-values. Gray cells are those
results that are significant from a hypothesis
testing perspective. If we zoom in on social
political attitudes, we see that if we use R1, which here just represents
a modeling choice, the results are almost
always significant. If we use R2, we never get
significant results. If we use R3, the results are mixed. In this way, it works like
a specification curve, but in a less visual
way but it helps us understand the impact of
modeling choices on the results. A sensitivity plot is a second classic example
we can consider. Now, strictly speaking, this is not a multiverse
analysis and that we're not including a multitude of study results in
our presentation. However, this is a good
way to show the range of input values and how they can impact an outcome of interest. It's a very common approach in many industries, including
financial services. As this is an applied program, I thought I would
share it with you. In this case, the outcome of interests is a financial metric, net present value, or NPV. The factors that can affect NPV in this case are
the individual rows. Each row illustrates the
overall impact the factor may have given a 20% point
swing in its value. In conclusion, I
want to return to the research of
Young to consider categories for our outcomes
of our multiverse analysis. When we conduct a
multiverse analysis, our conclusions can fit into
roughly three categories. First is that under
a wide variety of specifications,
the results hold. That is, the underlying finding or relationship
we have identified is consistently found in many different modeling
specifications. Now when this happens, it's generally good news because it means that our findings
are robust, they hold. A second possibility
is that we find one or even maybe a few
model components are driving the results. For example, if we're studying home prices and our model inputs include 30 possible factors, we might find that home
size and neighborhood are the factors that are really driving the significant results. This is a possibility for a multiverse analysis and usually means we should
study these factors more. The final option here is
the worst one, that is, we find the relationship we're studying is not very robust. One or maybe just a few
specifications out of the many that we model
yields significant findings. This may suggest the
finding is driven more by model specification
than by the data. That's it for this
lecture on examples of multiverse analysis. We'll
see you in the next one.