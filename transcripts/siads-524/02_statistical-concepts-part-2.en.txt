Welcome back. This lecture is the second
installment of some of the key concepts and vocabulary
we'll use in this course. As these terms are
used in the course, It's important that you
develop a firm grasp of them. Let's get started.
In this lecture, we're going to focus on
three general areas. The first is probability
and probability density. Second is how we plot probability and
probability density. The third is the t-Distribution
and degrees of freedom. All ready, I've used the
word distribution a lot. Let's just pause
and make sure we acknowledge that there are many important uses of distribution in statistics
and data science. This might include
how we model data. We might use them for
hypothesis testing. We certainly use
them to estimate population parameters
or predictive modeling, even simulation. In this lecture, we're
going to consider some of the distributions that are used when we present
uncertainty. Some of these distributions
are not as common. Others like the t-Distribution, are used in many different
statistical of settings. We're going to start with the cumulative
distribution function, commonly referred to as CDFs. Let's start with an example and then we'll get into a CDF. Suppose a British hotel
company is considering building a new hotel
in a tourist area. Now, it's inexpensive
area to build and manage. It wants data to
support the decision to either build the hotel
or pass on the project. Under a certain set
of assumptions, the finance team projects that this new hotel project will return 1.8 million
pounds per year. Now this is helpful, but management also
wants to know what the probability is that the project results in
an adverse outcome. That is, what's the probability that the project loses money? Or that the probability that the project loses
a lot of money, which they define as 2
million pounds or more. The finance team says they have this level of detail
in their analysis, but they need help
communicating it. How could we show Management
this information? This is where the cumulative distribution function
comes into play. The cumulative
distribution function provides the probability
that a variable takes a value less than or
equal to a given x value. Now it's useful when we want
to know what percentage of observations are at or
below a particular value. Another way to say
this is that it really focuses on value minimization. The relationship
plotted in a CDF is the percentile rank
associated with the variable. The important thing to remember, the CDF allows the user to
read directly from the y-axis the probability associated with achieving a given x or lower. Let's return to our problem. The finance team
needs to display the probability that the
project loses money. To illustrate this value, we look at the x-axis and
we find the value for zero, which represents
zero British pound, meaning the project breaks even. We can look at where x
equals zero intersects the CDF and trace that intersection point
over to the y-axis, which is expressed in
terms of probability that a value falls at zero or
less than zero in this case. Based on this
implementation of a CDF, it appears there's about a 50% probability that the project returns no
money or loses money. Continuing on, if we want
to see the probability that the project loses 2
million pounds or more, we find -2 on the
x-axis and read the associated cumulative
probability from the CDF. Here we see the probability
is relatively low, less than 5%, even
so, it's not zero. Management is very happy with the information we provided. They've come back with
a final question. What is the probability that the project returns 1
million pounds or more? To answer this, we need a
different statistical function, the complimentary distribution
function, or CCDF. As I mentioned, the CDF
provides the probability that a value is less than
or equal to a given x. What we need is its complement, something that works in
the other direction. A function that returns the
probability that a value might be greater than or
equal to a particular value. Again, this is called the CCDF, the complementary cumulative
distribution function. If the CDF focuses on
value minimization, the CCDF, focuses on
value maximization. Management wants to know the
probability that a value is equal to or larger
than 1 million pounds. Using the CCDF, we look for the
intersection point at x=1 in the CCDF curve. We trace that over
to the y-axis. We find the probability
is approximately 18% that a value in the data
distribution is equal to or larger than 1. Translated to business terms, there's roughly an
18% probability that this new hotel project returns Â£1 million or more per year. Now I'm moving on to
the next major tool, we have the probability
density function. A probability density
function or PDF, provides the probability
that a value of a variable will fall
in a given range. It's similar to a histogram, but it plots continuous
rather than discrete values. For example, we can use the probability
density function to find the probability
that the x-variable fell between -2 and 1, and represent that
probability visually as area. It is important to note
that the y-axis of the PDF plot provides probability density,
but not probability. Probably density can't
be read directly, meaning I can't directly infer the probability
associated with a given value in the distribution just
by reading the y-axis, I have to use the area. I should also note that
the probability that a random variable takes
on a value exactly equal to a particular number is zero because the probability of any single point is zero in a continuous distribution
like we have here. Another characteristic of the
PDF is that the total area under the probability density
function is equal to 1, indicating that the random
variable is certain to take on a value within the
range of the PDF. The probability density function is also always non-negative, meaning that it will always return values greater
than or equal to 0. Let's spend a moment and connect these two concepts that
we've discussed thus far, the PDF and the CDF. Then recall the CDF provides
the cumulative probability, that is the probability
that a value of a random variable is less than
or equal to a given value. The PDF provides the
probability that a random variable takes on a value between two thresholds, between one and two. If we integrate the PDF from negative infinity
to a given value, that probability is equal to
the CDF of a given value. That's how they connect. The next concept I want to talk about is degrees of freedom. As we map out the
world of statistics, there are many places where we encountered this term,
degrees of freedom. We might encounter
it in modeling, such as noting the degrees of freedom for regression model. We might encounter it in
inferential statistics, such as when we run an ANOVA, F-test, or chi-square test. We might encounter
it when we build distributions such as
the t-distribution, which we'll explore
more in this lecture. We even sometimes use the
phrase more informally, such as researchers' degrees of freedom in a
multiverse analysis. What are these degrees
of freedom anyway? Degrees of freedom can take on a different definition depending on the statistical context. Generally speaking,
the degrees of freedom is the number
of independent values that are free to vary when we're calculating some
statistical estimate. For example, suppose we have a sample space consisting
of three numbers 1,2 and 3. If we know two of the values, say one and two, and we know the mean equals 2, then mathematically, there's only one possible
value for the third number, and that is the number 3. Two of the values are free to
vary. They can be anything. But the third value is determined to satisfy
the desired mean. For this reason, Bruce Thompson writes that degrees
of freedom quantify how many scores in a
dataset are free to vary in the presence of
a statistical estimate. Depending on our context, the way we calculate
degrees of freedom varies. Sometimes it's related to
the number of parameters in the model or the number of
rows and columns in a table, like a chi-squared test. A general way to
conceptualize it is sample size minus the
number of parameters. In the case of the
t-distribution, which we use in this course, degrees of freedom are
found by taking N-P. In this case, we
have one parameter, the mean, so it's N-1. That leaves N-1 degrees of freedom for estimating the
underlying variability. If we have a dataset with just three values, 4,5, and 6, which would be a very
small dataset and one parameter we're
working with the mean. The degrees of freedom
for this t-Distribution would be three minus one
or two degrees of freedom. Let's talk more about
the t-Distribution as it's really important to the
work we do with uncertainty. It's not often that the world of statistics intersects
with the World of Beer. But that's exactly
where the story of the t-Distribution starts. The t-distribution was created
by William Sealy Gosset. He was a chemist
and a statistician who worked for Guinness
Brewing Company, is best known for his work in statistical techniques
with small samples like the ones he had in brewing. It was towards this end that he created the t-Distribution. Now, Guinness wouldn't
allow it's staff to publish scientific
papers under their real names or mentioned Guinness or beer or even
brewing in their papers, Gosset had to use the moniker
student for his papers. That's why we know it as the student t-distribution and not the Gosset t-Distribution. He was a contemporary and friend of both Pearson and Fissure. The t-Distribution looks a lot like the normal
distribution. The shape of the
t-Distribution varies depending on how many
degrees of freedom we have. We'll start by comparing
the t distribution to the normal distribution with very low degrees of
freedom, let's say five. In order to have five degrees of freedom for t distribution, we would have six data
points in our set. We subtract one
parameter, the mean, from the number
of data points to get five degrees of freedom. Now, at lower
degrees of freedom, the t-Distribution
has thicker tails and a shorter peak than
the normal distribution. This makes for a more
conservative distribution for small sample sizes. We use the t distribution of situations when we don't
know the standard deviation of the population or the sample size we
have is less than 30. As I mentioned, the
t-Distribution converges to the normal distribution as
degrees of freedom increase. We'll start with this view of a standard normal distribution
centered at zero. We'll overlay this
normal distribution with t-Distributions of various
degrees of freedom. Let's start with
a t-Distribution with five degrees of freedom. The differences between the two distributions
are not worthy with the t-Distribution having thicker tails and
a shorter peak. I added a second t-Distribution, this one with 10
degrees of freedom. It's peak is closer to that of the normal distribution and its tails are a little
less thick than the t-Distribution we had
with five degrees of freedom. At 20 degrees of freedom, you can see the convergence really starting to take shape. The two distributions have
some minor differences, but they mostly overlap. Here's a small multiple view of the t distribution approaching
the normal distribution. In the upper left, we have
five degrees of freedom, which would be from a
very small dataset and thus quite different from
the normal distribution. In the bottom right, we have 50 degrees of freedom. Notice the
t-Distribution is almost identical to the
normal distribution. We use the t-Distribution for
uncertainty representation, particularly for confidence
intervals where we don't know the standard
deviation of the population. That's often the case in
statistical analysis, as we often don't have access
to the full population. Wrapping up here, the
focus of this lecture was the statistical
foundations of uncertainty. CDFs and PDFs focus on how we calculate in visualized
probabilities for a range of values. They do that very differently. CDFs provide probabilities, probability density functions provided probability density, which is why we
calculate the area. The t-distribution helps us model uncertainty by providing a statistical distribution
when sample sizes are small and or we don't know the standard deviation
of the population. One used for the t-Distribution and uncertainty is to aid in the calculation of the width
of confidence intervals. We'll use these terms a lot in many of the lectures
in this course. If you ever have any questions, please refer back to
this lecture. Thank you.