Welcome back. Let's continue our exploration
and discussion of Bayesian data analysis
by looking at one of the first key components to Bayesian data analysis,
the prior distribution. We ended the last lecture with this quotation from
Michael Bentoncourt. He says, Remember that using Bayes theorem doesn't
make you a Bayesian. Quantifying uncertainty
with probability makes you a Bayesian. It is a fundamental to a Bayesian perspective to quantify and represent
uncertainty. Candidly, it's for
this reason that we're diving into the details of
Bayesian data analysis. If uncertainty is fundamental
to Bayesian data analysis, then we should consider in
our quest to understand, quantify and present
uncertainty. We've already
learned about using Bayes formula to calculate a new probability given some prior knowledge
in new evidence. But in this relatively
simple application of Bayes theorem, the value we produce
was a point estimate, a single probability without explicit consideration
of uncertainty. How does it work? How do
Bayesians quantify uncertainty? Or more importantly,
how is uncertainty fundamental to the Bayesian
data analysis process? What we're really asking is how we move from the application of Bayes theorem to
Bayesian data analysis. In this lecture, we'll begin
to learn how we can extend the fundamentals
of Bayes theorem to calculate not just
a new point estimate, but the new distribution
of plausible values. Bayesian data analysis employs the same general
idea of Bayes rules. That is, we update prior beliefs with new evidence to update our probabilities. The key difference being that
we're now using models and the probability
density functions or probability mass functions in place of numerical
probabilities. Instead of feeding points into our equation,
we're feeding models. Put differently,
the main idea of Bayesian data analysis is we use probability distributions to model uncertainty about
unknown parameters, things we want to learn about in a statistical model and then we update those
distributions based on observed data using Bayes rules. In making this transition, the process gets a little simpler and also a
little more complicated. First, let's simplify our
framework for analysis. We start with a prior
probability distribution over a hypothesis space, which represents
our beliefs about the hypothesis before
seeing any data, remember prior is before data. We then update this
prior distribution to a posterior distribution, which represents
our beliefs about the hypothesis after
seeing the data. The posterior distribution
is proportional, that's what that
little sign means, to the product of the prior distribution and the likelihood of the data
given the hypothesis. The posterior, the prior and the likelihood are
all distributions which we have to model. Our focus and Bayesian
data analysis are these three distributions. Let's look at these three
distributions before we dig into a Bayesian
data analysis example. On screen is a picture of
three distributions we'll be working with as we unpack
Bayesian data analysis. We have our prior
distribution in purple, our likelihood function in red, and the posterior
represented in green. Just like in Bayes rule, the posterior is the result of how we combine the prior
and the likelihood. The difference, of course, is that we are working
with distributions instead of single
probability point estimates. We will have lectures
that focus on each of these unique
distributions. This lecture focuses
on the first of these distributions;
the prior distribution. To see Bayesian
data analysis and action and to also learn
about prior distributions, let's consider a case study. This case study is focused on
college application rates. Acme University has
launched a degree in hotel management
and hospitality. It's run this program
for just a few years, but it recently worked to
update its website to improve the attractiveness of the hospitality program
to perspective students. Acme University has created
two new designs that it's considering to use for its
hospitality program website. The Dean of the School of
Hospitality is wondering how we determine which design
is more effective, design A or design B. We set up an experiment,
an A/B test, where we randomly select new visitors to
the website and we assign them either to the
A design or the B design. We run this for a short period. Over that period of time, we get about 100 visitors
for the A design, and 100 visitors
for the B design. We capture the application rate. What percentage
of these visitors actually apply to the
hospitality program? For the A design, we found that 22% of the new visitors
apply to the program. For the B design, a little bit less 18% of the visitors applied
to the program. Now, if we were operating under a Frequentist perspective, we could do a lot with
this data, just as it is. For example, we could calculate uncertainty around
these estimates. Twenty-two percent of the
100 visitors applied, we could say, what's
the confidence interval around that estimate? On screen, we have the formula for the confidence
interval of a proportion. Not even gone
through this before, but I have mentioned that different kinds of
statistics have different calculations
to analytically estimate the
confidence interval. Here's the one for proportions. You can see the similarities to other formula we've used to estimate
confidence intervals. You take the proportion
plus or minus a z value, which is determined by the
level of confidence we want, in this case, let's say 95%. Then we're going to
take the square root, and this is where
things are different, where we're going to take our
proportion, in this case, 0.22 times its complement, 1-0.22 divided by
the sample size. When we do all that math, what we find is that we get
22% plus or minus 8.1%. Or another way to say that, is that the confidence
interval ranges from 13.9% to just over 30%. Now that's the
Frequentist approach. Hopefully, that's very
familiar to you at this point. We're now going to build
towards the Bayesian approach. The main difference in the Frequentist and
Bayesian perspective is that under the
Bayesian perspective, the parameter of interest is treated as a random variable. In this case, the parameter
of interest is a proportion. We're going to treat that
as a random variable. Under the Frequentist
assumptions as we just saw, the variable interest is
treated as a fixed value, 22%. Put differently, the
Frequentist view provides a fixed estimate, whereas the Bayesian approach provides a
probabilistic estimate. It's a totally
different process. If we were to approach
this problem as Bayesians, the first thing we do is
actually not collect data. The first thing we do is
before we see any data, we must model our prior beliefs. As we've said several times now, the prior distribution
represents our knowledge about the parameter of interest
prior to seeing any data. The prior is the biggest departure from
conventional statistics, but also one of the most
interesting possibilities in Bayesian data analysis. It provides us the
opportunity to build on knowledge over time. Rather than have every analysis starting out with a blank slate, we can directly
incorporate what we know into our new analysis. Done well, this can be a significant advantage to
Bayesian data analysis. The choice of the prior
distribution is subjective and depends on prior knowledge or assumptions about
the parameter. Now the prior distribution
can be chosen based on maybe previous studies we've
seen about this phenomenon, it might be based
on expert opinion , or even convenience. The prior distribution can
be continuous or discrete, it can be unimodal
or multimodal, it can be symmetric or skewed. There's a lot of detail here. We're going to talk about
three general types of prior distributions. The first one is called
the subjective prior. The subjective prior
involves choosing a prior based on subjective beliefs or knowledge about
the parameters. For example, a researcher may choose a prior that reflects their belief that this
particular parameter they're studying is likely
to be close to zero. The next type of prior is
called the empirical prior. This involves choosing
a prior based on empirical data or from
previous studies. For example, a researcher
may choose a prior based on the results
of a previous study or a meta-analysis. The last type of prior, generically speaking, is
the non-informative prior. This involves choosing
a prior that is as uninformative as possible to
avoid influencing results. Examples of
non-informative priors include the uniform prior
or the Jeffreys prior. Now there are two
broad questions we can ask when determining
a prior distribution. First, what do we know? How much information do we have? What do we think is
a reasonable range, for example, for our parameter? In this example,
what do we think is a reasonable range for the application rate for
visitors to our website? Then the second component is how confident are we in
this information. Our confidence will
determine the shape and overall variance in our
prior distribution. There are many functions we
can use to create a prior. Now we're not going to
go over all of them, but one of the more
common choices is the Beta distribution. In Bayesian data analysis, the Beta distribution is a probability distribution
that's commonly used to model prior
distributions for other probabilities
or proportions. The Beta distribution is a continuous probability
distribution defined over the
interval from 0-1, so it goes from 0-1. It's also defined by
two key parameters, which we usually
call Alpha and Beta. These parameters can be used to control the shape of
the distribution. Now as you can see on screen, the Beta distribution can take
on many shapes and forms. It's very flexible. We're going to look
at three common forms of the Beta distribution. The first general shape
is the uninformed prior, where we use one for both our
Alpha and Beta parameters. Now we use this shape
when we do not have information or confidence
in our information, and we don't want to pass much information
into our likelihood. In this situation,
we're basically ignorant and we want our data to heavily influence
our posterior outcomes. This is sometimes a
common starting point, particularly if we don't know much about what we're studying. The next general shape I want to talk about is where we set the Alpha and Beta
values to five. In this situation,
we might believe that the true parameter
value lies around 0.5 but we also have a fair amount of uncertainty
around this belief. We want the prior to inform but with this amount
of uncertainty, our prior will also adapt
to new data over time. The last general shape we're going to talk about is where we set the Alpha and Beta
values to both 50. Now you notice that
this distribution is still centered around 0.5, but the variance is
much more narrow. We would use this prior
if we're confident that the true value lies
between 0.4 and 0.6. Now a way to illustrate
how priors can influence posteriors
and how priors can update with new data is to consider how we model
coin probabilities. Suppose we are given a
coin with two sides, heads, and tails but
what we don't know, is, is the coin is fair, meaning is it equally likely
to get heads and tails? We want to use Bayesian
statistics to model our prior belief and update it with data
as we receive it. We're going to run through
two examples here. In the first example,
we're going to start with a prior that's uninformed. We have the uninformed prior. It's just a flat line
across the range of possible values I have
for my coin from 0-1. We flipped the coin and
our first flip says tails. What happens is because
the prior is uninformed, it changes quite a bit. It tilts heavily to what
we're going to learn to be the tail side in
this experiment. We flip the coin again
this time we get a heads. Notice how the shape of the posterior changes
quite a bit again. In this case, what
it's telling us is that we're somewhat confident that the true value probability of getting heads or tails is 0.5 but there's a lot
of uncertainty around that estimate because we only
have a little bit of data. We continue to flip the coin. This time I get another head. You can see the
distribution starting to shift to the head side. I get another head shifting to the head side again,
continuing that process. Right now what it's
saying is this coin might not be fair four heads and one tails right now
with an uninformed prior and with a
little bit of data, maybe this coin really
favors heads, five heads. It's getting more distinct, more certain that this
coin is biased six heads. Finally, after six heads, we begin to get tails. If we ran this
experiment many times, what's going to happen over
time it's because I already know in the data simulation
the coin is fair. If I run this long enough, it's going to eventually
center around 0.5. You can see the Bayesian
model catching up to that. As we get more and more data, our overall uncertainty and
our posterior is going down, it's getting more
narrow and it's moving closer to
our true average. You could see by starting
with an uninformed prior, the posterior takes
many different shapes, particularly when we don't
have that much data. But over time that data will form the posterior
around its own image. Now let's run that
experiment again, this time using a
different prior. Our new prior is going to have a little bit
different shape. It's more informed but
still has some uncertainty. You can see it's
centered at 0.5, but what we're saying
is that there's tremendous amount
of uncertainty as to where the true value lies. Really what we're saying is
that true value still could lie almost anywhere
in the distribution. It's just more likely
to be in the middle. We begin to collect data. After our first coin flip, what we find is that
we get one tail. Again, notice that the shape of our posterior in this case still retains quite a bit of the prior shape because again
it's a more informed prior. We flip again, we
get another tail. It's again, still
keeping that shape. What it's doing is
it's shifting more to the tail side
because it's saying, well, based on the data, I think that true
proportion probably lies to the left of 0.5, but it's still retaining
some of that overall shape. We continue the
experiment, a third tail. Again, same pattern continues. A fourth tail, it's getting more narrow, getting more biased, but again, still
keeping that shape. Fifth tail. Right now we're
at five tails, zero heads. It's still retaining our
overall prior shape, saying that there's this
almost normal curve to our uncertainty, but we're well off into the tail side of
the distribution. Then we keep on flipping and
we start to get more heads. It's going to begin
to move back towards our original assumption that this distribution is going
to be centered around 0.5. Because just with one
head there you can see that our posterior shifted quite a bit because our prior is
influencing that still. After 20 heads and 26 tails, you can see that our new
posterior is getting closer and closer to our overall value
of 0.5 in the middle. Then if I continued
on in this case, I have 124 heads and 132 tails, you can see that
we're getting very close to that middle value. Again, just like
the previous one, that uncertainty is narrowing. Now you notice in
the second example, the journey was a
little different. In the first example with
an uninformed prior, our posterior was
changing quite a bit. In the second example, when we have a more
informed prior, our posterior was
slower to change. This is the lesson I want you to get around prior distributions. Now, before we get back to our example
with Acme Academy, I just want to say
one more thing. It may be a tempting
conclusion to say that while as n increases, our prior distribution
becomes a lot less significant because I think you saw some of that before. That's only partially true. Certainly, as our sample
size and our data increases, our data is going to
have more influence in what the posterior
looks like. But it also depends on
how much information that new data brings into our total Bayesian data
estimation process. That's really going to determine how the posterior shapes. It's not as simple as
saying with larger data, the prior decision
becomes less significant. Returning back to
our AB test example, what prior should we use
given what we just learned? What is our preexisting belief of what the application rate is? Again, how confident
are we in that belief? To again simplify things, we're going to stick with
the Beta distribution. We're going to pick
from three choices. Uniform prior,
representing a situation where we really don't have any prior belief as to what the true
application rate might be. We have what I call
a centered prior, where we think the
application rate is most likely to be around 50%, but frankly, everything
is possible. This is, again more informed, but still has quite a
bit of uncertainty. And then on the right,
we have more of a skewed prior where we're pretty confident the application
rate is going to be less than 50 percent and most likely to be around 20 percent. For this example,
we're going to use actually that skewed prior to represent our belief that the application rate is
likely below 50 percent. In the next lecture, we will consider
the next component in Bayesian data analysis, the likelihood function.
We'll see you there.