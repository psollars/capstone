Welcome back. Now that we
know a little bit about how to calculate a
confidence interval. In this last lecture
in this series, we're going to turn our
attention to discussing how we interpret confidence intervals and communicate their meaning. Suppose we have a 95 percent confidence interval that
we want to interpret, on-screen we have two
different interpretations of the 95 percent
confidence interval. The first one reads, there's a 95 percent chance that the true population parameter
lies within the range. The second possible
interpretation is that the procedure
used to create this confidence
interval will capture the true population parameter
95 percent of the time. Suppose we take 100 samples
from a population and compute a 95 percent confidence
interval for each sample, about 95 of the 100
confidence intervals generated should contain the
true population parameter. Take a moment and reflect on each interpretation
that I provided. Which one do you
think is correct? Unfortunately,
Definition 2 is correct. I say unfortunately because
the second definition is not nearly as clear and
as intuitive as the first. If I poll most users and many data scientists about the true meaning of a
confidence interval, you'll more likely get a
definition closer to Number 1. Why is the second
definition correct? It has a lot to do with
the frequentist view of statistics and some
philosophical matters. However, the basics is that under frequentist
assumptions, the confidence interval is about the process of calculating the confidence interval and not about any
individual interval. So we cannot say that
there's a 95% chance that our specific confidence interval contains the true
population parameter. From a frequentist perspective, the interval either does or does not contain the parameter. Bruce Thompson reminds us that one does not
equal infinity. Meaning that the 95
percent confidence comes from an infinite number of confidence intervals
of that size and doesn't apply to any one
confidence interval. If this seems less intuitive of an interpretation than you were expecting, you're not alone. What we really want to say is that we have 95
percent confidence that the true population mean falls within our
confidence interval. Unfortunately, that's
just not the case. Let's look at a visual
example to help illustrate the
process and perhaps identify a path forward
for communicating and interpreting
confidence intervals. In the serine theory website, you can explore
confidence intervals. It's a helpful
visual confirmation for how we interpret CIs. Suppose we start with
a sample that we want to use to estimate
the population mean. That's the population parameter we're interested in this case. If we want to
examine a 90 percent confidence interval for a mean, we can repeat sample from
the population and construct a 90 percent confidence
interval around each sample. You can see the site also
provides a running total of the confidence
intervals containing the population mean
and those that do not. One hundred and forty-three
out of a possible 1,431 do not contain the
population parameter. Which is what we'd expect for a 90 percent
confidence interval, 10 percent of the
confidence intervals do not capture the
population parameter. If you're thinking,
wait a second, if 90 percent of the
confidence intervals generated capture the population mean and
I randomly select one, can I say I'm 90
percent sure that the CI will capture
the population mean? Unfortunately, the
answer is still no. Once we move from
an infinite number of confidence intervals generated by the process to a
single confidence interval. We can't say we're
90 percent sure that that particular
confidence interval contains the population mean because of what
Bruce Thompson said, one does not equal infinity. The confidence level is
about an infinite process, not about an individual
confidence interval. What can we say then about
these confidence intervals? Over the years, I've developed a bit of a workaround to this
philosophical conundrum. When I build
confidence intervals, often 95 percent, I like to say it represents a reasonable range for
our population parameter. I don't specifically
mention 95 percent because that specificity gets us into trouble as I tried
to illustrate. Knowing that 95 percent of the confidence
intervals generated under this process should capture the
population parameter, I feel comfortable saying
that the specific CI generated is a reasonable range for the population parameter. Even though we don't
know if the actual CI we have does contain the
population parameter. I want to close with a few general thoughts on confidence intervals.
I have four of them. The first is that the
greater the level of confidence you desire, the wider the confidence
interval will be. Hopefully, by this point, you understand the intuition
and it's fairly clear. The more certain we
want to be to capture the population parameter with
our competence interval, the wider we must go. Secondly, as our uncertainty, that is our standard
error increases, so does the width of our
confidence intervals. On-screen, we have three
confidence intervals that are all 95 percent CIs. The confidence
intervals get wider as the standard errors increase. That's because we're
multiplying our critical values by larger numbers when the
standard errors increase. Number 3, a quick thought on visualization of
confidence intervals. We'll get more into
this topic later. But one way we measure
the quality of an uncertainty
visualization is how easy it is to ignore
the uncertainty. Uncertainty researchers
find that it's a lot easier for
people to ignore uncertainty if we just have a point estimate and a
single confidence interval. When we include a second confidence interval,
for example, layering a 66 and a 95th
confidence interval together, it's more likely that the
end user will appreciate the underlying uncertainty
around the point estimate. Number 4, one common
mistake in calculating confidence intervals is using
the wrong quantile values. A 95 percent confidence interval has two-and-a-half percent of the area of the
curve at each tail. In the picture above, we have a vertical
reference line at the 2.5 percentile and 97.5
percentile lines. Between the 2.5
percentile and 97.5 percentile is 95 percent
of the distribution mass, which is what constitutes our 95 percent
confidence interval. Why is outside that mass, is 5 percent of the data divided between the upper
and lower tail. The mistake I sometimes see
is students may put 5 percent of the mass in each
tail for a total of 10 percent total outside of
the confidence interval. This would correspond to a 90 percent confidence interval as opposed to a 95 percent
confidence interval. In conclusion, I want to end this series of lectures on the confidence interval
where we began. Which was looking at a piece of research by the Pew
Research Center on the relationship between disagreement and the
amount of likes, comments, and sharing
of social media posts. Hopefully, as you look at this graphic and you see
the confidence intervals, you have a better
understanding of not only, where they came from,
how we calculate them, but also how to interpret them.