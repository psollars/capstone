Welcome back. In the
previous lecture, we learned how to calculate a confidence interval
from a single sample. In this lecture, we'll
explore how to calculate confidence intervals
from multiple samples. When working with
multiple samples, we use a three-part recipe to calculate the
confidence interval. First, we need a statistic,
like we said before, this could be a
mean or proportion, difference in means, any number of things. Second, we need a
sampling distribution. Again, because we're working
with multiple samples, either through organically
obtained from a population or calculate a synthetically through bootstrapping will
have a sampling distribution. The third ingredient is
a level of confidence. Again, this can be anything, common value being 95 per cent. That's what we use
in this example. In this case, we're going to
explore what the uncertainty is around a precision statistic. Precision is a performance metric for
classification models. I'll explain that more
in the next few slides. Our sampling distribution will be derived through
bootstrapping. We're going to stick with a 95 per cent
confidence interval. Suppose we've built a
classification model that predicts whether it
will rain on a given day, given the current data, we feed lots of data
into the model and it outputs the probability
of rain for every day, which we convert into a binary, either rain yes or rain no. Now we can calculate
the precision metric, which asks what percentage of rain predictions
are accurate. Suppose we calculate
this for our model and we found it to be 72 per cent, 72 per cent of the
days we said it would rain, actually had rain. Now while it's helpful to
have this point estimate, we'd like to calculate the
uncertainty around that value. Because we don't have a
sampling distribution or even a simple formula to analytically calculate the confidence interval
around the estimate, we have to bootstrap the
data, feeding the model. Fifty thousand times
we bootstrap our data, fit a predictive
model to those data, and then calculate the
precision statistic and store it in a DataFrame, which we use to create a sampling distribution
of precision statistics. This sampling distribution and precision values provides us a means of calculating the uncertainty around
the precision value. Because we don't have a
sample distribution or even a simple formula
to analytically calculate the confidence
interval around the estimate. We bootstrap the data
feeding our model. Fifty thousand times
we bootstrap our data. Each time we fit a predictive
model to those data, we calculate the
precision statistic and we store it in a DataFrame, which we then use to create a sampling distribution
of precision statistics. This sampling distribution and precision values provides us a means to calculate the uncertainty around
the precision value. Now the advantage here with an actual sampling
distribution is we can measure the width of the
confidence interval directly. We don't need a formula. All we need to do
is directly read the values for our
confidence interval from the sampling distribution. We pull the precision
value associated with the two-and-a-half
percentile value inner sampling
distribution and we pull the value associated
with the 97.5 percentile value in our
sampling distribution. Between these two should be 95 per cent of the
precision values, giving us a 95 per cent
confidence interval. Using our bootstrap distribution
of precision values, we can find the
two end points for our 95 per cent
confidence interval. Our lower bound is 64
per cent precision, and our upper bound is
80 per cent precision. This means we have a precision
value of 72 per cent with a confidence interval
around it that ranges 65-80 per cent. This concludes the
lecture on calculating confidence intervals
from multiple samples. In our last video, we'll discuss general
interpretation issues with confidence intervals.
We'll see you there.