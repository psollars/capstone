Welcome back. In this lecture, we're going to review one of the key concepts from last week. We call it the big
idea of uncertainty. The big idea of this
course is that we use statistics to
estimate quantities or degenerate answers to
questions and we cannot be 100% certain in those
quantities or those answers. This is fundamental to, I think every aspect
of data science. For example, suppose you have an electric car battery
company that wants to make the claim that their new
electric car battery has a range of 500 kilometers. How would you
validate this claim? You might run an
experiment where you randomly select 100 batteries from the production line and test their average
range on a track. From this sample, you collect the mean, the
standard deviation, and the degrees of freedom and you might use a
one-sample t-test to compare against your
hypothetical population mean of 500-kilometer range. Using these datas
and assumptions, you can make a statistical
inference about whether the battery has a range
of 500 kilometers or not. Now, uncertainty is
fundamental to this process. To what degree is our
sample representative? To the assumptions of the
t-test in t-distribution hold. How certain are we
in our conclusions? Lots of uncertainty. Take another example. Suppose we want to predict
future snowfall in Michigan given historical
patterns we have observed. Now, we don't know the future, no one does but we can collect
data from past winters. We can structure the
data so that we're looking at the
snowfall over time. We might create a
forecasting model to fit the data and ultimately make an educated guess about future snowfall
given past snowfall. These estimates are also
encumbered with uncertainty. The problem with uncertainty
is that we often set it aside to focus on the point estimate of what
we're trying to study, a statistic, a coefficient, a prediction, or a difference. A point estimate again
is our best guess or best estimate of
something that we can't know for certain. It's important that
we communicate it, but without uncertainty,
it lacks context. There are many examples of where point estimates are
commonly reported. Likely the most
common is providing a point estimate of
sample statistics. In the example here we note that the mean
hours of sleep for adults in Greece is around
7.26 hours per night. We can also report point
estimates through models, like a regression model
here that relates to the size of a Spanish
home in square meters against its price in euros
and we can use that to say that perhaps a 90 square
meter home in this region should fetch about
â‚¬152,000 in price and we certainly
use point estimates in something as simple
as survey results. For example, 44.3%
of data scientists use SQL on a regular basis
according to a 2020 poll. The point is that in much
of statistical practice, we produce point
estimates for our output. The point estimate or single value is what
users often expect. The question is, however, how much confidence do we
have in this point estimate? The point estimate may be
the most likely value, but we wouldn't be
surprised if the true value differed from our point estimate
in some meaningful way. The challenge with
all the examples that we just stepped
through is that they ignore uncertainty and
that's not something that is ever a best
strategy in data science. What are our options with
dealing with uncertainty? As already mentioned, one is to not represent
it or ignore it. We're setting that aside
is not a good strategy. We could always gather more
data and try and minimize it. That's often not an option both practically or from
a cost standpoint or the third is estimated
or modeling and that's again what you're going
to learn in this course. Now, in week 1, we talked about this important concept called the standard error
and I just want to briefly review three ways that we can get to a standard
error calculation. First, we could use multiple
samples from a population, and we could calculate
the same statistic for each sample and produce
a sampling distribution. From the sampling distribution, we just need the
standard deviation, which is in this case equivalent
to the standard error. The challenge again
with this option is that we often don't have multiple samples from which to draw which brings us
to the second method. The second approach
is to calculate the standard error analytically. Depending on the
statistic we're using, we may be able to use a very simple formula to
estimate it analytically. Some statistics like
a mean or median have an explicit formula
from which we can estimate the standard error
from a single sample. We'll learn more about this method throughout this course. Final strategy I want
to mention is taking a single sample and resampling it thousands of times to create thousands of
synthetic samples. From each, we can
calculate the statistic of interest and form a
sampling distribution. We then can calculate the
standard deviation of that sampling distribution
to find the standard error. Of course, we call
this bootstrapping. Let's go through an
example and see how these three strategies
could play out. Suppose I'm studying the
salaries of data scientists in Germany and while I'm interested
in the full population, I don't have access to it
so I do what we always do. I draw a sample. I sample all these data scientists in Germany and from that sample, I calculate several statistics. I calculate the mean and I calculate the
standard deviation. Now, these two statistics
describe the sample and if it's randomly drawn
and of sufficient size, they can be used as a
reasonable proxy for my population's mean
and standard deviation. But again, how well does that sample mean estimate
the population mean? To estimate this, we need to find the standard
error of the mean. The standard error is again the standard deviation of the distribution
of sample means. Let's step through the
three ways that we can get to that important
standard error statistic. First of all, we could
do repeat sampling. This isn't always a
realistic option, but it is an option. Again, we've got this population of data scientists in Germany and they all have
a salary that we could draw from so we could
collect not just one sample, but we could go out and
do two random samples, three random samples,
and do that many times over the course
of a period of time. From each of those samples, we're going to calculate a mean and pile those means up to make a sampling distribution of the means and then we can
read the standard error or the uncertainty directly from that sampling distribution. That's an option certainly
that we could pursue. The second option is the bootstrap option,
the synthetic option. This is where we take
the one sample that we do have of data
scientists in Germany, and we re-sample it
thousands of times. You'll notice this time, instead of having a nice
smooth distribution at the top representing
the population that we're drawing from. We have a jagged
histogram representation of the sample that we do have that we're going
to resample from. Again, we draw the sample
thousands of times, and each time we
draw the sample, we calculate the mean. We pile up those means in a sampling distribution
of the means, and we can read the uncertainty directly from that distribution. We have the standard
error right there. Our third and final option is to calculate the standard
error analytically. Now again, I want to repeat. This is not always an
option that we have available to us
and the reason is we need a formula to calculate the standard error
analytically off of a single sample and there's
many statistics that we don't have such a formula so we have to use one of
the other strategies. In this case, the mean
has such a formula, the standard error equals
the standard deviation of the sample divided by the square root of the
sample size of the sample. Once we create that calculation
in the value from it, we can use that for
our standard error and produce any numbers that uncertainty calculations
and visualizations. That's our third option. You're going to learn
a lot more about that option during the
lectures this week. That was a brief
review of some of the key ideas from week 1. I hope you found it helpful.