I know that many of
you may have taken Data Mining I
quite a while ago. So let's refresh your memory first about sequence
data and n-grams. We know that sequence data is
the data representation to represent a set of
categorical items. But the categorical items
is not just the planed set. They are organized
in sequential order. So the data objects that can be represented
as sequence data includes curriculum paths that you are familiar with
in the MADS program, or a DNA sequence, or the session of search queries, or any other behavior
sequences of the users or a sentence of words where trace of user actions,
so on so forth. For instance, in this example, we can see that if you treat every course as the
categorical item, then we can organize
multiple courses into the sequence based on
the prerequisite structure. Sequence representation can be formalized into
mathematical forms. For instance, we can use uppercase X to
indicate a sequence and we can represent
this sequence as a set of categorical items, x_1 to x_n, and every item
is associated with an order. So x_1 is associated
with the first position, x_2 is associated with
the second position, so on so forth and you can
see that now a flat set becomes a sequence. And you cannot swap  the order of the items. We can also write
down the sequence in these mathematical
forms as we have introduced in the
introduction of this class. We have seen this example. We can easily represent the protein sequences of species into the
sequence like this. So you can see that
the highlighted one is actually the
protein sequence of the Coronavirus. And what's interesting about  this sequence representation is that first, you can describe
the properties of a sequence with sequential
patterns. And in this case, we can see that all
these binding sites, all these motifs are actually
sequential patterns. And you can also try
to align the sequences together and compute the
similarity of the sequences. If you still remember
what we have covered in Data Mining I, we have introduced the Levenshtein edit distance, which is essentially trying
to align two sequences. So in Data Mining I, we spent lots of effort introducing patterns.
And in sequence data, the patterns are
particularly n-grams. And n-grams are very powerful. You can easily extract n-grams
from any sequence data. For instance, text documents. Text documents are
sequence of words. If we can represent text documents as a sequence of words, we can extract n-grams
or skip grams as sequential patterns
to represent the properties of the sequence. In n-grams, n indicates the number of items
in this pattern. In skip-gram, we can actually
relax the requirement that items have to be adjacent to each other and we can
add gaps into the items. So for example, if we
extract the n-grams or skip grams from
text documents, we can get phrases, we can get language expressions, for instance, "Ann
Arbor," "hot dog," or, "deep neural networks,"
so on and so forth. Similar as other patterns, we can also describe
the properties of n-grams or skip-grams
with a few statistics. For example, we can describe the support of the
n-gram or the skip-gram, basically based on how
many records or sequences a particular n-gram appears
in. And in many circumstances, the support is normalized by the number of total
records in your database. We can also talk about count. The count of the n-gram is
not the number of records, but the total number of times that the n-gram
appears in your database. So difference here is
that the n-gram could appear in one or more
sequences in the database. So count is not necessarily
equaling to the support. We can also describe the
frequency of the n-gram by the number of times that this n-gram appears
in your database, divided by the total
number of n-grams. Sometimes, if you don't know how many n-grams
are there in total, we can also use frequency
equivalently as counts. If there is no ambiguity, I will use either way. But if there is, I will
indicate whether we're using count or the normalized
or the frequency. Let me give you one example. We all know this person, right? So Master Yoda is a
famous character in Star Wars. And there are many
quotes that he has said. So for instance, this
least three quotes of his as our data base. So you can see that
we are looking at three sequences. We can extract n-grams from
this sequential database. And what we need
to do is to count how many times that
each unigram or single words appear
in this database. How many times each bigram
appear in this database. And how many times
trigrams appear. If you think that the
database is large, you can use smart algorithms like a priori to help you
reduce the computing time. But now let's just
look at the counts. If you count the unigrams, we can see that there are
quite a few words like "the", like "dark", like "you", "path", and we can actually
write down their support, and in this time,
unnormalized support. We can also count
the number of times that they appear in
total in sequences. So you can see that for the word ''the'' it appeared
in three sequences, so the support is 3. But it appeared
actually 5 times. So the count of the
word ''the'' is 5, which is larger than support. There are also other
words like "path", which appears in 2 sequences and also appears only
once in both sequences. Both the support and
the count are 2. We can further count the bigrams. For instance, "dark
side" and "dark path", we can also count the support
and count of trigrams, that means the sequential pattern of 3 words, for instance, the trigram "the dark
side" has the support of 2 and a count of 3. Well, this is just a trivial
example of three sequences. In reality, in
production we usually count the n-grams from a very, very large database.
For instance, Google provide this
n-gram viewer service, that they actually collected all the English literature between the year 1900
to 2000 -- 2010 actually. You can see that you
can type in any n-gram into this search service
and get its frequency. For instance, if we look
at the trigram that we have just gone over,
"the dark side", you can see that Google has
plotted its frequency over time. And if we look at its
frequency by the year 2000, we can see that it's quite small, it's about 0.00005 percent, in the complete English
literature of that year. We can also query the frequency
of bigrams, "the dark", you can see the frequency is actually much larger
than "the dark side". And we can query the
frequency of another bigram. in case you're curious
about the "star wars". Then you can see that
the frequency of the bigram "star
wars" is quite small, is actually much smaller
than the bigram "the dark", so on and so forth. So the question is, how can
we describe this behavior of the frequency or the counts
of bigrams and trigrams, or n-grams in general? So in Data Mining I, we have
talked about many ways to interpret the interestingness
of patterns. For instance, we use correlation, we
use mutual information, and we use many other statistics. Here we're looking at three other very
interesting questions. First, if you look
at examples before, you may be curious
why some bigrams are always more frequent than
corresponding trigrams? For instance, why is the
frequency of "the dark" is always larger than the
frequency of "the dark side"? Another question is, why are some n-grams more frequent
than others? For instance, the bigram "the dark" and the
bigram "star wars"? Why is the former much more
frequent than the second? Well, we know that maybe
that's because people don't use the latter bigram that much. But how can we use statistics
to describe this behavior? This is the even more
interesting question that cannot be answered by any of the
measures we talked about before. What would the next
item be in a sequence? For example, you have already seen another quote from
you that answers that, "In the end cowards are
those who follow the dark _____" and the quote end here. And then you are now  trying to make a guess: What would be the word
that follows "the dark"? Would that be "the dark
side" or "the dark path", or even "the dark lord"? So this is known as
sequence prediction, which will be introduced
in the next video.