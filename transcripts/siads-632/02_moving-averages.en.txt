So we have talked about different ways to
discover trends from time in this data. We talk about freehand methods. We talk about regression methods. We talk about simple smoothing
based methods such as spinning. As we can see that binning
has lots of problems and this look at even more rigorous
measured is known as moving average. I'm sure they have heard
about moving average before. An idea is actually quite simple. Basically you want to smooth the data
using previous observations. The idea is that, if I observe
a large or small number right now, I don't trust that, right. I want to look at observations in
the previous years or in previous days. Because I trust the summary
from multiple observations. And of course,
I trust the recent observations. So that's why the recent data points
can be summarized, can be averaged. And in this case, were computing
the average of, given timestamp and we're computing the average
this moving overtime. You can see that we're talking
about summarizing data using previous observations. But sometimes when people are computing
moving average the sunrise over the visions before and
after the current timestamp. We don't recommend that because later on,
if you're making forecasting, if you are making predictions overtime series,
we don't want to look into the future. So throughout this class we
are talking about moving average. We're talking about using previous
observations to smooth your data. And moving average is also known
as sliding window average or smoothing, right? The basic idea is that you have a window
of a number of observations and then you compute the average
of this values in this window. And then you use that to surrogate
the value that you actually observed. And because the window is moving over time
then this is known as the moving average. You can also interpret moving average
as the way of using overlapping bins. So you can see that actually solves the
problem of arbitrary in binary right and the numbers of bins. Does this remind you of something? Right, I think this reminds us of the use
of ngrams, right, in the sequence data. If you remember that we are also using
sliding window to extract ngrams from, the sequence data and we even have the fancy measure
called shingling if you remember. So why do we need moving average? Because if we use moving
average to smooth the data, it could help us eliminate
some cyclic patterns or some irregular movements that are not
necessarily useful for decision. If you only want to know the trend or
more like long-term patterns, long-term cyclic patterns, bigger patterns,
than using moving average always helps. Let me give you some simple
examples of moving average. You can see that in this plot were also
plotting airline passenger data we're actually using the moving average with
different window size to smooth the data, right? You can see that in the orange curve were
using a moving average of Windows 16. In other words, we're using the previous
16 observations to smooth out the current observation, and you can
see that the pattern is fairly clear. You can also increase or
decrease the size of the window. For instance,
you can see it in a purple curve. We're actually using the previous
128 observations to without, the current observation, and if you do
that, the trend becomes quite flat. Why, because most of the cyclic
patterns are gone after smoothing. We can talk about moving, averaging more
rigorous way in more mathematical way. You can see that to compute
moving average of window k, given the original time series,
that is the least of time stamp values. We're getting another smooth time
series data that has exactly the same number of observations. So different from binning,
if you use binning, you're actually reducing
the number of observations. If you use moving average,
you're not losing any data point, right? But the numbers are different,
the values x prime are different. In particular, the new smooth operation,
let's say xj prime is now the average of itself xj and
the previous k minus 1 observations. In other words, were actually using
the most recent k observations. We compute the average and we use that
to smooth the current observation. So this is basically how
moving average works, right? XJ prime is the average of
the most recent key observations. An here K is actually window
size that we care about. Of course, if you're at
the beginning of the time series, you don't necessarily have k
in most recent observations. In that case, you can basically average as
many data points as available, so moving average is quite simple and it's actually
used by many different organizations. Especially when making budget,
planning, right? Well some organizations, right,
they're quite ambitious, right? Or adventurers, right, they're making
the plan of the expenditure of next year. Basically based on the income of previous
year, so you can see that there's no smoothing, but some of the organizations
are more conservative and actually they use the moving average. They make the plan of spending next
year using the moving average. Let's see. Are the income of the previous three years
of previous five years so and so forth. So in practice,
if you want to use moving average and if you have the goal of making
predictions right, making forecasting. Remember only used previous
observations because you don't want to look into the future, right? That is, data leaking. Moving average preserves the length of the
series, but it could actually do some data at the beginning as we see that there
won't be enough previous data points. Although, you can see that using
moving average we have to get rid of the arbitrary
boundary of bins, but moving average is still sensitive,
so selection of window size. As we can see from the example,
if we use different window size, we got different curves and moving average
could also be sensitive to outliers. If there's the extreme value
that says this year, right? If you don't use moving average,
it's only going to affect this year, but if you use moving average,
it's going to affect the next few years. Of course, this can be reduced
by weighted average, so the observation is discounted when
you're computing the weighted average. You can actually see there are two
extremes of moving average, one, using the window of only two observations,
right, the orange line. You can see that if you use
moving average of K equals two, it's smoothing the time series little bit, but it's not so different actually
from the original series. Versus if you used
an extremely large window. Then you can see that basically you're
just observing the [INAUDIBLE]. And so as we said that, we can also
weight the different observations where computing the moving average and
this is actually quite desirable, especially if you want to
remove the impact of outliers. Right, so compute the weighted
moving average of window k. Basically we're still transforming
a time series into the smooth time series of the same number of divisions,
but instead of simply computing the average
of the most recent K observations. We compute the weighted average that every
observation is assigned with the weight, and then it's normalized by the sum of weights,
right? So you can see that WK is the weight for
the current observation, wk-1 previous observation and W1,
right, the k recent observation. So these are the weights that we
assigned to the first, second or to the kth position in window. Right, and you have the freedom
to design this weights, but normally we want to respect
to the current observation. So we want wk to be a larger number
than that wk minus one so and so forth. You can see that the two examples
of weighted moving average, on the left part,
we're using uniform weighting. If we use uniform weighting
that's basically moving average, there's no actual weight, right? You can see that if you use
the window size 16, 32, 64 is gradually just removing all
the cyclic and seasonal patterns, and you're just observing the trend
line on the right hand side. We're looking at a plot of using
real weighted moving average, and this time the weight is proportional to
the 10th power of your index in a window. In other words, we're really putting lots of weight on
the most recent observation, right? Because their most recent right,
and then the weight decays. If there are older observations and you can see that even if we're still
using the same window size 16, 32 and 64. You can see that we are actually
preserving more and more cyclic and signal patterns, right? And that's because that we are putting
more To the more recent observations. So that's the difference between weighted
moving average versus uniform moving average. Well, if you do that, it's assigning different ways to
different of the regions in a window. But we still have a window, don't we? In this case, the weighted moving average is still
sensitive to the window size, right? As we can see from the previous slide,
that if we use different window sizes, we still get quite different curves. Can we get rid of this
sensitive parameter as well? Can we get rid of windows? Well, there is a way to get
rid of the window size. An idea is to summarize all
the regions up to now instead of just the previous k observations, right? By doing that, you're still transforming
the time series into a smooth time series. But in this case, the smooth value and
the j timestamp is no longer the summary, the average of
the most recent k observations. It is actually the average of
every observation up to tj, right? So this is why it's called
cumulative moving average, because it is cumulating through
the first observation of the time series. It's averaging all of
the regions up to now. So that's the only difference, and you can see that by doing this we
successfully get rid of the window. So these xj, xj-1,
2x1 are all observed values up to tj, and you are normalizing
the sum by j instead of k. And j is the number of observations so
far. So this is quite intuitive, right? But how to compute
cumulative moving average or just simple moving average
is not that trivial. Computing fixed size to moving average, in other words, moving average with
the window of k is relatively easy. All you need to do is to implement a loop,
right? Given the original time series X,
you basically write a for loop, right? From the very first observation
to the last timestamp, you basically compute the smoothed
observation xj prime as the sum of the most recent k
observations and normalize that by k. To illustrate that, at the first
timestamp, you have x1 prime = x1. At the second timestamp,
x2 prime = (x1 + x2)/2, right? So x3 prime is basically
average of three items. Now if you use the window of k, and
let's see using the window of three. Then from then on every time point,
you're basically adding three numbers and taking the division by three, right? So you have x4 prime until xn prime, you're always adding three numbers and
divide that by three. So in this case,
the number of computations, number of operations you have in this for
loop is actually proportional to what? To the number of timestamps, right? In other words, you have at most
k times n operations, right? Because you need k minus 1 additions and
one division, so we have k times n of operations. This is actually quite neat,
this is the linear algorithm, right? That's why we say that computing
fixed sized moving average is easy. If you want to compute the cumulative
moving average, life is harder, right? It can be costly. How so?
Can't we just use the same for loop to compute cumulative moving average? Well, if you look at that,
the difference is that when you're computing cumulative moving
average within your for loop. You're actually adding up all
the previous observations and then divide that by the number
of observations, right? Let me illustrate this a little bit,
right. You can see that x1 prime = x1,
no difference, x2 prime = (x1 + x2)/2, no difference, x3 prime, no difference. But from now on, right,
you're no longer just adding k numbers, you're adding j numbers. And j is longer than k,
j is the number of observations so far. And every time in the for loop,
you have one more observations, right? So x4 prime,
you have to add up four numbers, x5 prime, you can see,
have to add up five numbers and so forth. And the nest number xn prime,
we have to add up n numbers. In other words, the number of operations now is no longer
proportional to the number of timestamps. It is actually proportional
to the square of n, right? So you actually have half of the square
of n operations to compute the cumulative moving average and that is inefficient
if your series is long, right? So that is to say that, when you want
to compute cumulative moving average, you don't want to use a simple for loop. So how can I deal with that? How can I improve the efficiency? And the answer is our old friend,
dynamic programming. Using dynamic programing to compute
those most value, add the Js timestamp. You don't need to add up
all the values previously. You only need to have two numbers. One is the current observation xj
of course, right? And the other is actually a smooth
value of the previous ten point. How do I do that? Well, if you compute the smooth
value from the first time point, and then you gradually build up the smooth
values of the previous timestamp. You can always guarantee that when
you're computing xj prime, you should have xj
minus one prime rating. Let's take a look at this example, right? So the original time
series has N numbers and you want to compute the smooth time
series and also have N numbers. So for the first number, well,
to compute the X1 prime, you don't need to look at your
any previous values, right? You just copy X1. And then, now you have X1 prime ready,
to compute X2 prime, you only need to have X2 and X1 Prime. You don't need X1 anymore, right? And this is because you can
rewrite the average of X1 and X2 into X1 plus the original
items the previous smooth value X1 prime,
and dividing that by two. And then, you should have
X2 primarily in this table. To compute X3 Prime,
you don't need X1 and X2. All you need is X3,
the current observation and X2 Prime, that's the previous smoothed value, right? Why?
Because you can rewrite the addition of the X1, X2 and
X3 into the addition of X3 plus 2 times the previous
smooth value X2 prime. Because X2 prime is computed
as X1 plus X2, right? Similarly, once you have X3 prime really,
next time when you compute X4 Prime, you don't need the previous observations. All you need is X4,
the current observation, and X4 prime that is the previous smoothed
observation, one observation, right? And so on so forth. If you do this practice,
once you have x and minus one prime ready that's a smooted value of
a second to last time stamp. Then you can compute the next
element of the smooth series xn Prime just using xn and xn minus 1 prime. The previous smooth value so
you don't need to add up numbers. You only need to add up two numbers. Of course, you have to transform
the previous smooth value back to the sum. So how does this help us? You can see that from the left
hand side of the equations, you still have half of
N square operations. But if you use this dynamic programming, all you need is the right hand
side of this equation, right? And you can see that it
is also the square shape. Or the number of operations now is again
proportional to the number of timestamps, right? And that's goes nearly with
the seriousness, right? Why?
Because once you have X1 prime, you can compute X2 prime based on
X1 Prime and the current observation. Once we have X4 Prime, you can compute
X5 Prime with X5 and X4 prime, right? So you always just need 2 numbers
to compute the next smooth value. And in part,
we just need a number of operations that's proportional to the length
of the time series. So dynamic programming rocks again. Here's something more to know about
the community of moving average. You can see that if you
compute cumulative moving average, you don't have to make a decision about
the size of a window anymore, right? Because instead of looking
at the past K observations, we basically look at all the available
observation so far, right? Because by doing that, because we are
actually considering a much longer history then the smoothing is going to be more
robust to more recent outliers, right? Because if there is the outlier, once used average that over all the past
observations, the effect will be smaller. That sounds like a good thing. However, if you use
community of moving edge, you should understand that If there is the
outlier, its effect will be carried over, will be carried over to the future, right? Because one of the reasons
is always considered, right? When you're computing future values. OK, this may or may not be able to. Another thing you should consider is that
by using the cumulative moving average, if we don't add weights
onto the observations. Then basically the most
recent observation and a very old observation have the same
impact in the final numbers. OK, right, you're not distinguishing old observations
versus a more recent observations, right? Unless you add weights into
the cumulative moving average. So this is the example of our
cumulative moving average of the airline passenger data. As you can see that basically this
gives us the trend fairly well. And it hides most of the seasonal
patterns and the cyclic patterns. And the reason is apparently because that
once your average everything in history, right? You can see that outliers or the extreme
values up and down will be smoothed out. But you can also see that the general
trend at the later time points, right? Is actually very much below
the actual time series. And why is that? Well, this is because that's
the old observations not playing the major impact to the smoothed
values in the future, right? When you have mass of old
observations with small values, then that makes the smoothed value
of the future also small, right? So as a result that you can
see that we see the trend. But the trend is actually quite
below the actual time series, especially when we're at later stages. If you don't think this is a good thing,
then there's the way to solve that. There is a way to accommodate that. And that is known as the weighted
cumulative moving average. We want to wait different observations so that I want can we don't need
to worry about the window size. And on the other hand,
we want to control for the effect of historical observations,
right? We don't want the first observation, the first few observations to always have
a strong appearance in the future values. We do that by any weights, right? And the weights should actually respect
the current observation, right? Because it's the current observation. So we have to put the largest
weight to it, right? No matter what, it is what we observe. And then more recent observations
should be put a higher rate than older observations. Why?
Because they are more recent. Right, and we assume that more recent
observations are more important for decision-making. So mathematically, you can actually
compute the weighted cumulative moving average as the average
of every observations so far. But each of which is timed
with the weight w1, w2, 2wg. And then you normalize that by
the sum of all weights, right? So you can see that different from
the cumulative moving average, you have weights. And different from the weighted
moving average, the cumulative. If you have all the observations
instead of just K observations. And we usually assume that the more
recent the observation is, the more important it is. So we assume that no matter what we did, you assign the weight of
the current observation. Wj should be at least not
less than wj minus 1 and not less than wj minus 2 and
so forth, right? So by doing this, we're putting
restrictions to the current observation and we're adding higher weights
to the more recent observations, a lower weight to the older observations. And in fact in reality you can apply
many different weighting schemas. And the mixing of thinking about
this mathematically is to consider the weight of i, right? Basically, the ith item as the function
of the i of the time index, y. And there are several commonly
used family of weighting schemas. You can actually select from when you
compute weighted cumulative average, right? We're plotting the weighting schema here. So the x-axis are the index i, right? So whether this is the first
observation or the 10th observation. And the y-axis is actually weight wi. You can see that the one on the top
left corner is using the uniform weighting where every time index
has the weight of 1, right? So by doing that, this is essentially
the cumulative weighting average with no weighting or uniform weighting. On the top right corner you can actually
see a variation of the uniform weighting is actually known as
the truncated uniform. And basically only the previous four
observations have the equal weight and older observations have
the weight of their. And this is actually essentially
the simple moving average, right, where we have the window 4 remember and
you can use long uniform waiting for different timestamps. For instance, you can use the so called triangle weighting
is also known as linear weighting. You can see that the most recent
observation T 10 has the highest weight. And then the weight decays when you
are looking at when you're adding up previous observations, but
their decaying linearly, right? Or linear right here write and
it puts more weights, are more recent items for
fear of it's an old items. Another way of doing this is to use so
called exponential weighting, right, and this is the one on
the lower right corner, right? You can see that were still assigning
bigger weights on the more recent observations, and
with decay overtime exponentially. Right to the very first observation in
the rest of the class will actually talk more about exponential weighing.