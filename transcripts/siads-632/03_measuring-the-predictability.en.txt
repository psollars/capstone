So we have introduced
how to standardize or stationarize time series so that result is more stationary. The hope is that if
the time series is stationary or at least
weakly stationary, then making
predictions is easier. But we also said that if
making a prediction is easy, that does not mean that
the prediction is good. Before we introduce
the principal methods of making predictions
of time series data, we actually want to
know how to measure for the goodness of
predictions first. The goodness of predictions is also known as the
goodness of fit. This is because that
by making predictions we're actually fitting
a model to the data. The measurement is actually
quite straightforward. Suppose you have a series
of timestamp items. Then for every timestamp, you have a predicted value, that is y_i hat. We use this little
hat on top of y to indicate that this is
our predictive value, not the observed value. Actually y_i is the value that we actually
observed at timestamp i. Suppose that we have many
pairs of y_is and y_i hats. Then if we use the observed value y_i minus the predicted
value y_i hat, this is known as the residual, or this is known as the observed error of
your prediction. Suppose you have many
pairs of y_i and y_i hats, then you can compute the
sum of squares of this residual and that gives us the measurement of how
good the prediction is. The residual sum of squares
can be abbreviated as RSS or SSres and mathematically, those residual sum of squares is essentially computing
the square of the residual or of the error at any given timestamp and then you sum all
these squares up. So you can see that if
your predictions are very close to the actually
observed value, then the RSS will
be much smaller. If your predictions are actually quite far away from
the observed value, then RSS will be larger. So the smaller the
residual sum of squares, the better the predictions. Another related concept is
known as the 'root mean square error' or RMSE and this is actually the metric
that is used by almost every project
that's making predictions of the future. In fact, this is quite
connected to RSS. This is essentially
the expectation of the squared error and then
you take the square root. The reason you want to take
the square root is because that you have the sum
of the squared error. So mathematically, RMSE
is essentially the RSS, that is the sum of squares of
the residual divided by n, that is the number of
predictions you have made. So this gives you the
expected squared error, and then you take
the square root. This gives you the
square root error. The good thing about RMSE, and this is also why that
people like to use it in reality is because
it is interpretable. Because RMSE is directly comparable to the
observed value y, they're at the same skill. For instance, you have the actual stock market price
of one of the stocks is, let's say on average is one
over three and then the RMSE of your prediction
is actually $0.5, then we can actually compare the error to the original price, that means the error is below
one percent, for instance. In reality, people
usually compute RMSE, not under training data. Why? Because your
model is trying to minimize the mean square
error or the training data. It's actually computed
normally using hold-out test samples
and in time series is forecasting you can
interpret that is computed based on hold-out
data from the future. So you make predictions
about the future and then in the future you can actually
observe the correct values, and then you can actually measure the goodness of the
prediction using RMSE. Similar to RSS, the root mean square error is also
measuring the error. So the smaller, the better, the larger the worse. Another related concept
is known as R-squared. R-squared is liked by many
people because RSS or RMSE are both just comparable when your pocket is the same. What does that mean? If you're making predictions
and the same Tensors, then you can actually compare the error of two different
prediction models. Either two models are
predicting for different since you cannot really
compare their errors, and in fact if you want
to compare the error of two models or two
different targets, the results could be
misleading for two reasons, one of the targets may be added different
skills than the other, and because error is
comparable to the value y. If one of the target is
the raw market price, the other target is
the market return, is the raw return or lack return, then their skills
actually vary a lot. You can just directly compare the error of two different models predicting for two differences. Another reason is that, even if two tensors may
be of the same skill, one may be intrinsically harder to predict than the other. For example, predicting the stock market is
just intrinsically harder than predicting
for students grades. So in that case, just comparing the error
of two different models, of two different targets
may be misleading. R-squared actually
provides a way to actually compare
prediction models, even if they're not predicting
for the same target. Because R-squared majors, out of the total
variance of your target, how much variance
you can actually expand using your
predictive model? And you can see that this
really has nothing to do with the scale of your
target mathematically. R squared is just one minus
the residual sum of squares, divided by the total
sum of squares. This is to say, if you compute
the total sum of squares, that is the yi minus
mean of all the values, then you take the square,
you sum over that, over all the timestamps
that gives you the variance of the data of what you are
trying to predict. Your predictive model can only explain partly the variance, because it makes errors. In that case, if
you use this error, the squared error divided
by the total variance, and then use one
minus its ratio that gives you how much or what proportion of
the variance you can actually predict using
your predictive model. So in this case, R-squared is measuring the
goodness of predictions. The higher the better, instead of the errors, the
lower the better. There are more facts
about R-squared, for instance, R-squared is
always between zero and one. If R-squared is zero, that means basically you're
predictions is no good, it is basically rend. If R-squared is one, that means you are making
perfect predictions. So the predictive values completely explained
that the variance of the original values. That is why that R-squared is usually good to use to describe the general predictive power of the model no matter
what target it is. But the downside is that you can no longer interpret R-squared as the practical difference comparing to the target
opera prediction, for instance, if we say
that the R-squared of your predictive model
for the market is 0.001. That does not mean that
for a stock price 100, you're expecting a predicted
value that is in-between 100.01 or 100 minus 0.01. This is because that
R-square does not have a connection to the
actual predicted target, you cannot compare
R-squared with y, versus if you use the RMSE, the root mean square error, you can compare the error
with the actual target y. Also R-squared does not describe the predictive power of
individual features, it only describes
the predictive power of the entire model. In many cases, we do
need to understand the predictive power of
individual features and that cannot be achieved
by either R-squared or residual sum of
squares or RMSE, we need something else, some other statistics
to tell us that. In cases that you want to know which features are more
predictive than other, there are techniques
called auto-correlation, in the context of time
series prediction, they are quite useful. That tells you how much that a previous timestamp can predict the value of
the future timestamp. An Autocorrelation is essentially computing
the correlation of values taken at even
timestamps with certain lags. In this case, for instance, if your prediction target is y_t, the autocorrelation
measures the correlation over y_t and y_t minus k, a lag of k. More specifically, the autocorrelation of
y_t and y_t minus k, is essentially the covariance of the two values with a lag of
k numerized by the variance of y_t minus k. So
the variance of the independent variable y_t
minus k. You can see that what it does is
basically telling us how much that previous timestamp correlates to the
future timestamp. If you take different lags, you can actually get a vector of autocorrelation values,
instead of just one. In this case, you
can actually plot the autocorrelation
of different lags. That gives you something called the autocorrelation
function plot, that is known as ACF. For instance, suppose we measure the autocorrelation of the
airline passenger data, we can actually see
the plot like this. The X-axis is actually
the size of lags. That means, how much of the correlation there is if we use the previous
timestamp with lag k, to correlate the
future timestamp y_t. When that equals to 0, that means that you're
actually using yourself, to correlate with yourself. Of course, that
autocorrelation is always 1. You can use this
for sanity check. Then if you use the
previous obvservation to correlate with the
current observation, the correlation will
be smaller than 1. In this case, you can see that the correlation decays
when the lag is larger. If you look at particular values, at particular size of lags, this gives you autocorrelation
of lagged observations. This indicates that, okay, previous values do have a predictive power of
the future value of y_t. But with the increase of the
lags, the effect decays. What's interesting is
that, you can see there, sometimes it does not
decay all the time. Sometimes it actually
increases again. Especially when you have a
time series with seasonality, with similar patterns, then
you can actually find that. In this case, you can see
that around lag of 12, that is basically one year, the correlation is high. The correlation of
the value last year and the value of this year
is actually quite high. So autocorrelation function plot gives us such information. In fact, you can actually compute the autocorrelation function
plot over a long time. As in this case, if you increase the number
of lags from 10s to 50s, you can actually look at the curve of
autocorrelation over time. From this curve, even
if I don't tell you, you can probably guess that there's the
periodical pattern. The original theories may
actually have some seasonality. This is also to say that if there is seasonality in the series, then the covariance is not there, then the time series
is not stationary. You can see that in this plot, the predictive power
drops when that grows in this clear
periodical pattern. As the [inaudible] , what if we plot the ACF of
the white noise series. You said that, okay,
there's covariance. So your time series
is not stationary. What if we look at the
autocorrelation of the stationary that we know that is the stationary
time series? What would that plot look like? To do that, we compute ACF of the Gaussian white noise series that we've shown you earlier. This is actually the
result ACF plot. Actually, you can see that
all the values are around 0, except for the very first value. That is, the autocorrelation of the current
value and yourself. Of course, that's still 1. Then as long as you have any lag, a lag that is not 0. The autocorrelation
is almost zero. With this truly
stationary time series, the lack of visions have no predictive power
to the future. You practice, you still
have computing ACF. People like to plot with the so called partial
autocorrelation function plot or PACF. The reason
is the following. In reality, if one of the lagged value is correlated
with the future value. Say, if y_t minus one
is correlated with y_t, then that means y_t minus two is correlated with y_t minus one. If that's true, then y_t minus two is also likely to be
just correlated with y_t. What is this telling us? This tells us that, if there is autocorrelation
with lag one, slightly it would
be autocorrelation with lag two, so and so forth. Then the information we get from the ACF can not be very useful. My correction of this is to compute the so-called
partial autocorrelation. That is, to compute the
autocorrelation with lag K, we tried to remove the
effects of correlations with any shorter lags. In this case, the
autocorrelation of y_t minus i to y_t is actually computed conditional
on the autocorrelation of any observations with
the shorter lag to y_t. When we controlled
the autocorrelations, the predictive powers of
more recent observations, then we hope that the
predictive power of a longer other
observation is more real. If you compute PACF, this is the plot
that you can get. This is computed using the same data, Airline
Passenger Data. You can see that patterns
are quite different. Of course, the autocorrelation to the observation itself
with no debt is still one. To the autocorrelation of the
previous one observation. Although relation
with lag one is the same as ACF because there's nothing that
you want to control. But starting from the second, from the lag two. Because [inaudible]
y_t minus one, the autocorrelation of y_t minus two to y_t is expected
to be smaller, and in this case it's
actually negative. Having that predictive power, the autocorrelation of the closer observations
controlled, PACF gives you a
more real estimation of the predictive power of
even other observations. By looking at this, you can see that there are only a few observations
that can actually pass the confidence interval which is indicated
in the blue area, and some of the autocorrelations
becomes negative. This is because some of the predictive powers have
already been captured by earlier observations by
actually closer observations. Before we actually do
forecasting of time-series data, it is usually helpful to
plot the ACF or the PACF. The autocorrelations, or partial autocorrelations
tells us a matter of sense. It tells us that a
lag observation, y_t minus one may have
the predictive power of the current or the
future observation y_t. If that is the case, you could motivate us to
build a prediction model, the model should use lag observations to predict
future observations. Which makes a lot of sense, because what you earn next year depends on how
much you earn this year. Your grade next year, maybe depends on your
grade this year, and the previous
year, and so forth. This is quite intuitive. In reality, if you
look at ACF or PACF, they're usually multiple
lag observations with different size of the lag that are more or less predictive to the future value. What we wanted to do
is to combine them together to actually make
predictions of the future. Does this ring a
bell? I hope this reminds you about
angry and edge models. The previous K observations or N observations are altogether
predicting the next one. But instead of building
a probabilistic model like a language model because we're dealing
with real values, we have to use something else, and this something else
is known as regression. By using regression,
we can actually combine the predictive power of lagged observations to
make prediction of y_t.