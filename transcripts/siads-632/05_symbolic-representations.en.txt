I hope you enjoyed time
series alignment using DTW, the airport of Detroit. There are lots of
similarities between time series representation
and sequence representation. We just introduced dynamic
time warping and you can see how well it is
aligned to edit distance. In fact, you can
see that there are many other similar concepts. In time series, we have seen the patterns for
instance and that should remind you about
n-grams in sequence data. What about cyclic patterns? In fact, cyclic
patterns is analogous to skip your n-grams
in sequence data. Still remember skip-ngrams? Skip-ngrams could
actually have gaps, so that actually
perfectly matches cyclic patterns and that's
the long-term movements. They're repeating, but they don't have the fixed
period or frequency. There are many other
connections you can draw. That's really curious. That makes us ask the question, what if we can just transform
time series into sequences? Now this is possible. Well, we think it is possible because these two
data representation have so many interesting
properties in common. If we can transform
one into another, then we can actually make use of all the existing techniques
we have learned. This would be ideal because we always want to reuse
what we have learned. We want to reuse the
weapons in our toolbox. We also know that they have
very different properties. For instance, time
series concerned with real values and we are dealing with continuous observations
and in sequence data, we are dealing with discrete
observations not on the other matters and we are looking at categorical items. How can you actually transform time series into a sequence data? But in fact, this is
actually quite possible. Well, remember that we
have a way to transform continuous data representation into discrete data
representation. In time series, the
continuous representation is actually a function of the value as the
function of time. We can always sample
from this function to transform it into the
discrete representation, but this is only on
discrete observations. What about the values? The values X_1, X_2 to X_N
are actually still continues. How can we transform the
values into categorical items. It looks impossible but after many years
exploration of researchers, people do find good ways to
transform the real values into categorical
items and they are called symbolic representation
or time series. In fact, there have been many interesting
ideas of defining items of transforming the numerical values
into categorical items. If you can do that, you can transform time series
into a sequence. We talked about discrete
Fourier transform. The basic idea is to decompose
the time series into a mixture of waves with different altitudes
and frequencies. If we treat every wave as the
item then basically we can actually transform
the time series data into a bag of items. Remember that discrete
Fourier transform does not give you the location
where the items appear, it only gives you the
vector of these waves. That means using the
discrete Fourier transform, you can actually transform the time series into the items, but there have been
many other methods. For instance, the so-called piecewise
linear approximation basically breaks up
time series into pieces and then find linear you analyze that actually
surrogates the time series. In this case, you are
basically decomposing time series into items and
where items are local trends. You can see that it's another smart use of
time series patterns. Basically, you construct the item corresponding to a local trend in a time series and then
the time series is transformed into a sequence
of such local trends. We also talked about
wavelet analysis. Through wavelet analysis we're able to extract cyclic patterns. Well, if you're treat every wavelet in this
case as the item, then we have both
the strengths and the location of every wavelet and that gives us a
sequence of wavelets. Wavelet analysis can also transform time series into something similar to a sequence of items. There are many other approaches. One interesting approach is to find the approximate shapes in time series and then you
define the shape as the item. In this case, you can
actually transform the time series into
a sequence of shapes. All of these are trying to make use of the pattern extraction from time series and then you'll name every pattern as the item. That's how you actually
transform time series into a set or a
sequence of items. These transformations
are known as finding the symbolic representation
of time series data. Why they are called symbolic? Well, because we're
actually transforming numerical values into
categorical items or symbols. One particular very interesting approach is called Symbolic Aggregate Approximation
or known as SAX. This is developed by Professor Eamonn Keogh in
University of California. You can see that basically
there making use or time-series alignment
and they're making use of all the ideas we haven't
chosen in previous slide. To find the sequence
representation of time series. That happened in the
lower boundary of edit distance or DTW,
which is very sweet. It is also super-fast. The transformation
from the time series into symbolic representation
takes linear time. If you're interested,
you can actually look at the details of SAX. The basic idea of SAX is to fix the continuous
representation of the time series. Both the number of dimensions and also
the continuous values. Then you basically transform that into a sequence of items a, b, c, and d, for instance. Every item actually corresponds
to a range of values. The range of values are
basically determined by the distribution of the
time-series values. After certain transformation,
you can see that the continuous time
series is actually represented or
re-represented as what? As a sequence of items. These items are actually a, b, c and d could also use x, y, z. But you should
understand that there is a natural order
between these items. Once we can do is that, you can actually apply any sequence mining
techniques to time series. To summarize time
series similarities, we can see that there
are basically two ways, one way is to treat the time-series data
as if they are vectors, then you can actually apply
techniques we learned from sixes planning such
as Euclidean distance, Pearsons coefficient,
or cosine similarities. In fact, you can actually apply transformations such
as DFT and DWT. What is DFT? Discrete Fourier transform. What is DWT? Discrete wavelet transform. Because the results of DFT or DWT are vectors or matrices, then these vector-based
two scenarios can be naturally applied. The other way is to treat the time series as
similarly to sequence data. Similarly to compute the edit
distance in sequence data, Dynamic Time Warping, DTW helps you to actually find the
alignment of time series. The difference is how to
handle numerical data. In fact, there's
actually one approach to transform time series directly into the sequence of items. Based on the transform
symbolic representation, you can actually apply many sequence learning
techniques we have learned. Finally, remember that all of the techniques
can be applied to the original time series
and they can also be applied on the
smooth time series. Remember that you
can actually smooth the time series using
moving averages, using EMA, or using auto refresh. Once you are able to compute similarity between time series, you can actually use similarities
to build downstream, more complicated
are the main tasks. For example, you can support
time-series motif search. If you know that
certain time series have particular pattern, you can use that pattern as the search query and find out the time series that
have such patterns, such as spikes for instance. You can also use them
for classification. You can use k-nearest
neighbor classification or many other applications. Takes the input of similarities or you can actually
use them for clustering, because many clustering
methods actually takes the input of the similarity
matrix of a data points. There are many other
Data Mining tests that you can build upon time
series similarities. This is just one example of clustering different time series based on the similarities
between them. In fact, classification
and clustering are usually built upon the lower-level Data Mining
outputs such as patterns, such as similarities,
and later on we will talk about time
series models. I know that many of
you will be taking Machine Learning
courses concurrently. You can think about if you're
data, is time-series data, what kind of features
you can actually extract to fit into
the machinery model. To summarize time-series
patents and similarities, I want you to know
how to calculate Euclidean distance
of two time series. Based on simulation
you can calculate cosine or Pearsons
coefficient easily. I want you to
understand the problem of times series alignment and why it is actually important
to compute the distance. You should know that one way to find the optimal alignment between two time series is
using Dynamic Time Warping. DTW, you should know the connection of DTW
to edit distance. You should be aware that symbolic representation
transforms time series into sequences so that we
can actually reuse or these sequence mining
tools we have learned. You don't need to
understand the mass, the details of how
to do the transform, all you need to do is to
find the package that helps you to get the symbolic
representation.