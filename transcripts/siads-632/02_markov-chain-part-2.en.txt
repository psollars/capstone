Now, let's see a concrete example of generating an n-gram
using a Markov chain. Now, let's use our favorite
example, the dark side. Suppose we have three words, "the dark side", which
we are interested in. We make each of them a
state in a Markov chain, so three circles if you can see. Of course, the vocabulary
is larger than three words. There may be many
other words that we don't quite care about
in this particular example. Starting with the three states, Starting with the three states, suppose we have already
designed the states. We will first assign the probability to
each of the states. That indicates that
the sequence will start in each of these states. We call this state
of probabilities, a start distribution or
an initial distribution. For example, as we can
see that the probability that any word is "the" in
English literature is 0.046. P "the" is 0.046. This is one of the probabilities in
the start distribution. Of course, the probability that a n-gram will start with
"side" will be much lower, and n-gram will start with the word "dark" will be even lower. The red probabilities
are known as the initial or start distribution. Now, once we have the states, based on the states, and once we have the
initial distribution, the next thing we will
do is to add edges. The edges will be pointing from one state to any other state. For instance, you can
see that there will be three arrows pointing
out from the state "the", one pointing to itself. Because if you have
written down the word "the", then in the next word, there is the probability
that you will write down the word "the" again. The other two arrows
points to this word "side", and the word "dark", respectively. You can see then if
I have three states, then we will have 3 times 3. There will be nine
transition probabilities. There will be nine links, that links from one
state to another state. Then, what we're going
to do is to assign transition probabilities to
these arrows, to these links. These transition probabilities, because when you are
in one of the states, you have to go to one
of the other states. You have to go to
one of the states, not necessarily one
of the other states. You could come back to itself, but you have to be
somewhere in the next step. In that case, all the transition probabilities
that are pointing out from one of the
states has to sum to one. Why is that? Because
if you are here, if you're in Ann Arbor, tomorrow you could still
be in Ann Arbor but you could also be in any
of the other cities, but you have to be
somewhere in the world. These blue probabilities alone as transition probabilities or
transition distributions. In this case, there will be one transition
distribution for one state that points out
from this state to any states in the space. Because we have 3 times 3, that are nine transition
probabilities, so there's the hint to that. It can always be stored
with what? With the matrix. The transition
probabilities, well, you really store that
with the matrix, three by three matrix and
there will be three rows, three columns corresponding
to the three states. To summarize that,
given the Markov chain, what we are talking
about are the number of states as in a set of
initial probabilities, initial distribution,
the red probabilities. Then, some sets of
transition distributions, or M by N transition
probabilities. That indicates, suppose
the current word is x, how likely is that
an expert will be y? These blue probabilities are the transition probabilities. With this Markov Chain defined, with all the structure, the number of nodes
and the edges, and the starting probabilities, and the transition probabilities, we can now calculate the
probability of any given n-gram, using the Markov chain or
this bigram langauge model. For instance, if you look at the probability
of the trigram, "the dark side", right? We basically starts with
the word "the" right? Because the word "the" has no previous word. Then, what we need do is to use the initial probability thatâ€¨
is P(the), right? Now, the next word we are
writing down is "dark", because we have a previous word. Then, we need to use the
conditional probability, the transition probability that the word is translating
from "the" to "dark". So that is P(dark),
given "the", right? We can still look up this
probability from the table. Then, now, we have
written the word, "dark". The next word we wanted to
generate is "side", right? Because it has the previous word, what we need to use is, again, the transition probability
from "dark" to "side", or the conditional probability
of "side" given "dark". They're the same thing, right? This is how you can calculate the probability of the
n-gram language model. You can see that in this case there's no ambiguity, right? We're not skipping any
word and we're not trying to decide whether we switch from a unigram language model or a bigram language
model, right? Because only the first word, because it does not
have a previous word, it uses the unigram
language model. All the other words are using the conditional probability
of the next word, given the previous word. We can do the same thing. Remember the example that breaks up the trigram,
"the dark the" right? Now, we can also compute the probability of
"the dark the". That is the start probability,
starting from "the", And then times the
transition probability from "the" to "dark", and then times another
transition probability from "dark" to "the", right? You can see that looking at
the numbers from the table, if you compare these
two probabilities, you can see that actually, the first one, the
probability of "the dark side", is now higher than the
second one, "the dark the". That has corrected
the problem that we observe in the unigram
language model. Sounds good? This is why that Markov
chain could help us model the dependency
between the words. Then in reality, because one-word almost always
depends on the previous word, then a Markov model,
a Markov chain, works better than a
unigram language model in sequence prediction, right? Now, we know that if
we have seen the dark, it's more likely to see the
trigram of the, dark, side. This is how to generate n-grams using bigram language
models, right? I know that we have
always been talking about generating
words or sentences. Let's take a look at another example and is
also quite important, which is to generate DNA sequence with
Markov chain, right? As we know that in
a DNA sequence, we only have four
possible states, A, C, G, and T, right? These are the observations, the categorical items in
the DNA sequence, okay? Let's do another
different scene, right? Instead of generating the n-gram, let's see how we can generate
a complete sequence, right? What's different from a complete
sequence and the n-gram? With a complete
sequence or sentence, always start from what? From nothing, and it has
to end somewhere, right? As we have introduced, once we have decided which states are there
in the Markov chain, we add the transitions, we add the links from any
state to any other state. In this case, well, we have
4 times 4, 16 states, right? Because we are generating
complete sequences right now, let us introduce two more states. One of the state is called
the start of the sequence, S. We introduce this as
the auxiliary state. Now, it just help us to make our calculation
easier, right? Why? Because every complete
sentence in this case, we will start from this state. Then similarly, we
add another state, that is E. That means the
end of a sequence, right? Why? Because every
complete sequence will end with this state. You can see that by adding
these two helping states, we no longer need to worry too much about the initial
probabilities, because a sequence will
always start from S, right? The initial probability of S is one and the initial
probability of any other states will be zero. Does it make sense?
From the state S, there will be just four links. It can only transcend to
any of the states A, C, G and T, suppose we don't
have empty sequences. Similarly, for the state E, it cannot go to anywhere, because if you are
already in this state, that means your sequence
has already ended. There will be four transition per radius linking to the state E, from any of the states A, C, G, and T. By adding in these two auxiliary states
to the four original states, we have six states in
this Markov chain. Now we can generate a complete sequence
with this Markov Chain. As we said that to generate
a complete sequence, we'll always start with the state S. Once we
are in this state, we need to make a decision, which state we want
to translate into. In this case, basically, we toss the dice. We toss the dice of the
transition probabilities from S. This is a dice indicating that these are the transition
probabilities from S or conditional probabilities of any other state conditional on S. Note that S cannot go to itself because
it's the starting state. We toss this dice and then the dice will tell us which
state we're going into. Suppose, the dice is
T and then we have generated the very first item in a DNA sequence that is T, or in this case, the second
observation in a sequence. Once we are in the state P, we will use another dice
to decide where to go to, because this is a
dice and that's owned by T. We're no longer at S, so we cannot use their dice, we have to use T's dice. This dice, if we toss
it, it helps us that, you cannot go back
to S but you can go to any of the four states
plus the end-state. Suppose it tells us
that, "Go to A." Then we have generated the
second item in the sequence, A, and then we will
be at A right now. When we we're into the state A, we have to use A's dice
instead of T's dice. A's dice corresponds to the transition
probabilities from A to any state, including itself. We toss the dice again, and it tells us to go to G, so we have generated
another item, G. Then we use G's dice
to decide where to go to, and this time, maybe it
says that, "Go back to A." In this case we'll
be going back to state A and we have generated
another A in the sequence. Then we use A's dice again
to decide where to go to, and this time it says, "Go to C." Then we use C's
dice to decide where to go to, and this time it says, "Go to the end." Finally, we're at the state and that is the end
of the sequence. Once we're in the
end of the sequence, you can see there's no
transition probability coming out of the state. That means we have already generated the end
of the sequence, and then now we have generated a complete sequence
T-A-G-A-C. Sounds fun? Well this is nothing
more complicated, like playing a board game. Basically, every state has its own dice and it's
own dice corresponds to the transition
probabilities from their state to any state
in this Markov chain. Whenever you are at
each of the state, you will use the
dice according to that state to decide
where to go to next. Once you have translated
into another state, you write down the
item in that state, and then you make
another decision of next step using the
dice of that state. Making sense? This
is how to generate a complete DNA sequence
with a Markov Chain. The only difference between this Markov chain and the
previous work Markov chain is that we have introduced
two more states, start of the sequence
and end of the sequence, because we're generating
complete sequences.