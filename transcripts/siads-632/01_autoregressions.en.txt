Now we have introduced how to transform a time
series into hopefully, a stationary time series to make predictions and then how to measure the goodness
of predictions. These are prerequisites of actually building a model
to make predictions. From now on, let me talk
about a series or a family of time series forecasting or prediction methods known
as autoregressions. The basic idea of autoregressions is as what
we have introduced when we were talking about ACF or
PSF that we wanted to use different lag observations or observations in the past to predict for the
observations in the future. In other words,
autoregression builds the regression model that
uses multiple Y_t minus i, where i actually takes
different values indicating different
lag, to predict Y_t. Normally, we introduce p
as the critical parameter, that is essentially the max lag. Essentially, how much
you want to look back, how many observations you
want to consider when you are actually making the prediction about the future observation. Mathematically,
autoregression AR or ARp is actually a
regression written in the following way,
yt is the target, is the dependent variable, that equals to the
summation of yt minus i, where i takes the value
from i to p. That means, you look at the previous value, you look at values that
are two lags away, the values that are three
lags away, and so forth, until you have the window of p. Every previous observations
has the coefficient, that is Phi i, that gives you the weight
of each observation. Mu here is outside the summation. It is a constant. It is the intercept, that measures the linear trend. As we can see that, well, you have Mu added to the
very first observation, the second observation,
the third observation. That really indicates
the linear trend. Phi are the regression
coefficients of Y_t minus 1. Phi i corresponds to the weight of the previous
observation with lag i. It's predictive power on
the future observation Y_t. Phi i can be interpreted as the effect of the value at lag i. Finally, we have Epsilon t, that extends the error
of the prediction. This we assume is sampled
from white noise. This equation gives us
the regression that uses the multiple values of previous observations to
predict the future value, Y_t. The multiple previous
observations are weighted. The weights are Phi i. With this model, how can
we make predictions? We need to fit this
model to the other data. In other words, we need to learn the parameters. What
are the parameters? Mu, that is the linear trend, and Phi 1 to Phi p, those are the weights of different lag observations
in the regression. We need to learn them
from training data. The training data basic means our historical observations
of this time series. This can be really done through
least squares regression. Basically least squares
regression tries to what? Tries to minimize
the square of error. That is the RSS, that is essentially the measure of the goodness of the predictions. This is how to find
out the parameters. In testing or when you're actually making the predictions, you don't need to really sample anything
from white noise. Instead, for the out sample Y_t, you can just make
a prediction using the weighted sum of the previous values.
Weighted by what? Weighted by the coefficients, your estimate from the regression plus the estimated
mean, that is Mu. Then you don't really add this white noise
into the prediction. This gives you Y_t hat, that is the predicted
value of Y_t. With that said,
you can just apply AR to any of your
favorite time series now. In this example we are
applying the autoregression, and we use the parameter two. That means we only look at most two lag observation
to make the prediction. We apply this model on the
airline passenger data. Once again that
we're not going to predict for the original values. We normalize the data first. We make predictions
for log return. You can see that after the transformation,
using log return, the series is actually more
stationery, the blue curves. The orange curves are the actual predictions we
make using the AR model. Once we've trained the model, we can actually
make predictions at any given time point t. In this case, we're not really making predictions
for the future. We use the same data to
estimate model and then we make predictions of all these
existing timestamps. That gives us all
these orange lines. You can actually see
that the predictions, they do preserve the patterns
of the original series. Because you can still
see the predictions are more or less stationary and
if you watch carefully, you can still identify, you will see the [inaudible]
from this predicted curve. So it does preserve
all these parties. Also, you can see the mean of the predictive series is zero. You can see that the
predicted series is also moving up and down, so it does extend part of
the variance in a data, of course not all the
variance of the data. So we can see that predictions
are reasonably good. This can be measured by the RSS, that is the residual
sum of scripts. You can see there there
is [inaudible] scripts of this prediction by AR
2 is actually 1.5. So this is not bad. So this is the theory and the results behind
autoregression. But autoregression does
have it's problems. Well, first, you can
see that autoregression does not really explain
all the variation. In fact, it does not consider
the impact of outliers. Especially the
unexpected shocks on future values because they are all captured in the aerator. So if you just predict for the weighted sum of
the previous values. If you don't take into
consideration of the aerator, then you basically loose the
explanation of the outliers. As we know, the outliers
are actually the error. That means the outliers are where the observed values
are so much different from the predicted values. So this motivates us to think, what kind of mechanism or
what other model could help us cascade the impact
of shocks or outliers? If you remember, we
have talked about one of the tools when we're talking about extracting trends. That's known as moving average. If you have moving average
then any shock will also be cascaded into the future until you're out of this
window. Does that make sense? In fact, motivated by this, people proposed a
different type of autoregression model known
as the moving average model. We'll know the difference between the moving average
to compute the trend versus this moving average model that's actually trying
to make predictions. Suppose, all your
time series values are sampled from white noise. So this is the basic assumption behind stationary time series. The white noise
has the flat mean. Then after you compute
moving average, or you can compute
weighted moving average, we actually have that the
following equation has to hold. You can see that
the difference of the following equation says that the observed value YT, the actual value
YT in the future, is actually a
summation over what? A summation over white errors. So epsilon T minus
I are white errors. The summation, there are two elements in the summation and Q is the parameter that is known as the max lag
of moving average. Basically, this is
the window size of the moving average model. Within this summation, you
can see that the error, the white noise of lack observations are all carried over to the
current observation YT. But they're weighted and
the weights are theta I. So when theta I are timed to the white noise of the
previous observations. It's like that you are computing the weighted moving average. If you remember that
when we're talking about moving average or MA, the weights are predefined. There we said that in the future, we would actually try to
estimate the weights from theta, and this is how you do that. This is to use the moving
average regression model to actually estimate the weights
of your moving average. These weights are multiplied
to the white error. At time T minus I. Then all these weighted
errors are added to the error of the
current timestamp, epsilon T. Also, we add a mean, a constant mean that
is mu into this. This regression model is known as the moving
average model, you can see the
difference between the Moving average model and
autoregression model, AR, instead of using
previous observations, to compute this weighted
sum and then to predict yt, while using a moving average of the errors in the
previous observations, and use them to guess
the future value, yt. So moving from this equation, you can see that we also have a critical parameter of
the moving average model. Instead of p, we use q. In this case, q indicates the maximum window size
of your moving average. So whenever we call the
function MA as moving average, we give the parameter q. Once you specify it to value q, you can say that it's
essentially computing the moving average of the
previous q observations, to guess the value of the
future observation yt. Comparing to moving
average smoothing or EMA. Both both the regression and the smoothing method are trying to compute a weighted average, but the weights of
EMA are predefined, even though that we assume that weights decay over time exponentially, they're
still pre-designed. But in the moving average
regression model, the weights are not pre-defined, but they are actually estimated
through the regression. Moving average model is always
stationary. why is that? Because it's essentially a
weighted sum of white noises, often caused by adding
in the constant mean, it has the trend mean. But if you, detrend the
moving average model, you can see that,
because it's computing the weighted sum of
another of white noises. If the white noise
are stationary, then the moving average
model is stationary, that's why the moving
average model actually has very nice
statistical properties. But in reality. We know that making the prediction easily does not mean making the good prediction, and the particular reason
is that the error terms, epsilons, are not
really observed. If you always revenue sample from a white
noise distribution, then your random guesses might be very different
from reality. But if we cannot observe
the error terms epsilon, that basically makes
the estimation of the parameters very
hard, and in reality, estimating the
parameters Theta 1, Theta 2 to Theta n to
actually state up q, is actually much harder
than the AR model, where you actually compute
the weighted sum of the previous observations
because they are observed. This is an example of using MA, to using the moving
average model to make predictions of the
airline passenger data. Again, we first normalize
the data using log return, and then we use MA to
make some predictions. We first estimate the
parameters and then we apply MA to generate the predicted value of every observe timestamp. What's interesting here is that, you can see the RSS of
the MA model is 1.47, and if you remember, the RSS of the AR model is 1.5. That means, because
the predicted target is always the log return, they are comparable,
and that means the MA model actually
outperforms the AR model. Here to be fair, we
also used MA of two, and that means we
also just look at the previous two timestamps. You can see that
you can either use AR or MA to make predictions
of the time series, and then you can use RSS to measure the goodness
of the predictions. This is about AR and MA. The next model we
want to introduce is called autoregressive
moving averages. This is abbreviated
as ARMA or A-R-M-A. Well, for smart students, you can already observe that ARMA is essentially the
combination of AR and MA, and indeed, this is the case. This model basically
combines autoregression, AR, and moving average, MA, and trying to get the
benefits of those models, while resolving the both
of their limitations. Before introducing the
equation of the model, a careful students
would notice that AR, has the parameter
p that indicates the maximum number of observations, you
want to look back. MA also has the parameter q, that is measuring the
maximum window size of moving average. Combining them, the model ARMA has two parameters, p and q. If we write down the equation, it looks like this. In this regression, yt is
the combination of what? Of the weighted sum of the previous observations,
[inaudible] minus i. This part is essentially,
autoregression. You also have the parameter p, that is the maximum number of observations you
want to look back. Another part of this equation is essentially the weighted
sum of the errors. This is basically the
moving average model. You also look at the weighted moving average of the previous
queue of divisions, and then the major
parameters are theta i, those are the weights
of the moving average, and epsilon t minus i
are the white noises, in the previous observations. This part is the
moving average model. The two share two scenes. The other regression model, and the moving edge model
to share two components. One, is mu, that
is the trend min, and the other is epsilon t, that expense for the error. If we add the two
summations together, if we integrate them together, and add back the
mu, and epsilon t, that gives us the
regression of ARMA. You can see that
ARMA is essentially, combining the power
of AR, and MA. Then, if we use ARMA
to fill the data, we would actually expect a better predictive power,
better predictions. In fact, it is the case. If we use ARMA, estimator parameters,
and then applying ARMA, the learned model, and the observed timestamps of this algorithm of the
airline passenger data. You can actually see that, the predictions are actually
much smoother than before, and they do preserve
the patterns. If you look at the predictions
by your naked eyes, you would expect that they are better than a predictions of either AR or MA.
How to verify that? You compute the RSS. The residual sum of
squares of using ARMA to make predictions
is actually 1.02, and that is much
better than AR, or MA. That even reduces
the sum of errors of AR by almost one third. That is actually
quite impressive. To be fair, the parametres we use are P equals two,
and Q equals two, that means for both AR and MA, we just look at the
previous two observations. This is how to use AR or MA, or ARMA to make predictions. But as you said that before
you apply this models, you are actually normalizing
the times series first. Try to standardize them into stationary time series by using let's say log
return, or differencing. But in reality, what we need is the prediction for
the original values. They see the number of
passengers in the next year. This is quite easy. Once you do the transformation, and then you learn the model,
makes the predictions, and you've got a prediction
of the log return. All you need to do is, to transform the predictions
back through the original. In this figure, you can see that, from the previous prediction, that we're obtaining the
predictions for the log return, all we need to do
is transform the predicted log return
back to the row y, this gives us the
predictive value of the original time series. You can see that it really
preserves the shape, or the [inaudible] , the patterns of the
original time series. ARMA is fun. ARMA combines the power of autoregression,
and moving average. In practice, it
works usually much better than either of the models. But ARMA does have its problems. One problem is, of course, still the assumption of
stationary time series. In theory, either AR or MA, only works on
stationary time series, so ARMA can only be applied to the
transformed time series. When time series
are not stationary, it is harder to
apply ARMA directly. How to apply ARMA, if your time series
is not stationary? Use differencing, and hopefully, that will make them
both stationary. Instead of just applying
the transformations first, and then apply
autoregression models, are there models
that can directly consider the stationerization
of time series? Are there prediction
models that you can directly apply to
original time series? We will introduce
such a model later.