I hope that you understand the basic
intuition behind reservoir sampling, right? Basically, whenever you
receive the newer item, right? You include that into the sample
with the probability of K/N, where N is the current total number
of items in the stream, right? And then suppose you decidedly
include that into the sample, you have to kick out the other
item in the sample, right? With equal probability, right? So this is the very simple procedure
that basically guarantees, right? With proof that the probability of
all the items will be the same, will be equal, right? To present in this sample. So this is the process of the organic or
the basic general reservoir sampling. And, in fact, there are many extensions to this process
to handle different requirements. For instance, the concept rate is
one thing that we are interested in. As you can see that, we were talking about
some of the simplest sampling strategies. We say that we actually, they were
actually resulting concept drift, right? Because if you only keep the most
recent items in a sample, the recent items could actually distribute
very differently from the other items. But, in fact, this is not always the bad
thing because recent items could carry more weights, right? Because the recent observations may be
more relevant to decision-making, right? And because the data generation
process may have changed and also because the context may have changed,
right? So tweets five years ago will not
be talking about same topics as tweets recently. So if you want to make a decision on what
the general interest would be, right? You should actually weigh more
into the tweets recently. Similarly, for
the presentation election four years ago, the context has been very different
from the election right now. So you want to actually make predictions. You should probably make more, put more
weights into more recent observations. So concept drift is not
particularly a bad thing, right? Although that with reservoir sampling,
we're actually excluding that phenomenon, we guarantee that all the older items an
new items have exactly the same property to be included in the sample. But in reality, sometimes we do want to
give more weights on more recent events. And this actually can be easily done
by adjusting the reservoir sampling. How so? When sampling, you basically assign a higher probability
to more recent events, right? And that will actually account for
concept drift. If you do this, it will lead to
the so-called weighted reservoir sampling. You basically assign the higher
probability to the more recent event, so it's no longer K/N, it's probably
something higher than K/N, right? And then you would guarantee that older
events have smaller probabilities, right? But you don't want to exclude them
from the sample definitely, so they still have some
probability to be sampled. In this case, you're not just going
back to the simpler approach, only keep the most recent events. So how to solve this? Well, basically you can just make simple
adjustments to reservoir sampling, right? Remember that in reservoir sampling,
the probability that an old item, I, is included in the sample at time N
plus 1 equals to the probability that it was already in the sample, right? Probably that it was not kicked out,
right? Upon the arrival of the new item, right? There you have two very
interesting probabilities, one is PM plus 1, M plus 1. Now these are the probability
that a newer event and the newest event is included
in the sample, right? And to give more weights
to the newer events, you can just make it higher than K/N. Another probability that is fair,
that is equal, is the probability that you actually kick out any of the old
items already in the sample, right? That is 1/K. To give more weights and more recent
items, you can basically adjust this 1/K so that this is the probability
that favors other items. That means the older item is more likely
to be excluded, to be replaced, right? By the arrival of the new event. In that case, right? You can actually guarantee that
reservoir sampling still works but you can actually
accommodate concept drift. So the basic idea is to make 1/K, right? Dependent on both I and M, that is the
current number of events, in the simple. So the smaller i is, right? The older the event is,
the more likely it will be replaced. And basically what people do
here is to design the process so that the probability of an event XI to be
included in the sample is proportional to the function, right? More generally, the function is just the
function taking account of the index i and the current number of events N. And you can see that this function, f(i,
N), will decay when N minus 1 increases. That means if this item is older,
if this item is actually farther away from the current timestamp,
right? Then it is probably less relevant, right? To the current context, right? Then the probability that we should
include this in the sample of the data stream should decay. Does that ring a bill? What kind of decay? Well, people like to use exponential
decay, that is the probability, right? Or the function i,
N is proportional, right? To e to the power of times N minus i. And N minus i is basically
the distance from the historical event to
the current timestamp. Does that ring a bell? I hope so, right? I hope that will remind you of
exponential moving average, right? Where every time stamp would actually have
the impact to the future observation. However, their impact we
actually decay over time. And the decay follows
the exponential distribution. So you can see that how these concepts
in data stream, in time series and sequences are actually aligned. So reservoir sampling is very
powerful in practice, right? Basically you can actually apply this
to any data stream that you care about, just keeping the sample instead
of the entire data stream and you still want the sample to
be representative, right? Of the history, right? People don't put hard constraints
on sample size, right? So instead of keeping exactly K samples,
right? We could actually relax it so
that older invent may or may not be kicked out, right? And if you do this, you could actually
facilitate exponential decay much better. And in some cases you may
choose to kick out multiple events at the same time to
keep your sample compact. And it causes different strategies,
different policies. For certain policies, right? You can prove the bound of
the sample size, right? So there are amounts of mathematical
proofs with stream data mining, right? And you can prove that the bound, the upper bound of the sample
size is guaranteed, right? With certain types of policies. So this is all about
sampling the data stream. I hope that you now understand that
sampling data stream provides the generic solution to mining data streams. Because as soon as we can keep a
representative sample, that whatever data mining techniques you apply to the entire
data, you now apply that to just the sample, so you don't actually
need to deal with the entire history. I hope you understand how
reservoir sampling work and why it guarantees the furnace among
the new items and the old items. And I hope you understand the idea
of handling concept drift, especially using weighted
reservoir sampling and how these ideas connect back
to exponential moving average.