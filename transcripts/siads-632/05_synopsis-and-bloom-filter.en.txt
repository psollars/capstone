So we have introduced
the challenge of mining stream data and we use this particular example that, how can you deal with 500
million tweets everyday, with the capacity of
just 1,000 tweets? To do data mining from
this constant space, you could actually use sampling. Instead of dealing
with the huge stream, the basic idea is that you now
deal with just a sample of limited size and the sample would actually fit
into your capacity. So at any given time, the stream will essentially
become a set or a sequence, or a small time series, given what your sample size is. But sampling also has mass of limitations because in reality, the number of events we keep
in sample is usually much smaller than the actual number of events in the
entire data stream. If you compare 1,000 to
500 million per day. Though it's hard to the
small sample if it's too small to preserve more context
in statistical parties, rather than, for instance, just mean or standard deviation. In many cases, individual events could be
large and it could be complex. For instance, if you keep a sample of tweets then you have to
store all the words, all the metadata,
and so on, so forth. As it may or may
not be useful for a data mining task but they
will take your space anyways. In such scenarios that we know
keeping statistics may be even more efficient than keeping the actual
events. Why is that? Because we know that in
certain data mining tasks, what we care about
are the statistics. For instance, frequent
items we care about counts. Mining sequence data, we
care about probabilities. Time series data, we care about the regression
coefficients and such. In those cases, if we know that what we want
are the statistics, then why don't we just keep the statistics instead
of the sample of events? In fact, that if we can fit 1,000 tweets into the sample,
into a compacity, then you can properly
store mass of statistics, and those statistics could
be actually much more complex as they cannot be easily reconstructed
from the sample. What are the alternative
statistics that we can do to sampling events? We could actually summarize
the data stream with statistics and update
the summary over time. We could keep statistics
such as the appearance versus absence of a
particular events or counts of events, the mean, the variation, and the higher order
moments of your stream. We could actually update
this statistic over time, and based on these statistics, we can actually do
lots of things. We can even store
populistic models over time because populistic models are essentially a set of numbers. We can even store
regression models over time because a regression model is essentially a number
of parameters. So beyond these
simple statistics, we could actually store more complex data structures
such as sketches, such as models; either
populistic models, regression models or
other types of models or synopses as we can see later. So these are the
alternatives we can actually store to sampling
the real events, to keep a small sample of events. Synopsis of data streams
are also known as summary. If you don't like
this fancy words, you can just call them the
"summary" of your data stream. They are even more efficient than keeping the
events themselves, because the events could be complex and they could
take lots of space. Just individual events could
actually take lots of space. But actually the
useful information you store with these individual
events is limited. Synopsis or a data summary could actually help us to use
a small space to store lots of useful information because they can be more informative than small
sample of events. Building synopsis of data streams you'll rely on
randomized algorithms. So we're back into the world of using randomized algorithms. We have already introduced
a randomized algorithm, that is the reservoir sampling. The problem of this
randomized algorithms is that is usually infeasible to
give accurate answers. Of course, reservoir
sampling could, but we can usually guarantee
some error bound of this or that
randomized algorithms and we can actually prove
them mathematically. One of the typical statistics
or synopsis that we'll actually keep is
the appearance or absence of particular events. The basic application
scenario is to check whether the item or the event has
ever appeared in a stream. Remember that we're not going to keep the complete stream, right, because the
history is actually huge. You can also see that
this is hard to do, if you keep just a small sample. Because if you care about whether that event has appeared or not, in a stream, is
not something that just 100 sampled events
could actually tell you. For example, if you care about whether anyone
has used the word "serendipity," in research
papers in the past year, because we have done some
cool research on serendipity. Then you can't just
look at a sample of a 100 computer
science publications, of course you also cannot
keep all the publications, and look at them one by one. So how can you
solve this problem? Or you want to figure out whether James Bond has ever
connected to your server, you cannot just
sample a few logins, or you also cannot just look back the history whenever
we have such a query. Well, how do solve this problem? If the recovery, in other words, number of unique items is small, then a straightforward idea
is to keep a dictionary, and use the, for instance, the python dictionary to
indicate whether an item has appeared since the start of the stream and then you update
the dictionary over time. Then anytime you can query the dictionary by
directly asking, whether this item or this event has the
value one or zero, or even the counts of time that it
appeared in the stream. But this only applies when the vocabulary or the
dictionary fits into memory. In many cases that the
number of unique items, or unique events in a
data stream is huge. For instance, if you want
to store the dictionary over 500 million of words, which is possible in reality, 500 million unique tokens, than it is a hard task. So what can we do if we cannot fit the
vocabulary into memory, but we still want to
quickly access whether something has appeared or
not in the entire stream? Well, we can use something, an algorithm called
the Bloom filter, and the basic idea is actually
quite straightforward because building a dictionary is essentially building
a hash function, and Bloom filter is actually making use of multiple
hash functions. So lets talk about Bloom filter. As we say that comparing
to the dictionary, a bloom filter is based on
multiple hash functions, the basic idea is to
keep an array of N bits. N bits mean that we're only
storing the numbers 1 or 0, binary numbers, and
we keep the array. You can see that keeping
the array of N bits, is much more efficient, than keeping the actual
sample of events, because every particular event is actually much bigger
than a few bits. Then we know that if we
have one hash function, that will map any item, or any word, or any
complex document, or data object, into
one of the N slots, so this is the hope that the
hash function would actually randomly map any item
into one of the N bits. But the idea or bloom filter
is to use K hash functions, and hopefully that will
actually map every item into K slots instead of
just one of the slots. That actually creates
the fingerprint for this particular item, then at query time, all you need to do
is to query whether this fingerprint
is present or not. Basically to look at, this case notes should
have been filled, if this item has appeared
in the data stream, but if one of them
was not filled, then, we can conclude that
the item has not appeared. So this is the basic idea
behind bloom filter. Let's look at how it works. Suppose we still have
this stream of items, the string of colored squares, and instead of keeping
the sample of items, we keep the array of N bits, and we initialize
every bit with zero. So you can see this
is what we call a synopsis of the data stream. So we know that if we
have one hash function, like how the dictionary
data structure works, for any given item, uses x, for any given square
of the colors if we actually map this item
into one of these slots. If it's mapped into
one of this slots, then we could actually change
the value of this slot. But instead of using one hash, we want to use multiple hashes. Why? Because there are
always much larger number of items in a data stream than the number of bits
you can restore, right. If you only use
one hash function, then you will run
into a problem that multiple items will be
mapped into the same cell. Instead of using just
one hash function, we use K hash functions. Hopefully, the K hash
functions will randomly map the same item into K
slots into the array. In this case, the K slots
are actually colored as red. Remember that, all the numbers
are initialized as zero. Once we mapped the existing
item into K slots, we can change the values in
all these K slots into one. Now basically it
means that, okay, we've got a fingerprint
of the red square. The fingerprint actually
consists of these K slots. If you see the one in all
these slots that means, the red square has
error appeared. Similarly, if you're looking
at a different item, for instance, this yellow item. Using the same K hash functions, you will map this different
item into K different slots. Well, of course, some
of the slots may overlap with other
items, but not often. That's the key why you
use K hash functions. In this case, suppose the yellow square is mapped
into these three cells. Then you can see there
are some other cells have the value one already because some other items
were mapped into it. Some of the cells are still zero. Then we just change the values of all these K slots also into one. Of course, you can see
that for some cells, if the values are already one, then we don't make
changes because we're just keeping the bit array. We do this again and again. Finally, when we have processed
all the items so far, then we have hashed the newest arrival
into K slots again, and then we change
the K slots into one. You can see that, by
looking at our array, many of the slots will be
filled with one already. But some of them are still zero. Does that make sense?
If you're lucky enough that you have more
zeros than ones. This is what we do to
build a bloom filter, to process the data stream. You can see that we're
always just dealing with the newly arriving items. We never look back. We just keep updating this array. We just keep updating
these synopsis. Then once we're done, suppose we receive
the query anytime. The query is one of the events. We want to ask whether
this particular event has a red appeared
in history or not. The only thing to do
is to map this with the same K hash functions
into this speed array. Suppose we map this into
these three yellow slots, you can then check whether all of the K cells are filled with one. If they are, then that
indicates that it's possible that this yellow item has appeared in the stream. But not necessary, because
the K cells may be filled at different times with two different items
or three different items. But it's at least possible that the yellow item has
appeared in the stream. Well, that's probably
not that useful. But if we observe
that one of the slot, that's the current item
was mapped into is zero, then we can conclude that this query item definitely has never appeared in the stream. Why? Because if it had appeared, then all the K slots in general would have
been filled with ones. If just one of the cells
were not filled as one, then this item has not appeared. This is essentially how
bloom filter works. In practice, if you
use bloom filter, you should know that there
are possible false positives. That means that, the bloom
filter may say that, okay, this item may have appeared, but actually it has not. But there's no false negatives. That means if the bloom filter tells you that item
has never appeared, then it has never appeared. The two outputs from
the bloom filter are possibly present or
definitely not present. We can actually prove the upper bound for false positives. This is very neat. I'm not going to go over the
mathematical proofs. If you're interested, you can actually find it in a textbook. In reality, there have been many extensions of bloom filter, trying to lower this upper bound, trying to reduce the provable
error of false positives. But you should also
know that bloom filter only works if you're interested in knowing whether someone has appeared or
not in the stream. It does not work for others
entities such as counts.