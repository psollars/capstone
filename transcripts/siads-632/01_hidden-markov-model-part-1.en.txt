So we have introduced
sequence models including unigram models, bigram models,
also known as Markov models, and even higher order n gram models. Today we're going to introduce yet
another model for sequence data, and that is known as hidden Markov model. Hidden Markov model is very popular. I'd probably say that 10 years ago,
right? Not as popular nowadays, when the deep
learning models were invented. But it's still very widely
used in sequence modeling. Hidden Markov model, if you look at
literature, is also abbreviated as HMM. It is another family of Markov models. Well, as we said that, many models
are using the Markov assumption. The differences at what they consider
as the event, in the sequence and what they consider as
observed and not observed. The hidden Markov model assumes that, every item in a sequence is
associated with the state. So you can see that it's modeling
both the state and the item. And the events items in sequence
are discrete, they're sequential. This is basically why we introduce
hidden Markov models for sequence data. And those states are finite, so
they are finite number of states. Or they are countable. Even if the there
are infinite number of states, they have to be countable. But sometimes they are infinite, right? We're not going to talk about these
infinite Hidden Markov Models, in this lecture. But just remember, that sometimes the
states are countable, and probably not finite. And what's very important about
hidden Markov models is that. It assumes that the data
is only partially observed. So what is observed? The items are observed. What is not observed?. The states are not observed. This is why the Hidden Markov
Model is called hidden, because it models sequence
with hidden states, okay? What do we mean by that? Let me give you two examples. In one of the examples,
we're looking at DNA sequences. Well, we know how to model DNA sequences,
using the Markov model. Sure, but sometimes there's yet another
type of information associated with DNA's. That are genes. So genes are basically bonding
sites of unit items of DNAs. As you can see that
after biological analysis, people can actually label part of
the genome sequences as the genes. There may be another segment
of the DNA sequence, as another gene an in
between different genes. There maybe subsequences
that do not have such functions, okay? So in this example, we have both
the items ATC and G in DNA sequence. And we also have something hidden 
that is, which part of the sequence corresponds to a gene? Use our favorite example text. we can also see scenarios where
there are hidden states of the words. In this case, you can see that we are
labeling limbed entities, from the text. So you can see that from
a sequence of words, some of the words
associated with one type. Or sometimes multiple
types of entities, right? And some of the words
are not part of the entities. And whether a word is part of the entity
or not, is usually hidden to us. In more details, if we look at the gene
example, the DNA sequence and gene example, What we observed are  the
unit items, ACTG, right? And for every item  there is the hidden state. And that hidden state says whether
this item is part of the gene or not. So you can see that the hidden states are
basically a sequence of the same length. But it only has the binary indicator. Whether A is part of the gene, whether  the
next C is part of gene. So and so forth. In a text example, we can also see that there are things we observed. Words, right? W_1, w_2, to w_n. That's our
good friends. Old friend, okay? But for every word there  is the hidden state, that indicates whether this word is part of the entity. Well, sometimes there
are multiple types of entities. So whether this word is one of the types
of entities or not any of the entities. You can see that the hidden states
are basically entity types and they also form a sequence
of the same lengths, right? Of course, in that case,
the categories are much fewer. And hidden Markov models
are modeling these sequences. With both observed items and
hidden states. Of course, modeling hidden states makes the
generation process of HMM harder, right? Remember that in HMM, we have 
a sequence of observed items. Or we call it the output sequence, right? Big O equals to O_1 to O_n. So we use another notation
here instead of w, right? Because this is general and
this is observed, right? You can see that in text,
the big O could be w, right? Could be the words, right? And in DNA sequences, the O_1,
O_2, O_n could be APC and G. And formally, there is another state
sequence that's of the same length. We called it big Q, right? So big Q equals to q_1, q_2 to q_n. There are exactly n states and every state corresponds to
one of the outputs in order. Normally, this state sequence is either
not observed or partially observed. Remember, just indicate that maybe
some scientists have labeled that, okay, this part is one of the gene,
right? But they have not labeled the entire sequence. So why is it called hidden Markov model? We know why it is called the hidden model. Why is that called hidden Markov model? Apparently, the Markov assumption
still holds for hidden Markov model. And here, the Markov assumption is no 
longer indicating that the next word only depends on the previous word. It actually indicates that the next state
only depends on the previous state, right? So in other words, given any state in the
state sequence Q, right, let's say q_i, q_i, supposedly, it should be
depending on anything happening before that. Including the states
and observations, right? But based on the Markov assumption, we
assume that q_i is only dependent on q_i minus 1. That is the previous state. In our example, whether the previous
item is part of the gene. Or whether the previous word
is part of named entity. Okay. And q_i does not been done any
of the words, or any of the items. Right? As long as we
have the previous state. This is why the model is
called Hidden Markov Model, But there is another
assumption which is about the words or the items.
All the observations right? So the assumption made by HMM is
that once the state is determined, once we know that, okay, this word is still part of that
named named entity, then the output, that's which particular word it is, does
not depend on the previous observations. That means as long as we know that this
is that lm entity, then which lm entity which lm right does not depend on
previous states or previous observations? In other words, the probability of O_i, that is one of the items
in the output sequence, supposedly, it depends on anything that's
before it as well as the current state. But we assume that it only
depends on the current state, q_i, not anything before that. So both of the assumptions hold, then
the model is called a hidden Markov model. The generation process of HM based
on these notations will be related, but still different from the Markov chain. What's related? We still have the network
diagram where we have nodes. But in this case, the nodes indicates
states instead of the items, right? For example, in the named 
entity extraction example, we have three states, right? Whether the item is the person with
the word is part of the location or whether the word is neither the person,
nor that notation. So we have actually three states, okay? Remember, that in Markov chain We have three words. We have many words. And in the hidden Markov model,
we have three states. Similarly we have the
initial probability distribution, that assigns the probability to each of the states that indicates that,
at the start of the sequence. how likely that the first
word is the person. And the first word is part of the location. Or the first word is another word that's
neither in the person nor the notation. You can see that this is quite similar
to Markov model, to the Markov chain. Then the next thing is also
quite similar to Markov chain. That is, we're adding links
from nodes to nodes, right? So there is a link from any state to any
other state. And the probabilities associated with each of the links
are called transition probabilities. And similar to a Markov chain, every state
has its own transition distribution. That is for any particular q_i that's the
state we have a transition distribution that assigns probabilities from q_i
to any state q_j, including itself, right?
In other words, every state q_i has its own dice to decide where to
translate to, if you were in this state. We call this a white dice.
Why white? We'll see. So up to now, you can see that
the only difference between HMM and the Markov chain,
is that the states are hidden, right? We have nodes corresponding to
states instead of the words, right? We still have nodes, links,
initial probabilities and transition probabilities. Now let me give 
you another thing that's quite different, right? Another thing that's very different is
the so-called output probabilities. That is, remember that now we only have
the hidden states linked to each other. What about the actual words? Actual items? The thing is that when you are in
each of the states, right, there is the set of probabilities that
decides on which word you will write if you were describing a person. Which
words you will use if you were describing the location. And which words we will use
if you're describing neither of them. For example, for the person state,
there is the distribution of probabilities of all the
words given the state right? And you can see that this is essentially
a unigram language model. But this is the unigram language model, particularly
for that state, right? In other words, the state "person" has its
own word distribution. Has its own output distribution. And
that is what? That is the blue dice. We call it a blue dice.
And similarly, the state "location" has its own blue dice. The state
"none" has its own blue ice, right? So you can see that every node, every in state in hidden
Markov model, has two dices. The white dice decides on which
state you will translate into. And the blue dice decides on which output you
will generate or which word you write down if you were in this state.
Right? So this is how it works. You can see that a blue dice determines
what word you will actually write down if you were in this state. And the white
dice indicates which next state will be. Okay. And the hidden Markov model is
described by the initial probabilities, the transition probabilities, and
output probabilities, right? We use Lambda to indicate all
the parameters of a hidden Markov model. So with this denotions, you can
actually generate a sequence using hidden Markov model. Again, we start with nothing and then based on the initial probability,
we sample the state from all the states. So now we know that, okay, we want to write a word, and
the word is about the person, right? Once we determine the state, we then
generate a word using the blue dice of this state. Okay? And
see this is "yoda", right? So you can see that the word "yoda" is
generated from this state. Then the next thing we want
to decide on is not the word. But which state we will go to next.
And in this case, we will no longer use the blue dice of
"person", we will use its white dice. The white dice, the transition probabilities,
tells us which state is more likely to be the next, And suppose we sample one that
links us to the long state, the grey state that is "none". Neither the "person" or "location".
And then once we're in this state the next thing we will do is to
generate a word, right? And then we use the blue
dice of this state. And this time suppose
we generated a word "in", using the unigram language model, right? Then the next thing to do is to make
a decision on which state will we go to next. Which state will we go to next, right? And then suppose we use
the white dice again. And then luckily it says that stay here,
go to yourself, right? Then we stay in this state for
a little longer, right? And then for this new observation, we generate a new word from the blue dice
of this state and we got the word "the". And then we have to use the white
dice again to decide where to go. And then suppose dice says
go to "location". Okay? Then we will sample the word
from the state location. We use its blue dice and then generate
a word. And suppose that's the word "Jedi". And then suppose we throw
the white dice again. And then the blue dice again, right? That will help us generate a state,
an output, a state, and a word, a state, and a word, right? Alternatively, right?
A state and then this word. Another state and then another word, right? From state to state,
we use the white dice, right? We use the transition probabilities. From state to word, we use the blue dice. That's essentially
the unigram language model. And so on and so forth, we have
generated the full sequence with both the observations and the hidden states,
right? Both Q and O. So you can see that generative process
of HMM is not so much different. The only difference that now you have
two aligned sequence to generate instead of one. And the state sequence is
normally hidden right? But the HMM is actually much more
complicated than Markov chain or unigram language models. And
the parameters are much harder to learn.