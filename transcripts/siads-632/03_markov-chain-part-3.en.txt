Once you know the Markov chain, generating the
sequences becomes easy. You can also
calculate probability of any sequence
being generated by the Markov chain as long as you know the initial probabilities and the transition probabilities. But problem is that if you
are new to sequence modeling, if you have a new set of data, you actually don't know
the probabilities. It's not hard to find how
many states there are, basically you count
how many words there are in your vocabulary, or how many items there
are in a DNA sequence. But you don't know the initial probabilities or the transition probabilities, we need to estimate
them from data. Basically, to use a Markov chain, we need to find the
initial probabilities, that is each word as the
first word in sequence. What is the probability that we will see one word
as the first word? You'd have a partial sequence
or a complete sequence. For complete sequences, If we only care about generating
complete sequences, if we never try to generate
a partial sequence, then we can always consider S, the start of the
sequence as a word. In that case, the
initial probability will have the one value for S, and they're value for any other states or any other words. If we do care about
partial sequences, then we have to assign probabilities to any
of the real words. In that case, we need
to calculate Pw. A Pw_i can be easily
calculated in the same way as we estimate the
unigram language models. Basically, we count
how many times that the word w_i appear in any of the sequences
that you have observed, and we normalize that by
the total number of words. Suppose we start anywhere
in the sequence, how likely that we will be
looking at the word w_i. This is how to estimate
initial probabilities. How do we estimate
transition probabilities? You can see that because every word depends on
the previous word. We have Pw_ j given w_i. That means we need to estimate the probabilities of all these pairs of
dependency structure. If we only care about
the complete sequences, we can just add another state, E as the end of the sequence. The easy way to treat that is to consider the end of the
sequence E as yet another word. Everything will be the same. How to estimate the
probability of w_j given w_i? Well, basically, all
we need to do is to count how many times then the bigram w_i w_j has appeared
in all the sequences. Why? Because the
conditional probabilities of w_j given w_i depends on how many times then
w_j appears following w_i. We need to put the count of the bigram w_j to
the top of the bar. Then we divide that
count by what? By how many times that we observe any bigram with w_i
as the first word. You can see that after
this normalization, the conditional probabilities
of any word w_j, given a particular
w_i, will what? Will sum to one. In other words, the sum of the counts we
have below the bar is essentially the
count of w_i itself. Suppose we consider n over
sequence also as the word, then how times that we
see any word or any of the sequence following w_i is essentially how many times
that we see the word w_i. This becomes simple.
Basically, to estimate the transition probabilities
from one word to another, all we need to do is to
count how many times that both words appear in
order next to each other. We normalize that by the
count of the first word. We could do that for initial
probabilities too. How so? Remember that if we care
about complete sequences, we always have SSS state. If we care about
partial sequences, we need to see how likely that one word is used as the first
word in a sequence. We can do this in general by only considering S as
the stating state, but we do consider w_j, given S as the
transition probability from S. Does that make sense? This is essentially
the same with what we have seen with the DNA example. This conditional probability
can be calculated as the count of how many times that w_j is the first
word of a sequence, normalized by how many
sequences you have, or how many sequence starts. Basically, this is how
many sequences you have. This is how to estimate the initial probabilities and the transition probabilities
of a Markov chain. You have to do that
when you have data, you have to do that
when you have observed many sequences or
many sentences to give you a summary of
the Markov chain and its relation to the chain rule and
unigram language models. You can see that based
on a Markov chain, we're basically making different
independence assumptions to the chain rule. Remember that in a chain rule, which is guaranteed to be right, we're calculating the
probability of a run sequence as the probability of the words
one by one, give what? Given all its previous words. But in reality, this calculation
can be very hard because in this case we have to estimate all these conditional
probabilities from data. We see that this chain rule is always true because there's
no independence assumption, every word is depending on
everything in front of it. Now, we can add in some
independence assumptions to make this form easier for calculation and
easier for understanding. Well, you could assume that
this complete independence, that says the
probability of one word. Although in the chain
rule it says that it depends on every
word in front of it, now we can see that it does
not. It depends on nothing. That's also to say that the
probability of one word is completely independent to any other words in the sequence. In this case, we call it
the unigram language model, and it assumes
complete independence. This is a unigram model. To move from there,
we can see that this assumption is too strong because when we
are writing the words, we always depend on
some previous words. Probably, not the previous
100 or 1,000 words, but at least the previous
one or two words. In between no independence
and compete independence, we actually have a big space
called partial independence. For example, the
Markov chain assumes that the probability of
writing down any word wk, given all its previous words, can be simplified as the
collision probability of wk only depending on
its previous one word. If this is true, then we say the Markov
assumption is true. This is, you can see,
partial independence. Wk is dependent on
the previous words, but it's not dependent
on everything. Its neither no independence
or complete independence. A Markov chain model, because we care about
both wk and wk minus 1, it's also called
the bigram model. Well, you may say
that what if we play around in the space
that we assume that the generation of one word, does not only depend on
the previous one word, it depends on previous two words. I could do that. I
could also say that, it depends on the
previous three words. Well, you are right.
You can do that. In that case, you can basically introduce higher
order Markov chain. For instance, if you assume
that the probability of one word depends on
the previous two words, only two words, then you're
basically looking at the second-order Markov chain or a trigram language model. Remember the difference?
Markov chain equals to bigram language model, second-order Markov chain equals to trigram language model. Because in a Markov
chain we care about, how many words it depends
on in front of it. In bigram or trigram, we are talking about
how many words we're considering altogether. In this case, we are considering
three words altogether. That's the trigram
language model. You can extend this to third-order Markov chain or
four-word language model, or fourth-order Markov chain, or five-grand language
model, so on and so forth. Then you can reduce the
J-th order Markov chain. That basically indicates the
probability of writing down long word depends on
J-th words before it. This is the j plus 1
gram language model. All these space, all these assumptions are known as partially independence. That's in between
no independence, that is the chain rule, and that's guaranteed
to be right, and complete independence, that's the unigram language
model, is too strong. Is not always correct, is to strong but it's
extremely simple. For Markov chain,
second-order Markov chain, or J-th order Markov chain, we're basically trading
off the efficiency. With what? With the clickness, with the strengths
of the assumptions.