In this module,
we're going to explore density estimation. So if you've ever used the histogram, then you've already created
your first density estimator. Because the job of density estimation
is to take a set of observations in different parts of a space and
compute a probability, that we will see a particular event
in a particular area of the space. And histograms are one simple way
to create a density estimate. And then we'll look at those briefly and
then see how something called kernel density estimation can generalize
the idea of histograms. And those are a family of density
estimators that are called nonparametric, because they don't assume any particular
underlying distribution for the data. But then we'll also look at parametric
methods called Gaussian mixture models, as a different way to
do density estimation. And then we'll also cover briefly high
dimensional depth density estimation, so there are issues when you get into
high dimensions becomes particularly challenging to do accurate
density estimates. And then finally we'll do a very brief
overview, kind of as a prelude to the deep learning course just a summary
of how recent methods in deep learning have assisted in creating density
estimates for more complex datasets. So let's take a closer look at density
estimation, density estimation is used when you want to convert a set of
individual measurements in a domain. For example, suppose you have measurements
about seismic activity related to earthquakes in different
parts of a region. And you'd like to compute a probability
in that region of an earthquake occurring at any point, based on
the evidence you've gathered at these individual measurement points,
that's where density estimation comes in. The density estimate will estimate the
probability P of observing a measurement at a given location. So on the right here, I'm showing
an example of what one possible density estimate might look like for the
individual measurements on the left, and you can see that it's sort of a smooth
diversion of the individual observations. And now what we've been able
to do is assign probabilities, whether sort of low probabilities,
in these regions that are white, medium probabilities in these
regions that are more shaded and high probability events occur in this
area of the region that is the darkest. So the question is,
what method could we use to derive these sorts of
probability estimates? So what is the probability
density function? Well, for now let's just say that it's
a function that you need to integrate, to compute the probability
of seeing an instance or an event in a particular region. A probability density function is a
function that has to integrate to one over the entire domain of x, the domain
where you're making these observations. And so in a sense,
probability density captures how much of the total probability mass is
associated with a particular region. One thing to note about density functions
is that their range isn't restricted to being between zero and one. All that's required is that, the integral of the function
must be one over the domain. So here I'm showing
an example of the uniform density function on the real line. So this is a uniform density between 0 and
0.5, so point x has an equal chance of
appearing anywhere in the range 0 to 0.5. And in this case, the probability density function has
to sum to one over this whole domain. So in order for that to happen,
the value of the density function is actually equal to two over the entire
domain, because 2 times 0.5 equals 1, the probability mass of the density
function integrates to 1. The same thing would be true if we use a
more refined type of probability estimate. Suppose, instead of a uniform
distribution between 0 and 0.5, values around .25
were extremely likely according to some very
peaked Gaussian that's say. So again the integral under this
curve would still be equal to 1, it would just be squished
into a more peaked shape. And if you look at the maximum value
that this density function takes, it could be, for example,
somewhere around 16. So again, it's not restricted
to being in a particular range, it's the only requirement is that
the integral of the density function over the entire domain be equal to 1. To get the probability in a region
that an event might occur, you first pick the region, and in this case I'm
showing a two dimensional density. You pick the region, and then you integrate the value of
the density function over that region and the integral that comes out will
give you the this probability. Let's just say that x is between A and
B, and y is between C and D,
the probability of x being between A and B, while the probability
of y being between C and D is just the integral of
the probability density function here. There are two main families
of density estimators, two main approaches for taking a set
of individual observations and somehow computing a probability
estimate based on those observations. The first approach to density estimation
is a parametric approach, and in that approach, the assumption is that
the underlying data follow a specific parameterized probability distribution. For example, a Gaussian could also be
exponential Poisson, beta and so forth, to derive the density estimate, you simply
take the maximum likelihood estimate for the parameters of that family as
estimated from the given data. For example, for these points on the
right, in a parametric Gaussian approach density estimation, you would estimate
the sample mean and standard deviation. So it might look something like this, so that would estimate
a Gaussian from those points. In a nonparametric approach, which is what
we'll focus on first here, we don't assume anything about the nature of
the underlying probability distribution. Histograms are an example of
a nonparametric form of density estimation and we'll look at a different
one called kernel density estimation. In a nonparametric approach,
we would take these points, and we would consider fitting something
like a Gaussian, we might end up fitting a smooth version of the points using
a kernel function like we'll see later. For example, in a medical application
density estimation with one variable might be used to estimate the distribution
of a specific test score. In this case, let's say type two diabetes, the plasma glucose concentration
number from a blood test. Typically, glucose concentrations
are much higher in people with this particular form of diabetes. And if we gather individual test
observations where we have, in this case, one dot per patient, take these
measurements for different patients. And then we can construct a density
estimate to estimate the probability that anyone with that medical condition
has a particular glucose score. Even if we didn't see that specific
score in the original data set, so the density estimator will take
the original points on the left and then we'll construct this smooth
curve on the right which maps any given plasma glucose concentration
to an estimate of probability. So typically would expect to see for
this particular medical condition, a lower proportion of diabetic patients having
this level of glucose at 75, let's say. And then higher probability of
having this particular glucose concentration given
that medical condition, we can then gather another sample from
patients who do not have the condition. We can measure the same variable from
that set of patients not having that condition and we can make a density
estimate from those observations. So we might observe for the non diabetic
patients, they have a plasma glucose concentration with a density estimate
that looks like the one on the left. So once we have these two different
density estimates one for each condition. We can compare them they can be used
in further machine learning stages, such as part of providing features for
classification or regression. The more technical way to say this is
that density estimation calculates a continuous probability density
over a feature space given a set of discrete samples in that feature space. With the density estimate, we can then estimate how likely any given
combination of features is to occur. As a concrete example, we could build a
classifier with this one feature that took as input a measurement of the observed
variable the glucose concentration. And we could see which of the two
classes of patience was more likely, so at a measurement of 100, let's say. We can see that non diabetic
patients are extremely unlikely to have that concentration, whereas diabetic patients are much more
likely to have that concentration. So if we were to predict given
a particular measurement, which of the two classes
somebody would fall into? We could pick the more likely
class as a prediction, so we can see that at this crossover point
right here, any values in the observed variable that are greater will be
associated with diabetic patients. Because they have a higher probability
under this density estimate and any values to the left of this line will
be associated with non-diabetic patients. Now granted this is an extremely
simplistic model but it's an indication of how density estimates can be used as
input to processes like classification. Let's look at this classification
scenario a little more formally, suppose we have a binary class
variable C that we want to predict. In this case, we might want to predict
class zero, might be associated with non diabetic patients in class one, might
be associated with diabetic patients. If we want to be able to classify
a patient based on their observed x value for this test result,
we need to be able to compute the probability of each class
given that observation. Since we computed the density function, we now know the probability
of x given the class. So given that somebody is in class zero,
this probability density estimate for any given x, we now have
a probability associated with that. This is probability of x
given class equals 0 and probability of x given class equals 1. But in order to do classification, we need
them the other way around, we need to be able to predict the class from
the observation, so how do we do that? Well, this is another application
of our trusty Bayes theorem, so Bayes theorem will tell us how to
get the probability of a class given an observation from the probability
of an observation given the class. If a classifier is able to compute
the probability of each class given x, then it can predict the most likely class,
whichever is greater probability of C equals 0 given x, or
probability of C equals 1, given x. So we're interested in which
class has higher probability, we can drop the denominator because that's
going to be the same for both classes. So this is simplified a little
bit probability of its class 0, the non diabetic patients,
given x is proportional now to the product of probability
of x given C equals 0 times the probability of ability of 0 so
that's just Bayes Theorem. And we can do that for
each class, now you'll note, that this also requires us to specify,
a prior belief in the class, being 0 or 1 that can greatly affect,
our estimates for the posterior probability
of a class given x. We'll revisit that later, this is an example of a classifier
based on a generative model. Generative models models
each class explicitly, instead of modeling just
the boundary between the classes. So we have density estimate,
that's the model for class 0 and a second density estimate,
that's the model for class 1. Understanding this is also useful for
understanding Na√Øve Bayes classifiers, basically where each feature
has its own density estimate. So how do we convert a series
of individual observations let's stick to one dimension for now. How do we convert a series of individual
observations to that nice smooth density estimate that we saw earlier? Well, let's start by
considering how we might pull observations like this
to create probabilities. One simple way is to divide the domain
of the observations into bins, so we're moving toward the idea
of this being a histogram. So for example, we might pick a bin
size of this observations between 0 and 1 we might pick a bin size that is
0.5 divide the domain into two and then we would just look at what proportion
of all the points was in each bin. So in this case,
if there were five observations in total, then the probability that we derive
from this bin is three fifths. And the problem we derive from this band
is two fifths, so we've just arrived Very simple probability estimates from
these individual observations. You can generalize this idea with this
formula, which basically just calculates the fraction of total points per unit
area or unit volume of the region. So these regions define bins that you can
think of assigning each observation to. Now one issue if you work with histograms
you may seen before is that there are two problems, one of them is, if we
picked a bin slice that was very small, let's say 1 over 100 most
of the bins would be 0. But occasionally it have a spike wherever
there was a point it have a spike but everything else is up to 0. So, that's a pretty
high variance estimator They're constantly fluctuating
between zero and some constant. On the other hand, if you pick a bin
size that's too large, let's say in the extreme case, we pick a single
bin that's just the entire interval. Then we only have one
probability estimate, which would be what five out of five. So, clearly that estimate is too smooth. In some sense, the variation
is too low across the region. So we want to have some middle ground
where we pick a way to bend the points that captures the essential
characteristics of the underlying true density. Without being too high variance or
too low variance. So, of course, we've seen this before
as the histogram and that the key question of how wide the bin should be in
the histogram, do we want the same bin width everywhere or do we want to focus
on areas where there are more points. Do we believe that true density is
actually zero for empty bins or not? Should we smooth those
empty regions somehow? So those are questions that arise when
we're using histograms as density estimators. And so
we can see another example where we show the true density of this
variable with the green curve. If we pick a bin size that's too narrow. We'll get lots of local variation. If we pick a bin size that's too wide,
as in the bottom case, you can see we've lost the two mode
nature of the underlying density and it simply looks like
an increasing quantity there. You want something somehow to be able to
pick the right bins strategy that does capture essential properties of
the underlying distribution. So this flexibility leads to good and bad properties of histograms
as density estimators. On the one hand you don't have to assume a
particular underlying distribution family, you don't fit a particular
model to the data. So that's why it's non parametric model. You just have to compute some simple
statistics, the number of data points in each bin, store them, and
compute probabilities that way. On the other hand,
especially in high dimensions, the number of bins you have to track
is exponential in the dimensionality of the data space you can imagine how
many bins you'd have to track. As you start moving to two dimensions,
three dimensions, higher dimensions, higher dimensions are tricky. In higher dimensions, you have to be a bit
more clever about how you adapt, for example, with local
bandwidth to the density. And also the this this
property of histograms. Do you probably noticed you're trying
to use them at the bin boundaries at the extreme ends of the domain, there's
some annoying discontinuities where the probability simply drops to
zero instead of being smooth. So to overcome this problem of trying to
pick the best width of the bins using a fixed scheme, we're going to look at
the idea of kernel density estimators. Which allows regions to be centered
on the data points themselves. So these regions, you can think of each
one as being like a little Gaussian, centered on a point. So these regions can overlap. They have soft edges. Essentially, they define a little
probability distribution centered on the point. And so instead of having a fixed zero or
one, at a given point, you can think of it like
a smooth version of the point. And then if we want to
compute a probability, let's say, of some value of x that's here, we let the nearby points influence
the probability estimate greatly. And points that are far the observations
that are far from x will influence it weekly. More formally, if we decide to use
a Gaussian, it's a smooth each point, the probability estimate at a point
x is going to be just the value of the Gaussian associated with
each of the end points and then just added up and then averaged. So essentially what this is saying is that
each of the data points that we observed, the discrete original data points can
have some influence on our probability estimate at x, but that influence is
going to be mitigated according to the Gaussian around that point. So the farther that x is, let's say
from this point xi, you can see that in xi Gaussian, x has very low probability,
whereas in a different Gaussian, like this one right here, xj,
probability of x is a bit higher. So with this combination of Gaussians
added up average essentially. We're going to derive a smooth
version of essentially a histogram. So here's an example of what might happen. If we did that you can see this accords
pretty well with what we intuitively might expect. There's a lot of activity
in this area of the main so we would expect higher probability for
events, or measurements, in that part of the domain,
and perhaps over here as well. And we get this nice sort of a bumpy
sort of estimates that we might get for a histogram. We get this nice smooth estimate for
the density. So this choice of function to center at
each data point is called the kernel. So in this case we're
using a Gaussian kernel. But we could actually pick many different
possible shapes for that function that controls the influence that point has
on neighboring probability estimate. So this idea of being able to define
a Gaussian around each of the observed points, can be generalized to the idea
of a kernel function centered on the data point, whose width can be
controlled by some bandwidth parameter h. So in the case of Gaussian, we might
control the width of the Gaussian by varying its sigma parameter and
standard deviation. We can generalize that formula for
the density estimate. We saw in the previous slide by replacing
the Gaussian with some other kernel. And if we do that we get this formula. Essentially we're adding
up at the point x. If we want to determine
the density at point x, then we look at all
the observed data points. And let's suppose we want to know
the influence of data point 0 here. On our target x if x0 has
a Gaussian around it. Of a certain width we can see
that the probability of x, relative to that Gaussian is fairly low. And if it were some other kernel function
which we could use a triangle or a box a lot of different choices. Essentially the kernel defines
the influence that a point has on our side probability at x. And then by adding up all the influences
across all the observed points, we can get a smooth version
of density estimate at x. This bandwidth acts like
a smoothing parameter. So, you can imagine increasing
the width of the Gaussian to get a very smooth density estimator. Or if you have a very narrow
Gaussian around each point, the resulting density
estimate will be very bumpy. It can be high variance. So to use kernel density
estimation in scikit-learn you use the KernelDensity class
in sklearn.neighbors. The two main parameters you need to
be concerned about are the kernel, which in this case I've set to
a gaussian and the bandwidth parameter which sets how broad or how narrow
the kernel is around each point. Then as usual with scikit-learn estimators
you call the fit method on the individual observed points you Passing
all the points is matrix and it will estimate the kernel
density function. If you want to change the kernel
simply change the kernel property. There's lists of names that
I'll show you in a minute, but implement different shapes of kernel. So this one called top hat you can use for
example, and then if you want to do the key operation, which is how to get
the probability of a new instance, under the estimated intensity, you pass
in the new samples, you want to score and you call the method score
underscore samples. And this will output the log
probability of those points under the estimated density. Here's an example with two
dimensional observations. So here x is a Numpy array of
a series of two dimensional points. And I've also included as an example
of using Chrome density estimation for outlier detection. Two points that clearly live far
From the original set in x, so we take our original observations, call the fit method on
the kernel density object, and then we can compute the probability
of points using score samples. And of course, you can apply score samples
to your original training points as well. If you want to As well
as any new values of x. And you can see that under
the kernel density estimate probabilities of the training
points are quite high. And the probabilities of the new these
outlier points here are extremely low. So kernel density estimation
is a very useful tool. To detect unusual outline points, and
we'll discuss in a later module of course, how you can use kernel density
estimation to improve for example, supervised learning. In kernel density estimation. The choice of the bandwidth parameter
is typically more important than the choice of the specific kernel. Usually a gaussian kernel will be
just fine for most applications. Sickie learn provides a variety of kernels
as you can see here, different shapes. And you can see that the choice of
kernel is reflected somewhat in the shape of both the overall
density estimate that you get. And it's especially evident in
the tails of the estimated density. For example, in this diagram,
you can see the underlying observations that we're using as
the input to kernel density estimation. The gray represents the true density and
then you can see the various density estimates that result from
different choices of kernel. So let's look at the top hat kernel
in particular this box shape. You can see the kernel density estimate
the results from choosing the top hat kernel. For example carefully this you can see
that the density estimate itself has rather sharp edges so it's not smooth. It reflects the the sharp edges
of the current law itself, especially as you get towards the Tails
of the distribution and you maybe don't have a lot of underlying data that
fit can be particularly important. So they may be specialized
scenarios particularly. We don't have a lot of new line data and where it's important to fit
the tales of the distributions. Well, that a specific kernel
choice can make a big difference. But in general, as I said, Gaussian kernel
works well in most applications and choice at the bandwidth parameter
has a lot more effect on the accuracy of the density estimate
compared to the kernel choice itself. Kernel density is especially popular for
creating heat maps like this one. This is an example of a two dimensional
density estimate, where the underlying observations have to do with
earthquake activity around the world. We can see that using kernel density
will allow us to predict the probability of an event given its location. So now we'll discuss briefly the question
of how you evaluate whether a density estimator is doing a good job or not. We haven't really discussed that yet. Well, the way we can evaluate density
estimators is similar to the way we can evaluate other forms
of probability model and that is to compute held
out lag likelihood. So given a density estimator M and
held out dataset X with instances xi, you can compute how likely any
given Xi is under the estimated density probability of Xi
given him the estimator. You can do that assuming that
draws from x the instances Xi. Are all independent
identically distributed, the probability of seeing the entire
set of held out data is simply the probability of seeing each of
those instances together as a set, which is just the product since they're
independent, identically distributed. It's just the product of
the individual probabilities. And then we can just take the log of
that quality mathematical convenience. And that's the held out data likelihood. We can use the holdout data likelihood
as a quality measure to, for example, to find the optimal bandwidth value. So here's an example
that uses grid search. And cross validation to maximize
the held out log likelihood. So here we're doing a grid search
over a set of possible bandwidth. So we've decided to create an array here, a series of possible bandwidths,
we pass in that list of bandwidths and the kernel density estimator,
the Gaussian kernel we want to use, you pass in the cross validation method,
which in this case will be leave one out. Then we call the fit method using
the set of x random 1d data we had in the previous cell in this notebook
using the spritzers of cross validation to determine the optimal
value for the bandwidth. Is about 0.43, and it gives us back also
the data log likelihood that choice. So kernel density estimation was
an example of a nonparametric method. Now we're going to take a brief look
at a parametric density estimation that you might recall when we looked
at Naive Bayes classifiers. That was an example of a simple
joint density estimation. Were all the features were
considered independent. Each feature had its own density estimate. And then to compute the probability
of a set of features, we simply took the product of the
probabilities of the individual features under their density estimates. Well in this video, we're going to look
briefly at Gaussian mixture models. As implemented in the SK
learn.mixture.gaussian mixture class, Gaussian mixture model, or GMM is a widely
used density estimation algorithm. It's used all over the place. And the probability distribution
of X is model as a simple linear combination of Gaussian components. So each component has a weight WK, and then this is the individual Gaussian,
the Cath Gaussian mixture. You can see that it has
a mean covariance matrix. So the goal in density estimation in
this parametric situation Is that we're given a set of examples x
instances x 1, x 2, to x d, let's say. And now we have to estimate these
parameters, we need to estimate the center and the covariance
of each Gaussian in the mixture, we want to essentially maximize
the data log likelihood that the mixture model provides for
some given set of parameters. We're going to revisit this briefly when
we look at the expectation maximization algorithm, which can be used to
estimate these centers mew sub k, and covariances sigma sub k. So we'll discuss that as part
of our exploration later. There you can see there are a lot
of parameters to estimate here, we have the k minus one weight parameters. There are k minus one of them,
because they have to sum to one. So once you determine k minus one of them, that k one is constrained to be whatever
it is, that makes it sum to one. For each element of the mixture, we have
D parameters that are a mean vector. So there are k d parameters for the means,
and then, we have k times d times d plus one over two for
the Gaussian covariance matrices, assuming that they're all different, so there
are a number of parameters estimate here. Other methods besides includes
variational methods, the BSc criterion. We're not going to go into those in this
course, but they are fairly efficient. And as I said, the Gaussian mixture model
in general is a widely used technique. Here's an example of a Gaussian
mixture model with two components. So two Gaussians,
showing the negative log likelihood plot, that's predicted by
the Gaussian mixture model. So just in the one dimensional simple
case we saw, we have a set of points that are our initial observations, only this
time that they're in two dimensions. So this could represent earthquake
events in a particular region, or susceptibility to forest fires. Or some other whatever event you could
localize to call the Gaussian mixture model in scikit- learn,
it's part of the mixture library, and you simply specify the number of
components, in this case, two, and what type of covariance
estimates you want to make. In this case, we want to estimate
the full covariance matrix for the Gaussians in the mixture. So these blue points
are our training points. And so, we call the fit method on the
observed points that we saw in training, and then, once we've called the fit
method, we can use that score samples method to get the probabilities
of a whole set of points. So actually, in this case,
we've developed a whole mesh grid. So we want to plot this contour plot. So we're basically taking the probability
in every point on this mesh grid in order to plot this contour map. Now, this is another instance where we can
see that if we have most of the points fitting the Gaussians well, we can see that there's an outliner
here with much lower probability. So this is another example of using
density estimation for anomaly, or outlier detection, in this case, a
Gaussian mixture model with two components fit this observed data pretty well,
because as you can see, they were already in
two distinct clusters. So it was a natural fit for
a two component Gaussian mixture model, you can see that the regions of highest
probability, highest likelihood, the modes of those two distributions,
and you can see that they're Gaussian. So they have these typical
contours to come out. And you can imagine that if you
had other points over here, you could fit an additional
third Gaussian into the mixture. This is very much along the lines
of K means clustering, but it's a soft kind of a clustering. Where, instead of an absolute
assignment to a particular cluster, for a point you predict the probability, that the point was generated by
one of the K mixture components. So some of the advantages of Gaussian
mixture models are that the algorithms to estimate them, are very fast, and
work on very large scale data sets. The second advantage is that their goal is
to maximize the likelihood of the data. So the particular algorithm isn't
biased towards particular kinds of shapes of clusters,
locations of the means, and so on. So it will flexibly adapt to the structure
of your data rather than preferring some predefined structure types. Some of the disadvantages include
the possibility of singularity, so when they don't have
enough points per mixture, estimating the covariance
matrices becomes difficult. The Gaussian mixture model algorithm may
diverge and find solutions with infinite likelihood, unless you constrain with some
kind of prior, the covariance matrices artificially, in other words, if you have
a regularizer for the covariance matrices. There's also the perennial issue of
determining the right number of mixture components. So the Gaussian mixture model will use
all the components that it's given. Without additional knowledge,
you'll need to either use a holdout data set to compute the held
out log likelihood, or some kind of information
theoretic criterion like. The BIC vision information criterion to
help prefer simpler mixtures, for example. Toward the beginning of this video, I
mentioned the curse of dimensionality for histograms as you build up cubes and
hyper cubes, the number of bins
increases exponentially. We can somewhat mitigate
the curse of dimensionality and the cost of searching
a high dimensional space, if we're allowed to build an index
on the training data first. If we have an index that makes
finding the nearest neighbors fast, so we don't have to compare with all
endpoints in the entire data set, if we're simply trying to determine
the density of a local neighborhood. So two popular indexing methods
are K-D trees and ball trees, and both of these are supported
in scikit-learn. Generally speaking,
you can trade off computation time for accuracy via the a tall and
our tall tolerance parameters. You might be wondering how density
estimation benefits from the most recent developments in machine learning and
deep learning. For example, I just have one slide here to
give you an idea of the approach that can be used for density estimation with
more complex learning frameworks. So one approach is called
normalizing flows. And basically, the idea is you can
estimate an arbitrarily complex. Density estimation function by choosing an
initial simple density, like a Gaussian, base density, and then, applying
a sequence of nice invertible smooth transformations to obtain a series of
successively more complex densities. Eventually, you'll obtain probability
distribution for your target variable. I've included a paper linked here with an
explanation, if you'd like more details. So that concludes our tour of
density estimation methods. We'll see them reappear again and
again throughout this course, both for unsupervised learning applications,
but also, to improve supervised
learning in different ways.