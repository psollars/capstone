Let's talk for a minute about
search engines and what's required for a good search engine to find great results
for your query. Well, there are many ways
to express a concept, for example, suppose
you were interested in picking the best
stock market portfolio. But this might not be the
way that it's expressed exactly with the same keywords
in the ideal documents. An ideal document might refer to equities instead of stocks. It might refer to optimal things instead
of the best things. It might refer to choosing
instead of picking, and so on. This problem of having a difference between the
terms that are used in the original query and
the terms that appear in the best results is called
vocabulary gap problem. It's at the heart of what a search engine has to do when it compares someone's representation of their information need, a query, and the representation of an information
source, the document. There's the problem of having a gap between the terms in
the query and the document. There's also a problem if the terms are used
in different senses. It may be that the same keyword is in both query and document, but it's used in very
different senses. This challenge of overcoming
the vocabulary gap and doing correct matching in the case where words are
used in different senses, was the original motivation
for latent semantic indexing. The basic idea of latent
semantic indexing is, instead of doing a
direct keyword match between queries and documents, you would do some semantic
oriented matching between an expanded version
of queries and documents in some latent space. This assumes there's some underlying latent
semantic structure in the data that may be obscured
partly by how people choose words to express
those semantics. But by using
statistical techniques to estimate the structure, we can get rid of the noise, the randomness of individual
word choices and focus on the underlying semantics
of queries and documents. The statistical technique
that latent semantic indexing uses is exactly singular
value decomposition. Singular value
decomposition is applied to a large matrix of
term-document association data. Doing singular value
decomposition allows the arrangement of this
latent space to reflect major patterns in the data
of essentially clusters of topics and ignores the smaller
variations in the text. The result is that
both queries and documents get
projected, in a sense, to a shared latent space where semantic
matching can happen. Terms that don't
actually appear in the document may match with terms that don't appear in
the query because they're both in a shared semantic space. We'll see an example
of that shortly. To use this in a search engine, you would simply take
a query expanded with the topics that were found by latent
semantic indexing and then do ranking
or retrieval of all the documents in that semantic
neighborhood associated with the expanded query. I've included a link
to the original latent semantic
indexing paper here. But right now we're
going to look at some details of how
it actually works. Latent semantic indexing
has been investigated for use in lots of different domains. The nice thing about it
is it can be used on any document corpus involving
some conceptual identifier, so it could be words
in any language. It could be morphemes
from speech. It could be identification
codes corresponding to collections of objects
in larger groups. It's the basis for one semantic matching
for full-text search. It's been used to do
cross-language retrieval, matching papers to reviewers, clustering, filtering out spam, many different natural
language applications. Sometimes it's referred to
as Latent Semantic Analysis. Latent Semantic Indexing
is simply the use of truncated SVD on the
term document matrix x. Typically X is filled
with tf-idf weights. The input is a sparse n by
p term-document matrix and also the number of desired
latent topics K. In practice, tf-idf is chosen for the
term weights because that is what performs better
in practice typically. It's sparse matrix so there's no centering or normalization of x. Once x is set up with tf-idf
term weights, in this way, we simply run truncated SVD to get the factored version of x. Here's a more
graphical description. Here's the term-document matrix. We run truncated SVD
with k specified. The result of that is
a term topic matrix, which is the u sub k matrix, it has k columns. We have a topic document matrix that has k rows and p columns, where p is the
number of documents. This is the v
transpose sub k matrix that comes out of truncated SVD. Then we have a diagonal
matrix as usual with SVD. The interpretation of
the diagonal matrix is that it's a set of
weights for the topics. If we plug in an actual matrix, a seven by three
matrix in this case, we're going to
pretend that this is a document term matrix and I haven't put in
tf-idf weights here. I'm using simple frequency counts for now just to
illustrate the point. But what comes out of plying the truncated SVD to the
term-document matrix is, you can think of this
term topic matrix as a description of each topic according to
the weights of term. Each column has a set of
numbers that are weights for each term that indicate the importance of
that term to that topic. Likewise, the v transposed
Sub k matrix has weights that indicate how representative that document
is for a given topic. Or alternatively, for any given document what
are its topic weights? How are those topics
associated with the document? We can view latent
semantic indexing as a soft clustering by
interpreting each dimension of the reduced space as a cluster and the value that
a document has on that dimension as its fractional membership
in that cluster. In scikit-learn, doing
latent semantic indexing is almost as easy as just
doing truncated SVD. Of course, the step we have
to take first is preparing the x, the term-document matrix. We do that using
tf-idf vectorizer. We initialize the vectorizer with the text pipeline parameters
that we want to use. We call fit transform
on a list of strings which represent set of documents that we're
going to be processing. That will create a vocabulary of terms that are all unique terms
across all the documents. So we can get the
back that vocabulary, that list of term strings
that correspond to the vocabulary entries using Get feature names from the
vectorizer once it's been fit. Then we simply run truncated SVD by specifying
the number of topics. In this case, it's
specifying 200 topics. We run truncated SVD, that will get us back factored
version with u_k, Sigma_k, and v transpose sub
k. In order to get the matrix u_k from the
fit truncated SVD object, we call fit transform
on the document matrix, a transposed version just
to make the sizes match. To get u_k, we call LSI fit transform in this way to get
the reduced term matrix, the reduced document matrix. The transpose V sub
k we can get simply by looking at the
components underscore property of the fit LSI object
and the singular values in that diagonal matrix
we can get from LSI's singular underscore
values, underscore property. If we look at the shapes of
the objects involved here, we can see that we have
our original matrix x which is 11 k rows, 39 k columns. We can see that's
getting factored into the U_k that
has 200 columns, diagonal matrix that has
200 rows and columns, and then the transpose of k, it's got one column
for every document. That's how the original
matrix gets broken up into three new truncated
SVD result matrices. For large collections, running
Latent Semantic Indexing, we typically set the
number of topics defined somewhere
between 100 and 400. How does this address
the semantically based matching that we were hoping to do in search, for example? How do we use these
matrices that come out to do that matching? It's very simple with the
results from the truncated SVD, suppose we have an
initial query q, and suppose that we
have a TfidfVectorizer, so we can transform q
using the vectorizer, and we get some sparse vector with the various terms in q. Likewise, if we
have a document d, we can also run that
through the vectorizer to get sparse vector representing d, so that would be the
vectors that you see on the very right
of these expressions. Then we can take the matrix U_k, and we can simply
multiply through. We can take the document vector
d using the top product, we can multiply with
U_k and then multiply by 1 over all the weights
in the diagonal matrix, and the result is
another vector that's basically an expanded
version of d. For example, if d had a term here, here and here, those same terms would
likely be activated here, but it would also contain some additional expanded
terms that had related words, and the same is true for q, so you can think of q_k and
d_k as vectors that contain an expanded set of
terms that match the semantics of the
query or the document. Once we have these expanded
q_k and d_k vectors, you can simply match q_k
against d_k using say, cosine similarity,
just as you would for the original q and d vectors, but you're computing the
similarity of the query and the document by comparing
their expanded versions, q_k and d_k with
cosine similarity. This exercise is actually part of the homework for this week, but you should find, if you
implement it correctly, that the queries
that originally had no terms in common and had a
cosine similarity of zero, using the original
query document terms, have a significant non-zero
cosine similarity, and you compare the
vectors that you get from the previous
formula after you've expanded the query
and the document using Latent Semantic Indexing. You'll see that in the homework for this week, an example, of how that matching formula can aid you in this
vocabulary gap problem. You can take a look at
why these query and document expanded
variables are matching. You can find this
semantically related terms that was matching on, by computing the UU
transpose matrix. If you compute the
UU transpose matrix, or maybe more precisely
the U_k transpose matrix, you get this term-term
relationship matrix. The term-term matrix tells us the term expansion
behavior of the LSI model. You can think of it like
the first U operation, maps a term to the latent space, L_ k with k topics, and then we're mapping back again from that latent
space back to term space. You can very easily see what
the related terms are by computing the term-term
matrix with this top product, you're simply taking the
product of the U_k matrix with its transpose and using
this routine and in Python, you can compute for any term. We can look up, say
given a term index, we can get that row in the term-term matrix
and all the other terms in the row will give
association weights for how strongly those terms are associated with the term index, and that's also part of
this week's homework. That gives some insight into why Latent Semantic
Indexing might help information retrieval by finding semantically
related terms. If we do this with the LSI model trained
on a large text corpus, we can print the related terms associated for
example, with economy. In this particular corpus, we find that if you were to enter economy as a
query, for example, it would expand your query with these additional
related terms; government, people,
Clinton, and so forth. It tells you something
about time period that the corpus represents the
news articles from the 1990s. Similarly, a word like spending gets expanded
with some related terms; budgets, gasoline, some bunch of related
terms that come out of the LSI model and so on. The example we use
typically here is the 20 newsgroups
dataset again, we ask, in this case, we
want to compare it with topics that are found by other topic
modeling like methods. Here we've asked it
to find 10 topics, K is 10 in this case. You can see it acts very much like essentially topic
modeling method. You can compare this set of
10 topics that were found by LSI with the 10 topics that were found by some of the other methods we're
discussing this week, non-negative matrix factorization and Latent Dirichlet allocation. You can see that it does
indeed find coherent topics. Here's one about encryption, here's one about Windows
Microsoft software and so forth. If we look at topic 4 which is more apparently a
politically oriented topic; at least that's how we
can try to interpret it, we can see the
relative contribution to the topic for each term. We can see the term
like people has significantly higher
weight in the topic, followed by government
and some of the issues of discussion
in the news articles; gun-control, budgets, constitution,
healthcare and so forth. I think it's safe to
say that this can be interpreted as a political topic. Then using the v_k
transpose matrix, we can find what are the top documents
for any given topic. In this case, the document with highest weight for topic 4 is a White House press release, which would make
sense because it's a very political document. It talks about issues
of relevance to politics of the
day in the corpus. We're getting pretty
reasonable results from Latent Semantic Indexing. There's definitely some pros and cons for Latent
Semantic Indexing. We saw that by finding this latent space and
then essentially being able to expand queries and documents in that latent
space to find related words, definitely addresses the
vocabulary mismatch problem. It also finds this
concept space that's much lower dimension than the original
term-document matrix. One of the effects
is that when you expand queries and documents, you definitely can increase
search engine recall. You can actually find more relevant documents
that are out there. Often, sometimes the
precision increases as well because in documents that
are topically relevant, they tend to use some of
those related terms as well. If you add those related terms correctly to your document, query representations,
you actually might end up also
increasing precision. It's most impressive,
Latent Semantic Indexing when there's little overlap expected between
queries and documents. That's where the
semantic orientation of the matching really
gives most benefit. Some of the drawbacks
include that running SVD on a truly large
collection can be very computationally intensive
and that in some ways might have limited the adoption of Latent Semantic Indexing, things like the web, for example. It also assumes that the Singular Value
Decomposition goal of finding an optimal element-wise
least-squares approximation to this document matrix is the
right thing to optimize, but it may not be the right
goal for all problems. Finally, you can go
ahead and compute this SVD on this
very large matrix, but then when you start
getting lots of new documents, there's some more
complex strategy required in order to update these latent factors
approximately without having to recompute an entirely new
Singular Value Decomposition with all the new documents. There are incremental
update possibilities, but it's not necessarily
that straightforward. LSI is a powerful
illustration of the power of latent models to do things
like semantic matching, but it also comes with
the computational cost. We'll look at some other methods
for doing topic modeling and semantically matching
in other modules this week.