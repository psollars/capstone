In this lecture, we're going to cover dimensionality
reduction. We're going to talk about what
it is, why it's important. We'll take a high-level tour of three broad families of
techniques that people use to do dimensionality reduction
and then we'll zoom in specifically on Principal
Components Analysis or PCA. We'll also look at several different
flavors of PCA that have been developed to handle
more specialized scenarios. Let's start with
the basic idea of a dataset as a
spreadsheet or a table. Each row in our original data set X represents one instance,
could be for example, an employee record in the database that
you dumped out and each column represents
a different property of that data instance. It could be first name, last name, hire date, salary, and so forth. A particular row of this table, as we've seen before,
this instance has a bunch of properties. In fact, it has p of them. It's a p-dimensional
vector and we might sometimes use the term variable
or feature for a column. If it's being considered
as such in an analysis. Well, one very
important family of unsupervised learning
methods that transform the input dataset are known as dimensionality reduction
algorithms and as this name suggests, this kind of transform can take your original dataset
that has p columns. Let's say p is 200 and it generates an
approximate version of the dataset that we denote
to the X sub q and with, let's say 10 dimension. Well, the new dataset
has ten columns. These may not be columns
that are selected exactly from the
original p. They may be completely different
from the original p, but they capture some
essential pattern of variation that existed
in the original data set. You can think of them perhaps as a compressed version
of the dataset. In dimensionality reduction,
we might want to create a version except q that's much
smaller than the original? The question is, why
we want to do this? Well, one reason is that
you might be exploring dataset and having
fewer dimensions. It makes it much easier
to do an analysis. You can deal with three
or even two variables instead of 100. Another reason might
be you want to apply some technique to
your original matrix, for example, maybe a
supervised learning method, but p is very large. If p maybe, say a hundred, two hundred or a thousand, as we see in many
different instances in supervised learning. There's this curse
of dimensionality, which increases computation time as the dimension of
the data increases. One way people have dealt
with this is applied dimensionality
reduction techniques to the original dataset to get something that's much smaller in dimension and then that makes it possible to apply some of these more sophisticated
techniques to the data. Another reason you
might want to do dimensionality reduction
is visualization. That's a special case, a special type of projection. If you can reduce the dataset
to just two variables, that makes it easy to visualize and perhaps
you're looking for visually for clustering behavior of the
variables and so forth. You might want to
compress a dataset. That's another reason people perform dimensionality reduction. You can view it as a form of compression in some scenarios. You might want to reduce
the number of bits per pixel in an
image, for example. Then another reason people apply dimensionality reduction is
to find interesting structure that helps improve
the accuracy of downstream tasks like supervised
learning predictions. Quite often there's interesting
structure in the data that's useful to find and dimensionality reduction methods, as we'll see, allow
you to find that. What do I mean by interesting
structure anyway? Well, there are three general
types of structure that data mining methods and also
machine learning methods can find and exploit. Though in the first example, there might be global
cluster structure in the data and here we can see that the data is occurring
and obvious clusters. This might represent
this dataset group of customer visits to
a website and here what matters are the relative
distances between the items in the same cluster and the
distance between clusters. In the second example, we have some data that lies along a two-dimensional
ribbon in 3-D space. Here, all the data lies on
a low-dimensional sheets, so it's a
three-dimensional object, but the data itself only lies along this
two-dimensional manifold. To describe the position of
any point in the dataset, you specify two coordinates. You can specify how
far it is along central measure in
this direction, and then you can measure
its deviation from that central curve as
the second dimension. That's a second type of
more local structure. Then the third type of
structure will be interested in finding is correlation
structure so, how do different columns in a dataset correlate
with each other? You can see, it's
quite easy to see in the case of two features
that you can visualize this, but we have this Gaussian
like elliptical Cloud and by performing
dimensionality reduction, we can get an idea of how
the variables covariant. That's exactly what PCA is going to do that
we'll look at. You can imagine that as a
number of variables increases, this correlation structure
can get quite complex and we'll see how PCA will
help us with that as well. One concept I wanted to
emphasize early on is this idea of intrinsic versus
extrinsic dimensionality. This is a very important
term and it refers to the smallest number
of variables that are required to represent the data. In this example here, a data set may have a
dimensionality of two-dimensions. In this example, we were plotting this curve,
two-dimensional plane. but all the points lie
along a simple curve. In fact, if you wanted to represent any point on the curve, you could simply specify if this could be the
motion of something in through a surface
over time, for example. As it moves over the
surface over time, its position changes along
the curve and you can specify any point on this curve simply by
specifying a time, assuming with the point doesn't backup and go over
the same point twice, assume that it continues
evenly forward. It has an intrinsic
dimensionality of one. Similarly, you can look at this, what we call the Swiss
roll example over here. This is a three-dimensional
object and it's been plotted in
three-dimensional space. It says it has an extrinsic
dimensionality of three, but it's intrinsic
dimensionality is lower. Its intrinsic dimensionality is two because of the
reasons I just mentioned. All the points lie on this two-dimensional sheet here that goes around in a spiral. It's two-dimensionally,
intrinsically because takes two coordinates to specify
the location of any point. The location along one axis
here, this curve here. Then the second axis is
perpendicular to the first. So that's intrinsic versus
extrinsic dimensionality. There are three
main ways that you can group dimensionality
reduction methods, three main families of technique. The first way is called
feature elimination, the second way is called
feature selection, and the third way is
called feature extraction. We will look briefly at the first two and
then dive a little more and to depth on the third because that's
where PCA lives. In feature elimination, you
simply take some columns in your original data set and you reduce your number of columns, you reduce the feature space, simply by deleting or
selecting a subset of features from the original set according to some
arbitrary criterion. Sometimes this is useful
as a quick way to explore a specific pairs
or groups of features. Of course, this has the
obvious disadvantage that you'll lose useful
information about any relationships
between features that involve anything you've dropped. You might eliminate features
to explore a data set, but for purposes of analysis, generally eliminating
features can lose a lot of information unless you decide what to eliminate in
a principled way. Features selection is a slightly more
principled approach. It's usually based on a
score that you compute for each column or each feature with some kind of
statistical tests. The test measures how correlated is this feature with
other features, or perhaps you're trying to predict something
and you want to have the correlation of that feature with a gold standard label, but the bottom line
is you can compute some score and then once
you computed a score, feature selection score
for each feature, you can use the score to rank the columns by descending
importance and then you can pick the top k. Let's look at a specific example of a
feature scoring function. One example of a very widely used feature selection metric is information gain. You might recall information
gain as the criterion that sometimes is used to split
nodes of decision trees. It comes up again in clustering
and many other areas. It's incredibly important
measure in machine learning. It's important, I
think, to know it well. We're going to walk through
just a specific example of how you might decide to eliminate or keep some columns
of your data set, depending on how those columns are likely to help you with
a binary prediction task. This is an example of
selecting a subset of columns with a
particular task in mind. Basically, information gain, which I've shown here
as the feature IG, it measures the decrease in the label entropy H when the feature is present
versus when it's absent. A feature that has
good at reducing label entropy produces high information gain
when it's present. Therefore, it's a good feature for discriminating
between two classes. A poor feature, or
in other words, a column that you might
want to eliminate, would have a low
information gain score. If you look at the specific
example on the right, we have a data set with
two features, A and B, then we have a binary
ground truth label, let's say T. In our habit
of naming Python variables, you might call A and B
this part of the table x. The ground truth might
lie in variable y. A good feature is one that
basically splits the labels. The average entropy of the
labeled distributions in the splits is lower than the entropy of the overall
label distribution. In this case, after doing the calculations that
I've shown on the right, feature a has a much higher information
gain, than feature b. Feature a, in this case, has an information
gain score of 0.862, feature b has an information
gain of negative 0.338, and so in this case, when you're deciding
which features to select, you would select feature a and you would get rid of feature
b. I encourage you to work through this example
more specifically on your own to satisfy yourself
that this is true. We can take a quick
look here to see. We can compare, for example, h of t here corresponds to taking the
entropy of all the labels in the set and the conditional
entropy h of t given a, if you look at this
formula up here, that's how you would
compute h of t given a. Basically, what it
does is it walks through each of the
possible values of a, so zero, one, and two. It computes the entropy
of the subset of labels that are selected
when you choose a equals 0, a equals 1, a equals 2. For example, if we were
to pick a equals 0, this value here, you
would select a subset of labels being zero and zero. For a equals 1, you'd select the subset of labels
here, here, and here. So zero, zero, zero and likewise, value of a equals 2, you would
select t equals one here. You can see in this example, it's been setup so that the conditional entropy of
t given a is this quantity, here turns out to be zero. It perfectly splits
the values of t into completely homogeneous values so that the sets are
subsets of t that you get from selecting values of a, turn out to be all
zero or all one. They have an entropy of 0. The information
gain is, as I said, incredibly important and
we'll be using that number of times in the work
that's coming up. Now if you're interested, we're not going to go through
any of these in detail, but there are many other
feature selection metrics like information gain that people have
developed and they have different strengths
and weaknesses. If you're interested
in looking at text classification as an example of where you have thousands
and thousands of features, namely the words in a document. How do you do feature selection? In that case, I recommend
highly this paper by George Forman from HP Labs. In that study he walked through many different possible types of feature selection metrics
that you apply to words in text classification
and so you can see here, he included information gain, and that was one of the
better performance, but you can see there are
many other types here. If you're interested
in more background in how these work, chi-squared is another
fairly well performing one. You can check out this paper. It's a nice accessible survey. As you might expect, some words are more informative than others, if you want to
detect if a document discusses a particular topic. You can see how the other
feature selection methods, in addition, information
gain worked in his study. Regardless of the feature
selection score you use, you can compute it for
each of your features. Select the top k
features as you reduce dimension representation
and throw away the rest. This method has more
statistical justification than just arbitrarily throwing
away eliminating features. It still suffers from
information loss, and it's highly dependent on the choice of scoring function. But some forms of feature
selection tried to do is they try to
go a step beyond applying a score to features individually and they search over the space of all possible
subsets of features. These are sometimes
called wrapper methods, and they include methods like sequential forward
selection, genetic search. They basically repeatedly call a learning algorithm on a
validation set of data as a subroutine to evaluate the performance of various
subsets of features. Subsets of features that end up giving higher
performance would be ones that are
selected and the ones that don't contribute
will be discarded. The disadvantage of this
approach obviously, is that it quickly becomes computationally
difficult or impossible. It's very intensive in high dimensions so people have developed methods to try
to do it more efficiently, but you can't avoid the
high-dimensional cost with lots of features like text or image
problems, for example. I described three main ways to reduce the dimensionality
of your dataset. We looked briefly at
feature elimination, which we can now eliminate. I described feature selection using information gain as an example and now we're going to look at a class of methods that instead of just trying to select a subset of
existing features, actually tries to create new
features from the old ones. Let's take a look
into what that means. Remember, the goal
here is we want to do dimensionality reduction. We want to take our dataset, transform it into
a version that has fewer columns using these
new features as the columns, and these new features
should still be effective at capturing the original variation and structure in the
original dataset. But there should be
minimal redundancy and so we can express
this mathematically as saying that they are linearly independent and there
should be fewer of them, fewer features, than we had
in our original dataset. We can divide these
feature extraction approaches as they're called into linear and non-linear methods. The new features in
linear approaches are just linear transforms
of the old features. For example, you create
new columns that are linear combinations of the original columns
and then there is a class of non-linear
dimensionality reduction where you're allowed to use non-linear transforms of
the original columns. These non-linear methods tend
to be more sophisticated, potentially more
computationally intensive, but also just as we've seen with linear versus non-linear
supervised learning methods, the non-linear dimensionality
reduction methods can often find more complex structure
if it exists than linear methods and we'll look at some examples of that. Here's example of a feature extraction
function that takes a 5x5 integer pixel array as input and returns a single
integer between 0 and 9. It's a digit detector. It takes a grey-scale image
of a digit as input and so our input space or feature
space has dimensionality 25. There are 5x5 integers, 25 integers that make up our description of
the object and so this digit detector
reduces the dimensionality from 25 to one because the
output is a single integer. It certainly captures the
main interesting structure in the original dataset, namely, which digit is in the
array and that it does it with a lot fewer features. Just as [inaudible]
a fun question to think about is whether this particular example
of feature extraction is an example of linear or
non-linear functions. Let's start by looking at a very important and widely used linear dimensionality
reduction technique called principal component
analysis or PCA. There are a couple of ways
to describe how PCA works. An intuitive, more geometric way and then there's
a linear algebra way. What we're going
to do is to start, we're going to look
at the geometric way, the visually intuitive
way and then later, we'll look at the
linear algebra behind PCA as part of understanding a powerful general
dimensionality reduction method called singular value
decomposition or SVD, which is very closely
connected to PCA. Intuitively what PCA does, it takes your
original data points. Here I have a very simple
dataset with two features. It's a two-dimensional
dataset and imagine each instance is denoted by a point here in the
scatterplot and intuitively, what PCA does geometrically to these original data points
is it finds a rotation of the points so that the
dimensions are statistically uncorrelated and
after it does that, each data point can be
described by new coordinates relative to this new frame of reference in these
uncorrelated dimensions. Once we find these new
rotated dimensions, the nature of PCA is such that we can drop all but
the most informative coordinates that capture
most of the variation in the original dataset and
we still end up with a good approximation in the least square sense
to our original dataset. Looking at this
synthetic example, we have two features
that are somewhat highly correlated and when we apply PCA. PCA will find the direction in the data has highest variance. What that means is, if we think
about PCA trying to find, there are different ways
that you can project this data onto a line. Imagine there are all
these different ways. If you pick a different line. You project the data
onto the line and see how wide the data is
on that projection. For example, if we picked a line that went like this and then we projected every one of those
points onto that line. Then we looked at the variance
of the projected data, we would get a certain value. You can see that using this
line and call it candidate A, we get a certain variant that's smaller than if we had
picked a different line, which we'll call candidate B. This particular line,
when we project the points onto the line
and look at the variance, you can see the much
larger variance has been retained from the data. In fact, this line B represents the direction of greatest variants for
this particular cloud. Imagine after
projecting the data, we have this one-dimensional
representation where each point maps to its projected place on the line and if you look
on the right over here, we've just rotated the line, so that it's now horizontal. This gives us a
one-dimensional approximation to our original data set. If we take the 2D data,
the scatter plot, and project all the points under the single one-dimensional line. We get a one-dimensional
approximation that's in more mathematically, it's a good approximation in the least square
sense that minimizes the least squares error in the approximation to the
original data Cloud, which I've denoted with
these gray points. It's a one-dimensional
approximation because, we've eliminated one coordinate
from these data points. You can now just specify
a single coordinate on this new feature
that we'll call, feature 3, think of this as a
new column that you can us. By specifying the
value for feature 3, you can identify a data point in the original
two-dimensional set. One result of applying
PCA is we know the best one-dimensional
approximation to our original
two-dimensional data. In fact we've compressed the data from
two-dimensions to one, of course, we lose information
in this approximation. We've lost some information about the original second coordinate but depending on the application, that may be okay. We've certainly learned
something about the data by applying PCA, we found this direction
of greatest variation. That first direction of greatest variants that PCA finds is called the first
principal component. That's the long direction
of this cloud of points. PCA will proceed iteratively. It will first find in the
data the direction of greatest variance
and it will find this first principle component. Then it'll continue, it'll
look for the direction at right angles that maximally captures the remaining variance. In two-dimensions,
there's only one possible such direction at a right angle to the
first principle component but in higher dimensions
there are infinitely many. In higher dimensions PCA would find that second
principal component, the next greatest variance. Now, just to note that there could be two
equivalent solutions for principal component based on either a positive
or negative sign, you could have a vector that
was oriented this way or you could have a vector that
was oriented the other way. Which of those two
directions is produced by PCA can be arbitrary. In any case, one result of applying PCA is that we now know the best one-dimensional
approximation to the original
two-dimensional data. We can take any data point that used two features before in x and y and approximate it
using just one feature, namely its location when projected onto the first
principal component. Let's take a more
detailed look at the linear algebra
formulation of what PCA does. We assume our datasets is in a matrix X with the instances or individuals in n
rows and p columns. Where each column represents
one of the p observed variables that we often call
features or attributes. Remember, our goal is to reduce the dimensionality of X by
replacing the p features in the columns of X with
a new smaller set of q features that capture most of the interesting
variation in X. We're going to compute
those q columns with the original p columns. Also we're going to focus on linear dimensionality
reduction. These new q features
will be restricted to be linear functions
of the old features, and the linear function
of the old features is just a linear combination
of the columns of x. We can write any
linear combination of the columns of x as the sum from j equals
one to p of a_j, x_j, where x_j is the j column and a_j
is a real number that represents the weight or loading that that column has
in the linear combination. We'll denote the vector
of these loadings, a_1, a_2 up to a_p as a, [inaudible] that a
here and also here. The result of computing a linear combination
of the columns of x using the weights a_1, a_2, and so let me just clarify that a_1 would represent a real number corresponding to this column, a_2 would represent a number corresponding to this column, the weight on that column, and so forth up to a_p. The vector a has p
real values in it. The result of computing
a linear combination of the columns of x using
these weights, a_1, a_2 through a_p is
by computing x_a, which is a single
vector of n numbers, one for each individual
instance in the rows of x. You can think of this column as a new feature computed as a linear combination
of the old features. Just to clarify, if I choose a particular set of values
for a_1 through a_p, that means I'm choosing a
particular set of weights in the linear combination and the result is going
to be a new column, a new feature x_a, and there's going to be one entry in that new column
for every data point. By choosing different
combinations, different values for
a_1 through a_p, let's suppose we had a
second vector, a prime, that had a set of other values, a_1 prime up to a_p prime, that would be a different linear combination
of the columns of x, and that would result
in a new feature, essentially a new column, that also had n values, one per data instance. By choosing different linear combinations of the columns of x, we end up with
different new features. There are lots of
ways that we could pick different values for these weights and
each different choice would result in a
different new feature. What would be a desirable
property of this new feature? If this new feature were the only feature we
are allowed to use; we were told we can choose
exactly one value for a, one particular
linear combination, which one should we pick? For example, if we were reducing
the dimensionality of x from p columns to just one new
column using this feature, one good criterion would
be that we would pick the new feature whose values
have the largest variance. To see why this is
another way to think of the vector x_a is exactly as a projection that
projects all the points in xs rows onto the vector a. Just like the simple ellipse
example that we saw, except now we're in
higher dimensions. We want to find the vector a that results in the widest
possible shadow of x, when you look at all the
shadows of the points in x that were projected
onto the vector, again, we'll just use
a here as our example. We have all the points of x, when we take x_a, can think of that
as a projection of all the points of our
dataset onto this vector a, and we get a series of results, real numbers that denote the
position on the vector a. We want that to have the
greatest variance possible, just like in the ellipse example. Just like in the ellipse example, you could imagine doing
this a number of times. You'd find vector a_1 that
maximized the shadow of x, and then the vector a_2 that gave the second wider
to shadow and so on. Suppose we chose these vectors a according to this
maximization criterion? Suppose we did that
three times and we got a vector a, another vector, a prime, that's a
different linear combination of xs columns, and then we had a third vector, a double prime, which was another linear combination
with different values. In this scheme, each
data point in x now has to coordinate
representations. The first one is simply X_1, X_2, you can think of it as
it's original coordinates in the original data space, or we can take this vector and project it
onto each of these a's. [inaudible] We can
project it onto a, we can project it onto a, prime and we can project
it onto a, double prime. For each of these projections, we get a real number that
indicates the position of the point in X where it projects onto that
particular vector. If we project X onto
each of the three, a, a, prime and a, double prime
vectors that we have, we will get three coordinates. Essentially we've transformed X into a new coordinate space, we'll call X hat, where the first
coordinate is X_a, the second coordinate X_a prime and the third coordinate
is X_a double prime. The act of choosing these
a's effectively projects the original data
point into a new space whose coordinates
can be determined according to these choice
of the a, vectors. We agree that finding a vector a, of loadings that
maximizes the variance of X_a will produce a
good new feature. It's a statistical
fact that if we have a covariance matrix S
for the p variables, we can write the variance
of X_a as a, transpose S_a. The covariance matrix
S is a symmetric p by p matrix of real values
because by defining it, it contains the covariances of every possible pair of variables, namely every possible
pair of the p columns. For this problem to have a
well defined maximum solution, we need to constrain
the vector a, somehow otherwise the variance could be made arbitrarily large. The most common way
to constrain a, is to say it must
have length one, also known as a unit norm vector. Algebraically we express
this as requiring that a, transpose a, be equal to one. Now, we have our
optimization problem to find us the optimal vector a, the optimal linear combination
of the original columns of X that will get us the
optimal new feature. We want to maximize
the variance of X_a that is equal to a, transpose S_a and it's
subject to the fact that a, has to be a vector of unit norm. We can add this requirement about the unit norm to
our optimization by adding this penalty term
minus lambda times a, transpose a, minus one, we add that to the objective. If a's length is
greater than one, the penalty term will
add a negative number to the variance and thus
penalize that possibility. Our problem finally is
to find a vector a, that maximizes this equation one. Technically, lambda is
called Lagrange multiplier. We can use some basic calculus
using matrix notation. If we take the derivative of equation one
with respect to a, we get the condition
in equation two, S_a minus lambda a, equals zero or equivalently
S_a equals lambda a. What this tells us is that to maximize the variance
of the new feature X_a, we should pick a, so that
S_a equals lambda a, for some constant lambda. But this is exactly
the definition of an eigenvalue and
eigen-vector of X_a. The largest eigen-vector
lambda one, and corresponding eigenvalue a_1 give the largest
variance since a, transpose S_a equals
lambda times a, transpose a, equals lambda. Because remember a, transpose a, is our condition that the
vector have a unit norm, and so that will be
our new feature. By the way, note that equation
two is still valid if we multiply the eigenvectors
by negative one. The signs of all loadings
are arbitrary and only their relative
magnitudes are meaningful. Basically, principal
components analysis finds the series of vectors, which I have called
a, a, prime, a, double prime that will produce
the best new features. I'm going to make
a slight change in notational convention here and instead of calling
these different a's, a, a prime, a, double prime,
I'm going to call them. Principal components
analysis finds this progressive series
of eigenvectors a_1, a_2 up to a_p that
satisfy equation 2. These eigenvectors
are also orthogonal, and thus the resulting columns, which will be Xa_1, Xa_2, and so forth. Those are going to
be the new features. They're going to be
uncorrelated with each other because of
that orthogonality. We'll go into a bit more detail about why this is important, and the nature of
PCA solutions in a separate video on singular
value decomposition. We have the solutions to
this optimization problem. I'm going to call the
first eigenvector a_1 with its corresponding
eigenvalue Lambda_1. I'm going to call
the second solution with second largest variance a_2, and its corresponding eigenvalue, Lambda_2, and so forth. Remember that each of these represents a vector
of real numbers, which are column weights. We're using those column
weights to compute a linear combination of
the original columns of X. Generalizing this equation, we'll have Xa_1, Xa_2 as the
values for the new features. As I said before, you
can think of this like a projection:
projecting X onto a_1, protecting X onto
a_2 to get the value of each new feature for
each data instance. The vectors Xa_1, Xa_2, they are the linear
combinations of the original columns are in fact called the principal
components of the dataset. This Xa_1 is a
principal component 1, and the second one is
principle component 2. It's a bit confusing because sometimes people use the term principal components just for the eigenvectors a_1 and a_2. The weights in the
linear combination are the values in a_1, a_2, and so forth. In standard PCA terminology, we call the eigenvector a_1, the PC loading, so a_1
is called the loading. We call the elements in the
vector that results from the linear combination
Xa_1, Xa_2. We call those the PC scores. The values of the
new features for each instance are
called the scores, and the weightings in eigenvectors themselves
are called the loadings. I'm taking a little
bit of extra time with this just to make sure that the basics of what's
going on with PCA are clear. It's common to define
principal components as the linear combinations for the column centered version of X, which here we'll denote X star. The elements of X star
are thus X star_ij. That is simply taking the original entries
in the matrix X, X_ij, and subtracting from them the mean of all the
values in each column. That's why it's called
column centered. X_mean_j is the mean of all
the values in the j's column, and centering X in this way, it's just a shift
in geometric space. It's not going to change
the PCA solution, but it allows us
to connect PCA to a powerful technique called singular value
decomposition or SVD. That's because quantity n-1 times S is equal to X star
transpose times X star, where S is the covariance matrix. The singular value decomposition
of the matrix X star, it turns out that that finds the eigen decomposition of
X star transpose X star. In other words, we can solve our original problem of
finding the eigenvectors and eigenvalues of S by computing the singular value decomposition of the centered matrix X star. We'll look at the details for
SVD in a separate lecture. Finally, if you recall
the ellipse example, where we decide to compress our dataset by only
keeping the projections of our original 2D points onto
the 1D principal component, you'll see that if you only keep the first K principal components, and throw away the rest, you'll end up with an
approximate version of your original dataset. It's optimal in the
least square sense. The sum of squared
distances between the new approximate points and the corresponding old
points is minimize. Now we're going to look
an example of using scikit-learn to apply PCA to a higher-dimensional dataset, the Wisconsin Breast
Cancer dataset. As a reminder, this
dataset consists of instances that
represent measurements taken of cancer cells. The complete list of
features is on the left. You can see there's
some features that concerns a cell shape, and others related to other cell attributes like size and texture. In the original study, the researchers developed an
imaging tool to help them extract these features with
the aid of a human expert. As researchers, we might
be curious if there are groups of cells with
similar attributes. We're especially interested
to see if there are groups of cell features that are strongly associated with other malignant, or cancerous cells, or
benign normal cells. Let's take a look at some code from this week's
notebook to see how PCA can be applied
to this dataset. To perform PCA, our
first step is to import the PCA class from
SKlearn decomposition. Our next step is to
make sure the data is properly prepared before
we use PCA on it. To prepare your data for PCA, it's usually a good idea to
first transform the dataset. Each features range of values has zero mean and unit variance. We can do this here using
the fit and transform methods of the standard
scaler class as shown here. By default, in scikit-learn, PCA will automatically do
column centering of your data, but it won't do the
variance standardization. We recommend this
standard scalar step. The one time where
variants standardization might not be advisable is if you have a dataset
where the units of all the variables are the same. In that situation, doing the
variants at normalization may result in shrinkage of features containing
strong signals, and inflation of features
with much less signal. But otherwise,
scaling the data in this way is highly recommended. Notice here, since
we're just exploring the data and not doing
supervised learning, we're evaluating a model
against a test set. We're not splitting our dataset into training and test sets. But here's a really
important reminder. If you are using PCA
to find new features, to avoid data leakage, always apply it after you
do the train test split. This applies to any type of normalization, scaling,
topic modeling, or other preprocessing that
makes use of properties, or statistics across
the entire dataset. You always split first
and then process later. We'll revisit this rule multiple times in the future applications. After scaling the data, we create the PCA object. We specify that we want to retain just the first two
principal components to reduce the dimensionality
to just two columns. Then we call the fit method, using the normalized data. This will set up PCA so that it learns the right
rotation at the dataset. We can then apply this
properly prepared PCA object to project all the points in our input dataset to this
new two-dimensional space. You see that if we
take the shape of the array that's
returned from PCA, it's transformed our
original dataset that had 30 columns
corresponding to the 30 features into a new array that has
just two columns, essentially expressing each original data
point in terms of two new features that
represent the position of the data point in this new
two-dimensional PCA space. We can then just create
a scatterplot using these two new features to see how the data forms clusters. That's what's being shown here. In this example, we've
used the dataset that also has labels for
supervised learning, namely, the malignant
and benign labels on cancer cells that were
provided by human experts. This helps us see how well PCA serves to find
clusters in the data. Here's the result of plotting all the 30 featured data samples, but using the two new
features computed with PCA. The first feature is the
first principle component, along the x axis. The second feature is the
second principal component, along the y axis. We can see that malignant
and benign cells do indeed tend to cluster
into two groups, based on the principle
components found by PCA. In fact, we can now
apply linear classifier, so this two dimensional
representation of the original dataset, and we can see that it would
likely do fairly well. This illustrates another use of dimensionality reduction
methods like PCA to find informative features
that could then be used in a later supervised
learning stage. We'll continue to explore this
idea later in the course. To get insight into the nature of the principle components
that PCA found, we can create a heatmap, visualizing the first two
principal components of the breast cancer dataset
to get an idea of what feature groupings each
component is associated with. Note that we can get
the arrays representing the two principal component axes that defined the PCA space using the pca.components
underscore attribute that's filled in after the PCA fit method
is used on the data. We can see that the first
component is all positive, showing a general correlation
between all 30 features. In other words, they tend to
vary up and down together. The second principle
component has a mixture of positive
and negative values. In particular, we can see a cluster of negatively
signed features that co-vary together and in the opposite direction of
the remaining features. For example, here. Looking at the names, it
would make sense that this subset would
co-vary together. We see the pair mean texture, there's another cluster
over here, worst texture, a pair mean radius and
worst radius, and so on, these are all related to the shape or texture
of the cancer cells. You can check out the
code that generate this example in the
notebook for this week. We can get further inside into the relationship
between data sets variables and its
principal components by plotting what's
known as a Biplot. Because there are multiple
useful aspects to Biplot, and it's just a useful tool by itself for understanding PCA. I've created a separate video that you can explore
Biplots with. But ensure if your data are well approximated by the first
two principal components, a Biplot is a very handy
visualization that shows you both the projected
data, in other words, this scatter plot, you can see all the original data points
in this new PCA space. It also shows you
individual variables in the form of these vectors. By looking at the
angle between vectors, you can see how highly
correlated they are. You can also get lots
of other insights from Biplots and that's
explained in the separate video. Another important
tool to check how well PCA captures
the variation in your data set is to look
at the magnitude of the eigenvalues that are associated with each
principal component. The eigenvalue can
be thought of as a weight associated with
the principal component that reflects the proportion of total variation that
component captures. You can plot the eigenvalues for each principal component
as a plot like this, it's called a scree plot. A scree plot shows you
whether the first two or three principle
components captured most of the variation
in the data. If they do, the scree plot
will be steeply dropping, in the first few principle
components like it does here. Then typically,
it'll be followed by a brief bend in the elbow and then a long flatter section with small eigenvalues that
are less than one, all these eigenvalues
here are less than one. The point at which this elbow occurs shows you the
cutting off point. This is typically where a line at 45 degrees to the left
is tangent to the curve. Starting with the first
principle component to the right of this point, you can throw away all the remaining principal
components to the right. In that case that would be
these and all the rest. You can throw those
away without losing too much information in the data. If your scree plot
happens to have a long, flat shape where the elbow is more difficult to determine. One rule for selecting the most informative
principal components, it's called the Kaiser rule, is to simply take all
the principle components whose eigenvalue is at least 1.0, in which case that would be approximately eigenvalues
one through six here. In a statistical sense, the standard measure
of quality of a given principle component is the proportion of total variance that it accounts for in the data. This is easily available
using scikit-learns explained_variants_ratio_output
variable. This one right here.
With these ratios, you can create what's
called a proportion of variants plot to select the top k principal
components that capture at least 80 percent of the
variation in the data. In this example, the first
four components account for 79 percent of the total variance. The first five components account for about 85 percent
of the total variance. In this case, the first four
or five components might be good enough to give you an accurate approximation
of your whole data set. If you find that your data has a lot more than two or
three principal components, you might want to look
at alternatives to PCA, such as Kernel PCA that we
describe in a separate video.