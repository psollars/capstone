In this module, we'll explore
Hierarchical Clustering. Hierarchical Clustering
produces a set of nested clusters that are organized as a hierarchical tree, like shown on the left here. A tree is called a dendrogram and it records the
sequences of merges or splits that happen as you build up the hierarchy
of clusters. I'll go into more detail about those processes and this
tree structure shortly. For now, it's just important
to understand that hierarchical clustering
operates typically from the bottom up. So on the right, I'm showing
a set of six points 1, 2, 3, 4, 5, 6 along with the hierarchy of clusters that are
generated as you start with each point in its own
cluster and successively merge the closest nearby
groups into new clusters, and we'll talk
about the processes for doing that as well. Some of the strengths of hierarchical clustering
include the fact that you don't have to assume any particular
number of clusters. Once you build the
tree from the data, you can obtain any desired
number of clusters by cutting the tree
at the correct level, so what the dendrogram shows on the x axis is the set of original six points
and what the y axis shows is the distance apart that the two points
where that converged, so points one and three
were at a distance of 0.05 and the decision was made to merge those two together because they were the closest two points, the next closest to points
where at a distance of about 0.07 let's say, so they are merged
about that distance. The next group of
clusters that got merged were at this level and that merged item four with this cluster two-five
and that merge happened, the dendrogram shows you it happened at a distance
of about 0.17, we'll go into more detail about that step-by-step
process in a minute, but the point is
that you can pick the level which you are
satisfied that two groups of things should be similar or dissimilar and wherever
you cut the tree, you'll end up with a clustering that results from cutting it
at that particular level. So if we cut at this level, the clusters would be
these individual points, these two points here as a group and those
two points as a group. Typically we use hierarchical
clustering in situations where the underlying data supports a hierarchical
structure, the example I always use is biological sciences, because
genetic information, especially in an
evolutionary process, is very much a situation where things have
hierarchical relationships. So looking at relationships
between species and the different types of strains of a
particular bacterium, genetic material in
the bacterium can be a clue as to how the bacterium
evolved and so forth. There are two types of
hierarchical clustering, you can start at the bottom
with each point being in its own cluster and
then at each step, you merge the most
similar pair of clusters based on the cost
function or distance, you continue until
you have K clusters or else everything is
in one big cluster and that's called
agglomerative clustering because you're accumulating bigger and bigger
groups of points into clusters in this
hierarchical fashion, bottom-up, the other
approach is to do a divisive clustering
which is top-down. So you start with
all the points in a single big cluster
and then at each step, you decide how to
split the cluster into two smaller clusters
based on a cost function. Then you continue recursively
splitting each of the smaller clusters and so forth until you
have K clusters, or each point is in
its own cluster. Here's an agglomerative
clustering example that's bottom-up. So at stage 1, each point is in its own cluster. At stage 2, the
closest two points are merged into a new cluster. Stage 3, the next two
nearest things are merged, and these happens to points B and E. So they're the
closest two things and they get merged
into a new cluster. At the next stage,
item D and the cluster C-F are considered the two closest things in
this scenario now. So D gets merged with C and F into this bigger
cluster of three items. Now the rules that determine
whether two clusters get merged and what criteria is used will be something that
I'll discuss in a minute. There are multiple
different ways that you can specify criterion for
when you merge clusters. Finally, at stage five, you can see that point
A and points B and E, the group formed by B and
E was merged with point A to form this larger
three-point cluster. So finally, at stage five, we end up with two
top-level clusters. So we'll talk about how in
agglomerative clustering, the algorithm determines the most similar cluster
by specifying one of several possible what are
called linkage criteria. Alright, let's walk through
the stages of bottom-up agglomerative clustering
in a little more detail with a specific example. To start with, we've got a set of individual
points over here. Then we're going to maintain a proximity matrix
of point-to-point or I should say object
to object distances because eventually some
of these are going to combine into clusters. Eventually we'll be
tracking cluster to cluster distances in
this proximity matrix. We're going to maintain as
the algorithm progresses. Basic algorithm in step one, compute the proximity matrix. We start off with each point being its own
single-point cluster. Then we repeat this
loop which first merges the two closest
clusters into a new cluster. Then we update the
proximity matrix based on the linkage criterion, that tells us how far apart
two objects should be. We continue this,
we continue merging the two closest clusters until only a single top-level
cluster remains. You can see that a
key operation in this bottom-up
agglomerative clustering is computation of the
proximity of two clusters, and this is sometimes
called the cost function. There are different
approaches to defining the distance between clusters and these differences in the
cost function distinguish the different flavors of hierarchical clustering
that we can use. I'm not going to fill in the proximity matrix in
detail with distances, but we can see that
in an early stage, if we look at these, we can obviously see that some of these pairs
are close together. In an early stage, a number of these
close pairs would get merged into new
second-level clusters. That's denoted on the tree, the corresponding tree,
these individual points, this merging of the
tree like this, indicates that these
two clusters got merged together into
a single new cluster. Anytime you see two points
joining, like this, that means a new cluster
has been created out of whatever it was in
these sub branches. In this case, on the right, if we look over here, this new cluster was formed by a single point being combined with a cluster of two points. As we progress, you'll note that instead of tracking
individual points, now we're tracking clusters and their proximity
to each other. As this is progressing, we're tracking, for example, the distance
criterion between C2, this cluster, and
all the other ones. C2 to C1, C2 to C3, C2 to C4. Imagine from what we have some proximity function that tells us the cost or the distance of how far apart are or how
dissimilar these things are? That gets tracked in
this proximity matrix. For C2, for example, we will go about
measuring the distance to all the other ones and fill those numbers into
the proximity matrix. The question is now, what is the cost function
that tells us how good or bad to particular choices of cluster would be to merge
or not merge together. What's the cost function
that would tell us the desirability of merging or not merging two things
together and that which would allow us to fill in
this proximity matrix. This comes down to a
question of how can you define intercluster
similarity? Well, here are five
different potential ways to measure inter-cluster
similarity, a.k.a cost functions for bottom-up
agglomerative clustering. First criterion that you
could potentially use, is among all the different
points in these clusters. If you look at all the
distances pairwise between the points
and two clusters, what's the minimum distance? What are the closest two points, where you take one
from each cluster? What are the closest two points? Well, and you use that
minimum distance, whatever it is, as the cost function as the minimum distance
between the clusters. Alternatively, you could
use the maximum distance. You can say among all
the pairs of points, where one point is in the first cluster and the
other point in a second. Which pair of points has
the maximum distance? What is the maximum distance? You use that maximum as the distance measure
between two clusters. Of course, you could do
something in the middle and say, what's the average distance? If you took every
possible pair of points where one point
is in one cluster, the other point is in
the other cluster, you measure the distance, and then you did that
for all possible pairs of points and took the average, that's called the average
linkage criterion. That's another way
you could measure the distance between these two
clusters by their average. Instead of averaging across
all these pairs of points, you could first take the average, namely the centroid of all
the points in one cluster. You could take the
centroid, the average, the mean of all the points
in the second cluster, then you can see how far apart those centroids
were and use that distance as your measure
of inter-cluster distance. Finally, you have something
called Ward's method, which compares the total sum of squares that you obtain by keeping the two
clusters separate. By the sum of squares,
you take the sum of the squared distances of every point in one
cluster to the centroid. You sum up all those
squared distances, and you sum that with the squared distances and the other. You look at the scenario where the clusters remain separate. What's the total sum of
squares distances to those separate centroids and you compare that to what the
total sum squared distances would be if these two
clusters were merged. If the two clusters were merged, the new centroid for the
merged cluster would be here. You could then compute the sum of squared distances
from every point in the merged cluster
to the new centroid. Ward's method essentially says, what's the sum of
squared distances if the clusters are separate? What's the sum of
squared distances, if the clusters were
merged together? That difference gives you
this Ward's distance, that's also a measure of cost or inter-cluster similarity. I mentioned the word
dendrograms before. When you do agglomerative
clustering, it automatically
arranges the data into a hierarchy as an effect
of the algorithm. The hierarchy reflects the order, and the cluster distance at which each data point is assigned
to successive clusters. It's very useful to
be able to visualize this hierarchy using
these dendrograms. These can be used even with
high-dimensional data, maybe especially with
high-dimensional data, they're incredibly useful. One of the nice things about agglomerative clustering is that you get this nice visualization. Here's the dendrogram
corresponding to the Ward's method clustering of the previous
sample data set example. The data points are
at the bottom and they have numbers,
I've numbered them. The data points are at the bottom and they're labeled with letters. Y axis represents the
cluster distance, namely the distance that two clusters are apart
in the data space. The data points form the leaves of the
tree at the bottom, a new node parent in the tree is added as each pair of
successive clusters is merged. The height of the node parent
captures how far apart the two clusters
were when they're merged with the
branch that goes up. For example here, representing
the new merged cluster. Then these two are merged
in the dendrogram. The new parent is here, point D is merged
into that cluster. The new parent is represented
by that line that goes up. Note that you can tell how
far apart the merged clusters are by the length of
each branch in the tree. This property of a dendrogram can help us figure out the
right number of clusters. Because in general, we
want clusters that have highly similar items
within each cluster, but that are far apart
from other clusters. For example, we can see that
going from three clusters to two happens at a high y-value. Right now we started
off with six clusters, but by the time we've
merged and we're left with three clusters here. Namely one, here's two, here's the third
one, I'm going by these because these
are the parents here. By the time these
clusters are merged, it happens at a
distance 0.25 roughly. By looking at that height, we can see how similar the two subgroups were when
merged from three to two. That means that the clusters are merged at a distance that
was fairly significant, meaning they were
fairly different. Let's walk through a
specific example using the minimum distance criterion or the single-linkage criterion, that's the one that picks
the points that are closest in the clusters as they distance
between clusters. In single linkage clustering, we start out with
these six points, and what's the first
pair that gets merged? Well, it'll be these two
because they're the closest, this is indicated
in the dendrogram, by the fact that
these two are merged together in the dendrogram and we can see that
there are merged at a distance just greater than 0.1, which means the distance
between these two is simply 0.1 because they're
the only points in each of their
clusters at that point. Then the next step, the next two points to be merged
are five and two, because distance wise they're
the next closest so they are at a distance of
just less than 0.15, so they're merged
into cluster two and that's indicated
in the dendrogram by this part of the tree. Now the dendrogram is showing
us if we cut just under 0.15 we'll have one, two, three, four clusters
which we can see they do, we've got the two
individual points that are their own clusters, as well as the two ones
we've just created. So again, by cutting the tree at a given distance
and looking to see which parents are getting
cut at that point, you can count the number of
clusters you're going to get. Okay, so let's continue, what's the next thing
that's going to happen? Well, we could merge 0.1 into
this cluster here perhaps, could merge this
into this cluster. Remember, we're going to use the single linkage criterion, which says that we measure the distance between
clusters according to the smallest pairwise distance
between cluster entries. The two closest points between these clusters
are two and three, and that's at that distance, so looking two and
three are closer together than any
other remaining pair, for example, it's
a bit longer to go from four to three than
it is from two to three. These two clusters based on the single linkage criterion are based on the
minimum distance, are this far apart
because these are the two closest elements, so these are going to
get merged together, and that's indicated
in the dendrogram by the fact that my previous
event that merges three, six and two, five, and then soon after you
can see that the distance from four to that cluster was pretty close call
between these clusters or whether 0.4 was closer it's pretty close because the
distance was quite similar, and that's reflected in the tree. You can see that very
soon after cluster three is created with these
two sub-clusters, 0.4 here follows soon afterwards, almost the same distance
just a little bit farther, and so the next get merged in is 0.4 at a very similar
distance just a little bit more and
that happens at this merge event right here, and so on, so then
we eventually get merged with one because
that's the farthest away. The final clustering looks
something like this, so by the time we get
to the very top of the tree we have everything
in one big cluster. But it's nice because
we have a record of all the different merges that happened on the way to
creating this one big cluster, and that record is very useful to cutting the tree at a given inter-cluster
merge distance, so that's what happens when
you merge these points with single-linkage clustering
as the cost function. Okay, let's look for a
moment at the strengths and weaknesses of some
of these methods. In the case of single
linkage clustering, the minimum distance criterion, similarity of clusters under this MIN similarity
criteria is determined by the most similar members. It's a local criterion that
only pays attention to the area where two clusters
come closest to each other. More distant parts
of the cluster and the cluster's overall
structure are ignored. That means the
merge criterion for clusters under the min
similarity criterion is a local one. In that case, a chain of points can just continue
to be extended into a lengthy shape without
considering what the shape of the overall cluster
is that it's forming. This is called the
chaining effect. min similarity, single
linkage clustering is good for finding clusters
that are long and more complex and non elliptical. That's good as long as it's
a match for what's actually happening in the underlying data. You can see it in this
example here I've chosen a set of points that it has a long non elliptical structure. In fact, there are two
separate clusters here. Because of this local
merge criterion, where only the members that are closest to each other
are considered. As cluster is grown, it will grow towards the direction where these
nearest points are. It will quickly do this
elongated growth pattern, which is what is
characteristic of linkage. The same thing will
happen simultaneously in this cluster over here, it'll grow and along
this direction, for example, and so you'll end up with clusters that
are the correct, long, more complex shape. That's not a match for
the underlying data, however, that's a problem, and min similarity, single linkage clustering is probably not the right solution. Both single linkage and complete
linkage criteria reduce the clustering decision
to a single pair of points across two clusters. In some cases, single-link
clustering min similarity can also run into problems
with noise and outliers. If that noise occurs in such a way that it causes
this chaining effect and causes things to grow in directions that
aren't appropriate. The flip side to the benefits of single
linkage clustering, if the true clusters are
long and non elliptical, is that if the true clusters
are more compact and more globular and they have
a certain amount of noise, then this chaining effect that happens when you grow clusters according to the min
similarity criterion, can cause clusters that are supposed to be separated that
may have a little bit of noise in between this chaining
effect where it grows the clusters along directions where it can creep along locally, may turn out to erroneously merge clusters
that shouldn't be merged. Let's look at the
difference if we apply the complete
linkage cost function. The max distance
between clusters, being the maximum distance between any pair of points
in the two clusters. Again, we'll start by
walking through bottom up. We start off with every
point in its own cluster, and because each point
is in its own cluster, the max distance between any two is simply the same as the distance
between the points. At first 3 and 6 are going to get merged
again into cluster number 1. Now, what's the next
thing to get merged? The next thing to get
merged, so if you look at all the pairwise distances, so now the distance between this one-point cluster and
the new cluster 1 is going to be the maximum of any of the pairs
distances so it's going to be this distance here that looks to me like it's going
to be the longest distance. We can compare that, for example, with the distance from 2 to 5. The distance from 2
to 5 is definitely shorter than the distance
from 4 to this new cluster. Points 2 and 5 are going to be the next ones to get
merged together. Right now things
are going to start to get a little more interesting. Now we're going to look
at the maximum distance between this cluster point
and these cluster points, and here we're using the max
this time so the distance between these two clusters is calculated by this distance. The max from 4 to this cluster 2 is going to be the max distance between any points in two. It's going to be in this
case point 5 and of course, point 1 is really far away. Using the max criterion, cluster 1 is the closest so that's what's
going to happen next. It's going to merge point 4 with cluster 1 to
get a cluster 3. Here we are in the dendrogram, and that happened
at a distance of just between 0.2 and 0.25. As we continue, let's see
what will be the next thing. Now we have the max
distance so that the distance between
the clusters will be represented by this
distance here, the max. What's the max between
0.1 and this cluster? It's going to be
the farthest away. You can see 0.1 is going to
be closer to this cluster, so that's what's going
get merged next. Now let's look at
the strengths and weaknesses of the max
similarity criteria, the complete linkage clustering. Complete linkage clustering,
similarity of two clusters, is the similarity of their
most dissimilar members. This is the same as
picking the cluster pair whose merge has the
smallest diameter. The result in complete
linkage clustering is a preference for
compact clusters with small diameters over long, thin clusters like we saw with the single
linkage criterion. Unlike that single
link criterion, complete linkage is a
non-local criterion, the whole structure
of the clustering can affect how a merge is decided. The strength of complete
linkage clustering when you use this max criterion is
that it can be quite good at identifying
clusters that are globular, compact, even in the
presence of a bit of noise. However, the max complete
linkage criterion can also pay too much
attention to outliers. A single point far
from the center can increase the diameter of a
candidate merge significantly, and so it can dramatically
affect the final clustering. Complete linkage clustering can be quite sensitive to outliers. In addition, it may act, as shown in this case, to break up large clusters. What will show in
this case, we run complete linkage clustering
with this large cluster, it ends up actually incorrectly clustering these points in the large cluster with the rest of the points
in the other cluster that's supposed to be separate. So it's not good with
large clusters either. Finally, we're just
going to speed through the group average, hierarchical clustering, which takes the average distances. If we compute the average
distance at each step, three and six get merged again, five and two get merged again, four gets merged into cluster 1. Then one difference
is that 0.1 here gets merged into cluster 3 a bit sooner before everything gets
put into its own cluster. A group average clustering
is a compromise between using the min
and using the max. In other words, between
single and complete link. So in that sense, it's less susceptible to
noise and outliers, but it's also biased more towards globular clusters and
doesn't work so well if you have unusually
shaped clusters with very long or complex shapes. Ward's method is the
one I mentioned before, where we look at the
clusters that are separate, we compute the total within
cluster sum of squares, then we pretend that
we've merged them, and we compute the total
within-cluster sum of squares, pretending that
they'd been merged, and we look at the difference
of those two quantities. If we look at the
total within-cluster sum of squares for the
two clusters separately, suppose we're looking at
merging clusters C_i and C_j, for all the points
in cluster C_i, we compute distance
to the centroid. So R_i here is the
centroid of C_i, R_j is the centroid of C_j. Essentially, these
sum of squares within cluster distances measure how spread-out the cluster
is or how compact it is. Here is the sum of squares, pretending that we've merged those two clusters into a
single cluster C, i, j. Then we need to calculate
the centroid of this potentially merged
pair of clusters. You can see that it's looking at the difference between this quantity and that quantity. Let's think about what
this means mathematically. How can we interpreted it? If a cluster is really compact, then the sum of
squares distances of all the points in that cluster to the centroid will be small. If that's true for both clusters, we'll have to values
that are small. If we merge clusters
that are each tightly compact and far apart, then this sum of squares
difference is going to be much larger than
these two small sums. That indicates that it's not
good to merge the clusters. This quantity will be bigger. This quantity on the
right will be bigger than this quantity on the left. So you'd end up with a value of this
function that's small, or I guess potentially
even negative. What you want is, you want
this function to be large, you want this part to be large, and you want the
resulting merged cluster to have a small value. Anytime the merge will improve the compactness on average of the cluster,
that's a good thing. That's what Ward's method does. It's similar to group average
and centroid distance, but because it's aggregating these distances across
multiple points, it's less susceptible
to noise and outliers, it's also biased towards
well-behaved globular clusters. We'll see when we look
at k-means clustering, you can see that it's like a hierarchical analog of k-means. In fact, it can be used to
initialize k-means clustering. We'll talk about that when we get to k-means clustering
in another module. Here is a comparison of the four cluster similarity
criteria that we looked at. We looked at MIN distance
or single-linkage. We looked at MAX distance
or complete linkage, we looked at the group average and we looked at Ward's method, and if we do these four, we can track to see where
they're are different. All four methods merged
the two closest points, they also do the same for
the next two closest points, then they'll start to distinguish themselves
a little bit more. So Ward's method will merge 0.4 as will
group average and max, but the main criterion
as we saw before, will prefer to merge those two clusters based on
the min distance criterion, because they're
closer together under that criterion, than 0.4 is. We can continue, and this is the final clustering
that occurs. To perform agglomerative
clustering and scikit-learn, you import the agglomerative
clustering class from sklearn.cluster. When you initialize the
agglomerative clustering object, you specify the n
clusters parameter that causes the algorithm to stop when it has that number of
clusters remaining. Then you call the fit_predict
method on the data, and that will return the set of cluster assignments
for the data points. Scikit-learn doesn't
provide the ability to plot dendrograms, but SciPy does. SciPy handles clustering a bit differently than scikit-learn,
but here's an example. We first import the
dendrogram and Ward functions from the SciPy cluster
hierarchy module. The word function
returns an array, specifies distances spanned during the
agglomerative clustering, and we'll look at that format
of that array in a minute. Now that linkage
array can then be passed to the dendrogram
function to plot the tree. So it's very easy to simply take the original data,
call Ward's method, pass that output as the input to dendrogram to look at the tree. In particular, when you
call that word function, and you look at what's in
this resulting output, you'll get a list of
lists,that looks like this. What this does, is it describes the successive merges that happened to build the hierarchy. The first two columns
are node IDs. Now, let's suppose you
have n leaf nodes, n initial points, the
nodes 0 through n minus one are going to be
the IDs of those leaf nodes. They're not listed
in this output, the output begins with the
first merge of two leaves. What the output is
describing is it's describing each successive merge. In the first line, the
first two columns are the child nodes that got merged, the distance at which
they were merged, and then the total number
of leaf nodes that exist, number of points, in other words, that are in that cluster. You can see that indeed
it merged points 7 and 8 at a distance of 0.947, just less than one and
that created a new ID. Now that ID of that
merged cluster, if node 0-9 are leaf nodes, and this is where it gets
a little bit confusing, 0-9 are the leaf nodes. The node of the new merged points is going to be node ID 10, so the very first
line in the file, that's going to represent
cluster with the node ID of 10. The next one's going
to be the node ID of 11 and that comes from merging the next closest to point 0
and 1 at a distance of 1.11, so that's going to be node 11. The next line indicates that node ID 2 and node ID 10
were merged together, so node ID 2 is a leaf, node ID 10 is exactly what was
created in the first line, and so when you merge
those together, it's going to get
a node ID of 12. This is a little
more subtle and it's explained somewhat
in the homework, it's not terribly well
explained in the documentation, but the homework example
goes through this again. By looking at the succession of nodes that get
created from the merges, you get a description of the hierarchy which
can be very useful. Then you might be asking, well, what type of
hierarchical clustering would make sense to use
in a given application? My go-to method is Ward's method because it tends to give equal
sized clusters, you don't get a huge skew in cluster sizes where you
end up with two-thirds of points in one
cluster and then a few tiny clusters in the rest. It's also quite effective
for noisy data. By default, I would pick Ward's
method for most problems. If you have situation
where you expect the clusters to form
very long skinny groups, then single linkage may work
best because in those cases, the main distance is
ineffective proxy for cluster separation, and it will be an
effective distance measure for inter-cluster
separation if you don't have a lot of noise. If it is non-globular, not a lot of noise, and it's a very large dataset, you can consider using
single linkage clustering. Average and complete linkage
clustering perform pretty well when the clusters
are cleanly separated, but they have fairly
mixed results and I frankly don't tend
to use them very often unless I know for sure there's a specialized
application where if, for example, I really want to take the max
between the clusters. In general, for hierarchical
clustering top-down methods, which we really haven't
explored much in this module, they can be sensitive
to early errors, so if you pick a bad for split, where you split the
dataset into two, that can affect the
rest of the results, obviously so it can wreck the
entire clustering process, so it's sensitive
to early errors. On the other hand,
bottom-up methods can't see the whole dataset, they have a very narrow view of what's in a cluster
and so they may miss information
that would lead to better clustering
decisions overall. Bottom-up hierarchical
clustering, on the other hand, has the disadvantage
that it doesn't see the whole dataset at
once when it's trying to make clustering decisions
and so that may cause it to misinformation that would lead to better clustering
decisions overall. Where are we so far? Well, we've just looked at one major family
of clustering algorithms, the hierarchical
clustering algorithms. We're also going to look in another module at
K-means clustering, which is a partitional method
that's quite different. In general, we're
going to be faced, no matter what clustering
method we choose, with questions like, how
many clusters is best? How can we assess and
visualize cluster quality? How can we visualize and interpret the results
of the clusters? We'll continue to look at
those general questions repeatedly for different
clustering methods in this part of the course.