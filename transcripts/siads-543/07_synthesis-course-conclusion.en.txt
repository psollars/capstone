The purpose of this
short video is to review the concepts that we've explored in unsupervised learning. Talk a little bit
about how they will be used in future courses and also talk about some of
the deeper themes that underlie multiple
topics in this course. In unsupervised learning, we can divide the methods roughly
into two categories; things that transform
the data and things that find
structure in the data. We started off in this
course looking at transformation by
dimensionality reduction. We covered a few methods for feature engineering
but in particular, we focused on singular value
decomposition and it's closely connected algorithm, principal components analysis. We looked pretty
deeply into PCA and some variants of
PCA and using PCA to construct visualizations
of the data which are incredibly useful to get a sense for the
different variables, how they interact and
correlate with each other. We also looked at manifold
learning which was a way to project data from high dimensions to much smaller
number of dimensions. It's a form of
dimensionality reduction but it's done in a
way that it preserves distances in the original high
dimensional space and this is a type of constraint
that PCA doesn't have. The manifold
learning, its goal is to preserve the structure in the sense of relative distances or global distance structure. We looked at
multidimensional scaling, we also looked at
neighborhood preserving manifold learning methods
like t-SNE and UMAP and we saw that some methods
like t-SNE don't preserve things like cluster density or inter cluster
distances very well. It makes some very good for visualization but
not necessarily for preprocessing of data for
let's say supervised learning. We also looked at
density estimation as another form of
data transform. We looked at two major categories
of density estimators, we looked at non-parametric
ones which make no assumptions about the underlying
distribution of the data. Those include histograms
in the simple case, and then we looked at
a more general form of histogram called the
kernel density estimator that would smooth
out the bins across points to give you estimates
with less variance. Then we looked at parametric
methods which do make underlying assumptions about
the probability distribution the data was drawn from. In particular, we looked at Gaussian mixture models which are very widely used family
of density estimators. We apply those for things like outlier detection and
detecting covariant shift. The second major category
of unsupervised algorithm finds clusters or other
structure in the data. We spent a week in Week 2 looking at several different categories of clustering
methods that are all very different but
also in some sense complimentary and
they can actually be combined with each
other if necessary. But they make fundamentally different underlying assumptions. We looked at a generative method, k-means which turned out to be a special case of the EM
algorithm that we saw in Week 3. We looked at
hierarchical clustering which is particularly good for data that has an underlying
hierarchical structure. Unlike k-means, a hierarchical
clustering doesn't require you to specify the
number of clusters in advance. We looked at neighbor
based clustering methods and these are methods that don't make underlying generative
assumptions like k-means. They don't assume
hierarchical structure in the data like the
hierarchical methods. Instead they look at the local neighborhood
structure of points. They find connected
components in the graph of points nearby in
order to find clusters. We saw in week 4 how DBSCAN is a special case of something called
spectral clustering. It's a very powerful
paradigm that you can use to find interesting
structure in graphs. Finally, we looked at the
problem of evaluating the quality of clusters and how to label
clusters correctly. These are both problems that are critical to the correct
use of clustering. In the first case,
evaluating cluster quality. It allows you to
specify a criteria that make it possible to
choose a best clustering, or perhaps among a set of
clusters that might be best. Labeling clusters is important especially in user facing
applications where it's important to give an
accurate summary of what a cluster contains
for example in order to let users navigate or iterate on an analysis
of a cluster. Continuing exploring, finding
latent structure and data. In week 3, we looked at the expectation
maximization algorithm for incomplete data. We ran through a simple example where we looked at the problem of determining which coin was flipped in a two coin
flipping problem. But more importantly, that explanation
allowed us to see that some algorithms in the
course like k-means are actually a special case
of the EM algorithm. In fact, the EM algorithm
is connected to a number of methods both in this course and then for example in natural language processing. The EM algorithm isn't
the only method for doing maximum likelihood
estimation of parameters but it is one that is guaranteed to converge to
a local optimum and again, has been widely used and it's
fairly simple to implement. There are more advanced
methods for doing maximum likelihood
estimation of parameters, including things called
variational methods which you may be interested to explore on your own after this course. We looked in particular
at text processing because we cover topic
modeling as well. In order to understand
topic modeling and its operation on text
representations, it's important to
understand how to create a good quality text
representation and we did that by looking at the
vector as a class, the various parameters it takes, and how those
parameters connect to the multiple steps in the text pipeline
that they require. To convert a document into a useful vector of
numeric term weights. We then applied those texts processing steps once we have found vectors
for the documents, we then did topic modeling using both probabilistic and
non-probabilistic methods. In the non-probabilistic
methods camp, we looked at latent
semantic indexing and non-negative
matrix factorization. In particular, for latent
semantic indexing, we showed how in
order to overcome the vocabulary gap in information
retrieval, for example, it's important to be
able to match terms that don't maybe occur
exactly in the thing you're looking for and LSI let's you take your
initial query and so to smooth it out to incorporate related terms that do a
better job of matching. You saw a specific example of that in the week 3 assignment. We also looked at
probabilistic models for topic modeling,
in particular, latent Dirichlet allocation and Latent Dirichlet allocation is important understand
because it can be a very powerful
exploratory method. There are lots of flavors
around right now that take the original LDA model and extend it in different ways
for time series, other forms of data like
images, and so forth. By understanding Latent
Dirichlet Allocation, you'll understand the
general framework that people have been working in to create the
generative models of data. Finally, in week 3, we
looked at word embeddings, which in week 4 we saw
our simple case of self-supervised learning and word
embeddings fit naturally with the other work
that we did on topic models and text
representations. There are way to take
words and map them to low-dimensional dense vectors, which can then be compared like any other vectors with cosine
similarity, for example, or even cannon methods
and word embeddings are a central tool and some natural language
processing methods. Seeing them now both
an introduction that will be useful for deep learning, but also natural
language processing. Then in week 4, we focused on some of the challenges
that supervised learning has, like the cursive dimensionality with a lack of labeled data, and we saw how
unsupervised learning can help with some
of those challenges. In the case where we're lacking
enough labeled examples, we looked at semi-supervised
learning and in particular, label propagation to see
how you try to infer the correct labels
for things if you knew something about
related similar examples. We also looked at
self-supervised learning, which is a more general
case of word to vec. We looked at these pretexts
tasks whose goal is to get the system to learn a good representation
of a type of object, and then that representation is used for other tasks later. This is a very important
concept that you'll see again in deep learning. We also looked at
some other forms of unsupervised learning
and pre-processing. We looked at how to detect a data drift or covariant shift. In the case of the assignment, you've applied density
estimation to detect when samples that were coming in that we wanted
to predict something for, were very different from the
underlying distribution of the data that was used
to train the classifier. We also looked at
data imputation, which is incredibly useful
and important problem to solve where
there's missing data. You can use other
univariate, single column, or multivariate
multiple column methods for trying to infer
those missing values. We looked at some of the
risks of doing that, as well as some of the specific techniques
in scikit-learn that exist some of them very new to help do different forms
of data imputation. I wanted to mention as
part of this synthesis that there are a few
deeper ideas that I'm sure you've seen cropping up
again and again and I'll explain what I
think three of them are and you may have some
others that you've found. But for me, one of the
important ones is that a lot of these unsupervised
algorithms can be seen as different flavors of
constrained matrix factorization. We saw in latent
semantic indexing, PCA, non-negative
matrix factorization, even LDA, the latent Dirichlet allocation
can be written as a form of constrained matrix
factorization problem. Although we didn't go into
the details of that one, there's this underlying
unifying view in terms of matrix factorization
that is very useful. Why is it useful? It's useful because it allows
you to specify very explicitly how you want to constrain the solutions
to your problem. It brings to bear a number of numerical methods for matrix factorization
that already exists. If you can frame a
problem in terms of constrained matrix
factorization, you can bring to bear all of those existing tools
to solve the problem. I think it just gives
more insight into how you can decompose
a particular set of data into latent factors. There's not necessarily
one particular way. There are often multiple
ways that you can factor dataset and allowing solutions from constrained matrix
factorization helps make some of those assumptions
explicit when you're trying to examine the
structure of the data. Second deep idea that we've seen again and again is the
called the kernel trick, which is taking original
dataset and mapping it into some non-linear space where
you can then apply a fast, simple method like a
linear classifier or k-means to solve this
non-linear problem. We've seen the kernel trick in support vector machines
for classification. But we also saw how
it can be applied to something unsupervised
like kernel PCA, where you take your data and
in a very similar method, you map it using a
nonlinear mapping to shrink the data in some areas and expand
it in other areas. You're in this new feature space, but you don't have to actually operate in the feature
space explicitly. It's not necessarily
important to understand the underlying mathematical
details of how that works. But just to know
that it exists that you can apply what you can think of as a nonlinear
transformation to your data in order
to find structure. These tools exist and they're
actually fairly efficient. Kernel PCA, we saw a brief example with
spectral clustering. All of these allow
you to find structure in more challenging cases so it's important to
be aware of those. Finally, one of the other
deeper ideas is that we saw the idea of regularization
in supervised learning. How you can constrain the model family that's
fit to your data. Well, you can think of
unsupervised learning as a form of regularization except instead of constraining
the model family the classification
boundary for example, you can operate on the data. You can basically use
clustering for example to smooth the data by using global
or neighbor information. Then you can operate a supervised problem
on that smooth data. That itself is a form of regularization but
you're doing it by doing something to the data
rather than constraining the family of estimator that
you're applying to the data. That's a pretty
powerful idea as well. In terms of the
fundamental skills that we've covered
after this course, you should be able to correctly apply and interpret results from clustering methods and scikit-learn including k-means, agglomerative clustering, hierarchical
clustering and DBSCAN. We already covered
manifold learning, multidimensional
scaling and t-SNE, how to evaluate cluster results, how to correctly prepare
text for processing, and then using that with topic
modeling and understanding topic modeling and best
practices for its application. In addition to learning about
these different techniques, we covered some of
the trade-offs and assumptions that are inherent in these different methods
to help you understand when to apply them in
certain scenarios. We covered density estimation, focusing especially on just
a single random variable, which extends naturally
to higher dimensions. We looked at some interesting
visualization methods, especially biplots that
are useful with PCA. Then we covered a
number of methods you should be aware of for using
unsupervised learning, such as using density
estimation to detect covariate shift and when you might need to retrain
your classifier. Those are some of
the core skills and hopefully after this course, you will also
understand and be aware of some of the basic
mechanisms for using word embeddings
because it is used in somewhat in deep learning and natural language
processing, as I've said. You should be aware of the EM
Algorithm and what it does and how and why it's used and how it relates to clustering. You should be aware of a topic like semi-supervised
learning that there are methods like labeled propagation for inferring unseen labels, as well as Kernel PCA for doing non-linear
dimensionality reduction. Some of the material
is also meant to make you aware of self supervised learning
and its importance in finding rich feature
representations and again, that will come up
in deep-learning. You should also be
aware of issues around cluster quality and labeling
and interpretation, and we covered both in the
lectures and assignments, some methods for assessing when you have a high-quality
cluster or when you have high-quality labels for cluster and how to
interpret them. As far as how this
course prepares you for next steps
in the MADS degree, when you take
machine-learning pipelines, unsupervised learning
methods will provide powerful set of tools that
could be used before, during or after data processing in the machine learning pipeline. For example, you might apply to a very high-dimensional dataset dimensionality reduction method like PCA to provide a compressed set of
features that you then in the next step
of the pipeline, use to train classifier
on that dataset. We've seen some other
examples of how transformation and
finding structure in data with unsupervised methods can be effective pre-processing
methods in pipelines. In the pipelines
course, you'll be able to explore some of those. We've also looked at self supervised learning
and deep-learning is a natural connection next step to exploring rich data
representations, so our work on embeddings and latent variable
representations is exactly the thing that
deep learning methods do automatically and
in much greater depth, and so in the
deep-learning course, you'll see connections with what you've covered here already, and some of the more
profound advances in feature representation that
deep learning can provide. The text processing
work we've done, topic modeling and so
forth will be very useful in the applied natural
language processing course, which we'll go into
more depth on some of the processing steps we saw that include things like how
to parse the sentence, how to identify part of
speech and so forth, so richer representations
of text data. Finally, all these
methods will be useful potentially in
milestone two Project and exploring the data you have and transforming
it in some way, obtaining features and then
assisting with prediction. If that's the task that you
are choosing for the project. In combination with the
supervised learning course, after this unsupervised
learning course, you should now have many of the basic tools you'll need to tackle fundamental
data science problems you'll see in the future, including prediction, transformation and finding
clusters and other structure, both numeric and text data. You can see how supervised
learning methods and unsupervised learning methods interact with each other. For example,
supervised classifiers and regressors can benefit
from unlabeled data, unsupervised learning
methods, from dimensionality reduction
to label propagation. Thank you for all your
hard work in this course, you can be sure it will pay off, not just in preparing you for the next courses in this degree, but in many of your future
data science endeavors.