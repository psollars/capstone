In this video,
we're going to go into some detail. On the statistical method called
the singular value decomposition, it turns out that if you take
a data matrix, center it, and apply the singular value decomposition. You get principle components analysis. So PCA is a byproduct of
applying this powerful underlying method called singular
value decomposition or SVD. So in this video, we'll do a brief
review of some linear algebra concepts that sort of set us up to then go into
more detail on what exactly the SVD does. And it's an incredibly widely used
technique across statistics and machine learning. And so we'll also discuss some other
applications of singular value decomposition beyond just using it for
principle components analysis. A more general point that I'd like
to make before we get into singular value decomposition, is that matrix factorization is
everywhere in machine learning. So if you look at all the popular
methods for doing topic modeling, such as latent semantic indexing,
latent dirchlet allocation, non-negative matrix factorization. They're all different flavors
of matrix factorization. They just have a different objective
functions and constraints. Matrix factorization connects
together many different filtering, censoring problems, clustering,
topic modeling, and much more. So by familiarizing yourself with these
different forms of matrix factorization, you'll be able to recognize it again in
other methods used in data mining and machine learning. So let's get started looking at our
first matrix factorization method, singular value decomposition. On our way to looking at the essential
core ideas of singular value decomposition, I'm just going to give
a brief review of important linear algebra concepts and notation. So as usual, we're going to use
capital X as our variable name for the original data matrix. We're going to use X star as the name
of the centered version of the matrix, the column centered version. And we're going to use
X hat as the centered normalized matrix of our data set. Most of the time when we talk
about the matrix transpose of X, we'll denote that by X with
a capital T superscript. Occasionally, we'll have to
unfortunately resort to some other variations of notation for matrix
transpose, like X Prime, X star, or XH. And apologies in advance for any confusion
this causes, it's something that happens a lot in different flavors
of statistical derivations. But most of the time we're going to stick
with X superscript T for the transpose. Covariance matrix in this notation is
therefore X star transpose X Star. The correlation matrix is
X hat transpose X hat. And the cross product
matrix is X transpose X. A critical component of singular
value decomposition is going to be the use of orthogonal matrices. So an orthogonal matrix U is
a matrix that has its rows and columns as orthogonal unit vectors. Sometimes an orthogonal matrix Q is
called a linear unitary transformation. And you can think of it like a rigid
rotation or reflection in the space. It has the property that it preserves
the inner product of vectors. So if I have an inner product x,
y of two vectors. If I apply U to X and U to Y,
the dot product of those two resulting vectors is the same
as the dot product of x and y. It's known as an isometry
action in Euclidean space, which means it preserves distances. And it has the important property that
its transpose is also its inverse, or equivalently you can say that U
transpose U gives you the identity matrix. U transpose equals U inverse. So I'm showing here a very simple
example of an orthogonal matrix, which is a rotation by
90 degrees clockwise. And you can see with the example
in the bottom that if I apply this rotation to the .01,
it turns into the .10. So this is one of the simplest
orthogonal matrices. Use that in some of the examples to come. Now in general, any matrix M can
be seen as a linear transformation that acts on a vector V when
multiplied via dot product. So in this example, I have a very
simple matrix M consisting of two-by-two matrix with
four different values. And when we write down what
happens when we apply that matrix M to the matrix V here,
which I've denoted xy. You can see we just
simply linearly transform the first coordinate
from x to a11x + a12y, and likewise the second
coordinate becomes a21x + a22y. Or specifically if we have this
matrix M [4, 3], [-1, 2], you can think of it like a linear operator that
takes any vector xy and transforms it. Using the linear combinations that
are implied by the matrix multiplication. In particular, you can see what
the effect is of the linear transformation M on basis
vectors in the space. So for example, if we look at
the vector [ 0, 1] over here. We can see that when M is
applied to that vector, it gets transformed to the vector (3, 2). Similarly, if we look at
the other basis vector (1, 0), we can see that (1, 0) gets
transformed by M to the point (4, -1). So you can see how by
looking at the effect of M on the basis vectors in the space. We can get some idea of how M
transforms any vector in the space. How it might stretch it and
rotate it for example. And now we'll define singular
value decomposition. A singular value decomposition
takes an n by p matrix M, and factors it into three parts. A matrix U, which is an n by r matrix. A matrix Sigma, which is an r
by r positive diagonal matrix. And a matrix V transpose,
where V is p by r matrix. And here r is the rank of M. So the nice thing about
the singular value decomposition is that you can apply this
to any n by p matrix M. Just to be more specific
about the notation. The matrix Sigma that's the positive
diagonal matrix is often written like this, where the elements along
the diagonal are denoted Sigma 1, Sigma 2 and those are the singular values. Sometimes this diagonal matrix
Sigma is also written using S or D, and sometimes for
the input matrix we'll use X instead of M. So think about the action that we saw
in the previous slide where you apply the matrix M as an operator by decomposing
it into three separate operators. We can see a v here,
first gets mapped by this V transpose operator
followed by an operation using Sigma, followed by the operator U. So we've broken somehow
the action of this linear operator M into three
separate sub operators. And now we'll describe an intuitive
interpretation of what those three sub operators do. An intuitive view of how singular
value decomposition factors, the linear transformation M. Is that it decomposes M into 3 steps. So as a reminder, here is the original singular value
decomposition definition down here. And you can imagine a vector v
here that we want to transform. Originally wanted to
transform it with M and after we factor M with
the singular value decomposition. We can think about what happens
to V in terms of these three SVD operators that are created as
a result of the factorization. The first operator V transpose
can be thought of as a rotation. So if we imagine that this is
the original space that our data live in. Normally when we apply the matrix M,
you remember that example. Looked at what happened
to the basis vectors. Essentially what M does is
some combination of scaling, transformation, and
stretching, essentially on that original space, so
it does that in one step. However, the three substeps
that SVD finds break down that operation that M provides into,
first, a rotation that corresponds
to the V transpose matrix. So it rotates the original
space with V transpose. And then the second matrix,
the sigma matrix, the singular value diagonal matrix,
provides a scaling of that space according to
the magnitude of the sigma one, sigma two, and so on singular values. So that scales the space horizontally and
vertically in this case. And then the third operator
U is another rotation, so it rotates the rotated,
scaled space again. And the really amazing
thing is that we end up at the same result as the original M. So the intuitive view of
singular value composition is, any linear transformation
M can always be decomposed into rotation, a scaling,
and another rotation. So just to provide a concrete example, in case you don't believe me
that this decomposition works. Let's apply singular value decomposition
to a really simple two by two matrix. In particular, we'll use the same
matrix that we saw before, which takes an input vector and
rotates it 90 degrees clockwise. So in this case, the action of M
will be to take any vector and rotate it 90 degrees clockwise. So if the vector is here, let's say, it will rotate it 90 degrees clockwise. So that, for example, the vector (1,
1) will become the vector (1, -1). So we can actually apply singular value
decomposition to this matrix, and what comes out is the following. Singular value decomposition
tells us that M can be expressed as the product
of this matrix U, this diagonal matrix of singular values, and this matrix V transpose. Now each of these operators, U, sigma, and V transpose, has an interpretation
geometrically as well. And in this case,
what this is saying is that rotation 90 degrees clockwise is exactly
equivalent to performing these three rotate,
scale, and rotate steps. In particular,
V transpose is a transformation that will rotate 180 degrees about the y-axis. So it will flip things around the y-axis. In this case, the matrix sigma
is just the identity matrix, so it doesn't do any scaling horizontally or
vertically. And the matrix U, the other rotation, rotates 180 degrees about the origin,
so it flips x and y. And so we can confirm that geometrically. So there's a simple geometric
interpretation of this. If you imagine,
let's suppose we want to rotate by 90 degrees clockwise this vector here. What the SVD says is
that we can rotate it 180 degrees about the y-axis, mirror image. We can scale it with the identity,
so in other words, do nothing. And then we can rotate that
second vector about the origin, flip it so that it's reflected in
the origin, to get this vector. So you can see,
either we can apply M to do the rotation, or we can break it down as SVD
has instructed us to do into these three separate rotate,
scale, rotate steps. Right, I'm showing the implementation of
these steps so you can see that indeed. Even a simple two by two matrix
can be factored with the SVD. With some interesting interpretations
about what the components do, and we can also easily verify
that if we indeed multiply. These three matrices together
we will get back the original M. So that's pretty neat. You might be asking
yourself at this point. Well, there are multiple ways we
could have factored the Matrix M. What makes the factoring provided by
the singular value decomposition anything special? Well, it's special because
of the properties of U&V and the singular value matrix,
which here I've put a D and I'm going to write a Sigma over top of it,
just to remind ourselves that. It's the singular value matrix and
annotation. So what special are, for example,
that U&V are rigid rotations there orthonormal matrices
as we saw a few slides ago. We saw the properties of
orthonormal matrices. In particular,
U transpose U is equal to the identity and V transpose V is equal to the identity,
and VV transpose is equal to the identity. The squared elements of each
column of U&V sum to 1. The inner product or dot product
between any pair of columns in U is 0, their orthogonal. And the inner product between
each pair of columns in V is 0. And then D aka Sigma contains
a diagonal matrix with nonnegative decreasing elements. So the element at the very top left
of that matrix is the largest one and as you go down the diagonal
the elements get smaller and smaller. And as we saw before,
the diagonal elements of that diagonal matrix are the singular
values produced by SVD. The columns of U&V are called the left and
the right singular vectors, respectively. The singular value decomposition
is very fast and easy to compute, so any matrix X can quickly
be decomposed this way. It's also unique up to sign
flips of columns of U&V. You might be wondering what exactly
goes on under the hood to produce this singular value decomposition. Well, to do it quickly as
a pretty finely tuned operation, one way is called the QR algorithm. But there are lots of other variants
that are specialized and optimized for specific subtypes of matrices. QR decomposition can be computed
by process called Gram Schmidt. But there are also some more other
numerically stable methods, so that's the SVD in a nutshell. Let's take a look at some sample
applications and see why it's so incredibly useful. Let's suppose that you had
a really large matrix X and you didn't have room to store
the entire thing in memory. Instead, you only had room to store
a tiny fraction of the space. Well, you can use the singular value
decomposition to create an optimal approximation of the matrix X
in the least squared sense. So let's suppose we want
to take an extreme case and we just want to approximate X as best as
possible with just one pair of vectors. Well, you can do this with
singular value decomposition. You just do the SVD and use the first columns of U&V
multiplied by the 1st singular value. So if you think of the matrix X Here what we're saying is that we're going
to be approximating X using just a single vector, the first column of U.
And one singular value as a multiplier. And then the second
vector from V transpose. We've taken the original matrix X,
which could be, for example, an image and
we said we can approximate this image as the product of 2 one pixel
arrays plus this multiplier. So when we do this,
the approximation that we get using these two vectors when you multiply them
back you'll get approximate matrix X. And there's a theorem that says
singular value decomposition provides the best possible approximation X tilde to
the original X in the least square sense. So it's the closest approximation if
you measure approximation using a least squares objective. Okay, so it's always fun to try
these ideas on concrete example. So let's do the compression using
SVD starting with a 2 by 2 matrix. So we're going to start with
our matrix M here 0, 1, -1, 0. We're going to do the SVD on M,
to get back these three factors. And now we're going to compute the rank-1
approximation, to the original matrix M. Let's take in this case first column of U,
and the first column of V,
which is the first row of these transpose, and then we have our multiplier here. And we can multiply those altogether. And this is the matrix
we get as our rank-1 approximation to
the original 2 by 2 matrix. So you can see that it
actually is pretty close. The only difference is in
the upper right element here. Now I've written the product here
in terms of an outer product. So when you multiply U and
V transpose together, you get, it's an outer product, so
you get this full matrix as a result. So that was a simple example of
a rank-1 approximation to X. What if we wanted to use two pairs
of vectors to approximate X? In other words, how would we compute
the best rank-2 approximation? Well, that would have a very similar
form to the rank-1 approximation, except instead of just using,
the first column of U and the first column of V,
you would use, the first 2 columns. And we would add them,
there would be two terms in the sum, the first that we saw in
the rank-1 approximation and then an additional term using the second
column of U and the second column V and the second singular value
in the diagonal matrix. Now we're going to try this rank-2
approximation on a 7 by 3 matrix. But in the process of doing that,
I'm going to introduce something that's coming in a later lecture
called latent semantic indexing, as another application of singular
value decomposition to text. Latent semantic indexing is nothing
more and nothing less than applying singular value decomposition on
a matrix that represents text data. In particular, when we have
a collection of text documents, we can form a term-document matrix
that records for every document, some statistic about the occurrence
of every term in every document. By applying the singular value
decomposition to this matrix, we perform a sort of topic modeling. So SVD will find essentially a set
of latent factors that explain the text in the original corpus,
using a much smaller number of topics. So for example, we might have many,
many thousands of documents and many, many thousands of terms, but we may try
to find this value k, maybe at one or 200 topics that explain the language and
term usage in all the documents. So when we apply the SVD to text
data on a term-document matrix, the interpretation of the matrix
U is of a term-topic matrix that gives essentially defines, what terms
are important for each of the k topics. The singular values represent
a set of topic weights, and then the matrix of V
transpose records for each topic. What are all the different weights
of that topic within each document? Okay, so as promised,
we're going to now compute a rank-2 approximation to a 7 by 3 matrix. And what I'm going to do here is,
I'm going to interpret the 7 by 3 matrix as a term-document matrix, as an
illustration of latent semantic indexing. So let's pretend that we have three
documents and seven unique terms, and the first term occurs five
times in the first document and twice in the third document, and so forth. When we apply SVD we get, essentially
what is a form of topic modeling. So, in the matrix U we have,
in this case we've asked for rank-two approximation to the original
matrix X, so we'll get back two columns. Each column represents a sort of topic. And, each row represents the weight that
a particular term has in that topic. Likewise, in the third factor here,
for each of the two topics, there's a weight associated with
each of the three documents that indicate the weight that topic
has with respect to the document. So we saw how to compute
the rank-one approximation to M. Which used a pair of vectors
multiplied together to reconstitute an approximate version of M. If we allowed ourselves to
use two pairs of vectors, we've got a rank-two approximation to M. And you can see that, the general case is
to compute a rank-k approximation of M. You just extend this to include k terms,
exactly the same way. And typically here. As we saw in the case for
latent semantic indexing. K, which in latent semantic
indexing is typically 1, 2, 300, compared to a document,
where term count of thousands, k is much, much less, typically,
than the number of rows, or the number of columns
of the original matrix. So this is called the truncated SVD, where
we take a subset of the columns of U and V to create this representation
that approximates M. And you can see what we've done here,
we've taken an original matrix of data that had high dimension,
and we're compressing it. We're essentially figuring out
a representation, in terms of a much lower dimensional set of matrices, and
that's exactly what our goal was with PCA. And this brings us back to the point
from the summary of PCA that we saw. That, PCA of a matrix X,
is just the singular value decomposition on
the centered version of X. The columns of V are the PC
loading vectors. And the columns of U
are the PC score vectors. As you recall, we were seeking
loadings that maximize variance. It's important to note that,
principal components analysis is also done on correlation or
crossproduct matrices. So their versions of PCA take
singular value decomposition of the correlation matrix,
X hat transpose X hat. On occasion, the raw crossproduct matrix,
X transpose X, is used, and
that's typically with sparse data. In fact, we see that as the form that's
used with the plate semantic indexing idea that I showed you earlier. Keeping the first k components, PCA is equivalent to doing truncated SVD. One application of singular value
decomposition is to impute missing values, and we have a module on imputation of
missing values later in the course. But it's worth noting that, singular
value decomposition can be used for computing missing values. If elements of X are randomly missing, SVD gives you a simple and
effective way to impute them. Now, you have to be very careful when you
do data imputation of missing values. It's a complex topic. And, typically the best imputation
method will depend on multiple factors, how much noise there is, what the
covariance structure of the variables is, and so forth. But just to give you an idea of
how SVD is used in this case. One simple algorithm would fill in
the missing elements of X using, for example,
the column mean to get a new matrix X*. You would then compute
the SVD of X* to get a rank-k approximation with, say, k = 4. So you approximate X* with SVD. And then you replace the missing
elements in the original X with the corresponding elements
of the rank-k approximation X(k). And then you can repeat these
steps until the values converge. And I've listed a paper on the slide
here that's an example of this approach. There are many other applications for singular value decomposition
in data analytics. Here's another example. Say X represents a set of p-dimensional
points to which you want to fit a linear model with least squares. But first, you want to quickly test
if X is what's called multicollinear. In other words, if there are high
correlations between the features. This is really easy to do with SVD. You simply look at the singular values. If the singular value, add 1,1
divided by the singular value add p, p is huge,
then least squares is probably a bad idea. So essentially you look at the ratio
of the singular values, the largest and smallest. And if you do have multicollinearity,
you'd want to apply a more sophisticated regression approach like Partial Least
Squares regression or PCA regression. There are a number of SVD routines
in Python that will be using. Scikit-learn only has Truncated SVD. Whereas you can get a quote
Unquote Pure version of SVD, I can give you a full
decomposition in scipy.linalg. For dense data, you can use
column-centered SVD or Truncated SVD. And for sparse data,
you can just use Truncated SVD, doesn't center the data,
if you pass in sparse data. So it sufficient for sparse matrices. So I just found out again that
singular value decomposition suffers from what's called
sign indeterminacy. The signs of the components that you
get back depend on the algorithm and the random state. You want to fit instances
of Truncated SVD once, and then you keep it around to do any
transformations that you need to do. We'll revisit SVD using a number
examples in the notebook, and also, as I mentioned,
when we look at latent semantic indexing.