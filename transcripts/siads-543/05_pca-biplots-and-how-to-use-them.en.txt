In this video,
we're going to explore by plots, which are a way to visualize the results
of a principle components analysis. And the really neat thing about
by plots is they show you both the transformed data. What happens to the data after you
transform it into PCA feature space? And it also shows you the relationships
between the principle components and between the principle components and
the data. So it's incredibly useful tool to explore
a data set that you've analyzed with PCA. But it's also really good to look at by
plots just because they're very useful. Practice in applying
the concepts from PCA to understand the structure and data first. Let's do a quick review of
the linear algebra basis for PCA. Remember that our goal for PCA is to compute a new set of features
that we derive from the old features by taking linear combinations of
the columns in the original data set. And so we want to choose this
set of features in a way that preserves information about
variance in the original data set. But also does so more efficiently than
the original features that we want to essentially compress the data set into
this sort of smaller set of features. That we can use for
a more parsimonious representation. So what is a linear combination? It simply a weighted sum of things and I'm going to note here the linear
combination vector, a. I'm going to put it at the top here so the idea is that we're going to be taking
linear combinations of columns of X. So for example, if X had three columns, then a might contain point 3.3,
3.5 and 0.2. So we would be a vector if
there were three columns in X, then a would have three elements. Real numbers corresponding to
the weights that each column was given in the linear combination,
of course. There could be a negative
element of a as well. And so another way to think about this. If you look at the product of
matrix X with this vector, a. You can think of a as a vector. It is a vector and you can think of this operation of
multiplication like projection. So if X consists of these data points,
drawing little data points here. When we multiply X by a, you can think of this kind of as a
projection of all the data points onto a. And what's the result of the projection
we've taken the data points. And each data point now corresponds
to a real number on this line. Corresponding to its location
on the line described by a. And so, in our search for
this new set of features that will be a more compressed
representation of our data. We want a vector, a, as we discussed
in formulating PCA earlier. The vector, a, should maximize in
some sense have a maximal variance. So if you think back to that
example with the scatter plot, the elliptical scatter plot. The first principle component found the
direction of maximum variance of the data, and that's all this criterion
is saying right here. That whatever vector, a, this linear
combination of columns is, it should. Whatever the variance of that
projection is, this projection Xa. That variance should be maximum. So anyway, the PCA algorithm will go
ahead, it'll chug along, it will find and a that satisfies that criterion,
as well as the second criterion. So to constrain the family of solutions
that the norm of a has to be 1. So we will go ahead and
it will find a vector, a, will call this solution vector a1. And that solution vector a1
will be a vector like this, it will be a set of weights,
one weight per column and it will create a new feature. So the linear combination of those
columns of the original data set X according to the weights in a1
will produce a set of these values that are essentially new feature
values for each data instance. And PCA is an iterative process, so
after it finds the first solution a1, we ask it okay now find another
vector that has the second highest variance as a result of
applying it to the data. And so on, so we'll get a series of
solutions, to this optimization problem. And each of the values in these ai
these vectors, is called the loading, so that linear combination is just
a linear combination of weights. The weights are called loadings in
much of the statistical literature. And the result of applying those
loadings on x, are called the scores. So essentially the scores of the thing
that appear in the new feature space, it's what happens to
the original values of x, you transform them into these new feature
space, these ai features that you found. It just basically says what happens to x, what are the coordinates of x in that new
feature space spanned by that the aI's? Now when we run PCA, the loadings on the scores are available
to us after we fit the model. So if you see down here in Scikit learn,
we first take our normalized original data set, run the fit method and
that goes ahead and does that process of finding these
solution vectors a1, a2 and so forth. And then to get the scores,
you just simply call once this PCA has been fit,
you call transform on your data. And so this this x_pca what
I've called x_pca here those, essentially are the results
of applying the data projecting the data onto
the principle components. So these are the scores, so the scores
come out of the transform method. And then to get the solutions
themselves that were found by PCA, you can use this property of PCA
that is set once it's been fit, components_, make sure you
don't forget the underscore, it's often easy thing to overlook. But pcacomponents_ has its columns
that contain a1 and a2 and so forth. So we have the scores that are essentially
the transformed data in the a1, a2 and so forth feature space, and we have the
loadings, which are the values in each of the ai.
And what biplots are going to do is, they're going to show us both the scores and the loadings in one plot. Let's go back for a minute to the PCA example on
the Wisconsin Breast Cancer dataset. So you might recall that this
data set represents a set of measurements of cell images,
and there were, what? 30 some odd features which you can
see over here, they actually had an imaging tool like the one on the left
to help staff derive the set of features. So it helped measure distances and
sizes and dimensions and so
forth of these cells on the image. So they have an imaging tool and
the output of the imaging tool, with this set of features that came out. Now if we want to find out which of these
features are correlated with each other, we can get that information
very nicely with a biplot, so in this example what we've done is
we've taken the original 30 some odd features an run a principal components
analysis with two principle components. So again, think of a principle component
like corresponding to a new feature. So we've taken these 30 columns, and we've
taken some linear combination for each of these principle components represents
one linear combination of the columns. And the values of the weights in
the linear combination are shown in this heat map, so you can see that for
the first principle component, it's got one wait for
each of the original columns. And you can see that for example, the values that are round 0 or
the ones that are sort of pink. The ones that are above 0 in the linear
combination are sort of orange color and so forth. So that's the first principle component
that PCA found, that's the first weighted combination of columns.
And then the second weighted combination of columns you can see it
has lots of negative values, so these dark areas represent loadings on
the columns that have a negative Value. So this is a biplot and I'm going to walk
through exactly how to interpret this and then show you some examples that I
think are pretty compelling that give you an idea for just how much you
can derive about the relationships between variables in the data
set just from using a biplot. Okay, so the by plus called the biplot
because it has two components. The first component,
which is represented by this scatter plot. You see the scatter plot in the background
here, different individual data points. So these data points are the scores. So basically the come out of
if you take the original data, run it through PCA and
get back the transform data. Remember, we're using two principle
components, so the score is that come out will have two dimensions for the first and
second principle components. So we can see right away the coordinates
if you will each of these data points was a data point
in the original x data set. And now we can see, for example,
that principle component one, the coordinate for this point is -0.2. The x is projected onto the first
principle component, and then we can project it onto the second
principle component about plus 0.02. So whatever this point was in the original
space, it's been mapped to two features. The first principle,
quantum feature of -0.2. The second principle components 0.2 for
that for that point. And we can do that with every
single original data point. It has some corresponding
projection on this plot. And I'll explain more about what insights
you can get from that in a minute. So that's the first component
is the scatter plot. The scores of the data
according to this PCA. And the second component
is obviously these vectors, these vectors that you see
radiating out from the center. There is one vector per column
of the original data set, I also refer to these variables. Since each of the variables individual
data set representing the different measurements of a cancer cell,
appear as an arrow. To plot, for example,
let's take a mean fractal dimension. To plot the arrow corresponding
to mean fractal dimension that simply the loadings for PC 1 and
PC 2 for that variable. And more specifically to get the xy
coordinate of this of this arrow. The x coordinate you can
get from PCA components. The first column and
the first principle component and the y value for this arrow comes from PCA
components underscore the second column. So there's one vector per variable
in the original data set. So what can you do with this plot? So we've plotted the data
as a scatter plot and we've plotted the loading values for
the variables as well. So what can we do with this? Well, the first thing that's true about
these diagrams is that you can look at the cosine of the angle
between any two variable markers. By the way, we call these data markers and
variable markers. The things that we plot on this time. So a variable marker is just this vector. The cosine of the angle between any two
variable markers is the coefficient of correlation between these variables. Now I want to mention one warning here. What we're assuming is that
the first two principal components of the data
in this problem capture, let's say over 80% of the variance
in the original data. That's an assumption we're making. We're assuming that
the properties of these principle component vectors will hold if
the first two principal components represent indeed the majority
of variance in the data. If that's not the case,
these relationships may not hold. But assuming that the first two principal
components summarize the majority, let's say more than 80% of
the variation in the data. Then we can say the cosine of the angle
between any two variable markers and into vectors, is the coefficient of
correlation between those variables. In particular, If you think
about the properties of cosine, things that have cosine of 0 is 1. And then it approaches
1 as you get towards 0, the coefficient of correlation
will increased towards 1. So the smaller the angle
between the vectors, the larger the coefficient of
correlation is between those variables. So you can see in this example mean
symmetry, it's a bit hard to read, because they are so highly correlated,
so close to each other. Mean symmetry and mean smoothness
are the two variables here, and the angle between them is practically 0,
so they are almost perfectly correlated. Whereas something like mean concave
points and mean fractal dimension. If you look at the angle between those two
vectors, that's approaching 90 degrees and at 90 degrees,
what's the cosine of 90 degrees? It's 0, so that's the first insight
you can get from a by-plot. Is just how correlated different pairs
of variables are with each other. So you can also get some insight into
the relationship between each variable and each principal component. So remember that the principal component
is essentially just another feature, so you can think of it
like another variable. And then you can ask yourself,
for what settings is the value of the first principal
component, high or low? And if the first principal component
feature value is high or low, what does that mean? Are they correlated with that
value of that principal component? Just as we did for pairs of variables,
you can also look at the angle between a vector and the axis representing
the principal components. So we can see the angle
between mean concavity and the first principal component. So this is the first
principal component axis. The angle between principal component one
an mean concavity is very small, which means that when the value of PC1 is high,
mean concavity is also likely to be high, so there are correlated
with each other there. Similarly, you can look at,
this is the principal component 2 axis. And you can see that mean fractal
dimension, the angle between these two is quite small, which means that when mean
fractal dimension variable value is large, that the principal component 2
value is also likely to be large. So just as you see with
pairs of variables, you can also look at the relationship
between variables and principal components to see
how much in effect how much each variable contributes to
the principal component score. The third thing you can see
from a by-plot is you can look at the relationship
between data marker or data point and a variable,
and to get the original value of the data point for that variable. So when I draw the arrow for
this principal component, I'm drawing in positive direction. So if I start at 0 and
I travel out like this, a positive distance is in this
direction as opposed to say something like that in
the opposite direction. So what this means is if I would like
to know, we know the coordinates of the data points in PC space
after they've been transformed. And let's suppose we want to know, well, what were their original values
in the original data set? Well, to do that,
if I wanted to know what the mean fractal dimension was
originally of this data point, all I would do is project
it onto that vector. And the distance from 0 to that point
would be the reconstructed centered value. In this case, it would be positive
because I've drawn the arrows to be in the positive direction. And if you think about these vectors
extending in the opposite direction, which represents the negative values. Then the mean fractal
dimension of this point, if I project this point onto the extended
vector for mean fractal dimension, that's going to something negative,
so in this direction, it's negative. So it's some negative value. So you can do that with any point. To get the original value of
a variable in the original data set for any of these points, you simply project
it onto the vector for that variable. So you can imagine if you wanted
to completely reconstruct the original values for
the features for any data point. You could simply,
let's see if we take this point. If I wanted to get this points
original mean fractal dimension value. I would project it onto this vector and
get some positive number. And then I would project this. Let's say we wanted
the mean compactness value. Then I would project it onto this mean
compactness vector and I would get another positive value representing
its original mean compactness score. And I could do this for
this data point, I could do this for every single one of the vectors. And in that way,
I could reconstruct the original centered values that the data point had
in the original data space. So to explain this 4th insight, I'm going to make a little digression to
describe what Mahalanobis distance is. Mahalanobis distance is a form
of distance measure that accounts for
standard deviation in the space. So if you have a set of points
that is distributed normally by, let us to say with
the Gaussian distribution. And suppose we have
a Gaussian distribution, I'm drawing units of one, two, three standard deviations
away from the mean. Mahalanobis distance, you can think
of it maybe like a warping of Euclidean space to account for
the directions in which the data varies. So in Mahalanobis distance,
this point here, that's one standard deviation from
the mean and this point here, which is one standard deviation
from the mean, are both considered equal distance from the mean according
to units of standard deviation. Even though in Euclidean distance
the second point appears to be perhaps farther away, a straight line distance. So it's a farther straight
line distance to the mean. But because of the way
the points are distributed, it's just as close in Mahalanobis
distance as the first point. Describing two points that
have similar Mahalanobis distance means that there are similar from
the point of view of being in a similar band of variance with respect to
the underlying data distribution. So the insight here is that
you can look at the Euclidean distance in principal component space, and that Euclidean distance is
proportional to the original Mahalanobis distance in
the original feature space. So essentially, this is a way to
look at how the data clusters and where the distance measure you've kind
of warped it correctly to account for how things are correlated
in the original space. So we think of it like a warping of
Euclidean space, and we can see sort of clustering behavior once the variation in
the data has been sort of normalized help. So here's a fun example to practice what
you've just seen in getting some insight from by-plots. So this is a by-plot of
principal components analysis of the set of beer reviews. So I have a database of thousands
of reviews of beer according to different measures,
the taste, appearance, and so forth, and
the ratings were on a scale from 1 to 5. And so I took this data set, ran it
through PCA just to get some ideas of sort of exploratory analysis to see, for
example, how people's review ratings for aroma or palette or taste were
correlated with appearance for example. Let's apply the four types of insights
that we covered in the previous example. So the first type of insight is how the
variables are correlated with each other. So we can see that aroma,
palette, and taste, the angle between those is fairly small,
so the cosine of the angle indicates
the degree of correlation. So with a small angle,
more highly correlated. On the other hand,
we can see that these variables, the angle between these variables and
review appearance is quite large. In fact, it's between the review overall,
and the review appearance, it's almost 90 degrees, which
indicates that the overall rating for a beer is not at all correlated with
the ratings it gets for its appearance. So that means that there appears
that might have very low ratings for appearance, but getting very good
overall review and vice versa. So that's kind of interesting, so
we can look at these pairs of variables. We can also see the correlation
with the principal components. So by looking at the angle
between each of the variables and the axis of that principal component. So we can see that for the first principal
component, aroma So here's the axis for the first principal component,
we can see that aroma and palate and taste are all highly correlated
with the first principle component. And this group of four actually sort
of reasonably correlated with the first principal component. So you can think of the first principal
component kind of summarizing those four variables. Because when the value of that
principal opponent is high that will be correlated with a more extreme value for
these others. On the other hand, if we look at the
second principal component axis, we can see that review appearance, the angle
between review appearance and the second principal component axis is much smaller
than it is for these other variables. So we can identify the second
principal component is kind of primarily summarizing
the appearance feature for a beer. The third type of insight had to deal with
the connection between the data points and the variables. So you remember that we were
able to reconstitute a data points original value for
a variable by projecting that data point onto the vector for
that variable. And also recall that when I drew
these vectors, I drew them so that they were already going
in a positive direction. So anything projected onto this
vector that projects onto the part that I'm showing here would have
an original value that was positive. If you extend out the vector in this
direction any data point that projects onto this part of the vector would
originally have had a negative value. And then of course, I'm talking about
the normalized centered original value. But we can still get some insight here,
so we can say for example, are there any beers that have had
a very high overall review rating, but had a very negative appearance score? So because the data have been normalized, if the original score was negative
that means that below the mean. So let's look to see if the biplot can
tell us are there any beers that got extremely negative appearance scores but
got very positive overall reviews. So to do that, we're looking for negative appearance scores would
take the appearance vector here. Which the positive direction is
this way we're going to extend it out in this direction. And so any beers that had a very
negative appearance score would project to somewhere in
this range on that straight line. And you can see there are plenty
of beers that do that, so all the beers essentially that
are here would project down into the very negative appearance
score read part of this line. So we're also interested in beers
that have had a very positive overall review rating. And so that would be beers that
when you project them this way onto this line would be
somewhere in this higher review. So so we can see that there
are indeed beers that are like that. So I'm just going to recreate this for
a second here this so we can see for example, that here's some beers down here. Where their original appearance score if
you project them on the appearance line and again, I just want to reiterate these
are all positive original scores and these would be all negative. So these beers when you project
them onto the appearance vector they originally had
negative appearance scores. But especially the ones over here, you can
see that when you project them to overall review ratings, their original review
ratings were much greater than zero. And so this section of
beers right here represents an interesting thing
potentially to explore further. And finally, we can see there's interesting clustering
behavior between the points themselves. And you can see part of the pattern is
due to the discrete nature of the ratings through the ratings advance potentially
increments 0.5 in some cases. Or when you average them
over multiple reviews for a beer, it'll be some quantized value. That's what is creating this bend like
structure that we can see immediately using the biplot. So there's actually no direct method for
creating biplots that comes with scikit learn, but
we have provided in the accompanying notebook an example of the code that
created the biplots that we've just seen. There is a package called PCA in Python
that you can install and use if you like. It includes a biplot plotting routine
which I've shown here on the right, very easy to install and use,
although I haven't used it myself, some aspects of it are a little bit
more difficult to sort of customize. What I would recommend is just using
the notebook code that we provided, it's kind of instructive just to
go through it to see how it works. And then you feel free to use that code to
create your own biplots for your datasets.