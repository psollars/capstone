This module is an introduction
to topic modeling which is the name of a broad
family of methods for assigning topics to content. First, I'm going to go through some general context and how this problem has been
addressed in the past. Then we're going to go
into more detail on a specific method called
Latent Dirichlet Allocation. Then finally, we're
going to talk about how the quality of
topic modeling is assessed and how to tune the parameters and apply
it in scikit-learn. People have been tagging content with topics
for a long time. With the advent of the Internet, there was a great effort
on number of sites to categorize millions of
pages using human editors. The open directory
project was one of those efforts and in some cases they had
automated curation to help them with the tagging. Over time, people have also developed supervised
learning methods, training classifiers
with existing tag data, typically from that open
directory project, for example. The supervised learning
approaches are able to tag new documents with the
most likely classes as predicted by the classifier. You've seen multiple
ways that you could approach that in the
supervised learning course. In this course, we're
going to focus on the unsupervised learning
approach to topic modeling. In that approach, we're
going to gather a corpus of documents that have no labels, apply a topic modeling
algorithm and then try to interpret the topics that
result from the algorithm. There have been some
approaches that combine supervised and
unsupervised learning. They use a small number of labeled instances combined with many unlabeled instances and one instance of that we'll
discuss is called labeled LDA. Here's a brief look at a supervised learning method that classifies web
pages into topics. This was trained using the
open directory project and it shows Microsoft
Research is homepage. If we look at the homepage,
we see that there are multiple topics that
could apply here. Of course, most of the
content is about computers, but we also see topics
related to science, WorldWide Telescope, social issues and then reference educational
issues as well. Supervised methods can
classify web pages. It's multiple topics
in a hierarchy. It's surprisingly common
to see documents that combine two topics that might be considered otherwise
quite different so you can have topics
that combine into a single page on politics or health or science and
education, for example. That was an example of a classifier applied
to web pages that was trained from many documents that were labeled
by human editors. What happens if you don't have
labels from human editors? For example, there
is a large body of scientific literature that exists in journals from the past. What if we wanted to mine
scientific trends according to the topics of research
the articles represented? In other words, we
wanted to discover the dominant themes
in a collection. Well, this is where
unsupervised methods come in. Here's an example of some sample scientific
topics discovered with the technique we're
going to look at very shortly called Latent
Dirichlet Allocation. Latent Dirichlet allocation
was run on a corpus of articles from Science
from 1980 to 2002. Here are the top
five topics that it discovered in the course of
analyzing those articles. You can see that the topics form fairly well-defined
semantic groups. There's a first topic on methods; computer
methods perhaps. There's a topic about chemistry, about neuroscience,
astronomy and health. Perhaps it's a good time to think about what a topic
model actually is. In this case, I'm showing a topic model as a list of words. Typically there are also,
as you'll see later, scores associated with
individual words that indicate their relevance
to that topic. I've omitted those scores in this example but we'll
take a look later at some visualizations that do show the relative relevance
of terms within a topic. The other thing to note about this example is that you can tell I'm having to interpret these topics in real time
about what they might mean. When the algorithm for
topic modeling runs, it does produce these lists
of words but then that leaves the problem for a human to interpret what exactly
each topic represents. Sometimes that can be quite a challenge as we'll see later. Finally, I'll just note that
even though the articles in Science have been tagged with keywords during the publication, the keywords are first of all, fairly limited in number
and fairly specific. They may not capture the more interesting trends or underlying themes that
occur in an article especially if you
start tracking them over time and that was one of the goals of this
topic modeling work by David Blei and John Lafferty. It was to not just look at a snapshot for a whole
collection over a long range of time but to see how the topic
model changed and thus get some idea of scientific
trends and when they started occurring in
various publications. As you might imagine,
there has been tremendous interest in applying topic modeling to
social media texts. Here's an example,
applying topic modeling to 4 million tweets from 1,200 prominent Twitter accounts
posted over 12 months. You can either identify the topics in an individual
tweet with a topic model. Or you can pull all the
tweets from a user and create a profile of topics that the user frequently tweets about. The types of topics
that you can track on social media include
things like sentiment. You can have a positive topic and a negative topic which is essentially
sentiment detection. People have used that for competitive analysis
to see what people are tweeting about
products, for example. In this case, we can
look at three accounts. The NASA account that clearly
is talking about space, the World Health
Organization account that is clearly
talking about health. Macworld account, which surprisingly has
predominantly Apple, Mac and other Apple
product terms. Let's go back to what
topic modeling is, what topic models represent
and how they're visualized. For example, topic modeling is an unsupervised technique that discovers abstract themes that
occur in set of documents. One thing I want to emphasize
here is the concept of a document can be
extremely varied. Traditionally, we would talk about documents being webpages, social media posts,
news articles. With the increase in the variety and the volume
of different datatypes, a document could easily
be a lecture video. It could be a DNA transcription. It can even apply, as
we will see shortly, to things like the
color themes that are used in various
Lego Projects. Here's an example
of how one might visualize a set of
topic models and its relationship to
a document using four different types of
measurement or visualization. What this shows is a random article from a collection of
scientific articles. This one happens to
be on chance and statistical
significance in protein and DNA sequence analysis. The first thing you might surface with a document like this, once you've learnt topic modeling is the set of topic
proportions that characterize the
relative importance of topics to this document. Here's is the example
of how they can show different topic proportions. In this case, the bar chart here uses color and
I'm just going to, because the colors a
little bit hard to see. I'm going to link the set of terms representing a
topic with the color. Here's the green topic that represents residues
binding domain, helix structure type words. The most prominent
topic according to topic proportions is this one the first topic having
to do with sequences, or regions, and so forth, which isn't surprising
given the title. Then the second largest topic, according to proportion is the second one about
measurement averages, ranges basically different kinds of measurement or
statistical terms. The second type of visualization
or analysis you might surface would be a set of top-scoring words from the
most prevalent topics. That's exactly what
these four columns on the left are showing. The third thing you might surface would be the assignment of words to topics in the
abstract of the article. You get a sort of colormap in the abstract which summarizes the article that gives you a sense of the relative
topic proportion, as well as the relative
ordering perhaps. Finally, you can rank the top 10 most similar articles according to their
topic distribution. A topic distribution is an interesting alternative
representation of a document. You could match two
documents based on the overlap of keywords or
the overlap of their text. But once you've done
topic modeling, you can also represent a document as a
distribution over topics. To compare two documents
for similarity, you can actually compare
their topic distributions. That has the advantage of perhaps being a little
more semantically robust than simply comparing two documents based on
their surface texts, you might have a better chance
of matching two documents that fundamentally
discussed similar themes. If you were able to use
the topic representation, instead of relying
on the fact that keywords exactly matched in both. Here's another flavor
of topic modeling as applied to social media,
in particular, Twitter. The study used an approach
called labeled LDA, which extends LDA by
incorporating supervision in the form of implied tweet level labels where
they're available. That enables explicit models of text content that's
associated with hashtags, replies, emoticons, and so on. This work applied Labeled LDA, which is a partially supervised
learning model to map the content of a Twitter
feed into dimensions. These dimensions correspond
roughly to substance, style, status, and social
characteristics of posts. So in their paper,
they can characterize both users and tweets
using this model. In this particular example, they're comparing
the W3C account, that's a Web Consortium account with Oprah
Winfrey's account. They've divided the tweets into the four
categories of style, social, status, and substance. The substance category has to do with what
might be called the more concrete topics that
the person discusses. Status has to do with
what somebody is actively doing or not
doing during the day, or when they're
posting, or whenever. There are social
related words that an account might use to
talk about other people, or gatherings or emotions
related to other people. Finally, there's a
set of words that are more idiosyncratic or
more characteristic of that particular person's
writing as opposed to others. So with this representation, you can do different kinds of interesting navigation,
search, browsing, filtering of Twitter
content, and in general, in this short microblog
like content. To accomplish these forms of
unsupervised topic modeling, there are two general approaches
that are most popular. The first is a
probabilistic approach using generative models, where we model each
topic as being distribution over words
probabilistically, and then each document is modeled as a mixture of a few topics. The technique we're going
to look at shortly called Latent Dirichlet allocation is an example of this
probabilistic approach. Second type of approach is a matrix factorization
approach where there's no underlying probabilistic
assumption or modeling. We simply take a
document term matrix and decompose it into a small number of matrix factors, usually two. The resulting columns in a factor can be interpreted
as a topic model. We're going to look
at a technique called non-negative
matrix factorization. That's an example of
this second approach. We're going to start
with looking at the generative
probabilistic approach to unsupervised topic modeling. This section is going to be a little bit extended
because within it, I'm going to cover not just a specific
topic modeling method, but I'm going to give
you some background on generative processes
and machine learning, how they're visualized
graphically using something called
Plate notation. I'm also going to take
a brief detour to talk about specific probability
distributions like the Dirichlet distribution that are going to be important in understanding how
a technique like Latent Dirichlet
allocation actually works. The way we're going
to think about a generative approach
to topic modeling, is that we're going to imagine that there were a series of steps that we did that
created a document. Now, this way of thinking where you imagine you're generating a document according
to a certain process, may seem a little
strange at first because our goal is to be
reading the documents and analyzing them,
not creating them. But the reason that the
generative way of thinking is important is that, if you can think about a document as having been
generated by a series of processes then you can attach a statistical model
to each step in the process. Then you can in a principled way, combine these processes together
statistically to create essentially a very
rich statistical model for the content of
something like a document, and that's very important
when we're talking about genitive processes
for topic modeling. So the way we're
going to proceed, is we're going to define a few entities like a
topic model, a word, a document, we're going to
define a series of steps that allows us to view a document as having been generated
from these parts. We're not going to
worry at first about exactly what probability
distribution we're going to use for each of those
generation steps. For now, we're just
going to focus on the process itself
at a high level. So in the case of topic modeling, we imagine that we already have a set T of K topic models. Each topic model is a probability
distribution over words, so just like we saw in
some of the examples, you imagine you have
[NOISE] K lists of words, and there might be a probability associated with
each word in those lists. We have our K topic
models and suppose we're going to denote the distribution, so this might be a
word like science. There might be a probability
of 0.65, let's say. All these works together with their probabilities
from distribution, we're going to call that Beta
sub k for the kth topic. We're also assuming that there's some fixed vocabulary V of all possible
words in all topics, just to make things simple. Our goal is to view some document d as a mixture of all these
k topics somehow. You might also imagine d
to the k topics being like a bowl of words where
you'd reach into the bowl, pick a word at random according to the topic models
probability distribution, and that's the next word that you're going to
use on the document. That there will be one
simple generative model that uses the k topics to
create a document word-by-word. Furthermore, suppose we want
to generate this document d, so that it's n words long, so now we have to
think about what's the generative
process that we could write to fill those n words slots using the k topic models. Well, here's one approach. For each document d, the first thing we're going
to do is pick a vector, Theta sub d of topic proportions. You remember that topic
proportions histogram that we saw on the science
article where it had, for each of the k topics it had a relative proportion of the
importance of that topic. These would be the topics, and then there are
some proportion here. If you recall that topic
proportion visualization, what we're saying is that
for some document d, we're going to randomly pick a vector of those
topic proportions. Let's suppose we have
a set of k topics, let's say 20 and so we're going to say
we're going to pick a random vector that
has computers at 0.23. This document is going to have 10 percent proportion
of scientific words, five percent of
society and so forth. We'll pick these
topic proportions, and then we're going to
go ahead and generate one word for each of
the n word positions in d. We're going to fill each word slot by
doing these two steps. The first step, we're
going to randomly pick a topic based on the topics
document proportion vector. In this case, 23 percent
at the time, for example, we'll pick computers, 10 percent of the time we'll pick
science as the topic. Let's suppose that we pick science as the topic
for this next word, the word slot I should say. We fill the word slot by
drawing a word randomly from the corresponding
topic model for science. We're going to use the
notation Z_d,i to denote the topic that
appears in document d at the ith word position. Let's just walk through
this again carefully. To generate a document, we first pick a vector
of a topic proportions, what the document's going
to talk about in general, and then we fill each
word slot by first randomly picking a topic based
on this topic proportion. Let's say we pick society as the topic for the
first word in the document. Once we determined the
topic for the word, we'll fill that word slot with
a specific word by drawing a word randomly from that corresponding topic
model for society. The topic model for
society might have the word people might have
a probability of 0.7. Word party might have a
probability of 0.3 and so forth. There would be a pretty
simple topic model. But you get the idea. We have just defined a generative process
for creating a document d with n words over a
set of k topic models. That description of a generative
process for documents, I used words to
describe the process. There is a much
more intuitive way to describe generative
processes like that using a type of visual notation
called plate notation. You might find it very useful
to be able to interpret these diagrams
because they are very widely used in Machine Learning. In a moment, I'm going
to show you the notation graphically for
the process that I just described for
topic modeling. But before I do that, I'm going to start very simply by making this digression to show you what the most simple
generative processes look like in this notation, namely flipping a coin. This diagram on the left is a graphical representation of the generative process
flipping a coin. In this notation,
each circular node represents a variable. In the case of flipping a coin, we have a variable F_i, which is the outcome
of the ith flip. Then we also have a
variable which is a parameter theta sub h, which is the
probability of getting heads for that particular coin. The circular nodes are
variables and then variable A points to variable B. If B depends on A. Clearly, in coin flipping, the outcome of the ith flip FI depends on the parameter
for the bias of the coin. The parameter theta sub h, because clearly if we change
the bias of the coin, that'll change the nature of the outcomes for the
variable F sub I. These rectangles, or what
are also known as plates, show variables that
repeat together, and each plate is labeled with
the number of repetitions. If I wanted to show model
of N coin flips in a row, capital N coin flips, where we pick a coin that's fixed for all
the flips at the start. It's the same coin, all the flips use particular
setting for theta H, we would denote that
by this diagram so everything that's inside the box is repeated N times. We have N flips,
there are N outcomes. Then the final thing to note
about this diagram is that the variables that we
observe are shaded. In this case, we're observing the outcome of the coin flips, and for this particular problem, I've framed it so that we haven't observed the bias of the coin. We don't know the probability of getting heads for
this particular coin. We might want to
estimate that from the coin flip observations. In this particular example, the observed quantities are associated with these
shaded variables. The unshaded variables represent latent variables or parameters. Just to summarize, this diagram shows a generative process of flipping a coin where the coin determined once at the start, and then used to create a series of N flips that are repeated
using the same coin. How do you think
this diagram would change if we wanted to model the case where the coins
bias changes on every flip? That' be a bizarre case. Maybe the coin is made of ice, it's melting or something, something about the
coin is changing. Every time we flip it, maybe it's getting worn,
something comes off. In any case, how would we
model the fact that we want the bias here to
change for every flip. We'd have to move
it inside this box. If it happens every
time and time, we'd move it inside the box. Let's take a look at that case. Admittedly, it's
a little strange, but it's a good illustration of how flexible these diagrams are. You can play around with
these different crazy ideas. If you wanted to have the coins bias change at every flip, then we put it inside the plate, here now every flip, we get a different bias that leads to a particular
outcome on that flip, and that happens
repetitively and times. Then we might
imagine that there's a parameter alpha that controls the distribution
that we select, theta sub H, I from. Theta sub H, I is the bias
of the coin on the ith flip, and we need some distribution to control how that gets set. It could be uniform, it could be something else, but we imagine that there's
some other distribution that controls how this
bias varies per flip. We imagine that
there's what's called the hyper-parameter alpha for that distribution
that's outside the box. Determined once at the start, and then used to determine the variability the distribution of biases that might
happen in every flip. Now let's take a look at
the generative process that I just described
for topic modeling. Here's what that looks
like in plate notation. As a reminder, each of these circular nodes is a variable. Each variable A
points to variable B, if B depends on A. Let's walk through that
same generative process, but use this diagram as a guide. We said that to determine the topics that a
document D talks about, we're going to select a set of topic proportions
and we put that into theta sub D. That
set of topic proportions, then we use that to fill each of the N word
slots in the document. Here we go down here. This inner box, this is repeating the word slot filling process N times because there are N of those
in the document. We take the topic portions
for each word slot I. We determined what topic
should fill that word slot. Then given the topic, we select a word from that topic's topic model according to that
probability distribution. That gets us the word w d
comma I that we observe. We repeat this word slot
filling process n times. By the way, the k Topic
Models live over here, so that the selection
of a word depends on the topic that the word
is supposed to be from. Then the actual
selection of the word is also dependent on of course, that Beta sub k, the probability distributions of the individual words in
the kth topic model. Just like the previous
coin flipping example, there may be some
hyperparameters, Alpha and Theta, that
control our selection of topic proportions and then also control the nature
of the k topic model. You might be wondering
what this outer box is, this represents the
fact that there are multiple documents
in a collection. There are capital d of them. This whole process of
filling the word slots in a particular document also
happens capital D times. What I've shown you in
that plate notation, that generative process is exactly what Latent
Dirichlet Allocation is. It's a generative model where a topic or a topic model is
a distribution over words. Again, we haven't talked
about the specific nature of the distributions
we're going to use, we'll do that in a minute. For now, we're just
going to focus on the fact that we have topics which are distributions
over words of some form. Words are modeled as a sample from a mixture
model of topics. The mixture is controlled by that parameter, the
topic proportions. Each word is generated from a single topic once the
topic has been determined. Then different
words in a document can be generated from
different topics. That's one characteristic
that allows Latent Dirichlet allocation to model documents that talk
about multiple topics. Again, just to fix the notation, topics are defined by
these k parameter vectors, Beta 1, Beta 2. Those actually are the word
histograms that you saw. It's controlled by
different mixtures of topic proportions. It's controlled by
different topic proportions for each document separately. Then as we saw before, we have this z sub d i for the per word
topic assignments. You might be wondering now, why did we go through all this effort
to build up this model? Well, what's going to happen is suppose we have a big corpus of documents,
scientific articles. We've observed all the words that appear in those documents, but we don't know the values
of all of these variables. We don't have the topic models
are the topic proportions. What we're going to do is use the observed word
frequencies and documents, to estimate the variables
that we didn't observe. There will be an estimation
process which we're not actually going to
go into in much detail. But the outcome of that estimation process is going to be a set
of parameters in this generative model that best explain the words we observed
in the document corpus. When I described at
a high level the generative process called
Latent Dirichlet Allocation. I said that we
would randomly pick a distribution over topics, and then given a topic, we could pick randomly a
word from the topic model, but I didn't specify
what distribution I was using to do that
selection randomly. Now, I'm going to
specific and describe how each of these variables
is distributed. That will give you more insight into the nature of this model. Latent Dirichlet Allocation uses two main types of distributions, multinomial distribution
and something called the Dirichlet
distribution. I'm going to make a brief
digression to explain the multinomial and Dirichlet
distributions separately because they're also very widely used in machine learning, especially in applications
related to texts. Let's start with the
binomial distribution, which I think should
be familiar to you. In a binomial distribution, we have a series of n
independent trials. Each trial has two
possible outcomes. It's a binary outcome. The binomial distribution
can tell us what's the probability of observing any particular
combination of outcomes. Famous example of a
binomial distribution would be the heads and tails outcomes from flipping
a coin n times and there, the parameter theta of course
would be the coin bias, the probability of
coming up heads. We denote the outcome of flipping a coin
being distributed with the binomial distribution as this c tilde capital
B n comma Theta. There's a natural extension of the binomial distribution from two categories to k categories. Again, we have n
independent trials, but now, instead of each
trial outcome being binary, it's one of k categories, so now we can ask what's the
probability of observing any particular combination of objects from the
various categories. In this case, the
k categories could represent one of k
different words that are selected to fill the
word slot in the document. Now our vector Theta, instead of having a
probability of heads or tails, it's a vector of k probabilities
that still sum to one, but where the i-th element
is the probability of selecting an object
from the i-th category. Imagine you have
a big bag full of different words from
a fixed vocabulary. You reach into the
bag and there'll be some words that are more
frequent than others in the bag, words like the or
and for example, and there could be words in
there that are more rare. But at each step you, you reach into the bag
and pick out a word and the words are
distributed according to a multinomial distribution. Each word has a particular
probability of being selected, so when we saw a topic
model earlier example, it looks like a word histogram, each word had a probability, that's exactly a multinomial distribution in our scenario. We can denote
selection of words for a multinomial using
this notation. The Dirichlet distribution takes us a level up and allows us to specify a probability
distribution over multinomials. It's a distribution over
what's called the simplex, which you can think
of as a set of k positive numbers
that sum to one. It assumes that these k
components are nearly independent and they're not quite independent because there is this constraint that they
all have to sum to one, but it tells us, using the
Dirichlet distribution, what is the probability
of observing a given multinomial
distribution over k categories. The Dirichlet
distribution is typically denoted like this and the
input to the Dirichlet, the parameters to the
Dirichlet are in vector of k real numbers. The diagram on the right shows different Dirichlet distributions
on the two simplex, the equilateral triangle for
different values of Alpha. Think of each point in the simplex represents
three numbers that sum to one and just like you might
expect in a topic model with three words or a bag with three different possible
words in the bag, the probabilities
have to sum to one, you have to pick
one of the three, but each point in the simplex represents one particular
multinomial distribution. For example, if the words A, B, and C were all equally likely, so.33,.33,.33, sorry
for my handwriting, then this point right in the
middle of the simplex would represent the multinomial
distribution,.33,.33,.33, a simple topic model. On the other hand, what do you think this point represents here, which distribution
without represent? Well, that would
represent the case where all the probability
mass was on A, so it was 100 percent certain
chance of picking A and zero that we would pick B or C. Then you can imagine different intermediate
versions of this. The point is that every point
in this simplex represents a different multinomial
distribution over k categories. Now, this hyperparameter Alpha for the Dirichlet distribution, controls how peaked
or how spread out the Dirichlet distribution
is and that's what this diagram on the
right is also showing. It's varying the vector
Alpha in different ways. This vector at the top represents different choices for the
vector Alpha and essentially, when the vector Alpha has
lots of large values, the distribution of the
Dirichlet is very peaked, so you can think of this
like a contour map. It's very peaked here and
it's moderately peaked there, whereas if it's small, in particular, for example, in the case 1, 1, 1,
it's perfectly flat. Any multinomial distribution
here is equally likely, whereas in the
highly peaked case, the most likely
multinomial in this case would be the one where all, A, B, and C were all equally likely, so that's the Dirichlet
distribution. It's a distribution above
the level of a multinomial. What's the likelihood of selecting a particular
multinomial, particular topic model, or a particular set of k numbers
that have to sum to one? Now you have a complete picture of the Latent Dirichlet
Allocation model. You now understand what this plate diagram is just telling us about
the relationships between the variables
and you now also understand how the different
variables are distributed. The topic proportions are
just distributed according to a Dirichlet distribution
as are the Beta sub k's, the topic models and
then each topic model is Then each topic model is a
multinomial distribution. Where do the final
topics come from? Well, we have this very
elegant statistical model, we know what the
distributions are, we know what all
the parameters are, but the only thing we've observed are the words in the corpus. The Latent Dirichlet
allocation algorithm estimates the model parameters that best explain the words
that we observed. In more technical terms, Latent Dirichlet
allocation infers the posterior distribution over the latent or
unobserved variables. This posterior is a reversal
of the generative process. The algorithm is
trying to figure out what is the latent
topic structure most likely to have generated
the corpus that we observe. Latent Dirichlet
allocation estimates the posterior expectations
of these latent variables. The k topics, the pre-document
topic proportions and the topic assignment
for each word. Now, this assumes that you
would supply the number of topics k as an input parameter. We could also learn those
additional hyperparameters, alpha and eta to get some
information about the corpus. If we saw what the
values for those are, we could infer for
example something about the semantic diversity
of the documents. How much do the
topic distributions vary across documents. We could also estimate
eta which has to do with the diversity
of the topic models, how similar are the topics that best describe this corpus. Now, computing these parameters
is beyond the scope of this course but the
methods that are used are what's called mean-field
variational inference, another one is called expectation propagation or Gibbs sampling. You can read up more about those on your own if you're interested in the details. Scikit-learn has a Latent Dirichlet
allocation implementation. The inputs are a document
term frequency count matrix. The number of topics k which is the property n
underscore components. Then there are lots
of other tuning parameters that you could set but usually the
default settings are fine. If the collections you're
dealing with are pretty small, you can use the limited
form of grid search to search for the lowest
perplexity model. I'll explain that in a minute. Now, because this is a
probabilistic model, it's not entirely clear what
the meaning of passing in TF-IDF values is for
the input matrix x. In practices actually might
work but in principle, you're really supposed
to be passing in the frequency counts. Those are what would
be appropriate for a probabilistic model. The output from LDA in Scikit-learn comes in the
form of the components underscore property
and the ijth entry of the components underscore
property can be seen as a pseudocount that represents the number of times where j was assigned to topic i. I'll show how to use that
component in a minute. Now, like many of these
non-convex estimation methods, the results may vary each
time with the same input. It has to do with the stochastic nature of
the estimation process. So just be aware of that when you run LDA with default parameters. Here's an example of
how you call LDA. You import Latent
Dirichlet allocation from sklearn dot decomposition. You pass in the number of topics when you
create the object. Like all estimators you
call the fit method with your document term
frequency count matrix and then you get back the topic models in the form of this components
underscore property. Let's look at a
typical usage steps to get this actually working. You start off with
a set of documents, I'm going to use a
very small toy corpus here and you can find this example in the
notebook for this week. Each element in the list is a separate document and I've used this example in other areas. Each element of this
list is a document. We're going to take
that set of documents, use the vectorizer object. In this case, we're using
count vectorizer which simply takes the term frequency counts of each term in each document. We'll call fit transform
using the input documents that will give us a
term-document matrix, call Latent Dirichlet
Allocation with that matrix. Here's the routine for actually displaying the
topics that results. This shows how to
use the components, underscore property that gets computed during the fit method. Here we're using the GET
feature names method on the vectorizer
to get the list of all the words that are
in the vocabulary. To display a list of top
words for each topic, we simply iterate
through each topic. For each topic, we
iterate through, find the indices of the top words and then look up their feature
names and print them out. You can see that LDA, when it ran on this
little corpus, found that there were
two distinct topics. That's how we design
this toy corpus. Half the documents
approximately talk about animals and friends and the other remaining documents talk about technical
computer stuff. You can see that LDA actually found those two topics correctly. It's also handy to be able to
get topic document weight. What are the representative
documents for a given topic? You can get that by
using LDA transform. You can use transform on the original document-term matrix to get the document
weights per topic. This is an example
of showing you how to look at the output from that. In this case, there's
one column per topic and one row per document. We can see for each
of the documents. The first document scored very strongly on the first topic and not so strongly
on the second. By doing the right
sorting, we can say, "What's the document that has the highest
topic zero weight?" In this case, it was
the document here. The second case it picked this other representative
computer-related document as the highest scoring one. That's how you can get the
document weights per topic. Applying this on something that's a little more substantial, the 20 newsgroups dataset, I asked it to compute 10 topics. You can see that it found
a reasonably coherent set. This shows the term frequency
for topic number 4. We can take a look.
What are the top words? People, guns, government,
president, firearms, police. You can see this maybe has
to do with the crime bill, which at the time Clinton
was trying to get passed. Again, this looks pretty reasonable from the point of view of finding a coherent topic. If we look at some of the highest weighted
documents for the topic, it turns out to be a
White House press release on that crime bill. There's another useful tool that you might want to try out. It's called pyLDAVis. It's built to take the output of scikit learns LDA process, and generate useful
visualizations. I've given a link to it here. It does many different things. It tells you for each topic, what are the top
terms for that topic. It also shows you on the left, using multidimensional scaling,
a map of all the topics. In this case, this is showing about 50 topics and their
relationship to each other. You can study which
topics are clustered close together and which ones
appear to be more outliers. By clicking on a
particular topic, you can get this list
of salient terms. I included here an example
of LEGO topic models. I'm not going to go through this in detail in the
interest of time. I included this example to
show that the concepts behind topic modeling can go beyond
just words and documents. In this example, it's a
really great example of creative thinking and applying it to something you might
never have expected. Analyzing how the
brick colors of LEGO, as they were used in
various sets over time have changed or faded away. I encourage you to check out this blog entry and
the results for it. You'll see that the
techniques he applies to LEGO bricks are very much similar to the techniques you can see for
words and documents, except it's also a
lot more colorful. It's really cool to check out. It also runs into
the problem that anybody who uses
topic modeling has, and that is how to interpret
the topics and what to call these topics are clusters
of, in this case, bricks. We also talks about how
he came up with the names automatically for the
different color themes that he extracted
using topic modeling. It's a fascinating read and I encourage you to check it out. Now having described LDA, let's talk a little
bit about the pros and the cons of this method. While the pros include
the fact that it's very useful as a visualization
and exploration tool, it's been used to get
some real insight into the nature of, for example, scientific trends over time in journals
and things like that. It's also what I would say is reasonably efficient estimate. Although I have to say my
experience with scikit-learn, it's been relatively slow for some of the corpora that
I've tried and it's definitely not as efficient as non-negative matrix
factorization, for example, another pro is that it's easy to modify the generative
model and extend it for some specific scenarios
and there have been a lot of extensions
of LDA to handle all sorts of different scenarios like changing topic
models over time or other kinds of evidence besides just words and documents
like speech or images, for example, among
the cons for LDA are the fact that like other
topic modeling algorithms, humans have to interpret
the topics which can be not so easy sometimes. LDA itself is not
specifically designed or trained to optimize
tasks like classification. However, people have
developed extensions to LDA where they jointly optimize, the topic models as well as classification results that
used the topic models. The estimated topic vectors
come out of LDA as a result, may not be very good at
discriminating documents, so there may be a
good overall set of tools to use to describe
an explorer collection, but they're not optimized for maximal discriminative power between elements like documents. To conclude this module
on topic modeling, I'm going to discuss
three general challenges and also discuss some methods for dealing with
those challenges. The first challenge,
as I've said before is how to interpret each topic. Often topic modeling results in a few topics where
it's obvious what the interpretation is a few more where it's clear after
you think about it a bit, and often the other
remaining 50 percent are like noise topics that look random and maybe just exist as a byproduct
of fitting the model. Second question is
how did generate a topic label for these topics. Related to interpreting them, how do we pick the
best example terms or how to come up with summary
titles for topic models. The third question
is how to evaluate the quality and
effectiveness topic models, either relatively or absolute, and we'll talk about
two approaches: A data-based approach, computing the probability of a held-out data set
under the model, and a task-based approach where we use the topic
models as features for downstream task like
supervised classification. One approach to creating more interpretable topic models is something called turbo topics. Turbo topics is not
a widely known tool, it was developed by David
Blind John Lafferty in 2009. Basically what it does is
it takes an existing set of LDA unigram topic models
that use single words, and it computes a more
descriptive set of terms for each topic
model that uses n grams, two-word phrases,
three-word phrases and the turbo topics algorithm is based on something called
permutation testing. It can be used with any topic
model for which there's a latent topic assignment variable for each
word of the corpus. If you compare these
four topics that were found in the Huffington Post news
collection, for example, if you look at the standard
LDA unigram topic model, it's reasonably easy
to interpret these, but some of the terms in
them can be ambiguous, or at least there may be used in certain contexts
that's not clear, whereas if you look
at the same group of topics that had been run
through the turbo topics tool, you can see that it's
actually picking out more than just Obama. It's picking out
Senator Barack Obama. Phrases like the
Illinois Senator. Phrases like David Axelrod, like basically names of people. More than just would appear
in the list of single words, you can now actually
get people's names appearing in this turbo
topics representation, which alone makes them more precise and also other
important phrases. Tuber topics is one approach
that was developed to use multi-word expressions in
order to visualize topics. Now we turn to evaluating
the quality of topic models. In the absence of any
ground truth labels. There are two metrics that are very widely
used for evaluating the quality of topic
models with respect to their ability to model
a held-out test set. Those are perplexity
and log likelihood. Suppose we have a topic
model that's described by this topic matrix fee and
hyperparameters alpha. After you run LDA, you can score a
vectorized version of a held-out collection
of unseen documents, documents that were never
seen during training using the score method and that will return you a log
likelihood value. That is the sum over all the documents of
this log probability. Given the topic model it predicts the probability of the words
in the unseen documents, the log probability,
and then adds them all up and by varying
the topic matrix. Suppose you have different topic models you
want to evaluate, you can compute
the log-likelihood using the score function for each model on the held-out set and pick the one that has
the highest log-likelihood. The other measure that is related to log-likelihood
is perplexity. In case of perplexity, a lower number is good, but perplexity of a test set w, is simply e to the power of negative likelihood divided
by count of tokens. You can get the
perplexity of a test set w by using the
perplexity method. Higher log-likelihood is good, while lower perplexity is good. Neither of these metrics considers semantic
associations or other set quality attributes of the set of words that
are in a topic model. It doesn't measure
the coherence or the redundancy of words
in the topic model, it simply estimates the
ability of the model to predict the words in an unseen
collection of documents. One interesting note is that in a separate study on human
judgments of quality, a measure like perplexity
was not significantly correlated with human
judgment of topic quality. Humans must obviously take
into account a number of factors in judging the
quality of a topic model, like it's coherence or how specific the terms are
and things like that. Perplexity in some ways
is a simplistic measure, but it's one that is used
in many evaluations and it can be used to compare the relative effectiveness
of topic models. In particular, it can
be used to select the optimal number of topics
K. The approach here, is that you build one LDA model
for each choice of topic, and then you compare
the perplexities. Typically, the trend is that
as you add more topics, it gets better at
modeling the text, the perplexity of the
model will decrease. Perplexities scores that
are lower are better, and you can apply this to select the number of topics at which there's a
diminishing return, where perplexities
stops decreasing as dramatically
because the amount of computation that's necessary for these methods goes up with
the number of topics. You have to figure out a
trade-off between the number of topics and the overall
likelihood quality, as measured by perplexity. Actually, you can look
for an example of this in the Lego topic models article
that I've linked here. One other way of measuring topic model quality
that perhaps is maybe more connected with how
people might assess quality, is using natural language
coherence measures. There's an example of this
in one of the assignments. In topic coherence,
topic terms are theoretically more
semantically related in higher quality topics. They're not just a random
collection of terms, there's some underlying
semantic connection between the terms that makes that
collection more coherent. There are many different
ways people have proposed to measure the
coherence of a set of terms. Could also be used to pick the optimal
number of topics K, if that's an important criterion for the final use of the topics. Here's an example on the left, this is a high
coherence topic model, because the terms have clear semantic connections
with each other, while the topic on the
right is lower coherence, because there's less of a
connection between these terms. In the example in the homework, we use word embeddings to estimate the coherence
of a set of words, but there are many
other methods as well. To conclude, some best practices
in using topic models, it's a good idea to use
stopwords and term filtering, weighting, when you preprocess a corpus for use
with topic models, because these can
have a major impact on the nature of the topics. If you leave out
stopwords for example, it may be that your
topic models have too many words that are not
semantically interesting, lots of high frequency terms, because those are what are
used in the documents. Preprocessing to filter down to a more semantically
interesting discriminative set of terms can be useful. The initialization and
parameter settings for the topic models can
make a big difference. Like many optimization
based methods, different random initializations can lead to different results. In some cases, there may be
several reasonable choices for the number of topics K
depending on your criterion, there may not be a specific
one that's the best overall, it could be that there are
several choices that work equally well for your problem depending on the needs
of your problem. Scalability; there are methods that definitely scale
better than others. As I said, I found that NMF
scales better than LDA. But most methods have in common that the runtime to compute these parameter
estimates increases as the number of
topics K increases. Finally, in terms of
interpreting output, you can consider
further post-processing to reward solutions with
more coherent topics, or you can consider using a
method like turbotopics that can take an existing
topic model solution and make it more precise
by adding phrases or more contextual
features so that it makes it easier to interpret for whatever purpose you have. That concludes our
introduction to topic modeling and specifically latent
Dirichlet allocation. Next, we'll be looking at non-negative matrix factorization
for topic modeling.