K-means is a very popular
clustering method that's also very different in nature from hierarchical
clustering. K-means has a partitional
clustering approach, which means it assigns each point to one
specific cluster. There are soft
clustering versions of K-means that we'll discuss later. But for now, the goal
of K-means is to assign for every
point in the dataset, to assign it to one specific
cluster that it belongs to. Each cluster is associated with a centroid or center point. In these diagrams,
I'm going to mark the centroid typically with
a cross or an x like this. There's one centroid per cluster. Each point is assigned to the cluster with the
closest centroid. Typically, closest is measured according to the Euclidean
distance in the space. However, there are other variants of that where you can
have, for example, a weighted Euclidean distance, or even other types of
distance measures altogether. In K-means, you
specify the number of clusters, K, in advance. That's a very
important parameter. We'll talk about how to find the optimal value for K later. The basic K-means algorithm
is really simple. You start off by selecting K, the number of clusters, and then you select K points
as the initial centroids. These initial centroids can be chosen completely at random. There are some heuristics for choosing initial
K points as well. Just for now, imagine you can pick K points
completely at random. Then you form K
clusters by assigning all points to the closest
centroid in the space. That will give you a
cluster membership for each of the points
in your dataset. Then with those memberships, you'll recompute the centroid of each cluster by taking the centroid of all the
members of the cluster, and you do this repeatedly. You go back to the
step where you form K clusters again by assigning all points to the
closest centroid. You repeat these steps, you're forming clusters,
recomputing the centroid, forming K new clusters, recomputing the centroid again, and rather special thing happens, which is the centroids, will converge to a
stable solution. We'll talk more about
that later as well. Here is how the
iterations proceed in the case where K equals 3. It's a little difficult to see, perhaps depending on the
colors on your monitor. But there are three clusters being asked for here with
three different colors. These points here are in red. These points here are in blue, and these points
here are in green. Now, in the initial iteration, we simply picked
these three centroids at random. Completely at random. So they just happened to
be here in the space. Then the second step for each point in the
dataset, for example, this one, we assigned it to the centroid that
happened to be the closest. All these points here, the closest centroid is this one. So we assigned it to
the blue cluster. Similarly, all these points, the closest centroid is that one, so they get assigned
to the green cluster, and likewise for the red. Now, we have cluster memberships
for all these points. Let's take a look at the
green cluster in particular. In the next iteration, with these cluster memberships, we recompute the
cluster centroid. You can see on the left here, the initially randomly
chosen centroid was up here. Now, we're going to
compute the centroid of all the points that were
assigned to that cluster. You can see that it will clearly shift to the middle of this blob. In iteration two,
we'll show that here. We do the same thing
for the other clusters. You can see that the
blue points here, the initial centroid was here after the cluster
membership was decided. We can take the centroid of those newly minted
members of this cluster, and compute the centroid
of all the blue points, and that moves here, and likewise for the red cluster. Iteration three, again, with the centroids being
positioned where they are, you can see that we assign cluster membership
based on these new centroids. You can see that all of these red points that
were originally labeled here based on their distances to the new versions
of the centroids, there's a slight change in cluster membership so that now, the red cluster is
shaped like this. For every point in
the red cluster, the red centroid is
the closest one. Likewise for green and for blue. Again, for the next step
we iterate the process. Once cluster memberships
have been decided, we then take the centroid
of these new points. You can see this
is going to shift slightly here, which it does. We repeat this about
five or six times, and it settles into
a converged state where you can see that
it actually found what we would probably consider
reasonable clusters. It found three centroids
where labeling each point according to the closest
centroid results in three fairly
separate clusters, and it's magical
that this happens. There mean there is some solid mathematical reasons
why it happens, but it's cool that it just is able to find
these clusters with such a simple iterative algorithm using no human provided
labels at all. So we'll talk more
about why that is, and it's connected to something
called the expectation maximization algorithm that we'll discuss in a later lecture. Let's review the steps that we just illustrated
with the example. You pick k, the number of
clusters you want to find, and you start by picking
k random points to serve as an initial guess
for the cluster centers, and then in step A, you assign each data point to the
nearest cluster center. In this example here, we going to draw a line that's
equidistant between these. So all these points will get
put with the red cluster, and all these points will get put the blue cluster and so forth. Then after you've assigned cluster membership
to all the points, you update the centroids based on those
cluster memberships, and typically the centroids
will shift at first and then converge to a stable solution giving a clustering solution. Now let's look at an
example and see if you can predict what the outcome of
the next step is going to be. Here we have budget points, we have three randomly
chosen cluster centers, and cluster membership has already been assigned
to each point, most of the points are closest
to the green centroid, so they'd been colored green, but there are a couple
of points here, that are closest to
the blue centroid, so they got colored blue. Where do you think in
the next iteration, where we shift the
centroids so that we've taken the centroid
of each clusters members, the centroids will end up? I will give you just a
minute to think about it. Well, there are no
points colored red, so this centroid is
not going anywhere. There are two points
colored blue, so where is this centroid going? What's the centroid
of these two points? It's going to be right in
the middle between them. We're going to expect that
this centroid will shift to a new centroid position right there between the
two blue points, and we'll expect that
the green centroid will shift closer to the very
center of this cluster. We'll expect that the green
centroid will shift slightly so that it's more in the center of all of
those green points. Let's see if that turns
out to be the case. That's exactly what happened. Once we've shifted,
the centroids, we're going to reassign
cluster membership according to which cluster
centroid is the closest. So all the points that
previously were labeled green, now they're closest
to the blue centroid. You can predict what color
these will be turning. Likewise, these points here are now going to be closest
to the red centroid, so when we update
cluster memberships, that's exactly what happens. Now, can you predict what the next iteration
is going to do? It's going to shift
the cluster centroids, based on the memberships
that we just assigned. Here we have all the blue points. What's the centroid
of the blue points? It's right here. What's the
centroid of the red points. Well, the red points
are all over here, the centroid of that is going
to be somewhere over here, and what's the centroid
of all the green points, it's going to be
somewhere in the middle. We take the next step, and sure enough, centroids have shifted, and after we repeat this process for a
few more iterations, the centers converge
to a stable solution. Again, using a very simple
iterative approach, we get something that
actually looks like a reasonable solution for
these three clusters. k-means is implemented
in Scikit-learn using the k-means class in
Scikit-learn dot cluster. Here's an example of
taking three random blobs, applying k-means to them, and of course we
have to tell it in advance that there
are three clusters. But like all the other
estimators in Scikit-learn, we used the fit method to estimate the
parameters of k-means, which are the cluster centers, and you can get the
output from k-means, which will give the final cluster membership assignments to all the points using the
k-means dot labels underscore. Don't forget the underscore. Underscore means that it was
computed during the fit, so what's happening
inside the fit method, when you call the for method, it's going to do those iterations that
we just walked through until the solution converges. Then after it's convergent, it stopped the final
cluster assignments. In this case there
are three clusters. Each data point will have
a cluster assignment. If we call this
cluster 1, cluster 2, and cluster 3, each
of these points, we'll have a label. These will all have threes. These points will all be
assigned to label one, and these points will all
be assigned label two, and that's what's inside the K-Means dot
labels underscore. There's one label
for each data point, and it's assigned to the cluster that that point belongs to, after the algorithm
has converged. One distinction
that we should make here is between clustering algorithms that can predict which cluster new data points
should be assigned to, and those that cannot make
those kind of predictions. K-means supports
the predict method, and so we can call the fit and predict methods separately. With k-means, you can predict a cluster that a new
point will belong to. Other methods we look at like
agglomerative clustering. You have to perform the fit and the prediction
in a single-step. Here's the output from
the notebook code showing the results applied to
the fruits data-set, where we know the value
of K ahead of time. Note that k-means
is very sensitive to the range of feature values. If your data has features
with very different ranges, it's important to normalize. For example, using
min-max scaling as we did for some
supervised learning methods. K-means clustering in
the examples I showed seemed to work flawlessly
and predictably. In practice, there
are some issues you need to be aware of, because we can initialize K-means using any random set of k points. It's very important to know that different initializations can result in
different solutions. Will discuss the reason for this, when we look into the expectation
maximization algorithm. If you choose a different set of initial random centroids, the results you get, the clusters that
are produced can vary from one run to another. Typically what people do is they'll do multiple
runs and then they'll pick the clustering that resulted most frequently from
the multiple runs, let's say 10 or 20 runs. Another assumption
we have made here, is that all the features
are continuous values. In the default k-means, the centroid is typically the mean of the points
in the cluster. But if the features are
not continuous values, for example, it doesn't make sense anymore to take
the mean of something. Instead, the center must be an actual representative
data point and one variant of k-means
is called K-Medoids. It's very useful when
the mean of a feature isn't defined or
is not available. Finally, the closeness of points is typically measured
by Euclidean distance, but could also be
a measure such as cosine similarity or correlation or any other valid
distance measure. For commonly used
distance measures like the ones we discussed here, K-means will converge,
and typically it will converge in a small
number of iterations. Usually the results
often improved only slightly after the first
10 or 12 iterations. Sometimes the stopping
criterion is used until relatively few
points change cluster. The complexity of k-means is on the order of n times
k times T times d, where N is the number of points, k is the number of clusters, t is the number of iterations, and d is the number
of attributes. There are many variants
of k-means that use heuristics to improve
its efficiency, especially for very
large scale problems. Let me give you an example
of how k-means clustering can give different results
for the same set of point, depending on how
it's initialized. In this example we
have three clusters. I've labeled the initial original correct point membership with different colors. One initialization, K-means,
might find this solution. It thinks there's
one cluster here, one cluster here, and
one cluster here. This is something that it
arrived at by following that iterative procedure
that we looked at it before, until it converges. That's a perfectly valid
converged clustering solution. But it's clearly not
the optimal clustering. But the different initialization K-means would find this solution something that was much closer, to the optimal clustering. Again, this is a scenario that's quite frequent with
k-means and shows how sensitive the results can be to how you pick the
initial centroids. Here's another example.
I'm going to iterate, and you can see as we increase the iteration by iteration five, iteration six, it's
converged to this solution, and that was the one on the right from the previous slide. On the other hand, if we pick different
initial centroids, so in this case, the centroids randomly got assigned
here, here, and here. We're going to run exactly
the same algorithm, and you'll see that
as it steps through. You can see that it's found a very different clustering
mean, the first case. This is why multiple runs of
k-means are done in order to understand the solution
space of clusterings, that different
initializations can find. It turns out the k-means is a special case of more general
model-based clustering, where we assume
that the data were generated from k
probability distributions. The goal is to find the
distribution parameters. This generalizes k-means
because the goal of simple k-means is to
find the k cluster centers. In the more general case, we're also interested in
estimating the covariances, and this is what we call
Gaussian mixture modeling. Again, the assignment of
points to clusters is unknown, so we have a case
of incomplete data. That's exactly the type of situation where an
algorithm like, expectation maximization
can be brought to bear. We'll look at EM in detail
and its connection to k-means in a different lecture. In this more general
model-based clustering, the output from the clustering
is not only a set of the estimated
distribution parameters, it's probabilistic. We have
what's called the soft assignment of points to clusters, where each point is assigned a probability of
being in a cluster, and the probabilities sum to
1 across all the clusters. You can see that
the simple k-means is a special case where
probability of belonging to a specific cluster is one and the probability of belonging to the other
clusters is zero, that's called a hard assignment. Some general
guidelines for finding good optimal k-means
clusterings: Be careful about where you
place the initial centroids. One strategy is to place the first center on a
randomly chosen data point, but then place the
second centroid on a data point that's as far as possible from the first center. As you place
successive centroids, you place the j_th centroid
at a place that's as far as possible from the previous one
through j minus 1 centers. As I mentioned before, the second idea is that you
do many runs of k-means, each from a different random
starting configuration, and pick the highest
quality clustering solution that results from this set. Because each k-means clustering is defined entirely
by its center point, they can only capture fairly
simple types of clusters. K-means clustering tends to work well when the data
points form into groups of roughly
the same size with simple globular shapes
that are well separated. K-means will tend not to do
well if the data forms long, irregularly shaped
clusters, for example like for this example
on the right. If there was a cluster
that was a circle shape, combined with clusters that were different sizes
and locations, different densities,
and so forth, k-means doesn't do
well with those cases. You might be wondering, how do you decide whether
you apply k-means clustering versus
hierarchical clustering in a given situation. One guideline is that
hierarchical clustering is appropriate when there is an inherent tree
structure to the data. For example, if you're a clustering genomic data representing the
evolution of a virus, for example that will naturally
form tree structures. K-means clustering does not
assume a tree structure, so it would not be an appropriate
fit for that scenario. One strategy is to apply dimensionality reduction and
examine the cluster shapes. Are you looking for
similar spherical cluster, in which case you'd apply k-means or are there
longer chains, which case hierarchical
agglomerative might be more suitable? K-means preferred solutions where the clusters are of similar size. If you do dimensionality
reduction and you see that there are
very different cluster sizes, shapes, and densities
that can confuse k-means. Any kind of cluster
geometry that's more complex or where there are outliers can also
confuse k-means. You need to be able to
specify and test for a good choice of k
in your scenario. Another question is
whether you need to easily interpret the
clusters that result. In that case,
hierarchical clustering may have an advantage. Do you know the right
k ahead of time? If not, hierarchical clustering can help determine
the right number of clusters based on the
distance threshold that you think is
acceptable between groups. There's nothing
preventing you from using both methods together, so you can try several
hierarchical methods and see which gives the most
interpretable clusters. Then you can use k-means with a hierarchical cluster
centroids as starting points to clean up the
hierarchical cluster. That concludes our
overview of k-means.