In this module will look briefly at
a clustering method called DBSCAN, which is an acronym that stands for density based spatial clustering
of applications with noise. It works very differently
than the hierarchical and k-means methods we saw earlier. One advantage of DBSCAN is that you don't
need to specify the number of clusters in advance. Another advantage is that
it works well with datasets that have more complex cluster shapes. It can also find points that are outliers,
essentially noise, and that shouldn't reasonably
be assigned to any cluster. DBSCAN is relatively efficient and
can be used for large datasets. So how does it work? The main idea behind DBSCAN is that
clusters represent areas in the data space that are more dense with data
points, while being separated by regions that are empty, or
at least much less densely populated. The essential idea of connected
components comes from graph theory. The connected component is a component in
which any two vertices are connected to each other by paths. And are connected to no additional
vertices in whatever the containing graph is connected. Components can be computed efficiently in
time linear to the number of vertices or edges using a breadth-first or
depth-first search. There are three main steps
to the DBSCAN algorithm. All points that lie in a more dense
region are called core samples, and these are shown here in green
as an example on the top. The definition of which points
are considered core points by DBSCAN is controlled by two main parameters,
called min samples and eps. For a given data point, if there are min
samples, other data points that lie within a distance of eps, then that given
data point is labeled as a core sample. In this diagram we've shown
the eps distance as a circle around some of the data points. So here's the eps parameter, and
it has this radius, as you can see here. And we've drawn an eps radius around some
of these points, like this point here. And then all of these core points. So once its set of core points is found, all samples that are considered
closely connected neighbors for the core points are put
into the same cluster. In addition to points being categorized
as core samples, points that are within a distance of eps from the core points,
but don't have enough close neighbors to be considered core points themselves,
are termed boundary points. So here's an example of a boundary point. And these are shown in
the diagram labeled in yellow. And you can see this is
a boundary point because it's within eps of this core point, but it doesn't have any other
neighbors in that region. Finally, any non-core points that
don't end up being close enough to any cluster are labeled as outliers or
noise. And these are the ones in the diagram
like this one, and that one, and this one in bottom left there,
shown in red and label as noise. Here's an example of DBSCAN
on a sample data set. As with the other clustering methods, DBSCAN is imported from
sklearn cluster module. And as with agglomerative clustering, DBSCAN doesn't make cluster
assignments for new data. So we use the predict method to cluster
and then get the cluster assignments back. Here we've used the make blobs
function to sample 25 random points. And by default it will generate this
data using three cluster centers. The output from DBSCAN is
stored in this cls variable. You can see the cluster
membership values that DBSCAN has assigned in the bottom line. So when we dump cls, we can see this
array, which represent the cluster membership value that DBSCAN has
assigned to each of the points. And so there are 25 points and
there are 25 cluster membership values. So DBSCAN will assign a point
either to one of the clusters or a special noise category, in which
case the cluster ID is a negative 1. So all of the points that come back
with a cluster ID of negative 1 are considered by DBSCAN to be noise. And you can see that those three correspond to these three
points in the diagram. And we can see that DBSCAN did indeed
find three different clusters. So one obvious question would be, how do you set these important
parameters like eps an min samples? Because as you can imagine,
having the right settings for these can be critical to
getting reasonable clusters. For example, if you set the value of
eps to something that's much too small, that all the points in your data
set will be labeled as a- 1, they'll be labeled as noise or
outlier points. Because with a very small value of eps
nothing will be within that neighborhood. Conversely, if you set
eps to be too large, all points would be considered
neighbors for all of the other points, and so they'll all be put into
the same large single cluster. So here's one approach that
I've used to assess what some approximately good values are for
eps and min samples. So it hinges on the idea of looking at
a sample distribution of the distance values that are actually present
between points in your data set. So for example, you can sample
10% of your points, and then for each of those points, you can compute
the k-nearest-neighbor distance to, say, using a small k, say k equals 1,
find the nearest neighbor. How far is nearest neighbor for
each point for k equals 1 or 2, let's say. And you do that for other points, you'll
get a distribution of distance values. And then you can say, take the median
of this distance distribution and then set eps to some small multiple of
the median of your distance distribution, like three times. And then, given that setting, you can set
min samples to some small value like 5. And then you can do a validation pass, or
simply try increasing multiples of eps and adjusting min samples to kind of see what
the effect is on the cluster structure. So DBSCAN has some advantages and disadvantages compared to
the other clustering methods. A big advantage is that you don't need to
specify the number of clusters in advance, unlike a method like k-means. DBSCAN also works well with datasets
that have more complex cluster shapes because it doesn't have
any built-in distributional assumptions about the underlying data,
like k-means does, for example. DBSCAN is also great for finding noise
points that are outliers that shouldn't be reasonably assigned to any cluster. Some clustering methods attempt to
assign every point to some cluster, but DBSCAN will have the special noise
category that it can assign to points that are outside any cluster. It's also relatively efficient, and actually works pretty well for
large datasets. Some of the disadvantages include
the fact that it can be a bit slower than k-means or agglomerative clustering for
large datasets. In some implementations, the clustering
is not completely deterministic. Membership of the boundary points, for
example, can vary between runs, and it's not usually a big problem. The quality of the clustering depends
heavily on the distance measure used, as I mentioned. So that's why it's particularly critical
to choose a meaningful distance threshold for eps. DBSCAN doesn't work well with data
that has large differences in density. It's hard to find a single min samples, eps set of parameters that works well for
all clusters in that case. However, DBSCAN is a valuable member of
the scikit-learn clustering toolkit, and we'll investigate some of its uses in
real applications during this course.