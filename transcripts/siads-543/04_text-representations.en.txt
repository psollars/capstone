One of the big challenges in working with natural language, is the sheer variety of
different flavors that it has. It has differences in style, vocabulary, the amount of
noise, how it's formatted. There are often no
predefined features or internal formatting rules. The contexts in which the
text appears can be very, very different as the
examples on this slide show. You can have news articles, you can have articles
on social media, you can have text messages which have basically
their own language, scholarly articles, you have
email, the list goes on. In order to apply tools
like topic modeling, we're going to need some good
text preprocessing tools, because as we'll see, the nature and quality of the text preprocessing can really affect the quality and accuracy of these text
processing algorithms. Our goal is to use a topic modeling algorithms
in Scikit-learn. Well, those algorithms
expect as input, some numeric matrix that describes the documents and the terms that are
in the documents. The numeric matrix
typically has as its rows, the different documents
and as its columns, all the different
terms that could appear in those documents. Then each cell of the
matrix has a statistic that describes something about how that term occurs
in that document. But to get there, we have to somehow go from
a corpus of raw documents. We might have a list of tweets or we might have a database
of news articles. Somehow, that natural language in raw text form has to become
a document term matrix. Now fortunately, that
step is provided for us with a single class
called a vectorizer, that we'll discuss very shortly. But inside the vectorizer, there are a lot of
different steps that happen to process the text from its raw form into a numeric form that's
suitable for scikit-learn. I'm going to walk through each of these steps to
describe what it is, because all of these
have some form of parameter in the vectorizer class that controls their behavior. As we'll see later, these specific choices of these parameters that
affect how this pipeline is run can really affect the overall quality and
accuracy of your results. It's very important to understand what each of these steps does, and how changing the parameters changes the behavior
of that step. The first step in this pipeline
is called tokenization. The job of tokenization
is to take the raw text in its original form and split
it into individual tokens. Now a token is
supposed to represent some unit of meaning, and typically this
will be a word, or it could be a short phrase. In English, the way
that we obtain tokens often is by splitting
the incoming text, on white space or on
certain punctuation. Here's an example where I
took this news headline, "Breaking, the US
stock market is up 3 percent in AM trading." Using the traditional rules
for tokenizing in English, we break it up on
spaces between words. We might also eliminate some special punctuation
along the way. If we do that, in this case, we would go from this raw
text here to this series of individual words that result from breaking the original text
on these spaces here. If there are multiple spaces, then we might just treat
them as a single space. That's great for English. But there are many other
languages that have much more sophisticated rules for determining what
the tokens are. In particular, many
Asian languages, determining what a
word is, for example, can involve a fairly
sophisticated set of rules. Some characters that may behave as punctuation
in some contexts, may not be punctuation and
other contexts, for example, the pound sign can mean very different things
depending on whether you use it in an inventory description
or a tweet, for example. There are a lot of
context specific rules, that might determine how
text is to be tokenized. The second step involves what's called character
normalization. In this step, we take each token and then do processing on the individual characters
to convert them into a form that's called
a normalized form. Where words that have different surface forums
like for example, a word that starts
with a capital letter versus a word that starts with
the same lowercase letter. Essentially in many contexts
they mean the same thing, so they're collapsed into the same share token by
using normalization. We don't necessarily want
to treat them differently, depending on whether they
start with a capital or not. Now of course, there
are also cases where we do want to treat
things differently. But a lot of the time, simple tokenizers and text
processing pipelines will make simplifying assumptions
about the texts that may be inaccurate for some cases. But hold as a general
rule and so on average, the rules are reasonable. One of those rules is that all uppercase letters
get converted to lowercase in
English, for example. That's the simple form of
character normalization, and that's why it's also
called case conversion. For example, if we
took our list of tokens from the previous example, we might in the second step, convert breaking with a capital B to breaking with a lowercase b, and do that for
these other words. Now this is fairly
simple in English, but character normalization
can cover cases that go beyond just converting
upper to lowercase. There are also issues
of how to handle the many languages
that have accents. A French word like per say, how do you want to deal with
the accented e at the end? Should that be mapped
just to regular e or not? You can imagine many cases
where mapping an accent to an unaccented character
would result in ambiguity. What you'd map words
that should be different to the same normalized word and that might not be correct. Any sort of letter with an
accent you have to figure out, should this be normalized to a regular 'a' or not, and so on. For now we're just going
to assume the simple case of case conversion as
our normalization step. But you should definitely
be aware that the rules for case conversion and
character normalization can be much more complex
depending on the language. The third step, after tokenization and
character normalization is called stopword removal. A stopword is a term that's very common in a particular
collection of documents. In English, for example, typical stop words
are words like 'the', 'of', 'a', 'an', 'but', etc. The assumption that
underlies stopword removal, is that these kinds
of very common words carry little semantic
meaning and therefore, because there are so frequent, we can greatly shrink the size of a corpus that
we have to process by removing all of these very
common words because they take up a high percentage of the terms that
appear in a corpus. Now of course, this
assumption is not correct, and in fact over time, the trend in search systems for exam has been to do less
and less stopword removal. For the simple reason that these apparently
innocuous words can actually carry greater
weight in meaning. For example, if you
were to search for office versus the
phrase the office, which is, for example, a television show versus
office the software. But we're going to
introduce it as a step for some of these corpora, because for some of the tasks
that we do, it makes sense. The other thing about
stopword removal is that it varies
greatly with domain. The same set of stop words that things that might
be very frequent in one corpus may not be at all frequent in another
corpus and vice versa. For example, if you
had a biology corpus, the word biology itself, because it appears in every
document, every article, maybe a stop word for purposes of searching
in that collection. But it may be a very
important word in a very different collection
on social media, for example. Another type of stopper
that gets removed sometimes are very short-term. Any terms that are just
a single character sometimes are considered
as having little meaning, and so those are also removed as part of
stopword removal. If we look at our tokenized and character normalized
case converted things that are remaining from
the previous two steps, then you can see that in this example we've removed
the stopwords 'in', and in this case they
our stopword list also had the word 'up' so 'up'
is being removed as well. The stopword lists that are used to decide if a word
should be removed or not, typically come with software
that processes the text, and those stoppered lists
themselves are derived based on a statistical analysis of how frequent terms
are in the corpus. The next step in
the pipeline after stop-word removal
is called stemming. Stemming relates to the
actual meanings of the terms, and what it does is it normalizes words to
their root form. If you look at the example on the right, the words thinks, thinking, and thinker would
all map to the stem, think. Similarly, if you look at the first three
examples here, argue, argument, and arguing would all map to this stem form, argue. In some cases, stemmers
can make mistakes. Depending on the
stem you're using, the stemmer might
decide that because this word ends in an S that
it's a plural form of argue, and so it would map to the same stem as the
other three examples. But it's an important
step sometimes because these different variants semantically are highly related. For example, if you
are searching through a collection and
you happen to type, thinking instead of thinks, then you would expect
both of those to be mapped to the same thing
or to match essentially. Stemming allows
different variants of the same word to match. It's not included
with scikit-learn, it's natural language function. To obtain it, we can
use packages like NLTK, which is a natural language
toolkit for Python. In our example of
token processing, in this case what
the stemmer might do is it might normalize breaking to break and
trading to trade. The next step after
stemming is called min-max frequency-based
term filtering. You can think of it like a more refined version of
stop-word list removal, or at least gets a
different mechanism to achieve something similar to
what stop list removal does, and that is to prune
away terms that appear either very frequently in the corpus or very infrequently. The purpose of pruning
very infrequent terms is that they may be noise terms, unusual terms that
don't contribute much to the meaning
of a document. There are two ways you can specify min-max frequency
paste filtering. You can either set the
parameter to ask for a certain absolute
number of times that a term can
occur in a corpus, or you can specify it as a percentage of all
corpus documents. Finally, after these preceding
text processing steps, we're left with a set
of tokens that will be used to create the final
document term matrix. This final step is
called vectorization. Now, as we're processing
documents in corpus, we accumulate a dictionary of all the unique terms
that we've come across during the processing
and we accumulate these as we're processing
the documents. When we're finished
processing the corpus, we're left with a
dictionary that contains all the unique terms that survived the different kinds
of prunings and processing, and that set of unique terms
is called a vocabulary. Now, using this dictionary,
this vocabulary V, we can associate with
each term in a document, its index into this vocabulary. Once we've processed the corpus and we have the vocabulary, each term in the vocabulary
has a unique number that identifies its position in
the vocabulary, its index. For example, in the
vocabulary that was created during processing
of news articles, it might be that the
word break maps to index in the vocabulary of 435, the term U dot S dot maps
to index 7,893 and so on. The process of
vectorization creates a sparse vector that has dimension the size
of the vocabulary. For example, if there were
10,000 unique words that we came across that formed the vocabulary for a particular
collection of documents, then each document would map to a sparse vector with
10,000 dimensions, one for each possible term in the vocabulary that could have occurred
in the document. Of course, most words don't
appear in most documents so the vector D that describes a document is mostly zeros because it doesn't contain most words
in a vocabulary, it only contains a few. If a document d, for example, has five unique terms in it, it might have five elements that are non-zero in
its sparse vector. This simple representation of a document is called
the bag-of-words model. The reason it's called a
bag-of-words model is because, when we store the information about the term being
in a document, we don't record the position of the term or its relative
position to other terms, we simply record that it exists in the document somewhere. In effect, we are throwing all the words in a
document into a bag, all jumbled up together into a single vector that
forgets exactly where in the document and near what other words this
particular term occurred. Now the question next becomes, we have a sparse vector, what exactly are the
elements in the vector? I had said before that each entry in that 10
thousand dimensional vector represents some statistic about the occurrence of a
term in the document. Well, there are quite a
few different choices for what you can actually put into the vector as
far as a numeric value. The simplest choice is to simply record the presence or
absence of a term in a document by making the
entries in that vector binary, where zero means the term did
not occur in the document, and a one means that
the term did occur. Now, this is a
simple but it's also a very limited representation, it doesn't record at all how many times the term occurred in the
document, for example. We can refine this
representation a little bit by recording instead
of a binary number, an integer, that's
zero or greater, that records how
many times the term did occur in a document. This is called the
term frequency or TF. This simple representation of a document is called
the bag-of-words model. The reason it's called a
bag-of-words model is because, when we store the information about the term being
in a document, we don't record the position of the term or its relative
position to other terms, we simply record that it exists in the document somewhere. In effect, we are throwing all the words in a
document into a bag, all jumbled up together into a single vector that
forgets exactly where in the document and near what other words this
particular term occurred. The question next becomes, we have a sparse vector, what exactly are the
elements in the vector? I had said before that each entry in that 10
thousand dimensional vector represents some statistic about the occurrence of a
term in the document. Well, there are quite a few different choices
for what you can actually put into the vector
as far as a numeric value. The simplest choice is to simply record the presence or
absence of a term in a document by making the
entries in that vector binary, where zero means the term did
not occur in the document, and a one means that
the term did occur. Now, this is a
simple but it's also a very limited representation. It doesn't record at all how many times the term occurred in the
document, for example. We can refine this
representational a little bit by recording, instead of a binary number, an integer, that's
zero or greater, that records how
many times the term did occur in a document. This is called the
term frequency or TF. This TF based representation of sparse vector is used quite commonly in
machine-running applications, especially with
probabilistic models. The third even more
refined type of term weight is called
the TF-IDF term weight. Now, this multiplies the
term frequency of a term by an additional factor called the inverse document
frequency or IDF. Essentially what IDF does, is it captures the rarity
of a term in a collection. So if a term only occurs in a few documents
in the collection, it's IDF value will be larger, whereas if it occurs in many
documents in the collection, it will have a fairly low
IDF value close to zero. By multiplying TF by IDF, this term weight captures both
the notion that a term is important in a
particular document because it has a
high term frequency. It's also in some sense
an interesting term, it's a more unusual, a rare term if it has
a high IDF value. It has, it's a high
Information term as well as being important
for that document. TF-IDF turns out to be an extremely powerful and
widely used term weight for many different text
processing applications. How is the bag of words model implemented in scikit-learn? Well, transforming
a list of strings where each string
corresponds to a document into this numeric
document term matrix is easy with various flavors
of the vectorizer class. In particular, for the term
frequency bag-of-words model, we can use the count
vectorizer class. It takes as input
a list of strings, each of which corresponds
to a separate document. We used the fit transform method to tell scikit-learn to
process the list of documents. The output is exactly what
we want, it's a sparse, NumPy 2D array where the rows correspond to documents and the columns correspond to terms, and where each cell in this
matrix is filled in with the count of how many times each term occurs in the document. Here's a simple example. I have a very simple set
of training documents. All you need to do is you need to import count vectorizer, you tell count vectorizer what parameters you want for the text processing pipeline
that we saw earlier, I'll talk more about
that in a minute. Then once you've
created the vectorizer, you call fit transform, you pass it in a list of strings, and the output is a matrix, it's a term-document matrix. It's incredibly simple to
do this text processing, this pipeline with scikit-learn. Once you've fit the
vectorizer with your corpus of documents
that you passed in as input, there are a number
of useful properties or methods that you can
apply with the vectorizer to get useful information about
the vocabulary and how it corresponds to the columns
of the document term matrix. First of all, if we
look at the output of the vectorizer after
the fit transform, we look at the
document term matrix, we can get the shape
and we can confirm that the shape of the
document term matrix is six rows and 26 columns, six rows for the documents
and 26 columns for the terms. In fact, we can dump the
first row corresponding to document zero and we can see that the very
first document vector, the first row of the
document-term matrix is a sparse vector and it has
entries in columns 7, 8, 10, 14, 12, 6, and 16, the entry in that cell, you can see in this column here. As an example, we can see that the first document
contains a word with ID7. Now, how do we figure out what
term that corresponds to? Well, it's very simple
with get feature names. Get feature names returns a list of all the terms
in the vocabulary, in this case there are 26 terms. If you call get feature names
and get back that list, then you apply this
index of seven. You look up the elements seven
in the list, you will see, let's see 0, 1, 2, 3, 4, 5, 6, 7. You can see that the first document has as its
first term entry for a dog, actually, it's the
second term entry. It occurs three times in the
first document and so on. The second very useful property that gets computed during fit, as denoted by this underscore
at the end of the name, is the vocabulary property. The vocabulary property allows you to go in the other direction. With get feature names, if you give it as input, the term index or
the column number, it'll give you the string that
corresponds to that term. But if you have the term as a string and you want to
find out what the index is, then you can use this
vocabulary property, which is a dictionary. I have an example down here where we want to find out what is the column that contains the term counts
for the word cats. We can simply look up cats in
the vocabulary dictionary, and it will return
the number one. Meaning that column
zero is the first one, and column one is
the second column, and that's the one that
corresponds to cats. Similarly, we can look up the term monitor
in the dictionary and find out that it
corresponds to column 15. So with these two methods, we can access term
information either using a string or
using a term index. Just as we could create a CountVectorizer to
store term frequencies, we can create a
Tfidfvectorizer class to store tf-idf weights instead of term frequencies in the
document term matrix. Its usage is identical
to CountVectorizer. You pass a list of strings, the list of the documents, you call fit transform. The output is still a
document-term matrix, where the rows
correspond to documents, and the columns
correspond to terms. But the difference is that
whereas CountVectorizer stores simple term frequencies
in the table cells, Tfidfvectorizer stores tf-idf
term weights in the cells. Because for things like
text classification, tf-idf term weights
are usually a better, more effective
representation for text. We'll find ourselves using
Tfidfvectorizer quite frequently during the course of our assignments and
some of the examples. I had mentioned at the
beginning of this lecture, that this text pipeline
had many steps, and each of the steps had a number of parameters
that you could control. Well, the way you
control them is by passing them into the
vectorizer object. There are a lot of different
parameter choices, and I'm only going to cover a few of the most
important ones here. You can look at the
Tfidfvectorizer or other vectorizer documentation
for further details. I'm going to show you
some examples here of key methods like, if you want to manage the total vocabulary size, for example, for performance
reasons, you can set the max features property
of the vectorizer. What that will do is it will look at all the
vocabulary terms, see how often they occurred, and then take the top
max features number of them for inclusion
as the vocabulary, and it will drop everything else. The lowercase option controls
what you might expect, which is whether
or not it converts everything to lowercase or not. If it's true, it will
convert to lowercase. If it's false, it will keep words capitalized, if
they're capitalized. Here's an interesting parameter,
ngram underscore range. What it takes as an argument, is a starting and
ending range for the length of the phrase that should be considered
a term in the vocabulary. So if you were to
specify ngram underscore range of one coma one, that would tell the vectorizer to just keep single
words as features. Here, I've set it
to one coma two, which means to
include single words, but also two-word
phrases as features. That can be a really
important consideration for some kinds of text
classification problems. The min_df and max_df properties control what I described earlier as the term
filtering step, and again, as in the min_df case, you can specify an absolute
number of documents. That is the minimum number that a term can have before
it's eliminated, or you can specify
things as a fraction, as a percentage, so 0.95. In this case, if a
document term occurs, 95 percent of the documents or more, it'll get eliminated. So this sets lower
and upper bounds for the document
frequency of a term. So again, this is in
the number of documents or the fraction of documents
in the collection. Finally, the stopword
behavior can be controlled by
passing in a language. There are a number
of languages that have built-in stopword lists. You also have the
ability to pass in a custom stopword
list if you need to. These are just a few of the
many options available to control that text pipeline that's inside of the vectorizer class. In one of the exercises
that we have this week, you'll see how small changes
to these settings can make a big difference in the
text classifier performance.