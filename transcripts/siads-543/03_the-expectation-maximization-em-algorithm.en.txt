Before we get into
a discussion of the expectation maximization
algorithm itself, I'd just like to give a brief
review of a simple problem, the calculation of bias for a two-sided coin because
it's going to give us some valuable context for what the EM algorithm
actually does later. There are lots of
situations where we have a statistical model and the model is trying to predict
let's say a probability. The probability
estimate of the model is based on a set of parameters. In order to learn the best
parameters for a model, there are a number of
different approaches, but one approach is called the maximum likelihood estimate to estimate the parameters. In that approach, we try to find the value
of the parameters of the model that maximizes the likelihood of the
data that we've observed. In other words, if we imagine
that the data was generated by a process described by
the statistical model, so the simple example would
be flipping the coin. There's a single parameter theta, theta controls the
bias of the coin, how often it lands on heads, how often it lands on tails. We observe a number
of coin flips and our task is to estimate for this simple
statistical model, the maximum likelihood
estimate of theta. What value of theta best explains essentially
in an informal sense, the data that we observed. We're going to look at the
example of coin flipping as an entry into understanding
the EM algorithm. Let's start with the case of a two-sided coin
and we're going to use the observations that
come from a series of flips. We could either have
heads or tails. We are ignoring the possibility that
the coin will land on its edge because these
are hypothetical coins. We're going to imagine that there are only two
possible outcomes. We're going to flip a
coin a number of times and let's pretend we don't
know how biased it is. We don't know if it has got
a fair 50-50 parameter, 50 percent parameter
for theta or whether it's skewed towards
one side or the other. To start, we're going to
see a series of flips and we're going to look
at whether the flip comes up heads or tails and we're going to count
the number of heads that we see as well as the
total number of flips. Those are our observations
and now we want to figure out the maximum
likelihood estimate of theta. In this case, I'm calling
it theta A coin A because we're going to use
more than one coin later. How do we calculate theta
A and an estimate of it? By the way, we use this hat symbol typically
to denote the estimate of a parameter based on let's
say the sample data. One statistical model that essentially explains the
number of successes or failures that we might see in an independent Bernoulli
trials where there's one outcome or the other
with a certain probability P. One model for that Is
a binomial distribution. We can show with some
simple calculus that simply by computing the
sample proportion of successes that gives us the maximum likelihood
estimate for theta A and this is in
accord with our intuition. If we try to figure out
if the coin is fair, count the number of heads, count the total number of
flips and then we divide the number of heads by
the total flips and if it's around 50 percent, then that's our estimate for theta and we might say
the coin is probably fair. In this particular example,
we flipped the coin, we counted 19 heads and 32 flips. If you simply do the division you get an estimate for theta A, namely theta hat A of about 0.59 as an
estimate for the bias. That's the simple model with
a single parameter to be estimated and that's
a nice illustration of a more general principle of this maximum likelihood approach where you have some
parameters like theta that you want to find from assumed probability
distribution for observed data D. You want to find the estimate of theta hat that maximizes the likelihood or the probability of the data under that distribution given
the value of theta. In practice, we maximize
the log likelihoods. We take the log of the
probability of D given theta instead for
mathematical convenience usually since adding up regular numbers is easier than multiplying lots of tiny numbers. The example with the coin
toss was one example of maximum likelihood estimation of a single parameter,
the biased parameter. The main message here is that if your data
set is complete, if there are no
unobserved variables, it's pretty
straightforward to compute something like a bias
parameter because you have all the data you need
and you simply plug it through the formula and out comes the estimate
of the parameter. Let's continue with our
coin flipping example. Just to introduce
some terminology. In the coin flipping example, we had a parameter to
estimate Theta hat, which is the bias of the coin, then we haven't observed
variable x. X in this case is the number of heads in a sequence
of 10 random flips. Here's an example where we have, on the first set of flips, we might see five heads, a second set of flips
with the same coin, you might see eight
heads, so forth. There's no incomplete data
here, everything's observed. In that case, it's easy to
calculate Theta hat as we saw. We can calculate Theta
hat for each of these. We can look at the total. There are total of 29
observations of heads out of 50 flips total, and that gives us the MLE
solution, the estimate, since we assumed that x was distributed according
to a binomial distribution. That's pretty
straightforward. Now let's say your friend comes
back later with two coins and she
would like you to estimate the bias for
each coin separately, and you're allowed
to see which coin of the two produced
each sequence of flips. For example, you might
see that she gets coin A, does a series of flips
here and observes five heads when flipping coin A. Then you see she
switches to coin B, does a series of heads
and tails and sees eight heads when flipping
with coin B and so forth. In this case, the
number of parameters to estimate goes from 1-2. Instead of estimating just the bias parameter for one coin, we're going to be estimating
a bias parameter Theta hat_A for coin a and Theta
hat_B for coin B. The observed variable x
is still the number of heads in the sequence
of 10 random flips, and there's still
no incomplete data. We can see which coin she used, we can see all the
flips and so again, in that case, it's easy to estimate our goals of Theta
hat_A and Theta hat_B. For Theta hat_A, we simply add up all the occurrences of
heads given the coin is A, so in that case there
were 14 total here. We do the same when we
saw that it was coin B, in this case they were 15 heads. With these numbers, we
can estimate the value of 0.47 for Theta hat_A and
0.75 for Theta hat_B. It looks like a coin B is certainly biased in
favor of heads here. Even though we
added another coin, the estimate of this very slightly more
involved model with two parameters instead
of one will still easy to deal with because we
can see all the data. But now let's modify
again to say, well, what if your friend doesn't tell you which coin she used to
produce a series of flips. Suppose the only thing
she told you was that she selected between coin A and coin B with
equal probability. There's a 50-50 chance
that she picked coin A, and a 50-50 chance she
picked the coin B, but she doesn't tell
you which one of these produced each sequence. The problem is the same
as in the previous slide, you want to estimate
the bias of coin A, you want to estimate
the bias of coin B. You observe the number of heads in a sequence of 10 random flips, so you can still see
the flips of the coins, you just don't know
which coin it was. But now, instead of being
a complete data problem, we have some incomplete data. What is the incomplete data? Well, we're missing which coin produced an observed sequence. We'll call each of these
observed sequences, the flips X_i. We're missing which
coin was used, and so we're going to call
that missing observation Z_i. Z_i takes two possible
values, Z_A or Z_B. Hypothetically, for example, one possibility might have been that she flipped coin
A followed by coin B, followed by coin B again, followed by coin A, and perhaps followed by coin A. That's one possible set of values for this incomplete
data but of course, we don't see that column at all. Your problem is to estimate
Theta hat_A and Theta hat_B. It now looks
impossible to do that. How could we possibly estimate
those parameters if we don't know which coin was used? Well, it turns out
that we actually can estimate these
parameters and we can estimate them using the expectation
maximization algorithm, or sometimes we'll just call
it EM or the EM algorithm. The EM algorithm is an
iterative method to find approximately
optimal estimates of parameters in statistical models, when you have both observed
and incomplete data. The incomplete data
is often in the form of unobserved latent variable. For example, in our
two coin example, the unobserved latent
variable is that Z_i, which of the two coins with different biases produced
the series of flips? But you can imagine other scenarios where you
have incomplete data. For example, in the k-means
clustering algorithm, we don't know which of k clusters a particular
data point belongs to, so the cluster membership
for a data point is the incomplete data for
that particular problem. Or perhaps which topics
a web page is about, so we observe a web page, but it's considered a mixture of different underlying topics that we haven't observed directly. All these cases share
the property that we can describe them with a statistical model that has
a certain set of parameters. We have some observed data but we also have incomplete data. That's where the EM
algorithm really works wonders in some situations. I'm going to describe the
steps of the EM algorithm, and the initial step of EM seems so crazy that
it just might work. The initial step is just guess. Just take a wild guess
about what Theta hat A, and Theta hat B are, and we'll call those initial
guesses Theta hat A_0, and Theta hat B_0 because
we're going to be running at EM multiple
times, multiple iterations. We're going to call
the first iteration, Iteration 0 where we
initialize everything. The important thing here is that once we guess Theta hat A, and Theta hat B, we've
observe the series of flips. Now we'll be able to calculate, given the series of
flips we observe, and our estimates
for Theta hat A, and Theta hat B, the
probability that a particular coin was
used to generate X_i, that's what this denotes. Once we figured out an estimate for which of the
two coins was used, we can estimate the
expected number of heads for each coin, coin A or coin B for
that set of trials. We'll walk through an example more concretely in a minute. This step of estimating
a probability of the values that we didn't
observe the incomplete data, is called the E-step of EM. Essentially you can think
of it like in the E-step, we're filling in some estimate
for the unobserved data. In this case, which
coin was used. We start with a guess for Theta, we compute this probability of which coin was used A or B, and that fills in the
first column that we saw. Now the M-step is easy once we have this sort
of pseudo complete data. All the M-step is, is just our maximum
likelihood calculation with this sort of
pseudo complete data. That's one iteration
of the EM algorithm. We run the E-step to estimate this probability
of which coin it was, now we have those
estimates, we have a sort of a pseudo
complete data set, and we just do the
normal thing that we did when we had real
complete data to estimate the bias of Theta
hat A, and Theta hat B. We run the M-step to
compute the MLE estimate, and then we repeat
these two steps. Let me go back after the M-step, and we go back to the E-step, and do it all over again. We use the revised estimates
that we got in the M-step, and we plug those back in to the values we
use in the E-step, and we go back, and
forth, and iterate. Remarkably, our estimates
for Theta hat A, and Theta hat B get better, and better with each iteration. They converge to a solution that's a local maximum that
actually will converge. An intuitive way to
think of the E-step, think of it as filling in the missing data
with pseudo counts. If we can fill in
the missing data with the pseudo counts, then we have a complete data
set for estimation purposes, and we can just do our normal maximum
likelihood estimates using this pseudo complete data. First, how do we estimate
the coin probability Z_i given an observed sequence? After all, that's the key
column that's missing for us. Let's take a look at
that in more detail. Let's suppose that we take a completely random guess that
the bias of coin A is 0.2, the bias of coin B is 0.6. If we know the
biases of the coin, then for any trial X_i, any sequence of heads, and
tails that we observe, we can clearly calculate the probability of that sequence, assuming it was coin A because
we have the bias estimate, and we can clearly estimate
the probability of seeing that sequence if it was coin B because we have
that bias estimate. We assume that the probability
of a given sequence of heads is given by
binomial distribution. The probability of seeing a particular sequence like this according to the
binomial distribution, we look at the number
of heads that came up, in this case it was seven, number of tails was three. Probability of seeing heads was 0.2 according to
our initial guess, which meant the probability
of seeing tails is 0.8, and we have this many
possible sequences of that particular
combination of Hs, and Ts. Anyway, we can compute using
the binomial distribution, that probability of any given
specific observed sequence. We can do the same assuming
that it was coin B. Same sequence but now we're assuming coin B was chosen so now the probability of
seeing heads is 0.6. Probably this entails is 0.4. That's all great, but
what we really want to calculate is the probability that it was coin A
given that sequence, and the probability
that it was coin B given that sequence. Can we do this once we
know the probability of X_i given Z sub A or Z sub B? Yes, we can use Bayes Rule to calculate probability of Z sub A given X
sub i for example. Let's take a look
at how we do that. Our friend told us that the
probability of selecting either coin A or
coin B was 50-50. We know the probability of seeing coin A or coin B is a half, so our goal is to calculate probability that it's
coin A given a sequence. Let's just write that
out as Bayes Rule. This probability is equal to in the numerator probability of X given Z sub A times the
probability of Z sub A. In the bottom, what we've written here as an
expanded version of probability of seeing
the sequence piece of X_i. There are two cases, either
it was coin A or coin B. If it was coin A with
probability P of Z sub A, then that's the probability
given it was coin A, that's the probability
it was given to coin B. This whole thing
together is equal to probability of X sub i. This is just Bayes Rule and
expanding out the bottom. If you crank through, if you plug in the values
we calculated for P of X_i, given Z sub A and Z sub
B into this formula, we'll get the desired probability that it was coin A or coin B, given the observation and our guesses for the
bias of A and B. You can verify for yourself later that if we plug
all this through, this should actually
be three here. But if you plug all that through, the probability that it was coin A given the sequence
we saw is this much, probability it was coin B given the same sequence is
actually close to one. This makes sense
because given that our biased estimates were that the coin A was heavily
biased against heads and coin B was more
biased in favor of heads. If we see a sequence with
lots of heads and few tails, then it makes sense
that we would estimate something very
probable for coin B. If we go ahead and use exactly the calculation
to fill in this column of Z_i to Z_i's we'll do exactly the same
calculation we just did for each of these observed
series of flips. If we do that and
crank it through, we'll get these
probabilities that it was coin A or coin B and
obviously we're not certain so we have to express that column as a probability
between two possible events. Again, this example
that assumes that the current estimates for the bias of A and
B or 0.2 and 0.6. We're making good progress
on completing the E-step. There's just one more
thing that we need to do, and that is fill in
the expected number of heads that we would see given that it was
coin A or coin B. That's the next step
in our procedure. To fill in these expected counts, we want the expected number of heads produced by coin A
and coin B respectively. We simply multiply the
probability it was coin A by the total number
of heads that we observed, we can say on average, coin
A produced 0.58 heads. Whereas probability of
coin B is 0.8836 on average out of the
five total heads that we observed in this example, on average 4.4181 of them
can be attributed to coin B. This is a soft assignment, if you will, to the
coin that was flipped. We don't know for sure that
it was coin A or coin B, but we can assign probabilities. If we can assign probabilities, then we can assign an expected value
to the number of heads that we would see
on average from coin A. Now we just fill in
these two columns computing the expected value of the number of heads we can
attribute to coin A or coin B on average and then
we can add them all up. The expected number of heads to the coin A on average is 2.96, expected number of heads in
coin B scenario is 26.04, and now what we've done is we've created a complete dataset. We've reduced the problem of finding the parameters of
Theta head A and Theta head B to one that can use
a complete dataset. Now this dataset is complete in quotation
marks because we completed it essentially by producing pseudo data that
we didn't observe. But the E-step is the hard part, that's the one that
completes the dataset. The M-step is the easy one because now we're just
back to the problem of computing the maximum likelihood estimates for Theta head A and Theta head B from
the complete data. We're using our pretend
complete dataset to re-estimate the parameters for Theta head A and Theta head B. What do we get when
we plug in all this pseudo complete
data we just calculated? If we plug that in, we look
at the expected number of heads for coin A, coin B. We just use our usual
maximum likelihood estimator for the binomial distribution. Now, a revised estimates
for theta hat A and theta hat B are 0.1
approximately and 0.88. With these updated
parameter estimates, which we're going to call
theta hat_A t plus one. If the original theta
hat estimate was theta hat A t or theta hat B t, so we're going to use
the superscript t plus one to indicate the step at which these
estimates were derived. Then we just repeat. Now this example
of two coins is in the notebook that comes
with this week's materials. You can walk through these steps yourself in the Python code. The EM algorithm can be used to estimate the parameters of a probability distribution
where we observe samples from the distribution
like coin flips, but where some
variables are missing, namely which coin of
the two was used. Typically, these are cases
where the parameter estimation would be easy if we had
values for these variables, if we knew exactly
which coin was used. But instead, we have
incomplete data. The trick of the EM algorithm is that essentially it fills in the missing data to create
a pretend complete dataset. With this pretend
complete dataset, we're able to estimate using the maximum likelihood
the parameters we want. Then we plug those estimates back into the E-step
and just keep iterating and it will
actually converge to an estimate for the parameters. If we look at these
E- and M-steps, the E-steps are
focused on estimating the probabilities for
these latent variables. In this two coin example, it was the probability of Z_i. The M-steps are focused on the maximum likelihood estimation of the model parameters
that we care about. It alternates between these two. The E- and M-steps can be
viewed as a coordinate ascent, where at each step, it alternates between
estimating for the latent variables and then estimating as if we
had complete data. Now, the theoretical results
around the M can be shown that the complete
data log likelihood increases after each EM step. Each step that you take in EM is improving the
parameter estimates. It's guaranteed to converge
to a local optimum. What EM is doing is it's maximizing the lower bound
on a marginal likelihood. In effect, it's replacing, just as we saw in our example, a complete dataset D
what we'll call q, which is the pseudo data, the pretend version of that. If you look at the log
likelihood of the data, given the parameter estimates, the true log-likelihood
might look something like this and our goal
is to figure out. On the x-axis, we have the value of the parameter that
we're trying to optimize. We some optimal value
for theta that gives the maximum likelihood of the
data given that parameter. To get there, what EM does is it starts with a random guess, let's say we'll call
that theta old, and it computes this function. Essentially, it's
computing a function that is a lower bound on the marginal likelihood, this function right here, and we compute the M-step, we maximize our
position on that lower bound to get the new
estimate in the M-step. Then we plug the M-step results, this theta new, into the next
iteration of the E-step. What it's done is
that it's advanced data and now we're going to compute a new lower bound
on the marginal likelihood. The next M-step is
going to optimize that. You can see that as we iterate, it's optimizing a lower bound on this marginal likelihood. It's eventually going
to walk it's way to the true local maximum. That's a very
informal description of how the EM
algorithm is working. It's optimizing this lower bound, this function, instead of trying to optimize likelihood directly. There's no generic EM
support in scikit-learn, but EM is implemented to estimate parameters for Gaussian
mixture models. If you take a look at
this week's notebook, it'll walk through exactly
the two coin example that I showed you in
the previous slides. In that code, we've implemented the E-step function and the M-step function
so you can see exactly how those steps are implemented in Python that
we showed mathematically. You can run it yourself and
you can play around with how the theta values
are initialized. You can see how
quickly it converges, which is typically
within 5-10 steps. In the E-step, you can
see this is phase 1 where we computed the coin probabilities using Bayes rule. This is phase 2 of the
E-step where we used those coin probabilities to compute expected counts
of heads and tails. Then with those expected
counts of heads and tails that made it really
easy to compute the M-step, the values of
theta_A and theta_B. Here's a nice visual
representation of how the EM algorithm
does this hill-climbing. The x-axis shows the estimate
for theta_A, first coin. The y-axis shows estimate
for theta_B, second coin. I'm going to pick this
initial guess here, where Theta sub A is 0.6
and Theta sub B is 0.5. Then you can see when we run
the E-step to this output, this output shows
the updated versions of our estimates for Theta
sub A and Theta sub B. You can see it starts
off at 0.6 and 0.5. Then after one iteration of EM, it's worked its way to
0.71 and point 0.58. It should be somewhere over here. Then it goes here and
you can work it out. But essentially, you can see
it's hill-climbing behavior. I've drawn a straight
line to connect the initial guess with the
final converge solution. But if you actually
look at the path that EM takes in this parameter space, you can see what the
hill-climbing is doing. It's trying to climb this. This is a contour map
of the likelihood. Now note that the diagram
here is symmetrical. There are two optimal
solutions and that would correspond to exchanging
points A and B. That explains the
symmetry in this picture. We can also see what
happens if we start with a different guess for
Theta A and Theta B? Now we're guessing
that Theta A is 0.1 and Theta B is 0.3. Again, if you follow the steps that are output
from this function, you can see approximately here, I'm just doing this
approximately, but you basically see it climbing the hill toward
this local maximum. Here there are two local optima
and they're symmetrical. This is a very
typical scenario for EM where the
likelihood landscape, you can imagine if the model
were more complicated, it would be a very complex, bumpy looking contour map. Depending on where you start, where you initialize
your parameter guess, you can end up on a very different summit of
a different hill. That's why when you run EM, you should run it
multiple times with different initial
perimeter guesses. For each of the guesses,
you can maintain what the likelihood was for
that local maximum. After you run it multiple times, then you can take the
solution that found the maximum of those outcomes. In this case, they happen to
be symmetrical and equal. But you'll often find
that the landscape has some hills that are slightly
higher than others. By running multiple times with
multiple initializations, you increase your chances of finding a slightly higher hill, a slightly more optimal
solution for the parameters. Questions to ask yourself about any statistical modeling problem, where you have to estimate
parameters, well, what are the parameters
you're trying to estimate? What's the observed data and
what's the missing data? This will help guide you toward the algorithm that
makes the most sense. If you have a parameter
estimation problem where you have a mixture of observed
data and missing data, then EM is one approach you can use to estimating
those parameters. Now let's turn our
attention briefly to the k-means clustering algorithm. Because when we
first saw k-means, it appeared at least to me, to be magical the
first time I saw it. You do this iteration, this two-step iteration and somehow it magically converges
and it finds a clustering. Now it's not always
the best clustering, but often if the clusters
are well-defined, globular, and roughly of equal size, the k-means algorithm
works pretty well. Well now that you've
seen the EM algorithm, it wouldn't be a surprise
to you to learn that k-means is exactly
the EM algorithm. The unobserved data
in this case is, let's say for any
given data point, the unobserved data is what? The thing we can't observe is what the cluster membership
of any given point is. We don't know what cluster
assignment a point has, but we can use the EM algorithm to pretend that we
know what it is, just like we did with the missing information about the coin. We could iterate and find the
in this case for k-means, we want to find the location
of the cluster centroids. The parameters we want
to estimate for k-means are the x,y coordinates
of the K cluster centers. The observed data are
all the data points and the missing data is the cluster membership
label of each data point. We can translate this to the E and M steps we saw
for the coin example. Initialization, you
pick the number of clusters you want to find. You pick k random points to
serve as the initial guess. That's just like guessing
the initial values for the bias of the two coins. Except now we're trying to
estimate location parameters. We guess three random locations
for the cluster centers. Then in step A. This is where we create
our pseudo data, so we don't know what the
cluster membership is. So we pretend that each data point belongs to whatever cluster
center is closest. So we assign cluster membership estimates
to all the points. Now we have a complete data
situation, pseudo complete. That brings us to step
B, which is the M step. So if we know all of them cluster membership labels for the points, we can estimate the
cluster centers and that's when we update. So you can see how the
k-means algorithm maps exactly to the E and the M
steps of the EM Algorithm. Of course we repeat these steps until the centers converge
to a stable solution. There's a more general case of k-means called a
Gaussian mixture model. These estimates for the
Gaussian mixture models can be done with EM as well. So the assumption in the Gaussian mixture
model case is that the data points were generated by a mixture of K Gaussians. So what are the
parameters to estimate? We need to estimate the
Gaussian means that the x, y locations of the
centers essentially. But now we also have to estimate the covariance matrices
of the K Gaussians. So is the covariance matrix
like this or like that? That's the second thing that's added to the
estimation problem. The second set of parameters,
the covariance matrices. Now some simpler versions of Gaussian mixture models assume all the K gaussians have
the same covariance, but you don't have
to assume that, you can estimate separate
covariances for each as well. So this generalizes k-Means by also estimating the covariance
structure of the data. It can be done with
EM, as I said. So what are the observed data? Again, it's the data points. The missing data is the
cluster membership, the labels of each data point. I've given the link here
to the example that runs the Gaussian mixture model
scenario where it estimates the covariance matrices as well. That's easily doable in
scikit-learn as well. So topic modeling can also be computed using
the EM algorithm. We covered in this
week's examples of latent Dirichlet allocation, where we want to estimate parameters that
describe K different topics. We also want to estimate the per document
topic proportions. So that's two different
sets of parameters. What do we observe while we
observe words in documents? What's the missing data? The missing data is the topic
assignment of each word. So just as we used
Z_i to indicate which coin was
flipped, in the LDA, you can use Z_d,i for which topic is
corresponds to a word. You can use EM to estimate
these parameters. It doesn't scale particularly well to Latent
Dirichlet allocation. So there are other methods that researchers have used to
infer these parameters. So this is an illustration of how general the
EM algorithm is and how many different kinds
of scenarios it can be applied in these
incomplete data scenarios. So best practices for the use of this Expectation-Maximization
Algorithm. Em will always converge
to some local maximum, some optimum, but it can get
stuck in these local optima. So as I mentioned before, and as you saw in that diagram
with the contour maps, multiple randomized
initializations are always a good idea. Basic EM can be very slow to converge if there are
many latent variables. So people have developed
various methods to accelerate them and depending
on the scenario, it may or may not be
worth implementing those. There are other estimation
methods that are called variational methods that people use to estimate
parameters as well, but we don't cover
those in this course. Finally, I just wanna
emphasize that EM appears in many guises
across domains, especially in natural
language processing. It's been generalized
in various ways. So there's a method
called generalized EM, for example, that allows a more general form
of the E step. The E step can be constrained in various ways, for example. So I've given a link to a paper
here if you're interested in more details on optimizing
the performance of EM. So that concludes
our summary of EM, there are lots of
other scenarios where it's useful that we don't
have time to cover. For example, in survey data. Survey data you have
people who don't respond to the survey
and people who do so the people who
don't respond form a set of unobserved data
and in those cases, EM can help as well. So it's a really flexible, widely used algorithm that's really worth getting to know.