The original PCA method has
a number of problems if you want to apply it to more
complex real world datasets. So several important
extensions have been developed to address
these limitations. The first problem with
traditional PCA is that the results can be hard to interpret if you have lots
and lots of features. While PCA gives the
best possible variance maximizing representation of
a p-dimensional dataset in q dimensions with q less than p. The q new variables
it defines are linear functions of all
p original variables. While we were able to interpret the first two
principle components in the cancer dataset example, when the original dimension
p is much larger, many variables that go into the linear combination defining the q new variables have
non-trivial coefficients, especially in the
first few components, and that makes these principal components hard to interpret. So some variance of PCA have been developed for helping
people end up with q dimensions that are easier to interpret
while minimizing the loss of variance due to not using the principle
components themselves. In fact, there's a trade-off between interpretability
and variance. Here's some summaries
we're going to present of useful PCA variance. First, by introducing constraints on the principle components, we can enforce sparsity and we get a variant
called sparse PCA. We can get more interpretable
components via rotated PCA. We can handle noisy
input with robust PCA, and we can find more sophisticated nonlinear
low-dimensional structure with kernel PCA. There are also variants of PCA that can deal with missing
values in the dataset. For example, by modifying the
default PCA least squares objective function
so that it gives zero weight in the objective
to missing points. But we won't be covering this missing value
PCA in this course. One problem with the
usual PCA results, especially if you have
a lot of variables, is that as a visualization tool, they can produce results
that are hard to interpret because of all the
subtle variations in the variable values. Here's the example we saw from the Wisconsin breast
cancer dataset. The role of some
variables is clear but there are a lot of variables
with intermediate values. What we'd really like is for
PCA to give us results where the most important
clusters of variables are crystal clear for each
principal component. In fact, sparse PCA
does exactly that. It introduces constraints on the found components
so that most of their coefficients
are exactly zero, leaving only the most
influential as non-zero. There are many sparse
PCA flavors and we won't go into the details of
the underlying algorithms, but scikit-learn provides top
level library support for one implementation via
the sparse PCA class that's easy to use. This sparse PCA class
supports an L1 penalty, which as you might recall, is the same as used in lasso regression and just
like lasso regression, there's a sparsity parameter, Alpha, that controls the
sparsity of the solution. Larger values of Alpha force
more sparse solutions, and as you can see from the
example in the notebook, the sparse components
are much easier to interpret than the original
non-sparse solution. We won't be spending
a lot of time on rotated PCA since I
think for our purposes it's less practical
than sparse PCA for interpretable results and itself does not give sparse results. But rotated PCA is definitely a powerful and popular tool. So I'll give a summary of it here so you're aware of how it works. Basically, the idea is to
take the principal component vectors and find an
orthogonal transformation, essentially a rotation, so
that the rotated components associate each variable with at most one factor rotated
principal component. This makes the results
easier to interpret because each variable
is associated most strongly with one
specific component instead of possibly
multiple components. There are many possible
rotations we could apply to principal components, the most popular
optimization criterion to use for the rotation
is called varimax. The varimax criterion maximizes the sum of the variances
of the squared loadings. What the varimax criterion does intuitively is illustrated
by the example on the right. Finding the varimax
rotation informally means finding rotation of
the original vectors in which each variable marker
is either as close as possible to an axis or
as far as possible. You can see that in
the bottom image the original variable
markers have been rotated without
changing the angle between them so that
variable one over here is close to the x-axis after the rotation and variable two is close to the y-axis. You might wonder if PCA has a very specific
optimization criterion for finding components that
maximize overall variance. But then we modify that
solution somehow by applying this additional
rotation step. Is this still PCA? It's a slightly tricky question, but the short answer
is not really because the rotated components
aren't principal components anymore and their
directions are no longer guaranteed to be the
variance maximizing ones. Some of this language about
rotation and varimax might sound familiar if you've
worked with factor analysis. So this might be a good time to comment on the differences between PCA and factor analysis. These two are easy to
confuse because like PCA, factor analysis seeks a
lower-dimensional explanation of structure in a
high-dimensional dataset. However, its underlying mathematical
machinery is different, because what it seeks to optimize and the kinds of
structure it finds, are very different from PCA. I'll just give you a high-level explanation of how
they're different, since we won't be covering factor analysis in this course. PCA, as we've seen, solves a very specific
and restricted problem. The components it finds,
the principle components, are actual linear combinations
of the input variables, and they must be orthogonal, shows in successively, to
maximize the total variance. On the other hand,
factor analysis explores a much broader space of possible latent variables
and structures, and it does this using a variety of different
optimization routines. For example, in factor analysis, the factors can be
linear combinations, that maximize the shared
portion of the variance, of so-called underlying
latent constructs and the resulting factors do
not need to be orthogonal. Factor analysis
results, unlike PCA, depend on the optimization
routine used, the starting points
for those routines, and the constraints on the type of latent variables
structures to be explored. Simply put, there is not a single unique solution
and there are many ways and constraints one can
apply in specifying the kinds of latent factors
that might explain the data. The bottom line is that you
can use this rule of thumb, to decide what to use. You run factor analysis, if you assume or wish to test the theoretical model
of latent factors, that explains observed variables. Or you can run principal
component analysis, if you want to simply reduce your correlated
observed variables, to a smaller set of important independent
composite variables. A second major problem
with traditional PCA, is that it's very sensitive to huge outliers in your data. This is because PCA
tries hard to find principal components that capture all the variation in your data. But if your data's
variation is partly due to some severely
outlying points, it can greatly affect
the PCA results, and obscure the structure
that may be present in the regular non outlier data. These big outliers happen a lot in modern computing applications. From image processing, to web analysis, to bioinformatics, where corruption of input data occurs because of
sensor failures, malicious behavior or environmental
factors like shadows, color saturation, bright reflections on
an object, and so on. The solution to this
is called Robust PCA, unfortunately, while
people have been discussing it for
several years now, a robust PCA algorithm is currently not implemented
in scikit-learn, as of version 0.23. I just want to make
you aware of it and give you a high level
summary for now. The main idea of
one implementation called principal
component pursuit, is to attempt to find and separate the sparse
extreme noise, from the main underlying low
rank data automatically. This idea was published
in a 2009 paper, by, Candaes, Li, Ma and Wright. I've included a link to the paper in the
course materials, in case you're interested
in further details. More specifically for
principal component pursuit, you take a matrix M, that we assume is the
sum of two components, L and S, where L is
a low rank matrix, and S is a sparse, randomly distributed
noise matrix. In ordinary PCA, the
optimization goal is to find the best low-rank
approximation, L, of rank K, to the original matrix M, in the least-squared sense. This is captured
in the objective, by trying to minimize the
least squares distance between L and M.
Our assumption is, that the data M, may have a Gaussian noise
component that is small, so there's no sparse matrix S in the ordinary PCA problem. For principal component
pursuit on the other hand, our assumption is
that the noise in M can be much larger magnitude, but sparsely distributed
as captured by matrix S. This norm, with star subscript,
is the sum of the singular values
of the given matrix. The norm with the one
subscript is an L1 norm, that's just the sum of
the matrix entries, which we've seen in sparse
methods like Lasso Regression. As you can see, the PCP
optimization program, tries to find two matrices, L and S, that sum to M, such that L is low rank, and that S is sparse. There are a few other
technical assumptions about the nature of the
principle components, for the low rank component L, as well as the magnitude
of L's rank itself. The surprising thing is
that this method works for completely arbitrary
corruption patterns. They just need to be
randomly distributed. Also surprising, the author showed that this
decomposition can be accomplished effectively with
attractable convex program, and very little tuning. You can take a look at
the paper for details, we've included the paper
in the course materials. Finally, here's some results for two different image
processing algorithms, as shown in the original principal component
pursuit paper. In that paper the author
is focused on task, they had a main object with
predictable structure, like furniture in rooms or faces, and then some form
of sparse noise. For the background
detection task, the sparse noise was the
moving image of a person, that was not part of the
desired background image. For the face imaging task, earlier work in image
processing had shown, that fairly simple objects
that were lit from a distance, can be modeled by a
low-rank lighting model, with nine dimensions. Thus, the face images here, could be considered the
sum of the low-rank model, plus pixels that are
large magnitude, spatially sparse outliers, in the form of very dark shadows and very bright
shiny reflections. As you can see, the
robust PCA algorithm is highly effective, at removing these artifacts, in order to clean up
the original image. The face imaging task, is a nice example of how an unsupervised preprocessing
step, like Robust PCA, can be used to remove unwanted
artifacts in the data, and thus potentially
boost prediction accuracy of a supervised learning methods, such as an image classifier. Hopefully, robust PCA in some
form will become a part of Scikit-learns unsupervised
learning libraries in the near future. Final variation of PCA we will cover is called kernel PCA. Unlike traditional
PCA, which uses a linear transformation
of the input variables, kernel PCA can perform a non-linear transformation
on the input data, thanks to the application of
a nonlinear kernel function. The details of how kernel PCA works internally
are a bit complex, but the main idea is
exactly the same one that you may be familiar with from kernelized support
vector machines, where we introduced the use
of a kernel function to map the original dataset to a higher-dimensional space
with a nonlinear mapping. In that high dimensional space, the points became easier to
analyze with a linear method. Ironically, by
temporarily increasingly the dimensionality of the data with a nonlinear kernel mapping, kernel PCA is able to eventually reduce its dimensionality
much more effectively. Here's a brief review
of that kernel idea as it applied to classification
that might be familiar. It's a binary classification
problem in one dimension. In other words, it has one
feature, an x-axis value. Some problems like
this would be easy for linear classifier to
separate into two classes. Here we can just pick the
vertical line x equals zero. But for situations like this one, it might be impossible to find a single linear classifier that can perfectly separate the white points from
the black points. What can we do? We can't use x equals zero
anymore because that doesn't separate the
points perfectly. Well, one thing we can do is add a second dimension to the
existing data points. We can add a y-coordinate that's simply the square of the original
x-coordinate feature. In doing this, we've transformed our one-dimensional dataset into a new two-dimensional dataset. Notice we didn't need or
add any new information, we are simply computing
a new feature, x squared that's entirely based on the original feature x. This nonlinear mapping
is a simple example of applying a kernel function
to the original data. Now in this new
two-dimensional feature space, we can separate the two classes easily with a single
linear classifier. Now if we take the linear
classifier function enhanced feature space and use the inverse
transformation on it, we can see which points these correspond to in
the original input space. We can see that our
simple x-squared kernel, which is of course a
quadratic function, corresponds to a non-linear
decision boundary in the original input space. This is exactly the idea
that kernel PCA uses. Here's the analogous example
in two-dimensions and with three classes showing how
this kernel function maps what would be an
impossible problem for linear classifier to separate to an immediately
solvable problem. You might be wondering, these are for classification problems, how does this relate to PCA? What's an example of a problem that kernel PCA can do well on, but regular PCA can't. Take a look at the
figure on the left here. You can see the data points
on the left are located mostly along a curve
in two dimensions, namely, right here, like this. You recall that a curve is
a one-dimensional manifold. If you know the
curve, you only need one coordinate to describe the position of any
point on the curve. In this case, PCA cannot reduce the
dimensionality from two to one because the points aren't located on
a straight line. But the data is still
obviously distributed around a one-dimensional
nonlinear curve. PCA will fail in this, but kernel PCA can
succeed because it can find the nonlinear
mapping on the right that makes all the points lie on a straight line in the
new mapped feature space. Here, regular PCA can discover in that transform
space that the data actually do lie on a one
dimensional manifold. It's important to know
that just as with kernel support vector machines, using this kernel trick doesn't
require the algorithm to explicitly map all the points and operate in the high
dimensional space. It can do it implicitly, defining what the dot
product looks like in that enhanced higher-dimensional
space kernel function. But that's all behind the scenes. Here's how you use kernel
PCA in Scikit-learn, where we have two classes of points on concentric circles. You can see that calling PCA has exactly the same steps
as calling regular PCA, but with the addition of a
couple of extra parameters, the family of the
kernel function to use, in this case, we be using
a radial basis function. The kernel specific
width parameter, which for radial basis function
is the parameter Gamma. The original dataset
is on the left, and when we apply
regular PCA here, the results are
shown on the right. We can see that PCA is able to reconstruct the original data, but not detect that the
variation is actually along two separate
one-dimensional curves. Below, we can see that using a radial of basis
function as the kernel, kernel PCA can detect that
the data points actually lie on two separate
one-dimensional curves. How do you know in
advance whether to apply regular PCA or kernel PCA? Well, typically you would perform a task-based evaluation. If you're doing
dimensionality reduction as a preprocessing step
to train a classifier, as in the previous
concentric ring example,t hen you can
compare PCA to kernel PCA, according to which one gives the best final
classifier accuracy. Naturally, you'd perform model tuning on a validation
set to tune the kernel type and
kernel parameters before you do a final evaluation
on the test set. One final note is that kernel PCA has close connections with the clustering technique
called spectral clustering that we will cover
elsewhere in the course. Once again, the application
of the kernel trick provides a powerful generalization of
a linear learning method, this time for
unsupervised learning.