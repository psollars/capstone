We saw in the introduction to clustering
how sometimes the decision about what a good clustering is can be ambiguous
depending on the situation. Some situations there could be
multiple ways that you could arrive at a satisfactory clustering depending on
the kind of task that you're trying to do. The other aspect to evaluating cluster
quality has to do with whether or not you have ground truth labels that
you can use as part of your assessment. If you have existing labels,
then you can use those to determine the quality of the decisions
about membership in clusters. If you don't have ground truth labels,
you can still evaluate clusters according to some criteria that relate
to our original goals for clustering. Namely, are the clusters essentially
very densely packed, and are the clusters separated
well in the feature space? There's also the possibility of
doing a task based evaluation where the quality of the clustering
is evaluated according to how well it does as part of a downstream
tasks such as classification. And that could offer
another objective basis for comparing the quality of
different clusterings. For example, you might use
a clustering algorithm to decide on document topic labels that are used for
text classification. And so a good clustering algorithm
would be one that gave better features that resulted in higher
text classification accuracy. Critical question when deploying
clustering algorithms is what is the optimal number of clusters for
your problem? And this is very closely connected to
the question of cluster quality because if you have a measure of cluster quality, you
can try a number of different choices for the number of clusters and then pick
that number that has the highest quality measure according to
your choice of metric. There could be theoretical conceptual or
practical issues that guide your choice of clusters that you may have prior knowledge
from the problem that there exist. For example, five particular
clusters of types of customers that you expect to identify
from a given situation. But in general, we can also let ourselves
be guided by data driven methods. For example, in hierarchical clustering, you may have some idea of
acceptable distance or similarity between groups of things that
guides your choice of distance threshold. Once you've decided an acceptable
distance threshold which can characterize very dissimilar things or
very similar things. You can use the distance threshold as we
saw in the hierarchical clustering module to cut the tree at the appropriate point. And simply specifying
the threshold will automatically determine the number of clusters
that result after the cut. In the case of K-means another and
other non-hierarchical methods, there are a wide variety of statistics that
people use to estimate cluster quality. One of the main ones which is connected
directly to the clustering goal of picking tight clusters that are also
well separated from each other, is the ratio of total within groups
variance to between group variance. And then calculating that for
particular clustering, and then doing that repeatedly for different
choices of the number of clusters, and seeing what gets the best value
across the number of clusters. In some cases, depending on the type of
task and the goals of the clustering, you may focus entirely on
the within groups sum of squares. Typically, when you plot this and
I'll show an example in a minute, you get diminishing curve. As you increase the number of clusters
within groups sum of squares decreases, things get more tightly
packed around centroids. And there's a particular inflection
point after which adding more clusters helps reduce that
measure less and less. So I'll show an example in a minute. Just to explain briefly
each of these terms, the within groups sum of squares is
simply the sum over the squared distances of all the members in a particular
cluster to the cluster centroid. So for example,
here's the cluster centroid, and here I'm just showing one
cluster to start with. The distance to all
these points is summed, and so forth, etc, etc. So in this formula,
the centroid is denoted by. X bar and here were simply in
this term taking the square distances from each of
the points to the centroid. And then if we had multiple clusters,
in this case we have one, but if we had multiple clusters,
we would also sum up over all the other clusters that were out there to get
there within groups sum of squares. And that total sum would
be a statistic for that particular choice of clustering
that would kind of summarize how well packed the clusters
are in that choice of clustering. A typically as you start adding
more clusters within groups sum of squares decreases and
you can see intuitively why that is. So what I've done here is I've taken
the original set of points and then I've added the ability, I've increased the number
of potential clusters to 2. And I'm pretending here that I ran,
let's say K means and K means say found that
solution with two clusters. You can see what's happened is now that it
has the ability to add a second centroid. It would put it squarely in the middle
of this second group of points, and so now you can easily see that the sum
of squared distances here to these points. And these points in general has gone
down because each of them is much closer now to the respective centroid. So in this case, adding a second
cluster would have reduced the overall within groups sum of squares
compared to the previous example. The other important statistic
is between group variance. So this measures how well separated
clusters are and unlike the previous measure, the within group variance
between group variance doesn't sum up over the individual points, it
focuses on the centroids of the clusters. And essentially what it does
is it looks at the pairwise distance between the centroids and it's the sum of the over those squared
distances that creates this statistic. So as clusters get more separated,
their centroids are farther from each other and
the statistic will increase and typically what's done is we calculate
the ratio of these two quantities. So we calculate the ratio of
the between groups variance to the within group variance. So I'm simply taking
the between group variance, the thing that measures separation of
clusters, we want that to be large and we want the within group
variance to be small. So I'm showing two
examples here on the left. The first example is an ideal
clustering where the between group variance is high so you can see that
the distances between the centroids that determines the between group
variance there well separated, and so there's some of those
distances will be large, while the individual clusters have
elements that are very close to their centroids, so the within group
variance will be small for those, so the overall effect is to have a ratio
where between group variance is high, the within group variance is low and so
the overall statistic will be large. On the other hand, we can see in the example below
what a bad clustering looks like. Here what I've done is
taking the same points but recolored them to create different
cluster membership decisions. And you can see that the centroid for
the blue points, has by virtue of the fact that these new
blue points are now colored over here, that sense right is shifted here. Likewise these other
centroids have shifted and so that has reduced
the between group variance. The centroids are closer together now. Their distances to each other, are much
smaller, and so this is much lower. On the other hand,
the within group variance has increased. As you can see the distances
to their centroids. In general, has gone up. These outliers now are causing much
higher within group variance to occur. So taking the ratio of these two
quantities, we have a low between group variance, high within group variance and
that gives us a low ratio statistic and that's bad.
So you can see how combining these two types of variance together creates a statistic that gives some reasonable.
Ocean of overall cluster quality. And as I mentioned,
you can plot this visually. You can plot within groups sum of squares. So the statistic will go down as
things become more tightly clustered. And we can plot the number of clusters on
the X axis and this is a general trend for many clustering algorithms as I showed
you in that example with two clusters. As you had the ability to add more
centroids to your clustering. It's more likely that you'll find clusters
that are closer to those centroids, so the overall within groups
variance will go down. And typically what one method
to find the optimal number of clusters is to sort of see where
the bend in this elbow is. Perhaps at a 45 degree line tangent. And we can see that the optimum or clusters here by that
criterion would be 3 or 4. Because we have sort of diminishing
returns as we add more clusters. The rate of decrease of that within
group statistic it starts to slow and so you get especially diminishing returns
for adding more and more clusters. So the that's one method people
use to determine clustering based on the statistic. Another very interesting criterion that
is used to determine the optimal number of clusters is based on
stability arguments. In the upper left I'm showing
an example where we've sampled a bunch of points that in
reality come from 4 clusters. And those four true clusters
are denoted by these black circles. If we ask the clustering
algorithm to find 2 clusters. In this case, if we under specify
the number of clusters, it will could find a solution where the two clusters divide
the points with a horizontal line. Which would happen in this case, but if we took a different sampling
from the same training set. Again the sample that
was generated from four true clusters the next time we ran
the clustering algorithm with K = 2. On a slightly different
sample of the same data, it might find a vertical
line was the best separator. So you can see that as you take different
samples of the underlying data. The K = 2 choice would be
potentially not very stable. It might alternate between these two
possible solutions that would indicate that the choice of K
was not a great fit for the true underlying number
of clusters in this problem. Similarly, if you ask for
a number of clusters, that's too large, so in this case, let's say we ask for
K = 5 clusters. A similar thing will happen. Except this time when you ask
the algorithm to find 5 clusters, it will potentially find
a solution that divides. Let's say these two points. As well as the others. But if you were to sample
the training data again. Let's say we have a different
sample over here this time. And you ask it to find 5 clusters. It might choose this solution. So again, you can see that the choice
of clustering that results from specifying too many
clusters is also unstable. So this is just an example of
how clustering stability and its measurement can relate to
measuring overall clustering quality. Silhouette scores are widely used
method for assessing cluster quality. That doesn't require having
ground truth labels. So the key idea of a silhouette score for
an item and each item in a cluster across all clusters is given what's
called the silhouette score. So this is S(I) where I
is the index of an item. The silhouette score for
an item essentially measures how do similar it is to other items
in its current cluster with how dissimilar it is to items
in the neighboring cluster. And that's captured in
the silhouette score formula. If you look at the numerator,
there are two terms in the numerator. So this first term b(i). Basically, measures
the dissimilarity of the item to the closest other cluster
that the item is not part of. So this is the dissimilarity score. So typically items in other
clusters will have a higher dissimilarity with that item. So this number will hopefully be large and the second element is a(i) And
that's the dissimilarity of our current item with all of its fellow
cluster members in the same cluster. So hopefully that number will be low and
then we subtract them. So the rock silhouette score, then it's normalized by whatever
the maximum distance is a dissimilarity between the things in the cluster and
then the closest neighboring cluster. So typically, we might use
the centroid of the other clusters to determine this average dissimilarity. So when an item has a silhouette
score that's close to one, that means that its dissimilarity to
all the items in its current cluster is much smaller than the dissimilarity to the
items in any other neighboring cluster. If the silhouette score for
an item is near zero, that typically means it's sort of
under decision boundary where it could be sort of equally dissimilar
to items in one cluster or another. And if the silhouette score is negative,
that means it's probably very poorly clustered element where
it's in the wrong cluster all together. Now by calculating the silhouette
score for each item, you can then plot on a silhouette
plot shown here on the right. You can plot all the silhouette scores for
each cluster together to really nice visual interpretation that will
help validate your cluster analysis. So in this example, it's showing
the silhouette score on the x-axis. You can then compute the average
silhouette score for all the items in a cluster to get
an overall assessment of cluster quality. And you can see down here, this particular
item was likely miss clustered, because it has a negative
silhouette score and here's an example of applying
silhouette analysis using scikit-learn. So on the right, we have a clustering
problem where we've asked k-means clustering to find two
clusters on this sample data. And on the left,
it is the resulting silhouette plot that comes from that
choice of two clusters. And in this example,
it's reporting the silhouette scores for every point in each cluster. So all the points with cluster
label zero have their silhouette items scores shown here and
they're sorted, of course, by silhouette coefficient value,
the silhouette score. And likewise, the items in
cluster one have their all their silhouette scores plotted here and
then we can see what the average silhouette score is for
that choice of event. If we take the same data and
then apply k-means clustering for six clusters,
we can see again that with six clusters, it results in some silhouette scores for
items that are negative. So some items are getting clearly
misclassified with an equal 6. And in general, that's reflected
by the much lower silhouette score of 0.43 compared to the score
we got with just two clusters of 0.7. So this is an example of how you might
use silhouette scores to decide that for this particular problem,
the optimal number of clusters is two. Here's a measure of cluster quality that
requires ground truth labels to calculate. In fact, there are two measures here. Homogeneity and completeness. So homogeneity,
sometimes called cluster purity. Measures whether each cluster only has
instances in the same class within it. While completeness, measures whether all members of a class
are assigned to the same cluster. So this is a little bit like precision and
recall in machine learning evaluation. Things that are highly homogeneous. Would correspond to sort of
a highly precise clustering whereas clusterings that were complete,
were clusters that found all of the members that belong to a class
put them in the right cluster. So in that case, that would correspond
to having a high recall and both of these scores are between zero and
one. So in this example on the left, we have a cluster with the majority
class being a plus sign. Cluster two, we have the majority
class being triangles and in the On the right we have third cluster where the
majority class is consisting of circles. Let's compute the cluster homogeneity and
completeness for these three clusters. In the first cluster,
what is the homogeneity? Well, there are six elements in
the cluster, and of those six elements, four of them are in the same class. Ideally we would want all of them in
this cluster to be the majority class, the plus class. But we have two rogue elements
that made their way in, so we have 4 out of 6 being
in the correct class. So in that case the homogeneity
factor here would be 4 / 6. And then if we look at where all
of these plus class elements are, we can see we have four of them. We have four of them in the first cluster,
but then there are this one here and then
there are two in the third cluster, so a total of 7 plus class items
scattered across the three clusters. And so the completeness measure for
this class, would be out of the total of seven that there exist in all the clusters
it managed to keep four of them here. So the completeness would be 4/7. And you can do the same exercise for
the other two clusters. The second clusters majority
class are the triangles, and you can see that there
are eight elements in total, in cluster two, of which five
are the correct majority class. So its homogeneity calculation
here is 0.63 or 5 / 8. And in this case there are five triangles
total amongst all the data points. And all of them made it their way
into the second cluster correctly. So here are the completeness is 5/5, so
it's a perfect 1.0 completeness score and you can do the similar calculation
with the third cluster. The Adjusted Rand Index is another
measure of cluster quality that requires having ground truth labels. If you imagine having
a clustering of points called X. Let's suppose you have some clusters here,
and suppose that there's a ground truth clustering
called Y, of the same data points. So these is supposed to
represent different clusterings. According to the clustering algorithm,
and then the ground truth data points. So we have X and Y, the Adjusted Rand
Index measures the probability that X and Y will agree on a randomly
chosen pair of data points, and I'll explain what I mean by that. Suppose you were to pick two points here. That are in the same
cluster according to X. Or what we can do is we can then
look at ground truth cluster Y, and see what happened to those two points
in the ground truth clustering. And we can see that In the clustering X,
they were put into the same cluster, whereas in the ground truth clustering
they were put into different clusters and that means those two disagree. So to compute the Adjusted Rand Index, you essentially compute the two different
cases where the algorithms could agree. So in case a, you count the number
of pairs that are in the same ground truth class and
in the same clustering class. So for example,
we might have a point here, point here and Y, and if we looked at X,
we would see that those are both in agreement cause they also
appear in the same cluster in X. Then the other agreement cases the number
of pairs that are in different ground truth classes and also end up in different
classes according to clustering X. For example, there might be a point here,
a point down here and Y and those indeed also appear in
different clusters in acts as well. So that's another point of
agreement between those two. And so the Adjusted Rand Index simply
counts up all the pairs the degree in that fashion, and divides it by the total
number of possible pairs across all data points, which is see choose
two and that's basically what it is. There's a normalization that happens,
so the Rand index is the simple version. The Adjusted Rand Index includes an
additional normalization factor, to give a baseline of what the expected Rand Index
would be, if the labelings were random. A perfect labeling score under
the adjusted rent index would be 1.0. A bad score would be zero or
negative, down to negative one. The nice thing about the Adjusted Rand
Index, is that it doesn't make any assumptions about cluster structure or
underlying distributions. So we can use the Adjusted Rand Index to
compare clusterings from very different methods. Scikit learn supports a variety of
cluster quality metrics that don't need ground truth labels. One of them is called the Davies-Bouldin
score, lower is better for this kind of score. And this index signifies the average
similarity between clusters. Where the similarities of measure that
compares the distance between clusters with the size of the clusters themselves. And for the Davies-Bouldin score zero is
the lowest possible score, and the closer the score is to zero better partition
the clustering algorithm has achieved. Another type of cluster quality metric
that doesn't need ground truth labels is called the Calinski-Harabasz Index, also
known as the variance ratio criterion. So in the case of this index, a higher
score is better because higher scores are associated with
better defined clusters. The definition of this index is that
it's the ratio of the mean between clusters dispersion and the inter-cluster
dispersion for all clusters. Where dispersion is defined as
the sum of distances squared. So the Calinski-Harabasz Index is
higher when the clusters are dense and well separated, and it has
the advantage of being fast to compute. So calling these from Scikit-learn, you import metrics library and
the metrics library supports incredible number of different
metrics in Scikit-learn. In terms of clustering without ground
truth, we've seen the silhouette score, the Davies-Bouldin score and
the Calinski-Harabasz score. You'll note that all three of
these take the original data, as input in the first argument. And then the cluster memberships
assigned by the clustering algorithm in this labels underscore parameter,
as the second argument. Similarly, metrics with ground truth
are also found in the metrics library, and we've seen already the homogeneity score,
the completeness score. The Adjusted Rand Index and you'll see
here that the difference is that in addition to the labels that are passed
in from the clustering algorithm. You also pass in as the first
argument the ground truth labels that the method
needs to compute their scores. So that was sort of a quick overview
of clustering quality measures. Now I'm going to give a brief summary of
another very important aspect to using clusters in applications. And in some sense it's an aspect
of assessing cluster quality and that is the nature of clustered labeling. So for user interface or data exploration
tasks, humans need to interact with clusters, they need to be able to
understand them, interpret them. They need some kind of friendly
summary that gives an idea of what the cluster is about and
enables them to use them for navigation. If that's the purpose or
some other task like question answering. So how do we get useful labels for
clusters automatically? Well, this is a real challenge and
will talk about a few different methods that have been tried
to solve this problem. This particular example that I'm showing
on this slide is an example of what document clusters might look like as it
summarized by the Carrot Search Lingo4G clustering engine. So in this case, what the results
represent, are sets of documents that were returned for a query about security,
computer security. And clustering algorithm group them
into topically coherent subsets. And the colors in this case indicate the
popular documents in those sets that were clicked. And the labels that you see
like address port router or virus malware antivirus were
derived from the topically relevant high frequency
terms in each cluster. And so one of the things will
go over next is how do you pick the right terms to use
to do cluster labeling. So when we talk about methods for
cluster labeling, we can distinguish
between two basic types. Differential cluster labeling, which compares the distribution of
terms in one clusters documents, with distributions of terms
in all the other clusters. And then the other method
is internal labeling, which only relies on something within
a single cluster to compute a label. In differential cluster labeling,
there are quite a few different Statistics that you can use to flag
terms that are interesting because they are highly characteristic of a particular
cluster compared to other clusters. And in this example,
mutual information is one of those feature term scoring methods. There are other ones like chi
squared that you can use. There's a number of choices to compute
distinctive terms for clusters. For internal labeling,
there are two basic methods. One of them is to compute a centroid of
all the high-weight terms that are just in one cluster. So that doesn't use any information
about the other clusters. It simply focuses on what are the
prominent topics in a single cluster, on average, across all the different
elements in the cluster. The second general method
is to take a cluster, find a central representative article or
element. And then if it happens to be a collection
of documents that has metadata, or structure that contains, for
example, a title, you can then take the representative and use the article
title or the summary if it's available. For example,
news stories often come with a headline. In cases where that's available, it can be a very helpful summary that
you can use itself as a cluster label. There have been some other methods people
have used to automatically name or label clusters. One of them is to use an existing
hierarchy, such as Wikipedia topics or the Open Directory Project, to classify
documents into those hierarchies. And then use the hierarchies themselves
as a way to cluster documents if they're in the same subtree of
the same hierarchy, for example. Another issue is how to evaluate
the quality of these cluster names. Sometimes that depends on the given task. So what might be a good cluster name
in one type of task, for example, answering questions about a document,
may not be a effective cluster name for other tasks,
like quickly navigating search results. So your measure of success on the one hand
might be relevance or user satisfaction. On the other hand,
it might be something like time saved. So the method for evaluating clustering name quality can
be heavily dependent on the application. And there are some applications where,
in some cases, you don't actually need cluster labels. So if the clusters are simply
being used to do smoothing across different instances. So if you have incomplete data and you're using the clusters to essentially
borrow information from similar neighbors, so you can fill in the gaps
in your own information. And you're not even necessarily
presenting something to a person, you don't need cluster
labels in that case. Or there might be a case where
cluster labeling turns out not to be an effective way to even navigate, even if you are designing something
to present results to somebody. It really requires user studies to
understand the role of clusters, and how they can help or hinder someone's
ability to accomplish a task. So the benefits of clustering and
labels could be unclear, depending on what scenario. So it's worth thinking about
whether you really even need them. An increasingly popular way to get many
sorts of labels is by using crowdsourcing. And this applies equally well to
automatically naming or labeling clusters. So in this example, I'm presenting
some work by Lydia Chilton and co-authors from CHI 2013. And in that study, they used
crowd-powered methods to create and label object hierarchies and taxonomies. For example, they showed an array of 100 different colors to
a set of crowd workers. And then they asked the crowd
workers to complete a very carefully designed series of tasks. And the tasks were easy, quick for
the crowd workers to do, and they fell into three main categories. The workers were shown a color and
asked to generate a category for a color. Another task was to pick the best
category for a particular item. And then another type of task was to ask
for item category membership decisions. An with this workflow and
lots of crowd workers, they were able to build
a hierarchy that had 80 to 90% of the quality of a hierarchy
that was generated by experts. So this is another
approach to organizing and carrying out what might be
a multi-step challenge, in organizing data in
a form that allows you to Cluster and label the things
in your data set effectively. So that concludes our module on assessing
cluster quality and labeling clusters. And the methods that we've gone
over here fall into the last two steps of the typical
cluster analysis workflow. Interpreting and profiling,
relabeling clusters and assessing quality. These two final steps are a critical
part of the workflow. Because as we saw for
some of the clustering examples that they're used as feedback
in this iterative process. If we assess the quality and
it's found to be too low, that will drive changes
in the distance measure. Could change a decision on as
we saw the number of clusters, so it's incredibly important to
understand what types of quality measures are appropriate for your task. And also weather the interpretation of
clusters is important to your problem. And if so,
what type of cluster labeling algorithm might aid people in the interpretation. So we tried to cover the more
widely used labeling approaches and quality assessment
approaches to clustering. I encourage you to
explore the readings and the related papers in case
you'd like more details.