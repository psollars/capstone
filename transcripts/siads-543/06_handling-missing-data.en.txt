In this module, we're going
to discuss what to do when you're working with a data set that has missing data. So lots of ways that missing
values can occur in data set sometimes is the specific
code like a minus one, minus 999 that you
need to recognize. Often times the cell
is simply empty, or maybe it contains an NA or some other code and data is especially prevalent
in survey data, when not all survey
respondents answer a question or available for follow up
interviews, for example. But the vast majority of
Scikit-learn estimators that classifier is regressors dimensionality reduction methods, they all expect a
perfectly complete dataset with no missing data. So what do you do in that case, we've all had the feeling of being disappointed when we open a cool new dataset and
we see that there are missing values clearly
in some of the columns. One basic strategy
is to just discard entire rows or columns that have any missing
values at all. But the problem with
that, of course, is that you lose valuable
if incomplete data. A better approach is to
impute the missing values, which means to infer them
from the known data somehow. In the next few minutes, I'm going to discuss different strategies
for dealing with missing data and we're going
to focus in particular on a set of imputation classes
that are in scikit-learn. We'll go through some examples of exactly how to use those. Our general approach
is going to divide the problem of missing
data into different cases. So in one case we have what's called the
complete data case, where we only keep the rows that have no missing
values whatsoever. Here's an example
of an ozone dataset and you can see this
is actual dataset, how many actual missing
values there are, as some quite a reasonable
fraction are missing. You can see that there are not very many complete rows, in fact, there are hardly any and
we'll talk about when it's appropriate or more risky to consider this
complete case option. We'll also talk about univariate strategy where
our goal is to deal with one column or feature at a time and using known
values in that column. Like we can take the mean of all the values that
are missing or the median or we could even put
in a constant like zero. We can deal just with
this column by itself and fill in the missing values that way with those
simple strategies. But then there's a
multivariate approach, where instead of just
restricting ourselves to seeing the values
in one column, we can actually look cross
multiple columns to try to do a better job of inferring what exactly that
missing value might be. This involves supervised
learning and it could also involve unsupervised learning and we'll talk about those cases. Statisticians talk
about three categories of missing data and by the way, the field of missing
data imputation is quite a significant sub
field within statistics. The three categories that have become common to
refer to over time, just so you're familiar with
these terms if you see them. The first case is called
missing completely at random and in that case, the missingness of the data is not related to any
specific variable, it's a completely
random phenomenon. So I'm going to use
the example here of, let's suppose you're
collecting measurements using a weighing scale together with the weights of some
specimens in the field, in the missing completely
at random case, the missing data in your data file isn't
due to any property of the scale or where you
went to get the specimens. Instead, it's completely
random corruption, let's say in your data file that's due to a software error. This is a fairly
unrealistic assumption, but it's one that is
frequently assumed for certain imputation
techniques. The second type of missingness is called Missing At Random and I think a better
term for this is Missing At Random conditionally. What this means is that the propensity
for a data point to be missing is related somehow to
some of the observed data. In particular, it might be conditional on one
of the variables. So let's suppose you
have an addition to your weighing scale producing
a weight measurement, the weight observed of specimen. Suppose there's also variable
called surface type, soft or hard, which indicated with a surface that you place the scale on when you made the measurement. It might be that when
placed on a soft surface, your weighing scale produces more missing values than when it's placed
on a hard surface. So the likelihood that
a data point is missing is tied to that other
variable, the surface type. That's called
conditionally missing at random and most missing data is in this particular category. To continue with our
weighing scenario, the third category of missing data is called
Missing Not at random. Missing Not At Random, that case the probability of some missing value varies for a completely unknown reasons. For example suppose you're scale suddenly or maybe gradually
wears out on its own, and starts always producing missing values no matter
what the situation. There may be no
specific features that caused or are associated
with the probability, of missingness of the data and some factor you
didn't observe. It Curzon surveys for example, people with weaker opinions or some other individual factors, might respond less often to a survey or to
particular questions. This case is the most complex. In order to figure it out we need more data about
missing this causes. We need to do a what
if analysis to see, how sensitive the results
are, to different scenarios. And we need to do
an analysis to see if missing values are indeed, tied or not to particular
feature value ranges or collections of features and in some subset of
the feature space. Most simple imputation methods, such as replacing a value
with the column mean, assume this m car category, the completely random,
cosmic ray type random, accidental data
loss, which again is a restrictive and usually
unrealistic assumption. But it does allow us to state certain
statistical properties, like lack of bias in the case that that assumption
is actually valid. There are a vast array
of techniques to perform data imputation
and statistics. The goal of this module
is to make you aware of some of the highlights the
key concepts and methods, as well as, practical
applications that you can implement
with [inaudible]. Option 1, is one that many
of us have probably used, which is called
complete case analysis, you keep only the rows that have no missing
values whatsoever. One thing to do right away if you're considering
this option, is to look at the proportion of all rows that have
missing values. You can just do a
simple dropna method, to a data frame for example, using axis equals zero. You can see is that one
percent, is it 50 percent? Typically you would consider complete case analysis only if the number of missing values, was relatively small, in
other words you don't want to lose too much data. Now there's a theory and
statistics that says, in the M Card Case that
completely random case, using complete cases is
statistically unbiased. But given that assumption
isn't realistic, instead a very practical
thing that you can do, is you can do an analysis using PCA or multidimensional scaling, to see if the complete
cases systematically vary, from the entire data-set. Are the complete cases if you isolate them filter them out, and compare them you
could project them, into two-dimensions
as an embedding, as a multidimensional
scaling visualization. You can see if there's a noticeable difference in the areas of the
visualization space that the complete cases live in, versus the rest of the data-set. If you do see a difference statistically between the complete cases and
the whole data set, you should be aware the
bias that may be incurred, by focusing only on
the complete cases. For example if you send
out a survey by email, only a fraction of the people will return the questionnaire. You can think about
what factors might be associated with
completed responses. Maybe people who have more time, more income, of a
certain age bracket. These are coarser variables that are correlated
with each other. Just be aware, that you
should know the quality of your complete case
data and how it differs from the entirety of
the data set that you have. The second option
for dealing with missing data involves using specialized data wrangling
or cleaning tools, if human effort is available. This approach can be effective, if a lot of the
missing data can be inferred by human
inspection in other words, if other rows have obvious clues about the right value
for the missing value. For example, if the
state field, is missing, in a particular row
for the state column, the state itself may always occur in another
field like the title. A person can easily detect
these similarities, and use that extra information to fill in missing state value. These data wrangling
tools actually allow you to write scripts, to help automate this process. If you'd recognize, regularities between
one column and another, you can write a script that will always extract for example, the state name from one column, and fill it in state column
if that value is missing. This approach is also pretty effective at fixing noisy data. Things like misspelled
places and names, corrupted dates, or numbers. Anything where a person
looking at the data like this, can infer from context what the correct version of a particular missing or
corrupt data point might be. This approach is
not so effective if replacements for
the missing values aren't particularly obvious. In some situations, we
have a third option where the learning algorithm is able to handle missing values. There are some
learning algorithms that are designed
or maybe just by their nature are implemented in a way that handles missing
values automatically. Decision trees and related
tree-based algorithms. One such class of learning algorithm for
classification and regression. Just to quickly illustrate
this idea for decision trees. Suppose you have a
decision tree that has a node where it's
testing the value of some variable X_1 and test it to see if
it's less than 10. If it is the instances
assigned to this bucket, and if it's greater than 10, it's assigned to that bucket. Of course, it could
be different tests for different ranges of X_1. The case where an instance comes along and has a missing value, then in one variable, it might get rather randomly assigned to one of the
children or it might get assigned to the
child node that has the largest
number of instances. The details of how this works vary according to the various specific algorithm
of the decision tree. But the point is that
there are methods that handle the
missing value without having to do any
imputation whatsoever. The other scenarios
where you have a learning method that optimizes some objective function like multi-dimensional scaling, in that case, the
objective function adds a weight between two pairs
of points, let's say. This weight is set to one if the dissimilarity between
points x_i and x_j is defined. If there's enough of the features between x_i and x_j to compute a dissimilarity, then we keep that pair. If it turns out there's
missing data and we can't compute dissimilarity then w, i, j is set to zero. The algorithm
automatically filters out pairs of data points that either do or don't contribute to the
overall objective. This is a little more
subtle than just skipping a data point altogether because we're dealing with
pairs of points. In some cases, a feature value maybe missing from an instance, which you might still be able to compute the similarity if the similarity can be based
on any remaining features. For example, if we're
dealing with a database of store locations and we define the similarity of stores
by their distance apart. The location feature of a particular store
might be missing. But there could be another
feature that store has, like the location of the nearest
Starbucks to that store. That could still give
information about how close it was to a second store. If they were both close
to the same Starbucks, then you can still infer distance or similarity
between those two. The data points are still useful. We're not removing
the data points, we're filtering on
the interaction between the pairs of data points. Option four is what we're
going to focus on in a bit more detail and
that's the one where we actually impute missing values in a specific and careful way. As I mentioned before, one of those simple cases is just restricting ourselves
to a single column, focusing on the i-th
feature column by itself, inferring values in that
column that are missing, just using the known values
in that same column. Of course, there are risks to simple imputation strategies like just replacing a particular
feature with zero. This plot here illustrates why in the case
where, for example, we know the x-coordinate
of a point, but we don't know
the y-coordinate, so we set the
y-coordinate to zero. That will give us a
bunch of points that essentially we're
adding to the data set, circled in red here. Suppose the rest of the data set, the normal points that have no missing features
look something like this, before we introduce these
points with y equals zero, the mean of the distribution was zero and standard
deviation was one, and the correlation
between x and y was 0.6. However, after we introduced these extra points
with y equals zero, you can see there's some
significant changes in the correlation
between the variables. Essentially, unless you do something very
carefully like this, you can really radically change the distribution and the
interaction characteristics, the covariance structure
of the variables. You're essentially
introducing noise of a very biased variety. Let's look at how
to do that kind of simple univariate
imputation in scikit-learn. The very first straightforward
way is to just use the simple imputer class that lives in the impute
library of scikit-learn. What I'm showing here is an example of a wrapper function. Let me walk through this example to show you exactly what's
going on here because it makes use of
something called the make-pipeline that joins together two
scikit-learn objects. First of all, let's
just take a look at the simple imputer object. With this object,
you can cause it to take an action to fill
in specific missing values. Here you specify in the missing values property
what to look for in the data. In this case, we're
asking it to replace any occurrences of np dot nan. We'll skip over the add
indicator for a minute, I'll explain that
in a later slide. Then you pick the strategy. In this case, we're picking the strategy of constant value. When we specify constant value, is specify what value to fill
in for the missing cell. There are four possible choices for the strategy property, which I've listed here. The constant strategy,
which is the one shown but then you can
also take the mean. It will take the column mean and replace any missing
values with the column mean. This could only be used, of
course, with numeric data. You can also take the median, which is useful numeric data. You can also have
a strategy which you call most frequent or you replace a missing value using the most frequent
value in each column. This one can actually
be used with strings. Let me explain briefly
how you can apply the imputer with an existing
regression algorithm. Essentially what you do, if
you think of the pipeline as a process where
data is coming in, you pass it through
some transformation, and then you can call
some other action. What we're doing here
is we're creating a imputer object as the first
element in the pipeline. The second element in the
pipeline is a regressor. By tying them together in
a pipeline in this way, it tells learn to take the
output of the imputer, whatever comes out of the imputer and that becomes the
input to the regressor. Essentially what's happening
is instead of giving the regressor the original data, which has the missing values, you use this make pipeline to stick an imputer in
front of the input. The regressor only sees data that has the missing values
filled in by the imputer. This is really handy way
to seamlessly integrate an imputation step to
create this new estimator, which is a little mini
pipeline having two elements. Once you create this new
estimator using a pipeline, you can treat it like
any other estimator. For example, in this example, I passed that pipeline estimator
into cross vowel score. This really nice design allows us to take the x and y data that have missing values, call cross-validate score with the special estimator that has the computer and the regressor in a little mini pipeline and just use it like a normal
regression method. This generic sequence of taking a imputer and a regressor, forming a little mini
pipeline with them, calling cross file score
is something you can do without knowing any
particular type of imputer or regressor. You can make it into a function. What I've done here
is just creating the simple imputer object, passing it in to
this, gets scores. There's some sample code
in the notebook for this week that shows in more
detail how to, for example, call different types of
regressors if you want to compare the effectiveness
of the imputation with different regression types. This is the simple imputer
class, and so I can learn. Very easy to use. I wanted to just take a minute to explain the missing
indicator class. It's a special purpose
class in SKL learn impute that's actually poorly documented and that you might
never end up using, but if you do need it, it's very handy in
certain situations. I'm just going to
take a minute to make you aware of it and
describe what it does. The missing indicator
class's job essentially is to flag columns that
have missing values. When you call fit on a missing indicator object
with a set of data, what it will do is it will, just guess I'm calling on x1. I'm passing x1 into
the fit method. When the missing indicator
fit method runs, it will flag all columns
that have missing data. Then with that fit
missing indicator object, we can then transform
different dataset. The Transform method will produce an indicator vector for
each instance, each row, showing which of the columns
that were originally flagged during the fit as
having missing data in them. A house also have missing
data in the transform input. If you look at this example, you'll see that what
we're doing here, I'm printing out the results of checking for missing data in x2. Let's look at the output. What happened was when we fit the missing indicator
class using x1, there are four columns
in this array in total. It discovered that columns zero, two and three had missing values somewhere
in that column. If you use the features underscore property
of the indicator, that's what it will tell you. It'll tell you that in the X1 data that came
in when it was fit, columns zero, two and three had some missing value
somewhere in them. This set of column index numbers goes with the output
from transform. If we look carefully,
you'll notice there are three numbers here. There are three
corresponding columns in the output of transform. What this is telling us is
that when we passed in X2, with reference to the columns
that were reported as having missing data in X1. If you go through and say okay, for the first row of X2, is there missing
data in column zero? No. Is there a missing
data in column two? Yes, there is. So
that set is true. Is there missing data
in column three? No, that set is false. The second row of X2. Is there missing
data in column zero? Yes. So that set
is true and so on. It's a little obscure,
but actually in certain situations
when you need to track the occurrence
of missing data, which columns have it, it can be incredibly useful. This brings us back
to my mention of the add indicator property
on the simple computer. If you set add
indicator equal true, the computer will create one of those indicator
objects to pass along in the pipeline to the
regressor just in case the regressor needs to know information about missing
variables in the input. Normally, they regressor
would simply see data where the missing
values had been filled in. But there could be
some situations where a regressor
could use information about which columns
had data that had been replaced to make a better regression
decisions, for example. That's how the add
indicator property does. This is an unsupervised
learning course, and we've been talking this
week about the theme of how unsupervised learning can improve supervised learning. We've also seen the EM
algorithm and if you recall, the whole point of the
EM algorithm was to deal with incomplete
data situations. The EM algorithm can be used for certain kinds of
computation scenarios. For example, suppose we
have conducted a survey, we have a bunch of
missing responses we want to estimate
probabilities from the responses using a
particular statistical model whose parameters we
want to estimate. If the data were complete, we can easily estimate
those parameters. But since we have missing data, we have responses that
weren't filled in. Some observations
aren't complete. We can use EM where we guess
some initial values and then do that two-part iteration
step that we covered, where you fill in the missing
data with pseudo data. Takes of creative pretend pseudo-complete
data-set that allows us to estimate the parameters, to update that estimate and then we can
iterate in order to infer maximum likelihood
estimates for the parameters. There's no direct support for these kinds of computers
and scikit-learn. But you should be
aware of this approach exists and is viable. It's particularly
useful, as I said, for survey data and I've included a couple of extra
slides right after this one on the slide deck with specific examples
from survey analysis, there's also a link to details if you'd
like to know more. The next level up in our imputation strategy is going to be to do multivariate
imputation. In multivariate
imputation, we can model each feature
with missing values as a function of other features in other columns to estimate
the missing value. The class we're going to use in Scikit-learn is called
the iterative computer. The iterative imputer
operates as follows. First we pick a feature column to be the output y. I'm going to say that this first column is going to be the output column. These are going to be the
values we're going to predict the missing values. We'll have the other
columns be our x-matrix. This will be the y target, and we'll fit a regressor, let's call it R on x y for
all of the known values of y. That might look
something like this. If we look at all the rows where all the values in
the columns are known, we get a training set for
a regressor that per-takes the values in x and
predicts these values in y. This is the training
set for our imputation. We train the regressor
on rows that have known values of y and then once
the regressors has trained, we use the trained regressor to predict the missing values of y. These rows. Then, this is why
it's an iterative process, we go back to step one with
a different feature column. Let's suppose now we've filled in all the missing values that
we could with this column, then we might switch to this
column being the output Y. We do the same thing, we select all the rows that
are complete that have values for y and we make
that our regression target. By iterating across
different columns we will fill in missing values, one column at a time, until as many of the columns are processed as we want
in the dataset. Here's an example in scikit-learn and how to use the
IterativeImputer. One really important
thing to know about the IterativeImputer class is that it's still an experimental
class in scikit-learn. You need at least version
0.22 for scikit-learn, and of course we
have installed that for you in the
Coursera environment, but be aware of that if
you're running locally. In order to use it like
normally you would just need that line where you import iterativeImputer
from sklearn.Impute. But because it's an
experimental class, you also need this
line It's critical. From scikit-learn experimental import enable Iterative computer. There are a number of familiar
properties that are the same as other imputer objects
in the imputer libraries. Again, you can specify
the missing values, you have this ad
indicator property that we looked at before. There's a new property
called N nearest features, and that's the number
of other features, number of other columns to
use to impute missing values. These features are
drawn randomly with a probability that's
proportional to the absolute correlation
of the target column with the potential feature
columns are the input. Now one thing that's really useful is this property called
the Estimator property, where you can actually pass
in a Custom Estimator, so a custom regression method, for example, you could
pass in a random forests regressor into the
iterative imputer. Here's an example of
that in practice, so this is the
Estimator property. That's really useful
because that allows you to control which supervised
learning method is used for that prediction. That prediction that takes all the columns next and
the target column y. To do that, that
regression you can customize it by passing in the object that you want to use to
perform that regression, so it can be as
sophisticated as you'd like. Again, I'm using the make
underscore pipeline function. Here what I'm doing is I'm
using ridge regression, I'm creating a pipeline
that has a ridge regressor, and I'm connecting
it to an imputer, and this is the
iterative imputer. Just as we saw before, you can create this
new estimator, that's the set of
pipeline objects, it's just like any
other regressor. Pass it into cross-class
score and anything else that takes a
regressor or estimator, it's appropriate
and the way you go. Again, there are
multiple examples of using an iterative
imputer with different forms of Estimator in the notebook for this week. Also in the notebook, there's a systematic comparison of different imputation methods. What the simulation does is
it takes the full dataset, it corrupts it with
random missing data, and then it runs various
imputation methods like simple imputation,
I's always 0, mean and median imputation, iterative imputation and then another method called
k-NN imputation. Obviously the full data performs the best
in the regression, but some of these methods
like the k-NN imputation, in this case the mean, actually perform
surprisingly well. This is just one scenario,
I think in general, the simple imputation methods overall don't perform as well as the more sophisticated
methods that use iterative
imputation especially, I think the state of
the art is for example, used to use random forests, but of course, there's a much
higher computational cost for that increase in inequality. That was our tour of data
imputation in scikit-learn, we looked at several options, doing a complete case analysis, and the importance of checking the complete data
instances and comparing them to a dataset that
includes everything. Looking for systematic
differences between the complete data and
the rest of the data. Data wrangling
software, and we looked at a couple of scenarios
where that might be useful. Data algorithms for dealing with missing values that are
baked into an algorithm, those are useful to know about. Finally, we looked at
some specific examples in scikit-learn of using both univariate imputation and multivariate Imputation.