A persistent problem
in machine learning, as models get more sophisticated, they have more parameters. They take more training data
to tune those parameters. Labeled training
data is expensive, often it's done by humans and deep learning algorithms in
particular need a lot of it. We have this problem, we have increasingly sophisticated needs for predicting everything from, voice recognition to
autonomous vehicles. Autonomous vehicles, actually
is the case where getting training data for cars
is incredibly expensive. You can't crash a car
every conceivable way, you can't necessarily drive
it to every possible place, let alone label those things. Labeled training
data, is expensive and that's partly why we've been exploring in this
unsupervised learning course, the usefulness of unlabeled data. We've seen lots of
different ways, how unlabeled data
can contribute to our understanding
of data and also to our ability to predict
things from data. Deep learning
methods, especially, are an area like I said, where this is important. We're running into a
problem, how can we get large quantities of labeled
training data very cheaply? That is the question we'll explore in self
supervised learning. But before I continue, I want you to take a
look at this slide and I have listed here
several different tasks. Now, these tasks all
have something in common and I want to see if you
can figure out what it is. There are a variety of tasks. The first task here, for example, what
goes in the blank? He put the freshly baked
blank in the toaster. The task here, is to figure
out what word goes in that space or we might
take a look at this task. You're given a
photograph or an image and you have to say what was the original
orientation of this object, so obviously you have to
figure out what the object is and what its context
is, and what makes sense. Another type of task is, what will this scene look
like in five seconds? You might imagine a scene
while driving on the highway, you have other vehicles,
they are moving around at certain velocities. Can you predict five seconds from now what the view on
the road will look like given the previous
five seconds of video? Similar application in traffic, lots of data from
traffic cameras, drones and so forth,
mobile phones, that track positions of
cars on the freeway, can you predict what the
traffic is going to look like, in large massive vehicles
in five seconds or five minutes or an hour from
a set of existing data? I might give you a
picture and ask, what goes in the
missing rectangle? Or I might give an
image that's been scrambled in some way and
ask you to unscramble it, give me back the original image. I think you're sensing
a pattern here, these things all have
something in common. What that is, is that they're all self supervised
learning tasks. Humans, have a need
to predict things and machines in trying to serve those needs, build
predictive models. If you think back to our
description of structure, lot of the things that
machines are trying to predict things for have
interesting structure, whether it's an image with a
scene and lots of objects, whether it's text and
natural language structure, lots of interesting
structure in the world to try to model and
predict things about. The role of self supervised
predictive learning, is to take something as input and some object that has
interesting structure, a sentence, a view of
traffic, a picture. Then, we remove a
part of that thing. We might remove a
blank in the sentence, we might take out part
of a picture of a bird, we might remove the
next five seconds of video and that thing we remove becomes our
prediction target. We're given the rest of
the structure that we observe and we're supposed to predict the thing that's missing. We now have a supervised
learning problem, in that, can we predict r, the thing that's been removed from the other remaining
parts of the object a? Now, the cool thing about this is that we didn't have to exert any effort to get
this labeled example, since it was part of
our observed input. We actually had observed r, so we know what r is. But from the point of view of the prediction algorithm,
it doesn't know. From its point of view r is, you can think about
as labeled data. For example, the missing
word in the sentence, that's the ground truth. Even though the
training algorithm, can see some examples of
that in the training data, we can also hold out some as we can for supervised
learning problems. It didn't take any effort to get this series
of labeled examples, since it was part of our observed input in
the first place. We're doing supervised learning, but we haven't had to go
out and ask human judges, for example, for labels. It's the structure of
the problem itself that allows us to create
these self supervised tasks. More generally, self
supervised learning tasks predict any part of the
input from any other part. This holds for time
as well as space. In the video example, we might see several seconds of video or minutes of video. We want to predict the future, what's going to
happen in the video five seconds from now,
from the past video? It could be the distant past, it could be the recent past. However, we can also
think of a task where we see five seconds of video or we see an image and
we are asked to predict, what just happened before
this point in time? That would also be a
self-supervised learning task. Or we might have an object that has a structure
that has something on top, something on the bottom in space. We might ask the self-supervised
learning algorithm to predict the top of something
from the bottom of it. In general, any occlusion, can we predict the occluded
part from the visible part? We can pretend that there's a part of the input
that you don't know. Can the algorithm predict that? Before I say any more about self-supervised learning
and why it's useful. I want to make a brief digression to summarize deep learning. Because deep learning is the thing that's driving
a lot of this need for huge masses of
self-supervised labeled data. The key thing about deep
learning architectures is that they combine a very sophisticated automatic
feature extraction phase with a supervised learning phase. In traditional
supervised learning, you are the one
that had to figure out what features you had to put in as inputs to your algorithm or
perhaps somebody gave a set of structured
features to you. Maybe they were an expert
in a particular problem, they helped you
develop the features, what things would be more important for
prediction and so on. The thing about deep
learning is that it can do that feature extraction
automatically. To do that, it uses a hierarchy of multiple
feature extraction layers. It starts from very primitive
low-level features in initial layer that capture low-level features like
edges and patterns. Then each feature layers
output provides the input to the next layer which captures
higher-order features. Combinations of
edges and textures, and patterns that form higher level parts
of the structure. At the very end is
a set of layers, one or more layers that do the
final supervised learning. They take that multi-layer feature representation
that was learned. That's what goes in as input to the supervised learning model. Here's a picture of
what that looks like. This is an example of
a digit recognizer. It takes as input, an
array of grayscale pixels, which in this case you can
see it forms the digit 3. There are two main
phases in deep learning. There's feature extraction phase, the automatic phase
I talked about. Then there's a classification
phases, the final step. The classifier is the thing
you might think of as a more traditional
supervised step. Just like any other
supervised algorithm, it takes a set of features
as input and predicts, in this case a digit
from 0-9 as the output. The special thing that's
happening in deep learning is this feature
extraction component, which you can see has
multiple layers here. Now, you can imagine that each of these layers has a
set of hidden units, many hidden units,
and many weights. When the deep learning
algorithm's trained, it uses some form of
differentiable loss function and gradient descent
on a massive scale to take feedback from
classification errors, back-propagate that
into these many, many weights that exist in
all these different layers. Essentially it performs
gradient descent in this huge space of weights. It's jointly learning
to classify, but it's also learning as
part of that optimization, a really excellent feature
representation for, in this case, handwritten digits. This is an example of how the hierarchical nature of the feature representations
comes into play. In this case, this is a paper by Honglak Lee and colleagues. You can see this is the low level of the
model of first layer learns small low-level
edge type features. The second layer starts putting those low-level features together into slightly more
recognizable parts of a face. You can see set of eyes, nose, and so forth appearing
in the second layer. Then the third layer,
it puts everything together from the
second layer into these prototypes
of complete faces from the lower level features
in the earlier layers. This is true whether
you're classifying cars or elephants or chairs. You can see each of these. In the case of the car, it learns to detect things like wheels and doors, and windows. In case of chairs, it learns how to do identify
the backs of chairs, the legs, and the
seat part and so on. The cool thing is that there exists something
called transfer learning, where we could treat each of these object problems as a separate classification problem of separate chair classifier, elephant classifier,
car classifier. But it turns out
that we can learn a more general
representation that works for lots of different
kinds of objects, not just a particular category. That's where self-supervised
prediction tasks come in. Sometimes it's called the
pretext prediction task. It's something that should be
appropriate to your domain. If you're trying to do image
classification, for example, you might pick a pretext task that's related to images such as, can you detect how this
image was rotated? The whole point of
a pretext task is, we don't care so much about doing perfectly on the pretext
prediction task. But what happens on
the way to trying to predict that pretext
prediction task. This intermediate representation, all these layers get learned. We've seen in many studies that the model, this
paradigm actually, if you take a self-supervised
learning task, the quality of the latent
variable models that get learned in this intermediate
representation is quite good. In fact, even better. The same pre-trained
intermediate representation can be used for many
different prediction tasks. Suppose we wanted to predict handwritten letters instead
of handwritten digits. Well, what we could do is we
could have a pretext task, let's say the image rotation, that trains something that understands more about images and what is invariant in them, what a typical low and
high level features, and images of objects or symbols, then we could do that and
train it for that task. It would learn those
different low, medium, and high level feature
representations that we saw for objects
in that example, and we can freeze that intermediate representation,
all those weights. We're talking about a
huge set of weights here, they get frozen and learned, got learned and then frozen. We can use this pre-trained
representation now as our feature extractor for
this new classification task. That is the powerful aspect of self-supervised learning and something called
transfer learning. The fact that we can take
this feature extraction step, this black box, and reuse it for lots of different
classification problems. It means that if we want to
classify handwritten digits, we don't have to go through
a whole training phase, just for that problem. We can reuse the
intermediate representation we learned from
digit recognition. All we have to do is find a few training examples
for handwritten digits, in order to train the classifier, the mapping between the output of the feature extractor
and the labels, just like we would for any
supervised learning problem. But we need a lot less data
because we're not also using the data to learn an
intermediate representation. All we needed to do is
essentially do something very standard with a supervised
learning problem, that it can be done for
many different tasks. Here's another view of that transfer phenomenon
that I described. Here we have a pretext task, we have a dataset
that has no labels. We design a pretext task to fill in the blank in an image, or do something related, that gives us a pre-trained
model for the pretext task. Then we freeze this
pre-trained model. We can move it over here. It's exactly the same model
from the pretext task, and we can use it now in this new target task to predict
certain kinds of objects. Here we might have a much
smaller set of labeled images. The learning task is much easier because we've already pre-trained the key part of the model, the representation in this
self-supervised task. All we need to do is have a few labels for the inputs here, and then do our normal
classification learning here. We only have to learn the
weights for this classifier, instead of the entire
deep-learning system. Same thing goes for text. This is where
language modeling and word embeddings become important. In typical texts
learning situations, there's an encoder
that takes the text and encodes it into some
latent representation, which can be later decoded. In this case, having the pretext
task fill in the blanks. This is exactly essentially what many word embedding
algorithms do. By having this pretext task, predicting which words
go in the blank, or predicting which words are in context given the
word in the blank, we can get this intermediate
representation, which we call an embedding. We've seen just how
useful word embeddings that representation can be for many different kinds of
lateral language problem. When should you consider designing one of
these pretexts tasks? While in situation where getting labels is very expensive, you should consider maybe
doing one of these. If the input data
is unstructured, if it's images, texts, sensor data, anything
that requires a complex, automatically derived
feature representation. Again, it's a situation where you need lots of data to train
the deep learning model. You might also consider
these self-supervised tasks, if the intermediate
representation that you would learn
could be useful to any number of multiple
downstream prediction tasks. In the case of embeddings, those are very useful
for many NLP tasks, part-of-speech tagging,
analogy predictions, semantic matching, and so on. That's self-supervised
learning, it's a very active area of
research right now. It's something that will be
more and more important in the future as the data needs and learning algorithms
continue to increase.