In this segment, I'd like to walk you
through a specific example that gets to the questions of accountability and governance that we've been
talking about so far this week. The example is the case of Tay,
an artificial intelligence chatbot deployed by
the Microsoft Corporation in 2016. Tay, I'm going to warn you in advance,
this segment is going to show you a slide that includes
that includes hate speech, it includes Holocaust denial,
and slurs against Jewish people. I think it's worthwhile to show what
actually happened on the slide, but I'm absolutely condemning the use
of slurs and Holocaust denial. I just wanted to warn you
that that's coming up. The context is this, Microsoft Corporation
as an experiment in natural language processing and machine learning
developed a system that they call Tay, which is meant to evoke
a name of an adolescent girl. They used an image of an an adolescent
girl to represent Tay. The idea of the experiment was that Tay
would interact with people on Twitter, and there would be a real time process by
which Tay's interaction with people would feed a machine learning system that would
teach Tay how to talk more realistically. And Tay was specifically
targeted toward adolescents, so the idea was that it would be something
that would be helpful for young people. As Tay was depicted as a person,
and at the time young people participated in Twitter, now I think
they've moved to other platforms maybe, but at the time they did enough on Twitter
that this was a useful experiment. However, some of you may
have heard of this example, immediately after Tay was released to
the public things went quite wrong. So people who would interact with Tay
would sometimes type inquiries that really had nothing to do with any of the topics
of the tweet I'm about to show you. So they could even ask completely
innocuous questions, like do you like a certain kind of movie or do you
think that Ricky Gervais is an atheist or are factual questions or
a preference or opinion questions, and Tay would respond with
really rather shocking material. So here's a tweet that is an example
of Tay's response to someone query. Now, what happened? Well one of the things that
happened is that Tay was you could say under attack by a group of
people on the Internet that knew that Microsoft was rolling
out this technology and decided to organize to teach Tay hatred. So they taught Tay a variety
of offensive words, a variety of slurs
against different groups. They taught Tay to deny the Holocaust, they taught Tay basically just
to be extremely offensive. And they did that by systematically
engaging Tay in Twitter conversations that would feed
the natural language processing machine learning system
with hatred in other words. Now, this is where the case gets
I think really quite interesting. Here's another tweet,
it's not about Microsoft or Twitter, but it bears on our conversation. It's a tweet and it says, mike Schroepfer, Facebook's chief technology officer told
the Financial Times the company is now mapping out potential threats from bad
actors before it launches products. So this is a tweet commenting on
a public statement by Facebook's chief technology officer. You might see the data is 2018, this is partly when Facebook
was under a lot of pressure from Congress in the United States
over the Cambridge Analytica scandal. This is the response I have for
it, which I think is very funny. This is from Michael Isaac who is
a New York Times reporter, and he says, was this not part
of the launch cycle before? Why would you make that statement in 2018? Are you crazy? I mean I'm reading in but
I think this is what he's trying to say. Why would you not assume that
potential bad actors are going to try to mess with you before 2018. So why is that a new thing? So the reason this is relevant to Tay
tweets is that Microsoft launched Tay Tweets, and the discussion in
the technical community was about why would you not assume that something
like this was going to go wrong? It seems like from what we know, teaching a robot to learn conversation
from random comments on the Internet, if you spend a lot of time on Twitter or
on the Internet, as I do, looking at memes and things, there is
a lot of horrendous stuff out there. No matter your definition
of offensiveness, I predict there is a lot of
offensive stuff out there. And so knowing that,
why would you deploy such a system, especially targeted toward children,
without having some sort of plan? It doesn't seem that Microsoft had,
obviously they didn't have the expectation that this would happen
because it was very embarrassing for them. They did pull Tay from Twitter, but you could say they were pretty slow
about it because Tay was already spewing hatred to anyone who would talk to
Tay for some time before they pulled it. Although they did pull it
within I believe 48 hours, but the problems developed within 24 hours,
and so it's surprising that they didn't have any other way to deal with this other
than to shut the entire system down. It just seems like maybe they just
weren't expecting it to go that way. I think the issue though
that's interesting here for us is that there are a lot
of instances where other systems like this have run
into similar problems before. This is a news headline
from Adweek from 2015, and Coca-Cola had its automated tweet
campaign which was supposed to, it was called #MakeItHappy. And it was supposed to allow you to
spell out some text that you provided in fun shapes that would I guess
also be shown drinking Coca-Cola. The problem with it though is that
a publication name Gawker, and then also some other
people again on 4chan, took advantage of the #MakeItHappy
Twitter campaign to produce a lot of material that was branded with
Coca-Cola but contained hate speech. In this example, the Coca-Cola
bot is outputting the text of Adolf Hitler's Mein Kampf in
the shape of whimsical creatures. So given that we already see things like
this happening whenever there's some kind of system like this let on the Internet,
the question arises as to is Microsoft's team who produced
Tay's AI responsible for this? I mean how much should we say that
they should have foreseen this? And again, I think this is where the table
that I introduced in a previous segment is useful because the arguments devolved
around to the plus and the minus on the vertical axis as is this something
foreseeable by the technical community? The defenders of Microsoft said, well it's
true that this had happened in the past and that the Internet maybe is
a trash fire, as one person said. It's never really happened this fast and
you're really kind of blaming the wrong person, because after all,
there was a concerted effort by 4chan, by people with bad intentions, to attack
this system and produce the hate speech. So in some ways aren't they the speaker's,
the people who were attacking the system? Aren't they the ones that
you should be mad at, the ones that are actually writing
the hate speech that Tay learned? Their response was, again from
the technical community, no, not really. I mean it's widely known that these people
exist and that they talk a lot on Twitter. So if you're going to deploy a chatbot,
especially if you have targeted it for adolescents, you need to have some
sort of safety plan in place so that it doesn't learn hate speech and
regurgitate it. Microsoft actually did follow that
advice later with a later iteration of the chatbot that was released on
Twitter and ran for about two years. It's interesting the way that it would do
that is it had a list of reserved topics, and it would immediately
stop talking about anything on its list of reserved topics. So anyway, this is an example of
a real world scenario in which people have talked about that table and
the distinctions made on that table and tried to reason about who's responsible. I'll end there.