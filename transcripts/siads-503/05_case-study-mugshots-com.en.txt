Hey there, we're back and I have one final case study
for this week. In this week so far, we've talked about
the origination or the provenance of data. We've then highlighted
later in the life cycle, the problems of aggregation in making it hard to
understand the provenance. Just now, we talked a little bit about the
disposition of data. So data's final resting
place or how we might think about maybe this producing problems if the data
is able to be resold. The last thing I'd
like to cover in this week as a concept
that's quite important, is the idea of forgetting. You could say this
is a synonym for disposition because it's about the final resting place of data. It's an idea that's grown in importance in currency
and data science. The idea that there
might be a right to be forgotten or there
might be some right that you have to erase all the data about
you no matter where it has moved or where
it is been resold. So that's really an
interesting issue and I don't have that
much to say about it, but I just wanted to
highlight some of the interesting ethical
dimensions that this puts out. So in Europe, the right to be forgotten was actually
enshrined in legislation. It's interesting because it's often framed as a protection for the little person or someone who doesn't have a lot
of power and they want control over
their reputation. Since it's actually
been rolled out, most of the people
taking advantage of the right to be forgotten have been celebrities
trying to control the press coverage of themselves. So that's not actually probably what the intent was for
that kind of legislation, but the idea was that if someone makes a
Google search result, search for you, that you should have some say maybe in
how you're depicted. Especially in the United States, we have this idea of public
figures sometimes have different relationship
to publicity under the law than people who
are not public figures. So if you don't thrust
yourself into the public eye, maybe you should
have more control over your own facts and image. This is something
that's increasingly being debated in data science. The implication for you
as a practitioner is that as you're working as a
data science professional, if you think about gathering data or taking in data
or originating data, if that's a part of
your work process, it might also be
useful to be sure at some point you have some
mechanism for eraser or removal. It's interesting that the
social media platforms that we've talked about
a few times this week, initially had no
way to delete data. So once they gathered
some data about you, you had to, I guess, let them have it forever. Over time because of
outside pressure, many of the social media
platforms have actually added a way for you to erase
your data on the platform, although they still
make it pretty hard. I mean some of the platforms, there's a whole series
of steps and a lot of, are you sures and then
they still don't delete the data for like 30 days. Of course, that also doesn't
help you if the data are sold or had been sold, it's not clear how
you would even know where to go to get
that data erased. But let's take a particularly
harmful ethical instance of forgetting that I think
is an interesting one. This is a company
called Mugshots.com. The company's business
model is fairly simple. What they do is they
gather public records. Of course, public
records are public, that's the word public in there. So they gather public records
about arrests and then they aggregate these
public records on their website at mugshots.com. Then they do a variety of search engine optimization
strategies to try and push results from their
websites up in search results. So let's take an example. Let's say that you're
one of the people who was arrested on Mugshots.com. If you were arrested
for say drunk driving, Mugshots.com would then find that arrest photo and that arrest record
in a court record. They would digitize it if
it's not already digital, but usually it's digital now. They would put it on their
website and then they would try to create web pages automatically and they
would try to interest web search engines like Google and Bing in those web pages. The goal would be if
someone typed in your name, your mugshot would
appear on the website. So that's their their MO or
their procedure as accompany. The source of revenue is
that they then ask users, visitors of mugshots.com,
to pay them between $400 and $1,500 to
remove the pictures. So their business
model is that they put these public records
online and try to drive publicity to them. But if you pay them from $400
to $1,500, they'll stop. I think this case is not really an ethically
problematic case. I think most people think
of this as blackmail, but blackmail technically has to be about a crime and
these are public records. So I think the reason I find this case interesting is that, it reveals that we have a certain notion of
the definition of the word public and that that notion is really
dependent on technology. So as data scientists, we are in a field where
technology is changing very very rapidly and all kinds of new techniques are
made available. So for example, just
a few years ago, it would have been
hard to believe that a computer would do a good job
of understanding my voice, but now it's completely routine. Similarly with
facial recognition, may be other areas, emotion detection, these kinds of things. We're moving ahead
in leaps and bounds. But the thing is a
lot of our norms and laws are premised on
a particular moment in time and so you could
say that public at one time meant that there was a file in the basement
of the courthouse. The file in the basement
of the courthouse would require someone to
go to the courthouse. So yes, it was public, but it required significant effort. Maybe you would have to
pay a fee in order to see the public record
and then there would also be additional cost in
making it widely available. Certainly, it's something
that would have been possible 20 years ago or 30 years ago to find
Mugshots and promote them. But really, it's
something that because of the decreasing cost of circulating this information
and gathering it, it's created a new
definition of public. So it might be that
what we think of as public is not actually
what we want as public. So what this leads to
then is a question of, do you really want to make some datum or item of information available
in perpetuity? Do you want a plan to think about how it
might be removed? Especially if we
consider minor crimes, it seems ridiculous that
we would want a search for your name to come up with an extremely minor crime
mugshot from a long time ago, that that would be maybe
the most useful thing. I mean, we actually have
in the law a statute of limitations on crimes so that after a certain
amount of time has passed, you can no longer be charged with a crime if you haven't been, even if you're guilty because the crime is perhaps too minor. That's one reason,
there are others. So I think the issue for us
then as data scientist is, what is the forgetting mechanism? So when we have a data set, it might be equally important to think about how things
are removed from that data set as opposed to only focusing on how
the data are gathered. This sometimes talked
about in terms of the right to be forgotten or forgetting just
generally or eraser. I don't have much more, but I just thought it
wouldn't really be a complete week
unless we highlighted the importance of this issue.