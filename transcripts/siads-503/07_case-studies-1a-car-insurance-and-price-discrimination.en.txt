I'm back and still wearing my pin. In this segment, I'd like to give you a more concrete
example that deals with some of the ideas. We covered about classification and
cumulative disadvantage. You could call this the 1A example
because it refers to address numbering. I'll show you a picture here of a street
address in Europe that says 1A. Here's the example,
in actuarial science or in other words in the insurance industry,
the insurance providers often create a statistical model to stay solvent and to
be sure they earn a return on investment. They try to predict how often there's
going to be some adverse effect. Like if they provide auto
insurance as in this example, they have a model that tries to estimate
how likely different people are to get into a traffic accident
that they'll have to pay for. Now, this example is taken
from the Netherlands. I'm not sure why there are so
many examples from the Netherlands in this class, but
that's just where I happen to find it. And the example is from it's
sometimes called the consummate and bond auto insurance investigation. The example comes from
the fact that in Amsterdam, it was discovered that some of
the auto insurance companies were charging higher prices to people
whose street address ended in A. Now in Amsterdam, as in many places, most street addresses are numbers,
so I don't know about you. But I live in a street address that's a
number and if you have an A in Amsterdam, I believe it's more likely that this
refers to a basement apartment. So A1 is often the one directly
down a flight of stairs. It could also mean that your house
is subdivided into apartment. So it could differentiate
an apartment from a house. So once it was disclosed that
you pay more for auto insurance when you have an A after your address
that you provided the insurance company, people started asking well,
why, what's the rationale for this particular decision? And we imagine that there is some
statistical model in which perhaps even I don't know, someone did put in,
does the address have an A in it or not. Like I'm not sure that that's the case but
it could happen, sometimes when constructing these
models people just put in everything. They put in everything they can think of,
they probably already would have whether something wasn't apartment or a house, but
they just put anything they can think of if it makes the overall
variance explained go up. I guess our topic this week is,
is that a good idea? There was a public outcry about the fact
that auto insurance is more expensive for people whose addresses ends in A. And people thought that it discriminated
against the poor because basement apartments are cheaper and they're
more likely to be rented by students. Maybe you have a feeling
about this as a student. The interesting thing for our purposes is that, the poor is not
on the list of protected classes. So, the poor, it's not illegal to
discriminate against the poor in fact, you could say that it's normal business
to discriminate against the poor. We call it price discrimination
in economics, but it's still very unpopular and
it could be unethical. In this case, the insurance industry said
we put all kinds of things in our model and the model is how we ensure that we
stay solvent and decide what to charge. We're not inherently discriminating
against the poor but the public said well, are you suggesting that poor
people have more accidents, and this was extremely unpopular. If this didn't hit close to home, let's try another example related
to discrimination and the poor. In the United States, there have been
a number of instances where it's been revealed that if you buy things
from websites, the websites are using whether you have an Apple
computer or not as a proxy measure for your income, and they use this for
price discrimination. So if you buy things online and it turns out that your browser is running
on a an Apple, I guess they think well, if you can buy an Apple, it's more
expensive than a PC in most cases. So you can afford the higher price for
our product this again, there's nothing illegal and you know having an Apple
versus a PC isn't on the list of protected classes, but let's reason
as to why this makes people so upset. I think there are two important
things to keep in mind, one, there's an element of deception. Some of these sites imply that
you're receiving the best price and the idea that different people are being
offered different prices seems to be unfair because consumers have developed
an idea that they're all receiving the same price or the best price. I don't know if that will change in
the future as machine learning and data science and price discrimination
become more prevalent. A second reason that sometimes people
would object to this is they'd say, well the reason there's an ethical
problem with price discrimination, we like to think of society as generally
progressive and we don't like to penalize people for things they don't have anything
about, don't have any control over. So it's this idea of merit again. People have actually argue the opposite. They said that price discrimination
is more ethical because the surplus therefore generated allows
the corporation to provide service to more people than otherwise would. And so
price discrimination is very ethical and charging rich people more and
poor people less. But if we reverse it and
we charge poor people more, usually you do run into
these ethical problems. There's a famous example that
you might want to know in early French economics when price
discrimination was first discussed. One of the moral problems
of it was detecting when discrimination would be possible. I don't mean illegal or
unwanted discrimination. But if you want to charge rich
people more and poor people less or you want to charge people according
their willingness to pay which is what the economists say is the best thing, how
do you detect their willingness to pay? One of the strategies in price
discrimination has been to make the cheaper offerings so unpleasant that there's no way that you
would want them unless you were poor. If you had any money at all, you would
not choose the cheaper option because it's noticeably worse on
some important dimension. And so this has also created an ethical
problem for price discrimination because people sometimes believe that the minimum
offering is below some standard of decency, that people are are being
actively disrespected. The example given in early economics
was that railroad companies in France used to price their cheapest ticket, rather than provide benefits to the rich
exclusively for the more expensive ticket. They decided that they would
differentiate their classes of service by stripping the roof off of
the railway carriages for their worst class of service and
locating it directly behind the engine. So that when the train was moving,
the lowest class of passenger would be soaked with rain and
their their bodies and clothes would be continually drenched
in the smoke from the engine and with burning cinders
from the engines exhaust. And so, people said your price
discrimination is unethical because you have removed from
the poor basic dignity and in order to separate them from the rich. It's been argued that data science can
solve this problem by more accurately detecting someone's willingness to pay but
then you get into the problems of harmful discrimination based on history,
based on the protected classes. You also get into privacy which we
talked about during a different week. The 1A example, I think the interesting
thing about it for your analysis is that we have discrimination based on something
that's not a protected class, but it turned out to be extremely unpopular. And probably, I would have advised
the company involved to be careful about something that's
going to charge the poor more. It turned out to be a big black eye for
the insurance industry in this case, and it led to government regulation. And you can see I think from our earlier
discussion why people generally think this is an ethical problem. So my message to you in closing is,
don't focus so much on the list of protected classes
that's more of a compliance issue and don't focus on the protected
contexts exclusively. Like housing or employment, but rather
try to figure out morally what it is that we want people to be judged on and when
something would be perceived as unfair, and pursue that strategy for
your analysis. So, thank you very much.