Hello and welcome back. This week I have some case studies for you. But I also have I think what
you could call some methods. That means that I'm going to use
some of the lecture materials for this week to talk about
strategies that you might use to address issues of accountability and
governance. Now, there's a lot of strategies that you
might use and it was difficult to decide what we wanted to focus on because we don't
have unlimited time in this class. But I think the most useful
things that we're going to focus on for you are we're going to do
one segment on transparency and we're going to do one segment on auditing
particularly external auditing now transparencies was an easy choice because
that's one of the solutions to problems identified about ethics in a variety
of data science situations. So I'll give you an example of what
I mean when I say transparency, what exactly am I proposing that should
be disclosed? In a variety of data science situations where there are ethical
problems, sometimes transparency discloses the involvement of humans at all or
what they're doing, what the data are or where they're sourced or
where they came from, the model or the inferences that are made, or sometimes
even the presence of the analysis. Let me go through that again,
but with a little more detail. Sometimes in case studies we've talked
about where there are ethical problems one of challenges that's been identified is that
people don't understand what is going on. And so transparency has often
been proposed as a solution. There's even a movement called
the right to explanation and there are a variety of transparency laws that
could apply in a data science situation. Some of these issues are a little less
obvious than you might think though. And that's what we'll be doing when we
talk about transparency in this segment. For example, human involvement is the first bullet point
that I listed on this slide. In the past, some data science analysts
have used human involvement or the lack of human involvement to
justify their decision-making. I'll give you an example. Google used to - it'd be really important that Google would
claim whenever anyone complained about the search results that were returned
that the process was entirely automatic. And so I think what was meant by
that was that they were saying because our algorithm
delivered these results, and no humans were involved, there could be no particular
bias generated by humans so we know that it's fair because it's
an algorithm. An algorithm meaning, in this case a series of steps executed
in a computer. That's ridiculous, right?
I mean we could design an algorithm
that is itself bias. So the idea that there are no people
is not necessarily a reassuring thing. It actually turned out later on
that, it, that Google was lying. So they did actually have
human involvement on specific search results
that were being returned. And this led to, among other
things, some fines for them other things that are that are a
little harder than they might appear. So one of the things that people might
say is that they're confused as to why an analysis produced a certain result and data scientists have responded by
saying well, what we need is a better, better way to share information
about where the data came from, what are the data, how the model works, what inferences were
made. These have turned out to be very hard problems and some of them will be
addressed in an additional course in the degree program or
additional courses actually, but it turns out that explaining
the operation of a statistical model, although sometimes we require
that in statistics classes, as the models become more complicated, it can be really quite challenging.
The last idea about transparency I want to highlight is that
sometimes it can be a transparency initiative to disclose the fact that
there is a data science analysis. Some people, it turns out, can be unaware. Even if they're using products that
are generated with a data science analysis that something is customized for them. There was a remarkable study of Facebook
users that showed a few years ago that most, the majority of Facebook users
surveyed, didn't understand that Facebook was selecting some status updates from
their friends to show them and not others. So whenever they wouldn't
see a status update, they just thought it was their fault and
that they had missed it. So broadly speaking government's
regulators, nonprofits activists, and users have all agitated for
more transparency and you see this in interactive systems
that depend on data science because you see these little buttons that
you really didn't see five years ago or six years ago with a little I. A lowercase i in a circle or
a question mark in a circle, or maybe a phrase like why am I seeing this? So transparency is a big and
exciting movement in data science. We're just not completely
sure how to do it. For the remainder of this segment. I'd like to talk about some pitfalls and
strategies for transparency. I'm going to use that by talking about
The Limits of the Transparency Ideal, which is an article by Ananny and
Crawford. I'm also going to add in ideas
from other sources though. So although I'm using title
The Limits of the Transparency Ideal, I'm going to refer to other things. I know it's kind of pessimistic to
always talk about limits or pitfalls or mistakes, but I have to deal with the way that the debate in data science ethics has
evolved and that's kind of where we are. You can also make this less pessimistic
by thinking about these pitfalls or limits as as in some cases, they can be they can be reversed so
you can say if I'm careful to avoid this pitfall or this problem then that
could be a strategy for transparency. So don't get too hung up
on the negative phrasing. I have seven limits for you. The first one is that "It's Impossible to
disclose something that you don't know." The reason that that's a limit
of the transparency ideal, I mean, it seems pretty obvious, right? So I think it's useful for data scientists
to have in their arsenal when they talk about they talk about transparency
as a route to accountability, be sure you set expectations
relative to other domains of life. We don't actually understand why
a lot of things happen in the world. So let's take an example from
the media. It used to be back in, ancient history, there were these television executives
who would decide what you would see on television and
everyone saw the same thing. Now, it's much more likely that there's
some sort of data analytic process that determines either what is recommended
to you personally or even you know, what content is produced for
services like Netflix. So now it's a result of
data science when before it was the result of
a television executive. Well, it's important that when you say we
need to understand how the data science works, we didn't understand how
the television executive works. We can't pry open their brain. They would make decisions and maybe they would be able to give
you some sort of explanation. But that also could have
been a rationalization. Generally when decision makers who are
expert make decisions, and they're human, they may not have a really
well developed explanation for why they're doing what they're doing. And so saying that data science
should disclose exactly what's happening, it might be
helpful to step back and say well actually, you know in life
we often don't know what's happening. So why did the television executive
cancel my favorite show could be an insoluble problem. Like you may not know exactly
why that decision was made. It may have been made by
many people in concert. Each of them may have had a variety
of factors on their mind. They didn't have a clear ranking
about which factor is important. Maybe all of them were
important to some degree, once you try to explain that to someone
if the number of factors gets very large, the number of decision-makers
gets very large, the system gets very complicated. Who knows why the favorite show
was canceled. A second limit of the transparency ideal is that
the more complicated the analysis and the more people or institutions or
entities or data sets are involved, the more the transparency becomes
quite challenging because as Weizenbaum famously said, a computer scientist,
"Complexity distributes responsibility." So he also said one of my favorite
quotes about computer programming, "Long programs have no authors." So what I think he meant
by that is at some point after the program gets to
some level of complexity, it's very hard to identify
who is responsible for writing a particular
instruction to a computer and a lot of systems that involve
data science now work this way. So that's an important limit that you have
to keep in mind when you're considering applying some sort of strategy like
transparency. Another limit that's useful to consider is the idea that to be transparent
usually implies that you're moving some information across a boundary. That's like a professional or
an expertise boundary. Usually the people who want explanations
are not data scientist they're users or managers or regulators. And so when you say we're going
to make something transparent, it doesn't just mean, "Tada! I've shown it to you." It also might mean that you're changing
what the assumptions you might have about the knowledge someone needs or has
to sort of interpret what you're sharing. So if I just say, okay go on GitHub and
here's the code, user. That's what we're doing. What does that actually mean? And so this is actually where a lot of
transparency initiatives might break down because you also have to transfer
the expertise to understand what you're trying to disclose and
that might be challenging or even impossible so
it could be that your you are disclosing the information, but you didn't
disclose the expertise to the right audience that was necessary to
understand the information. So the implication there that's really
key that I want to sort of put my finger on for you is that you've got
to know what the audience is. So if you don't know the audience,
you don't know if you should be pitching it for regulators,
the layperson, other data scientists. So you have to figure that out and that
leads to a second point that was implied a little bit by the Ananny and Crawford
piece that I keep referring to but it isn't really there and that's that,
when we say transparency, in your mind, you often think about a requirement to
disclose something. "Tada! Here it is," but true transparency in some substantively meaningful sense doesn't
just require disclosure. It requires an audience that takes in that
information and does something with it and you should always think about
a transparency initiative as a combination of the disclosure and the audience. The reason
that's important is that you could imagine that transparency without an audience
is just an empty gesture. It's meaningless. It doesn't actually help anything and
in fact, and this leads to my next point, another transparency ideal limit
of the transparency ideal is that if you don't have an audience transparency
can actually create an ethical problem. Let me give you an example.
If you reveal information about a system that's an important
system in government or in industry or something like that and
there are problems with the system, so for example, the transparency reveals
that there's some extremely illegal or at least unwanted or really negatively
perceived thing going on with the system, if nothing happens as a basis of
that all you've done is damage the reputation of the system and
you haven't worked toward really much of a resolution of
the problem. You could say knowing about the problem is the first step and
that's true and that is something but really if the problem exists and nothing
is done you then create the sense that nothing will ever be done and that there
is no accountability for the system. So you have to think about the audience
and you have to think about the recourse. Another example of that same
last two slides, earlier we talked about ex-post versus post hoc,
or see Ex-ante versus
post hoc accountability. Those are very different audiences. So thinking about ex-ante anticipation
of what might happen post hoc also is thinking about like, after there is some harm what happens?
I think the issue there is that even it - within the category of post-hoc
there can be very different audiences. For example when you fly in an airplane, there's a black box recorder.
The Black Box recorder contains data on it that
we might expect that no one ever looks at unless
there is a terrible disaster and they'll look at that after a disaster.
That could help prevent future disasters, but it's not going to help the people who
were involved in the disaster that caused the crash of the plane that led
to the Black Box being examined. So are you providing what we might call
"forensic accountability" after a disaster or are you providing what
we might call a routine transparency where there is some monitoring service like a watchdog or a user group
or a non-governmental organization or an internal kind of a tiger
team like structure that's checking to be sure that your
system is doing what it's supposed to do. Those are very different strategies. So you need to think
about who's the audience what's the recourse if
something is found. Second to last actually of The Limits to
the Transparency Ideal is the idea that transparency is temporal and I alluded to
this when I used the Black Box example. So things are transparent when they're being
revealed and they're being received and understood by someone at a particular
point in time, you know, if nothing happens at
the right point in time, you could imagine that the data that are
revealed either they could become unusual. They could be lost. They might be uninterpretable
because the context changes and no one really remembers, like things
can shift from being a problem of governance and transparency to being
a problem of archives and history. So transparency is that is temporal and
the farther away we get from the operation of a system the more that we
struggle to understand what was happening. Even if there was a good system
of transparency in place. I'll give you an example there. I mean have you ever worked at a job where
you had to take over from another person you might have all of the information
about the system already with you, but there's going to be all kinds of things
that you don't really, you're not really sure about, especially if
that person isn't available to guide you. Then imagine
that you have to take over a system where several generations have
passed since it was designed so that it's becomes very hard to look into
the system and you really change modes from just sharing and having an audience
to really kind of this digging in and trying to figure out well what the hell
happened here, what's going on? It's more of like a reverse
engineering in that case. And then finally, I guess there are technical
limitations to transparency there. There are things that can't be
disclosed because we don't have, for example, good ways to measure
the thing that is desired. People want to say, want
us to say something like, how much does a particular factor
matter in an analysis where there are many factors? There are statistical and
technical ways to answer that question, and in some analyses the question
might not be as meaningful or it might be counterproductive and yet
there might be a desire for that question. Like how much does poverty
matter, does being poor matter? There are different ways
to answer that question. And in some sense many of
them could be true, but they're also, you could say that
all of them are kind of limited. So sometimes what's desired for
a transparency just may not really be possible to provide and
it's our job to be really circumspect and thoughtful about the technical and
professional limits to what can be said. There's a movement right now, for example, that sometimes called
"explainable AI" and in that movement researchers have proposed
doing things like sharing, sharing a variety of statements that help
people make sense of really complicated multi-factor systems by highlighting
specific factors of those systems and the influence of those factors. However, in some sense that's really
reducing the information we're conveying, like if I say, "This factor was this
important", if there were a hundred or 200 or 300 factors, the other factors also
did something in some of these analyses, and it might be just as true and and
maybe even more important to say, "These 47 factors acting, together
in a particular way, actually were very important for
this result." However, once we get beyond a few factors,
it's very difficult to explain in English to someone what's going on and
to hold it in our minds. So there may be technical limitations to
transparency in terms of what our summary statistics or diagnostic statistics can report, and we need to be honest about what we're leaving out when we
pursue a transparency strategy. So that brings us to the end of
this segment on transparency. I hope it was useful for you.