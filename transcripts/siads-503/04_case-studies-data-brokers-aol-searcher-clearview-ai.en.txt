Hello again, in this segment we're going
to move from the beginning of the life cycle of data to the end. Those are both key points where we're
going to interrogate this question about, is data science inherently or likely systematically to leave
certain things or people out? We're also going to focus more on
the second framing question for this week, which is about whether or not data can
ever really be said to be portable. So the issue that we raised
in the previous segments for this week was really about where
data come from and how you get data. What we'd like to do now is a link
that idea to where data end up. We were talking about the beginning,
now we're talking about the end. The way we might think about that
is using the phrase aggregation. So aggregation just means putting
data together with other data, obviously, maybe obviously. The reason I want to highlight it for you
is that aggregation is a little bit like a landmine or a pitfall, or whatever
metaphor you want, in data science ethics. Because whenever you merge data together, it's kind of very likely that it will
become a little harder to disentangle the provenance of particular rows or
instances of data in the data set. So let's let's talk through some examples. And so the reason I have called this
particular segment provenance and aggregation is that I want to point out
how way at the end of the data life cycle you can sometimes make moves
that make it hard to see what problems happened at the beginning. And that's a good example. So as an example,
I have here a screenshot of Acxiom, excuse me ax-i-um, I think it's ax-i-um,
yeah, which is a data broker. I've used data brokers in my research in
the past, although they're not this one. I just want to highlight that there's
an ecology of data brokers that if you haven't been working in this area,
you might not be aware of. But there are companies whose
business model is aggregation. So their job is to find
data about people and to add this all together into one massive
bucket of data that they then resell. And so they have a variety of inputs,
including stuff from social media sties, stuff from studies that they commissioned,
stuff from public records. I mean one of the key sources of data
in the United States is credit cards, credit cards are a huge source of data for
the data broker industry. And in this front page of the Acxiom
website, they emphasize marketing. But people do use these data for
all kinds of purposes. The interesting thing about Acxiom and
these companies, I mean, I didn't use Acxiom, but
I used a competitor to them. And I was kind of baffled, as an analyst,
when I started using their interface, because one of the things that they do
is they offer ways to categorize people. And it's a very very long list,
but generally, maybe because, I don't want to say terrible things about
marketers, but I mean I might have to. So I think marketers tend to not have
as much data science experience. And I think that was the target for
the interface I was using. So marketing tends not to be, I mean, as quantitatively grounded
as some other disciplines. So immediately, for me, as an analyst, the way that the data were aggregated
raised all kinds of flags and concerns. So some of them raised concerns because I was kind of concerned that there
were variables in there at all. So I was surprised that there were rows
for things like sexual orientation, which could be seen as against
the law in the United States, depending on how you use that. And there was no warning that popped up or
anything about it, it didn't say be careful, race, be careful, don't use
this in this way or it's a crime. It just was a giant list of data and
you could pull out what you needed. So I was surprised that some data existed,
but then there were hundreds and hundreds of hundreds of variables you
can buy from Acxiom and the like. And many of these variables really have
no explanation, so you can click and click and there's really not any kind of
explanation of provenance in the data. So some of them,
as a data scientist and a professor, I have to say I'm pretty skeptical. So some of the rows in the data set
that I examined were things like likeliness to buy a particular product in
the next seven days, and things like that. Or interested in moving away to
a different state, or something like that. And so these are pretty notoriously
hard to measure, in some cases, and it would really help if there was some
indication of how you were getting this. I mean is it that people were
searching for moving companies or is it that they filed
a change of address form, which is a public record
that they might use them in. I mean these are different kinds of data. Was it added together and
some sort of predicted model? There really was no accountability. So just want to highlight that one
of the problems is that because there is no connection from
the source to the use of the data. It just makes it very difficult for the analyst to know if you're
going to do a good job with this. If you're going to come up with the right
answer or who you're leaving out. So I guess this is just a warning about
aggregation as a point that tends to obscure accountability and
also provenance origination. And makes these kinds of ethical
problems harder to solve. In some jurisdictions aggregation
itself may be illegal. In the European Union, for example, data protection laws protect people
from their data being resold. In the United States that
generally is not the case, and so we have a little bit of
a different experience here. But as an analyst, who feels a responsibility to come up
with good analyses in the right answers, a lot of the aggregated data,
I think, is just sort of garbage. So watch out for that. They'll charge you for it, too. Another part of the life cycle that
I wanted to talk about at the end, is the idea of sort of thinking a little
bit more about where data end up. So earlier this week we talked
about where it began, and I just mentioned its aggregation. But I'd also like to highlight
where it comes where it ends up. And you sometimes call this retention or
disposition, and there's another word I'll refer to later,
forgetting. But retention and disposition refer to the
fact that in some processes you have data that are kept for a fixed amount of time,
and that's sometimes a privacy protection. Sometimes it's a law enforcement
requirement, in some jurisdictions. So you might have a requirement that the
cell phone company retain data about which customers were using which cell phone
towers for a certain amount of time, but then it's deleted. And that's partly to save
cost of storing data, but also perhaps as a privacy measure so someone can't look up where you were
seven years ago, or something like that. Disposition is sometimes thought of
as the final resting place for data. Of course, if your data can be
resold with no restrictions, it might have many dispositions
all over the place. And it might be very difficult for
you to understand where it ended up. So let's go through a couple of
interesting case studies here, I have two. This is a somewhat famous case,
even though it's an older case. It's sometimes called AOL Searcher
4417749, pretty catchy. This is a famous case from a long time
ago, because at one time researchers thought that it was okay to anonymize
data just by taking the names off. There really was not as much knowledge
about data anonymization and the risks that were posed
by poor anonymization. And really, now we think of
anonymization is very difficult and perhaps impossible, but
at the time this was not the case. So in 2006, some researchers at
an internet service provider, called America Online, decided to
release the web search histories of their users to further academic
research about search engines. And so they removed the identities
of the searchers and assigned all of them a number,
4417749 was one of the searchers. And the reason this became an interesting
data science ethics case is that reporters at the New York Times we're
reading through this data set and they thought, you know,
it's really quite easy to identify people from their search history,
even if their names are removed. And I don't know if you're aware, but
your search history is extremely personal. So here is some of the search
history from Searcher number4417749. You see dog urinates on everything,
foods to avoid when breastfeeding, bipolar depression and medical leave,
fear that spouse contemplating cheating. These are things that this
person was searching for. The New York Times actually knocked
on the door of this person, was able to determine their address and
find them. And I think maybe to protect themselves
they agreed to participate in the story, which you can look up. But they said they did a lot of
searching for their friends. I don't know if that's true or not, but
it's a convenient canard that would give you some sort of after the fact
deniability for your search history. You might have a look
at your search history. Google, for example, allows you to
examine your own search history. If you dig into the settings,
there's a page for it and it's really quite revealing about you. So the reason this is in the retention and disposition is that I think
it's an important case. Because it shows that,
it's hard for me to believe, but it shows that at one time people
were just not able to see that this was identifying information,
this was sensitive. And so the earlier way of thinking about
this was that you would retain data forever or as long as you wanted to. And it was totally up to you and that people really didn't
have rights to their data. So if you wanted to
resell it that you could. And I think we're now in a moment
where that is changing and sort of the the the beginning case that's somewhat
well-known in data science of that change was this 2006 AOL Searcher 4417749. One other thing I'd highlight about this
case is that here in the United States, for example, earlier technologies
have had extremely different,sort of, laws and mental models for how we
think about the disposition of data. So for example, in the United States,
if you go to a public library and check out a book you might check out
a book that tells a lot about you. So I mean a famous book in the United
States is called The Anarchist Cookbook, and it's a manual for bomb-making. And it's a quite popular book,
it's available in your public library, I would bet. Because legislators were concerned
about law enforcement's ability to know things about you via
the books you check out, I mean that book perhaps is particularly
worrisome to law enforcement. They passed a particular law
that says that you cannot, if you're a librarian, you cannot
disclose the borrowing record of someone at a public
library without consent. And so it's interesting that
this was such a concern then, but as we move into the Internet, this law seems quite out of step with
what's currently happening to our data. Remember that Google can store and
sell and release all kinds of data,
like your search history. Which, I think,
I would argue is somewhat equivalent to what the legislature was concerned about
with this restriction on library records. I mean, so
currently if I check the library, if I checked the book, The Anarchist
Cookbook, out of the library, I would have this legal protection for
no one knowing about it. But if I typed in the Anarchist Cookbook,
I would not have that protection, if I typed into Google. Let's let's do one more example
of retention and disposition. So this is, again, sort of rather than
looking at where the data originated, you look at where it ends up. This is a very recent New York Times
study, a story, excuse me, about a company that the headline says, The Secretive Company That
Might End Privacy as We Know It. So it's a very successful company
in terms of hype, called Clearview. The motto is, technology to
help solve the hardest crimes. Here's their web page, and
it's sometimes called Clearview AI. It's interesting because it's
a facial recognition technology company that offers its
services to law enforcement. And the interesting thing about
it is that it it gathers all of its data about people
that is then analyzed and resold to law enforcement from
public information on social media. So for example, here's a picture of
a face detection algorithm running on Creative Commons image from Wikipedia,
that's the Wikipedia founder, Jimmy Wales. So if you typed in, if you showed,
excuse me, not typed in. If you show Clearview AI a picture of
Wikipedia founder, Jimmy Wales, it would try to identify him on the basis of this
photograph, which was available publicly. One of the shocking things about Clearview
AI is that it violated all of the terms of service of social media companies
in order to assemble this data set. So Clearview AI would scrape public
data from anything that was available, regardless of what legal terms
were attached to that data. So for example,
if you made a settings change on your social media account,
like your Instagram profile. Or you made a settings change on your
Facebook profile and it became public, it's quite possible that Clearview AI
scrapers would grab all of that data and use any faces that they found as grist for
their analysis. That's tricky because the companies
involved have user agreements and the user agreements specify that you're not
allowed to do any scraping, for example. And you're not allowed to to gather
information for purposes specifically like this one from social media,
but Clearview AI did it anyway. So in the New York Times story,
the journalist, they ran at the journalists face and
found a bunch of images from social media. And he was quite surprised
by how effective it is, and in fact, the company has
promoted these testimonials from law enforcement that says,
we had these unsolved crimes, ran them through Clearview AI,
came out with the right answer. So it's interesting,
the question I have for you is, what is the ethical
dimension to the data here? Because data was created for
one purpose, in this case, like illustrating a panel
discussion about Wikipedia. It was then scraped, it was a scraped
in a way that, in many instances, was intentionally against the rules. But it's not super clear how to enforce
those rules in the United States context. So you violate the terms of service and there's some legal ambiguity as
to the consequences of that. So Clearview AI seems to be going
on kind of a business plan that, is we got the data so we've got it,
and we're going to use it. And so maybe it came from some other
thing for some other purpose, but we're going to go ahead and use it. The challenge made by the New York Times
reporting is that we have never, as a society, really thought
about information in this way. The idea that if something was ever made
public online that it now goes into one giant database. I don't know how good
their scrapers are so it might be a little bit of a hyperbole. But we had a certain meaning
assumed with the word public, when I make something public. But Clearview AI, their meaning of
public is was it ever public, and no matter under what conditions
it all just goes into the hopper. So the issue there is that
after the data has been repurposed without
permission of the users or the people pictured,
do you have any recourse. State laws on this vary, there are actually some state laws
the United States, I understand, in Illinois particularly,
that might have some bearing on this. But really, I mean, I think
the challenge is that it's ethical, that it's beyond the pale in terms
of its ethical implications. We haven't got a good rationale for
how to put this genie back inside the box now that there is this alleged
data set that has all this stuff in it. And so people are quite worried. I'll I'll stop there.