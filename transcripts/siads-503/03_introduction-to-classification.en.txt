Hi again. Welcome back. I've got some interesting
case studies for you. As I said, when I introduced the material for this
week, this week, my part is really to
present some cases, and I think they're going
to help us think through the provocative
framing questions. Now, I know that you've read
already in 501 and probably, it'll come up in other
classes about things like bias or fairness. So that's also the
topic of this week. But one reason for that is
that it really bears more talking about than you get
in a brief introduction. So we're going to try
and dig in a little bit on the earlier things
that you did in 501. The way I'd like to start
that is by thinking about this idea of
classification. I don't want to get too
philosophical on you, but I think it is
worth just pausing for a moment and thinking about how pervasive and
important classification is in thinking. It's this kind of thing that
we all do all the time, and the way that we do it carries certain kinds of baggage into
our data science analysis, and that is something
that if we're ethical, we need to think about,
we need to account for. So what do I mean
by classification? In the most basic sense, I just mean that in order to do anything with data, in fact, in order for data to exist, you have to make
decisions about it that distinguish some
data from other data. Those decisions
you might think of in terms of categorical data, so you might think of
classification as about a category, like putting something into
a high risk or a low risk, or a high-income or a low-income, or perhaps, a color;
red, blue, green. But by classification,
I don't just mean that. I mean that simply
by deciding what the variables are
and naming them, and even with a variable that has no representation
other than a number, deciding things like how big that number is allowed to be, if you have to specify
something like a datatype in programming or in memory or
something like that. So really, I just
mean the big picture, distinguishing some
things from other things. Now, it's been said by smart people that to
classify is human, and that one of the things
that we do when we think is that we divide things
up and distinguish them. We think about this versus
that in all kinds of contexts. I think I'd like to
get a little deeper than our introductory analysis in 501 where we talked about the
idea of data a little bit, and I'd like to think
instead about classification as a vehicle for reproducing
our assumptions. What I mean by that is
that even if we think that a system of
classification is neutral, there's often some way in which something
about our history, it could be about our cognition, it could even be
about our politics, or it could definitely
be about our culture, makes its way into the
classification of that results. So that's tricky for
data science because we often imagine ourselves
as out to analyze, and describe, and explain, and predict the world. That's a very universal thing, the world, but when
we look closer, we often find that a lot of our analysis are actually bound by history and culture and
our own personal experience. I'll just give you some
brief examples of that. One example that doesn't even
merit the term case study but just a simple example that's commonly given is gender. Here in the United
States this year, if you asked me what
the genders were, I'm not sure I would be
super confident in saying because there's a movement to expand our notion of gender, sometimes called the
non-binary movement, that means that gender
shouldn't be classified as simply male or female. Now, maybe, if you'd asked
me when I was a young child, I would have quickly
answered male and female, or man and woman, or
something like that. But if I was in a
different country, it's not clear that I
would do that either. Some countries already have more than two
classifications for gender. So even with something
as simple as that, you'd see that deciding on
how to organize your data is betraying that you're not from one of the countries
where there are three genders, you're from one
where there's two, or maybe, now in
the United States, you're thinking about
this movement to make gender a non-binary value. So that's just a
very basic example, but let's go into an example
in a little more detail. Here's a news article that was
published in The Guardian, and the headline
says, Facebook still suspending Native Americans
over 'real name' policy. It's an interesting situation. So social media networks have waffled on the
idea of whether they should be attached to your
real identity or if they should allow you to create
multiple false identities. So for example, I use Twitter and there
doesn't seem to be any enforcement about whether
you're actually using your real name as
long as you're not impersonating someone
famous and they get angry. Whereas on Facebook,
there's been a movement for a while to try and be sure that Facebook accounts are actually tied to
real offline identities. If you look at other countries, you have apps like
WeChat, for example, which would require verification with a bank account in China, if you're using WeChat in China, and then that's really
going to tie you to an identity as separate
from the platform. So as Facebook decided, as accompany that
it wanted to pursue a strategy where people were authentically using
their actual identities from other platforms
and from offline, they tried to figure
out, technically, how do we implement what
they call their real-name policy which is when you
represent yourself on Facebook, you should give your real name. So we're not completely sure about this because it's
proprietary information, but what it looks
like they did is that they hired some
smart data scientists to come up with a system using likely machine
learning classification to identify things that were likely names and things
that were unlikely names. That's the pretty basic machine learning application
that you might do. I mean, there are
other ways to do it. You could imagine a giant list where you have approved names, like a whitelist of names and a blacklist of non-names that
would be more primitive, but I suspect they use
something more like a machine learning classifier, and you could feed in
things like birth records. So the census, for example, in the United States, lists
names on birth certificates, and that's probably
an indicator of what names are real in
the sense that if it's ever been registered
on a birth certificate, it was probably
someone's name once. So that's the basic
setup of the case study. The issue that I think you may
have intuited already from the headline is that because of the problems
I outlined earlier, that in general,
any minority group, we're going to have less data
about the minority group. Minority, meaning a
smaller part of the whole. The Facebook system
turned out to be extremely bad at identifying
some kinds of names. So I had it on good
authority that some of the big losers in a real
name policy were Armenians, I'm not sure why, I have no access to the model, but also Native Americans. Then there was some
publicity around people who don't use their real name but they do so for an
extremely good reason. So for example, some
people don't use their real name because
they're being stalked, they've been the
victim of violence, and they need to hide
for some reason, perhaps, something like
abuse in a relationship. So there might be
a very good reason why you're not using
your real name. Another category, I guess, is that some people
change their name, and they intentionally
change their name to something that doesn't
seem like a typical name. So all of these
cases are predicted extremely badly by
the Facebook system. So the key takeaways
of this case, one is that classification is the way that you smuggle your worldview into
your analysis. I'm not saying that
because I want to make you feel bad about
having a worldview, I'm saying that that's
just the way it is. We only can understand
things using our culture, and when I'm talking to you, it's just very unlikely
that I'm going to use the assumptions of
the culture I don't know, I'm going to use my own. This points to the importance
of being aware of it. This is really an
important point, I'm not saying we should strive
to have no culture and to be objective because I think that that is totally unrealistic. Instead, the current
thinking is that we should try to figure out how to acknowledge that we're all
coming from somewhere, there is no neutrality in
the sense of classification. What we should do
is to try and be aware of the framing questions
that I told you earlier. That maybe, we have a bias
toward a majority view of, in this case, what is the name. The Facebook case is
an interesting case, not because the air that they
made is that surprising, it's a little surprising
because usually, if you were a company, I think you would not want to
concentrate your bad press on your name policy discriminating
against minorities. So I think, the fact that Native Americans were
upset as opposed to some other category of
people was particularly bad. You'd think they would
have thought of that. I mean, I understand that
Native American names tend to look very different from non-Native American
names in the United States. So it is something that
is a little foreseeable. We do have that topic on
this syllabus as well, when are things foreseeable? Nonetheless, it seems like Facebook could may have
maybe figured this one out. But nonetheless, it's
not that surprising that you would have a
minority impact like this. I think, one of the other
reasons this case is worth talking about is the way
this policy was implemented. I think, the issue
here is hubris. So the policy was implemented
in such a way that if your name was detected
as being an unlikely name, you would receive a
message like this. The message said, "It looks
like you're not using your real name," and the headline was actually, "Update your name." Now, I mean, I think that if Facebook looks at
your name and tells you, "Update your name," but
it is your real name, I don't think they
didn't really think through the user interface here, because this particular dialog
made users really angry. You might manage to
explain to a user, "Hey, we get it wrong sometimes, and this is just what we think," but that's not what
this dialogue says, it says that, "Your
name isn't real." So this particular message implemented by Facebook
turned out to be somewhat catastrophic for
their public relations because it galvanized opposition
to their real name system because people would immediately go on to Twitter and start swearing about how Facebook didn't think that their
culture was real. So the reason I highlight this is that
I think this hubris is not just something
that is required in the data science
analysis but also in its connections to how the user, or the consumer, or the operator, or the manager experiences that analysis and
how it's presented. So I think that is what makes this a particularly
interesting example.