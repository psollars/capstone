Now I'd like to talk a little
bit about the assignment for this week and particularly how
to do well on the assignment. The assignment for this week is about
something called the What-if tool. The What-if tool is a software
package produced by Google. And one of the interesting things about
it is that it's designed to help people diagnose problems with ethics in datasets
particularly with machine learning, but also with other kinds of statistics. I wanted to introduce it
a little before we got into it because I want to be clear about what
our goal is with this assignment. This class is about data science ethics. So I'm not actually trying to get you to
do a data science analysis of a data set. Although we are going to talk about
a data set, we will use a data set but I'm not actually trying to get you
to do a data science analysis. We're also going to cover material that I
don't expect you will have covered yet. So we're going to to use
a machine learning model, but I'm not expecting that
you know what that is yet. You might have covered it already
depending on when you take this class, but it's really not necessary. So what we're trying to do with
this is an ethical analysis, more like a software review. Let me be clear about what I mean by that. There's a movement, that's the kind
of exciting movement that some people are excited about, to try and take
the principles in classes like this one, the principles about justice and
fairness and ethics and to include them in the workflow
of data science more explicitly. Now in some software packages, that
means that they're actually buttons or check marks or analyses in the software
that instead of being derived from what we've learned about statistics are
derived from articles that people wrote about ethics,
which is an interesting thing. Now, it's also a controversial thing. There's a number of these packages
that are currently out and I expect more to come. The reason it's controversial is
that the area of data science ethics that we're talking about is still so
unsettled. There's been a concern that if you
rush to these software packages, you might actually do more harm than good. For example, some people have called
these software packages fairwashing or ethics washing because they
are worried that a manager or an administrator might say I want you to
run your project through this software. I want to be sure we do this ethics
analysis with this software. But then if the ethics analysis
isn't really any good or if it's just not applicable to your
problem, you might create the false impression that there were no ethical
issues with your project when in fact, there are very serious ethical
issues with your project. Now, I'm not paranoid
by raising this debate. There's a lot of good precedent for it. In the United States about well,
I guess around a little over 20 years ago, there was a concern about
privacy on websites. And the response of the industry
in the United States was a voluntary self-regulation
system that was called TRUSTe. The way the TRUSTe was
originally implemented, TRUSTe was a little badge that your
website or later web platform would get that would say that it adhered to
the TRUSTe principles for privacy. And that would reassure
people they think good, that seems better than if
it didn't it here, right? However, the way the TRUSTe was
implemented, it just required websites to have a statement
about what happens to user data. It did not require that websites do
anything useful regarding privacy. It just required that there was
a statement about what was going on. And so for example, you could get a TRUSTe badge on
your software or your web platform if your statement about privacy was users
of this platform don't have any privacy. We will gather as much data as we can and
we will sell it to the highest bidder or in fact, we might give it away. We have no good security practices and
won't secure user data or anonymize it at all. Now that would be acceptable under
the original TRUSTe code and you would get a little green badge
that said TRUSTe for privacy. That's ridiculous, right? I mean you're with me,
right, that's ridiculous. So I think people are concerned that
if we rush to the software solution or a certification solution,
this might happen again. So what we're doing in this part of
the class is like a software review. Imagine that you were
asked by your manager or your peers to look into whether it would
be a good idea if your organization adopted one of these supposedly
ethical software packages and then how would you go about
making that decision. And what we'd like you to be
critical about, what would it do and what would it not do, how would it
help and how would it not help. You can come to your own conclusion as to
whether these things are a good idea or not because you're working in
an area that is very unsettled. I don't think anybody knows really. The tool we're going to talk about
is given away for free by Google. And the advantages of this
particular tool are that it's supposed to help people that don't
actually have that technical background. So it's meant to help people without
a technical background inspect a machine learning model, maybe without
taking the machine learning classes. And it's relatively I don't know,
beautiful as a visualization tool. It produces some nice visualizations. I mean the other advantages of
this one are that it's fast and it's really pretty easy to implement and
it ties into Google's platform. So if you're already doing a bunch
of computing with Google's platform, it's pretty easy to tie this into it. And that's probably why they give it away
to make their platform more attractive. What I decided to do for
this is a little a little complicated, but I think it'll help us. So what I decided to do with
this is to use the example of the COMPAS case study and data set
that we have assigned for this week. Now, you'll remember we had an assigned
reading that was the ProPublica report about the COMPAS system
that predicted recidivism. These are two sidebar images
from the ProPublica report. Now, there was a controversy about the
COMPAS system described in that report. And what I'd like to do is I'm going to use the same kind of database to
work through the What-if tool. It's a little tricky though. I've done something kind of tricky or I guess the What-if tool people
have done something kind of tricky. And that what they did is they kind of are
simulating the ProPublica analysis, but it's not actually the same analysis. So if you read carefully the assigned
article and the notes that aren't assigned that describe their analysis, they used a
variety of tools like logistic regression. But we're not using those tools. So we're actually using
machine learning classifier. So don't worry about that. I'm just saying that what I'm really
trying to get at is more like a software review. So we're going to kind of pretend, go through a data set about recidivism
that we obtained from ProPublica. And then we're going to think through it. But in fact, we're not actually
analyzing the COMPAS algorithm. If we worked at the company that produced
the COMPAS algorithm, we would be able to do that, but we're kind of doing
this from the outside by simulating it. So there's a little bit of slippage there. But I don't think it matters because
really what we're trying to get at is, can you use this software
to make conclusions like those that were made in
the reading that you had? You'll remember one of the key problems
identified in the reading was error and how error was handled between groups,
particularly people who are black and people who are white. There was also some discussion
of a skew in risk scores. So these are images from again
the reading that you had. What we're going to do is we're going to
look at that in a different way with this visualization tool and try to come to some
conclusion about whether it's a good idea to have these tools and
what their pros and cons are.