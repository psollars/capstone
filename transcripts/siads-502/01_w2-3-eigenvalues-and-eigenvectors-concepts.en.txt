Math methods for Data Science, Eigenvalues and
Eigenvectors: Concepts. This lecture is going to cover ideas behind eigenvalues
and eigenvectors. Next lecture we'll talk more
about the mass equations that go into solving
eigenvalues and eigenvectors. Eigenvalues and eigenvectors
are at their heart a matrix vector product
and transformation. For instance, say we have
a matrix a and a vector v, and we want to multiply
a times v. In this case, we would calculate our
matrix multiplication. Our result would be a vector. We would take our.products
for each row, negative one times 2 equals negative two plus
1 times 2 which is two. So negative two plus 2 equals 0. Negative one times 0 equals 0, plus 1 times 4 equals 4. What matrix
multiplication does when we're multiplying a
matrix by a vector, starts with our given vector
here, negative one, one and multiplies that vector by essentially changing
the direction. So our new vector is zero, four. So that becomes
our transformation of this original vector. Essentially multiplying
a times v changed the direction and length of the vector and pointed it
in a different direction. A may contain some kind of
linear transformation which is often used in
computer graphics to change the visual
perspective of an image. Importantly when
you take a vector and multiply it by a matrix, it will point in some other direction which
is true for most matrices. Let's do the same
calculations but with a different vector
and then visualize. Here we start with
a vector one, one. We're multiplying it by a matrix. We will get the values, 1 times 2 plus 1 times
2 should give us four. 1 times 0 plus 1 times
4 also gives us four. So what does this new
vector look like? This new vector goes from
the origin up to four, four. Which is in the
exact same direction as the original vector. You can see that
these resulting vectors are on the same line. The vector didn't change direction but it did
extend the length. This is a special and rare case. It isn't a property of matrix a or a property of
the vector v. It's a property of the combination of a times v. This is
called an eigenvector, where the word Eigen
is German for self. It's an eigenvector
because when we multiply the matrix
by the vector, we're on the same line
as the original vector. Either in the same or
opposite direction. Here's another way to visualize. Here's a two-by-two
matrix a and here's a vector v. We
multiply a times v and we get a vector av
which points in a different direction
as vector V. This is not an eigenvector. Here's the last picture we have an eigenvector where
we took vector V and multiplied it by matrix
a and ended up with a vector in the same direction
as the original vector. Here's the eigenvector formula
where a is a matrix, V is a vector, lambda is a scalar or a single number multiplied
by that same vector. With a two-by-two matrix, this might not be very
impressive but think of a 500 by 500 matrix and multiplying that matrix
by a vector is the same as multiplying
some single number by a vector. A matrix has
as many eigenvectors as the number of rows or
the number of columns, since in this case we
have a square matrix. When we talk about
the entire set of eigenvectors that
belong to a matrix, we use different notation
with capital letters and a capital lambda with
a capital W. Let's return to this example
where we have A and we have V and we have a times v. We know that
a times v is a scaled vector. If we write this a
slightly different way, we can see if we
pull out the number four and write
the original v vector, we have the eigenvector formula. Where A times a vector is equal to lambda times
that same vector. In this case lambda equals four. Eigen decomposition
is the method of finding the eigenvalues
and eigenvectors. It's not typically done by hand. The Eigen decomposition of
a square symmetric matrix, a square matrix being
one with equal rows and columns and symmetric matrix, meaning the off-diagonal
elements are equal or symmetric. These eigenvectors
are orthogonal, meaning that the.products between any given pair of
eigenvalues will be equal to zero or in other words they
meet at right angles. All eigenvalues are real values. You will have as many eigenvectors
as there are columns. Here's an example of a square but not symmetric matrix and an example of a square
symmetric matrix. The off-diagonals
do not have to be zero but they do
have to be the same. This is great but our Data are often not square or symmetric. We could take a data matrix and multiply it by its transpose. The fact that we get
a square matrix should be straightforward from what we know about matrix multiplication, that when we multiply an m by n matrix and an n by m matrix, the inner dimensions
will cancel out and we should create
an m by m matrix. But how do we know that the resulting matrix is symmetric? Here's a quick
mathematical proof. Going into this proof
you do need to know that when you have matrices that are being multiplied and
then are being transposed, you can distribute
the transpose but then you have to swap the order
of the input matrices. So in other words, if we have A transpose times A and we're trying to
transpose the entire thing, this equals a
distributed transpose. But then we flip
the two matrices. So this first value move
to the second position, and the second value move
to the first position. So we can see A transpose
times A, all transposed, is equal to the second
A transposed times the first A transpose
which will cancel each other out and we simply
get A transpose times A. This also works
the other way if we started with A times A transpose. When a matrix is equal
to its transpose, that means it is symmetric. This is the definition
of symmetry in matrices. Why is this interesting? Well first, it gets us to
a square symmetric matrix. When we have observations
by variables, the matrix times its transpose
is not only square and symmetric but it is also
a co-variance matrix. This happens because
multiplication is computing the.product between the variables for each person
and for every other person. This works both ways, A times A transpose or
A transpose times A. One note: if you are aiming to calculate a
co-variance matrix, you will want to mean
center all the variables in your matrix before calculating
your co-variance matrix. I'll show you why this is
important in a few slides. Let's get back to
our eigenvector formula. When we're using data matrices, which themselves aren't
square matrices, we can replace A with A transpose A or A times A transpose which
is a single matrix. Essentially this is
the same equation. If A is a data matrix and A transpose A is the
co-variance matrix, then the Eigen decomposition
gives us something special. The eigenvectors
and W will point to the biggest directions
of variance and the data matrix A transpose A. These are also called
principal components. Say we have some data in
a two-dimensional space, that is we have an x
variable and y variable. The data are clearly
correlated but this coordinate system may not be the best way to
represent the data since the major variation is
occurring not on the x and y axis but on a diagonal
through the set of data. With the largest amount of
variation occurring along this first vector and the second largest
amount of variation occurring on the second vector. In fact these are
the eigenvectors. Calculating a principal
component analysis on these data would require first calculating
co-variance matrix and then Eigen decomposition
to get the eigenvectors. We would get this first vector
in the direction of the most variance in
the second vector in the direction of
the second-most variance. The second picture shows what
would happen if we rotated it so the axes corresponded
to the eigenvectors. I noted in the last lecture
that we need to mean centered data before calculating
the co-variance matrix. If we didn't, then the Data Cloud would not
be centered at the origin. The first eigenvector would
point to where the Cloud is and therefore would not represent the direction
of the most variation. Not a very useful piece
of information.