Math methods for data
science optimization, Loss Functions. Our goal with
optimization is to get a function that works well not only on our training dataset, but on data the function
hasn't seen before. We can compare
our labeled input to our predicted output
to calculate an error. How we compute that error is actually called a loss function. There are no universal
loss functions. Loss functions tend to
quantify how wrong we would be if we used the model to
make a prediction from x, when the correct output is y. This depends on a lot of factors. The presence of outliers, the choice of machine
learning algorithm, time efficiency of
gradient descent, the ease of finding derivatives, and confidence of predictions. Here are a few examples of loss functions for regression
and classification, though there are many more. We have mean squared error,
mean absolute error, and huber loss for
regression; and hinge loss, entropy, and exponential loss
for classification. We're going to walk
through the intuition of loss-function using a mean square and absolute
error for regression, and a hinge loss for
classification examples. Mean square error
is commonly used for regression as
a loss function. The mean square error
is the sum of squared distances between the target and predicted values, or in other words between the observed and expected values. The MSE is only concerned with the average magnitude of errors irrespective of their direction. However, due to squaring, predictions which are very far away from the actual values are penalized heavily in comparison to less deviated predictions, plus MSE has a nice
mathematical property which makes it easier
to calculate gradients, which we'll look at shortly. Does this formula look familiar? When minimizing
an objective function, we also call it a loss function. This is the plot
of an MSE function where the true
target value is 100, and the predicted values
range between negative 10,000 and 10,000. The MSE loss on
the y axis reaches its minimum value at the
prediction on the x axis of 100. The range is from zero to
infinity, for the loss. We might write the loss function for MSE to be a little more generic and not apply to
a two-variable situation. In this case, we have
the minimization over all of the coefficients
to be one over n, times the sum of all of
the deviations between the observed y values based on the prediction by
the x variables, and the actual
observed y, squared. We're going to let y of h
be the equation h_w of x, and our loss function or script l in the range of
all real numbers will end up being a real number, and we're trying to minimize the average loss for each output. We can rewrite our
mean-square error function as a loss function by
substituting out our y_h. One important note is that these functions are
generally convex, so there is an actual solution. If there are multiple minima or a concave section
of the function, it becomes much more difficult
to find the solution, though there are methods to do so which are beyond
the scope of this course. The mean absolute error is fairly similar to
the mean squared error, though the mean absolute
error is the sum of the absolute differences between the target and predicted values. Again, the observed
and expected values. This measures the average
magnitude of errors in a set of predictions without
considering their direction. This is the plot of the mean absolute
error function where the true target value is 100
as in the example previous, and the predicted value
ranges between negative 10,000 and 10,000. The MAE on the y-axis reaches its minimum value at the
prediction on the x axis of 100. The range of the MAE loss is
between zero and infinity. If we use this loss function and consider
the direction as well, that would be called
the mean bias error, which would be the sum
of the residuals. The range of that
error would also be between zero and infinity. Basically using the squared
error is easier to solve, but the absolute error is
more robust to outliers. Intuitively, we can think
about it like this. If we only had to give
one prediction for all of the observations that
tried to minimize the MSE, then that prediction should be the mean of all of
the target values. But if we try to
minimize the MAE, the prediction would then be the median of all of
the observations. We know that the median is more robust to outliers than the mean, which consequently makes the MAE more robust to
outliers than the MSE. Thinking about loss functions
for classification, we could look at the hinge loss. This is the loss function used in support vector machines. In simpler terms, the score of the correct category should
be greater than the sum of scores of all of the incorrect categories by some safety margin, usually one. Hence the hinge loss is used for maximum margin
classification, of which support vector
machines are one. Although this function
is not differentiable, it is a convex function which
makes it easier to work with the usual convex optimizers used in machine learning. Let's look at an applied example
of the hinge loss. Consider an example where we have three training examples and
three classes to predict; cat, dog, and bird. Each of these images will run through a different algorithm, and we're presented with the loss from the SVM loss function. We need to evaluate
which algorithm is best. Remember these are
three different algorithms. In this first algorithm, we know the correct
answer is a cat, in the second, we know
the correct answer is the dog, and in the third, we know
the correct answer is a bird. Which means the other two
categories are incorrect. So we'll define these
incorrect categories as s_1, s_2 for each of
the algorithms separately. Let's take a look
at the cat image. In order to find the loss, we have the sum over
all categories of either zero or that category's value minus
the correct categories value plus one across all categories with an incorrect categorization. In this case, we first need
to find the maximum value of either zero or 1.49, which is our first category that is incorrectly classified, minus our correct
classification plus one. This should give us the max
of either zero or 2.88, plus, looking at
the other incorrect category, the max of either zero or 4.21 minus negative
0.39 plus one, which would be the max
of either zero or 5.6. In these cases, we
have 2.88 plus 5.6, which gives us 8.48, which is a high loss, which is an indication of
an incorrect prediction. We can see that if
we use the values, the highest value here, the greatest likelihood that was predicted was that
this image is a bird, which is clearly incorrect. Let's do the calculations
for the dog example. To look at this dog example, we take the first category
wherever there was an incorrect classification,
negative 4.61, and our correct
classification 3.28, which we can fill in in the
second part of this equation, and our second
category where we had an incorrect
classification, 1.46. This will give us the max
of either zero or negative 6.89 plus the max of either
zero or negative 0.82, which gives us a loss
of 0 plus 0 equal to 0. In this case, we had
a correct prediction that the image was a dog
and it actually was a dog. Let's do the final bird category. For this last image of the bird, our actual bird
classification was a value of negative 2.27. So we're going to put that
into our formula for loss, and our incorrect
classifications were 1.03 and negative 2.37, which gives us values to
find either the max of 0 or 4.3 plus the max of
either 0 or 0.9, which will give us 4.3 plus 0.9, which gives us 5.2, which
again is a high loss, which makes sense because this algorithm predicted
that this image was a cat. Loss functions can be used to evaluate how good
an algorithm fits our data. The choice of the loss
function depends on factors of your problem. Including outliers, time
efficiency of gradient descent, the ease of finding a derivative, the confidence of predictions, and the choice of
machine learning algorithm.