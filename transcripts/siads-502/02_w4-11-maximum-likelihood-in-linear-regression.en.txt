Math methods for Data Science, Bayes' rule and
Maximum Likelihood: Maximum likelihood in
Linear regression. Maximum likelihood estimation
is a technique used for estimating the parameters
of a given distribution, using some observed data. For example, if a population
is known to follow a normal distribution but the mean and variance
are unknown, maximum likelihood
estimation can be used to estimate them using a limited
sample of the population. MLE does that by finding particular values
for the parameter, the mean and the variance
in this case, so that the resultant model
with those parameters, mean and variance, would
have generated that data. Let's return to
a normal distribution, aka a Gaussian distribution, where we have a normal
distribution with sum mean Mu and sum variance
Sigma squared. The probability density
function is f of x, given Mu and Sigma
squared is equal to one over the square root
of two Pi Sigma squared, times e to the negative x minus Mu squared over
two sigma squared. From a PDF function, we can get the probability
related to a data point x. So by substituting values for x the relevant probabilities
could be obtained. We can think of a PDF as a model and that they are
defined by their parameters. In this case the parameters
being Mu and Sigma. Just a word of warning, this lecture is going to be a bit math and equation heavy. First, we're going
to walk through a very concrete example. Say we have several
data points 3, 5.5, 8 and we're trying to figure out what distribution
they come from. We'll say we know there from a normal or Gaussian
distribution, but what are the parameters
of that normal distribution, Mu and Sigma squared. Let's start by assuming some values for Mu
and Sigma squared. Let's say our first guess
is that the mean is 5.5, and the standard
deviation is four. Our second guess is
that the mean of that parent distribution is 11 and the standard
deviation is eight. In addition to being able to calculate probabilities
from a PDF, it's very important to
understand that likelihood is also calculated
from PDF functions. But by calculating
the joint probabilities of data points from
a particular PDF function. That is, we can write
likelihood calculation, as the likelihood
of parameters given data is equal to the product from i equals 1 to n
of the function of the data given the parameters. So for our first guess
we're assuming Mu is equal to 5.5 and Sigma squared
is equal to four. Therefore, we can write our
function as the likelihood Mu equals 5.5 and Sigma
squared equals four, given our data points x
equals 3, 5.5, and 8. This is equal to the PDF evaluated at the point
x equals three, times the PDF evaluated at
the point x equals 5.5, times the PDF evaluated at
the point x equals eight. To simplify, we can write
that the likelihood, the parameters are 5.5 and
four given x equals 3, 5.5, and 8 is equal to f of x equals three given 5.5 and 4 the mean in the standard deviation
we're estimating, plus f of x equal to
5.5 given our estimated mean 5.5 and 4 as our variance, excuse me, plus f
of x equals eight given 5.5 as our mean and
four is our variance. So let's evaluate our PDF
with this information. I've written our known values
up in the top over here. So let's take our PDF and substitute in all
of our known values. One over the square root
of two Pi four, times e to the
negative three minus 5.5 squared over
two times four or eight, times one over square root of
two Pi four times e to the negative 5.5 minus 5.5
squared over eight, times one over square root
of two Pi four, times e to the
negative eight minus 5.5 squared over eight. Let's do simplification
where we can. We will get that the likelihood of
the Mu value equal to 5.5, and the variance equal to four, given x values of 3, 5.5, and 8 is equal to 0.091 times 0.199 times 0.091, which is equal to 0.002. Let's do the same thing
for our second guess. For our second guess ,we
are assuming Mu is equal to 11 and Sigma squared
is equal to eight. We're evaluating at x
equals 3, 5.5, and 8. So I'm going to go ahead
and write out the PDFs for each of these values of x. So after evaluating each part of this PDF and multiplying
those likelihoods, the likelihood that the mean is 11 and the variation is eight, given our data is 0.000004, or four times ten to
the negative fifth. We can see comparing
the likelihoods from our first guess and
our second guess, that our first guess has a much higher likelihood of being true given
the data that we saw. But how can we calculate the precise values for
Mu and Sigma squared? In mathematics to
find values regarding optimization, derivation is used. So what we do is, we write the likelihood
calculation function for all the known selected
data points as follows. L of Mu,Sigma squared
given 3, 5.5, and 8 is equal to the product
from i equals one, to n of the function of the x i, given Mu and Sigma squared. In other words, we
substitute in our values of x and write out the products without substituting
in a Mu and a Sigma. We can do some amount
of reduction here by pulling out one over
the square root of 2 Pi Sigma squared cubed times e to
the negative 3 minus Mu squared over 2
times Sigma squared times e to the negative
5.5 minus Mu squared, over 2 Sigma squared times e to the negative 8 minus Mu
squared over 2 Sigma squared. Next, we're going to take
the natural log of both sides. So we have the natural log
of the likelihood is equal to the log of our first value times
the log of our second, third and fourth value
in our products. Again, we can do
some simplification here since our logs will
cancel out with our es, and we can take
negative 3 natural log of 1 minus 3 times the natural log of the
square root 2 Pi Sigma squared. If you remember your natural log rules when we have division, we can separate them
by subtraction, minus 3 minus Mu squared
over 2 Sigma squared minus 5.5 minus Mu
squared over 2 Sigma squared minus 8 minus Mu
squared over 2 Sigma squared. Then we can do further reduction. Negative 3 log of 1 cancels out, and we have the negative 3 log square root
2 Pi Sigma squared minus 1 over 2 Sigma
squared times 3 minus Mu squared minus 5.5 minus Mu squared minus 8 minus Mu squared, which we can then expand. We can combine like terms so that we have
a differentiable function. Now to find the maximum
likelihood estimator for Mu, we'll do a partial derivative of the equation with respect to Mu. Because we're doing
the derivative with respect to Mu, the first part, negative 3 log square root
of 2 Pi Sigma squared, will drop out as a constant. We can do the partial
derivative with respect to Mu of the function 1
over 2 times Sigma squared times 103.25 minus
33 Mu plus 3 Mu squared. This derivative will be equal
to 1 over 2 Sigma squared. The 103.25 will cancel
out and we'll have 0 minus 33 plus 6 Mu. We'll then set this equation
equal to 0 and solve for Mu. So we can multiply by 2 Sigma
squared on each side, subtract negative 33, so that we have Mu
equal to 33 over 6. So our maximum likelihood
estimate for Mu is 5.5. Now that we have
the maximum likelihood estimate for Mu of 5.5, we can plug that into
our equation and go back and partially differentiate
with respect to Sigma. So to simplify, we have 3 log square root of
2 Pi Sigma squared minus 1 over 2 Sigma
squared times 103.25 minus 181.5 plus 90.75, which is equal to negative 3
log square root 2 Pi Sigma squared minus 12.5
over 2 Sigma squared. In doing this partial derivative, we can split
our equation into two and find the derivative
of each section. I'll move the negative values out in front of
the derivatives also. Further simplifying, we have the partial derivative
with respect to Sigma of 3 log square root
2 Pi Sigma squared, equal to negative 3 over Sigma. But we still have
to do the partial derivative of the second part. So the negative partial
derivative with respect to Sigma of 12.5 over 2 Sigma squared is equal to negative 12.5 over Sigma cubed. Now we can put
these equations together. We can combine these two values, negative 3 over Sigma plus
12.5 over Sigma cubed. Plus because we were subtracting a negative value and we're going to set
this equation equal to 0. 0 equals negative 3 over Sigma plus 12.5 over Sigma cubed. So 0 is equal to negative
3 Sigma squared plus 12.5. Sigma squared is equal to 4.167, and the square root of that
will give us a value of Sigma equal to 2.04. So based off these calculations, we had several data points, three, 5.5, and eight. We knew there from
a normal distribution, but we didn't know
what the parameters of the normal distribution were. So we use maximum
likelihood estimation and we calculated that
the maximum likely values for Mu and Sigma squared
would be 5.5 and 4.167. So we've seen a demonstration of the maximum likelihood
estimation method. How should we apply this
to linear regression? Well, we've seen how to calculate the best parameters for
a normal distribution. We can use this method to find the best model parameters in
a linear regression model. In our previous example, we were given three values
and knew they came from a Gaussian
distribution, and thus we knew the PDF. In linear regression,
what should we use? In linear regression,
the independent variable, y, is assumed to be
normally distributed. The mean that we're estimating is the function y hat or H_w of x_i. This mean is thus
not a fixed value. But for every x input, there'll be a number generated by the function as its mean. In other words, for
every observed point y, we will have
a predicted point y hat, the distance between
which is the error. The mean will be the y hat and so for this particular
value of x, we are estimating a distribution where the center is around y hat. We do this for every observed
and predicted value of y. So here, the line at
y hat is essentially mapping the moving average of moving normal distributions. As a side note, remember this is a linear model because there are only variables with
degree one and lesser. If these were
variables with degree two meaning x squared or higher, then it would not
be a linear model. We can consider the y hat data
as a normal distribution. But this time the mean values of that normal distribution
will be themselves or y hats since they
follow along and perfectly on top
of the y hat line. So the variance of these
y hat data will be zero. We can write this in
vector form such that the vector of y hat values
is equal to the matrix of x values regardless of how many predictor
variables we have times the weights or the coefficients in front
of each of the variables. In other words, we can
write this as y hat has a normal distribution
with a mean of x times w and a standard
deviation of zero. Other assumptions of
linear regression include, that the errors or
the distances between the y hat and the y values
are normally distributed, have an equal variance, and the mean of the residuals or errors is zero.In other words, we can say that the
errors or Epsilon have a normal distribution
with a mean of zero and a variance
of Sigma squared. So if we're assuming y
is normally distributed, and we know y hat is normally distributed and the errors
are normally distributed, and we want to estimate u of
y and Sigma squared of y, we also know that y is equal to y hat plus e. So we can find the expectation of y by calculating e of y
or the expectation as equal to the expectation
of y hat plus errors, which is equal to the
expectation of y hat since this is a constant plus
the expectation of the errors, which is equal to xw plus zero, which is equal to xw. So the mean or the expected
value of y is equal to xw, and the variance of y is equal to the variance
of y hat plus e, which is equal to the variance of y hat plus the variance of e, which is equal to 0
plus Sigma squared, which is equal to Sigma squared. So we know the variance
of y is Sigma squared. So we can say that
the y values have a normal distribution with a mean xw and a variance
of Sigma squared. Now we just need to calculate the maximum likelihood estimate
of xw and Sigma squared. Instead of three data points, we now have n data points, each and the dimension
d. We can write the likelihood as
estimating the mean xw and the variance Sigma
squared given the values of x as the product
from i equals one to n of the function y_i given
x_i_w and Sigma squared. So let's calculate the MLEs for xw and Sigma squared as we
did in the previous example. But note that here we
have n data points. In the earlier example, we only have three, and each of these data points
are in dimension n. Here I'm going to
plug in x_i_w is our estimate of mean
and y_i as our x value. Therefore, my likelihood
function is going to be the product from i
equals 1 to n of 1 over the square root
of 2 Pi Sigma squared times e_negative y_i minus x_i_w squared over 2
times Sigma squared. Then I'm going to
expand this for all of the data points in the dataset. So now I will have 1 over square root 2 Pi Sigma squared to the n times e to the negative
sum from i equals 1 to n, of the y_i minus
x_i_w values squared, all over 2 Sigma squared. To substitute back in the vector form ,I'm going to
have 1 over square root of 2 Pi Sigma squared to the n times e_negative y minus xw
transpose times y minus xw, all over 2 Sigma squared. Now to estimate
the best set of weights or the weight matrix
or the estimates for each of the variables in
the model at the same time, we're going to
partially differentiate the equation with
respect to W. So I'm going to take
this equation and set it equal to 0 after differentiating
with respect to W. So my first step is to take
the natural log of both sides which will give me negative n over 2 times the log
of 2 Pi minus n over 2 times the log
of Sigma squared minus y minus xw transpose, y minus xw, over 2 Sigma squared. Now I'm going to partially
differentiate with respect to W. Since there's
no w in these first terms, they'll cancel out and
the differentiation. We have 1 over two
Sigma squared d, y minus xw transpose, times y minus xw, dw, which reduces to
1 over 2 Sigma squared, y squared minus 2x transpose, wy plus x transpose xw squared which we can reduce
to be 1 over 2 Sigma squared, 0 minus 2x transpose y, plus 2x transpose xw. Now that we have
the partial derivative, we're going to set this partial
derivative equal to zero. If we multiply by
2 Sigma squared, we get 0 minus 2x transpose y, plus 2x transpose xw equal to 0. Solve for w, we get w equal to x transpose y over x transpose x, which can be written as x transpose x_negative
one, x transpose y. So we have found the optimal
values for w in our model. This is the main aim
of linear regression since once we find the w matrix, we can predict the coefficients for each of the x variables.