Math methods for data science,
calculating eigenvalues and eigenvectors in two by two dimensions. Normally I try to stay away
from theoretical math. But in this case, it's going to be nice to
see the connection between what we've done in the theory behind it. Let's say, we have a matrix A that's
in the set of all complex numbers with dimensions n by n. Suppose that, matrix A times a vector
equals lambda times a vector, we could rewrite this as A times
x equals lambda times x. For some scalar lambda in the set
of all complex numbers and nonzero vector x in the set of all
complex numbers in the nth dimension. Then, lambda is called
an eigenvalue of A and x is called an eigenvector of
A associated with that lambda. For any eigenvalue problem, there is an equivalent
polynomial root-finding problem. Theorem eigenvalues and
characteristic polynomials. Lambda is an eigenvalue of A, such that the determinant of
A minus lambda times I equals 0. Here's a proof,
lambda is an eigenvalue of A, A times x equals lambda times x for
every nonzero x vector, A times x equals lambda times x
equals A minus lambda times I. Where I is an identity matrix
of the same dimension as A, all times x equals 0,
that is the determinant of A equals 0. A minus lambda I is singular,
meaning this is non-invertible and the determinant of A minus
lambda I is equal to 0. Let's see what this actually looks like. Here's a 2 by 2 matrix A,
where we have the values 3, 1, 6 and 8. We're going to solve for
the roots of this polynomial. In order to solve for
the roots of this polynomial, we need to calculate the determinant
of A minus lambda times I. This is equivalent to solving for the determinant of A, so we'll substitute in our matrix A minus lambda times I. So I is an identity matrix,
meaning ones are on the diagonal and zeroes are on the off diagonal and we're
multiplying it by a constant value lambda. Doing some algebra,
we get the determinant of 3 minus lambda. 1, 6, and 8 minus lambda. In order to calculate this determinant, we solve 3 minus lambda times 8 minus lambda minus 6 times 1. So in other words, we have a polynomial. 24 minus 3 lambda minus 8 lambda plus lambda squared minus 6, which if we rewrite we have, lambda squared minus 11 lambda plus 18. And if we solve for
the roots using the quadratic formula, we would get a lambda equal to 2,
and a lambda equal to 9. These roots are the same as
the eigenvalues for this matrix. Once we know those eigenvalues,
we can solve for the eigenvectors. Here we know our original matrix A,
our first roots and our second roots. And we know that A minus lambda I
is equivalent to this matrix here. In order to solve for these eigenvectors,
we need to plug in each eigenvalue separately and do row substitution
to figure out the solutions. So first, let's substitute 2 for lambda. 3 minus 2, 1, 6, 8 minus 2. This creates a new matrix 1, 1, 6, 6. I'm going to call this new matrix B, so
we don't get confused in our solution. Here we have matrix B, which is also the same as matrix A minus
lambda times the identity matrix. In order for this to be useful for
our eigenvectors, B times sum vector needs
to equal a zero vector. So let's solve for that vector. We have 1, 1, 6, 6, and where solutions need to be 0, 0. I want ones on the diagonal and
zeroes on the off diagonal, and so I'm going to take -1
times row 1 plus row 2 and replace row 2,
which will give me a matrix 1, 6, 0, 0, and 0, 0. Here we have values of x1,
x2 which will give me this eigenvector. And in order to solve, I have x1 + 6x2 = 0. You'll note that this didn't fit our
typical format of having ones on the diagonal and
zeroes on the off diagonal. But I can pull out this system of
equations here since it's only one equation and
I can solve for x1 and x2. Let's substitute x2 = 1 in this equation. So now we have x1 + 6 times 1 equals 0, so x1 is equal to -6 when x2 is equal to 1. This becomes one of our eigenvectors, -6, 1 corresponding to a lambda equal to 2. Let's solve for the second eigenvector. We have knowns of the original matrix A,
our first root, our second root. And again, A minus lambda I. Let's substitute in 9 or lambda 2 for
our lambda in our final equation. Now we have 3 minus 9, 1, 6, 8 minus 9. This will give me my matrix B equal to -6, 6, 1, -1. With our new matrix B, which is
equivalent to A minus Lambda times the identity matrix, we can solve for
B times x equal to 0. This should give us -6, 1, 6, -1 times sum vector x1, x2 equal to the vector 0, 0. In other words, we have -6, 1, 6, -1 and 0, 0 to use for our row substitution. Let's take 6 times the second row
plus row 1 and replace row 1. This should get us 0, 0,
0 in the first row and 1, -1, 0 in the second row. We can switch row 1 and row 2, and end up with 1, -1, 0, 0, 0, 0. We didn't have to flip those rows but we can pull out this
formula to then solve for x1 minus x2 equals 0. So in other words, x1 equals x2 and the smallest vector here would be a 1,
1 where x1 equals 1 and x2 equals 1, and so
we have an eigenvector of 1, 1. We need to go back and check to
make sure that with a lambda of 9, this is actually the eigenvector
that corresponds to this eigenvalue. So here I have my matrix A times this
eigenvector that I just calculated and I'm going to check and
see if this math works out. 3 times 1 plus 6 times 1 is 9, and 1 times 1 plus 8 times 1 is also 9, and so if I pull out the 9 is my lambda, the matrix I should get is a 1, 1. Therefore, my math checks out and I now have my second eigenvalue and
my second eigenvector. So let's summarize this, when we put
everything together using our formula for eigenvalues and eigenvectors. I start with matrix
A multiplied by sum vector and that should be the same as some
constant times that same vector. In this case, I had lambda 1 and x1 and lambda 2 and x2. Lambda 1 and lambda 2, 2 and
9 are eigenvalues of A, they're also the roots of the polynomial. And x1 and x2 are the eigenvectors
associated with lambda 1 and lambda 2. The good news is,
most often eigenvalues and eigenvectors are calculated
by the computer. However, it's very nice to see
the foundations of where these values come from.