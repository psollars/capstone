Math methods for
Data Science: Bayes' rule. Bayes' theorem is one of
the most important rules in probability theory
used in Data Science. It provides us with
a way to update our beliefs based on
the arrival of new events. The classic example of
Bayes' Theorem is in medicine. Using Bayes' theorem, we can
determine the accuracy of a test by taking
into consideration how likely a person
has to have a disease, the sensitivity and
the specificity of that test. Bayes' theorem is an extension of probability theory in terms
of conditional probabilities. We read the left side
of the equation, called the posterior, as the conditional probability
of event A given event B. On the right side,
the probability of A is our prior or the initial belief
of the probability of an event A. Probability of B given A is the likelihood, also a conditional probability, which we derive from our data. The probability of B
is a normalization constant to make the probability
distribution sum to one. The general form of Bayes' rule
in statistical language, is the posterior probability
equals the likelihood times the prior divided by
the normalization constant. In other words, Bayes' theorem gives us the probability of A given B or how often A
happens given B has happened. When we know the
probability of B given A, how often B happens
given that A happens. How likely A is or the
probability of A and how likely B is or the probability of B.
Bayes' theorem in action. Let's look at an example where
we have event A which is that there is a fire and
event B is that we see smoke. Probability of A
given B would tell us how often we have a fire
given we see smoke. Probability of B
given A would tell us how often we can see smoke
given there's a fire. We know fires are rare, say 0.02 but smoke is
somewhat common, 0.10. Think of grills or barbecues. We also know that there's a 0.9 probability of smoke given
that there is a fire. What is the probability
that there is a fire given we see smoke? Let's write down what we know. We know that the probability of B given A is equal to 0.9. We know the probability
of A is equal to 0.02. We know that the probability
of B is equal to 0.1. If we apply Bayes' theorem, the probability of A given B which is what
we're looking for, is equal to the probability
of A times the probability of B given A over
the probability of B. One way to memorize
Bayes' theorem is to think AB, AB, AB. So let's plug in
the values we know, 0.02 times 0.9 over 0.1. This should give us
0.018 over 0.1 or 0.18. So the probability that there is a fire given we
see smoke is 0.18. Let's take another look
at Bayes' Theorem but a little more visually. Let's say we have
two events A and B. We know that
the probability of A, so the probability of
this entire circle of A is equal to 0.4. We know that
the probability of B, which belongs to this entire
circle of B is equal to 0.3. We know that the
probability of A and B, the intersection here, is equal to 0.2. We can calculate
the probability of B given A using these numbers. So the probability of B given A is actually
just the probability of A and B over
the probability of A, which we actually
have these numbers. The probability of A and B was 0.2 over the probability
of A, which was 0.4. This is just 1.5. We also can calculate
the probability of A given B as the probability of A and B
over the probability of B. So in other words, 0.2 over 0.3 gives me 0.667. Now, what if we have
Bayes' theorem? We could calculate
the probability of A given B as the probability of A times the probability of B given A over
the probability of B. So plugging in these numbers, the probability of A is 0.4, the probability of B
given A is 0.2 over 0.4, all over the probability of
B which is 0.3 which equals, and cancel these out, 0.2. over 0.3, which 0.667. See how all these
equations fit together? Let's look at another example
that may be more practical. Let's say we have
a distribution I've created on what time people are awake based on activity tracker data. I'm trying to decide whether or not it's a good idea
to call someone, like myself, at 8:00 a.m. I smooth this curve over all individuals and
we'll update it with new information to model the probability of someone
like me being awake. Here's my curve smoothed
over all individuals for the probability of being awake across the time
6:00 a.m. to noon. Remember, I'm trying to see if I want to call somebody
at 8:00 a.m. In this case, the probability
of being awake is 0.09. Well, let's take some
information about myself. I'm not in college. Therefore, if I update
this curve based on some prior knowledge
that I'm not in college, the probability of me
being awake at 8:00 a.m. is now 0.22. You also know I have a job. Therefore, I have to be at
work and the probability of me waking up at 8:00 a.m
has gone up substantially. It's now 0.43. The final piece of information is that I have
a two-year-old child. The probability that
I'm awake at 08:00 a.m. now has increased to 0.89. It's an okay idea to call me
now at 08:00 a.m. based on this information and
the updated probabilities we've used from a Bayesian prior.