Math methods for data science, Bayes' rule and
maximum likelihoods. What is a maximum likelihood? Maximum likely estimation is a technique used for estimating the parameters of
a given distribution using some observed data. Essentially, we're trying to fit a distribution to our data. There are many different types
of distributions. Discrete variables have probability mass
functions or PMFs. Continuous variables have probability
density functions or PDF. Our first PMF is
a Bernoulli distribution. The PMF of a Bernoulli
distribution is equal to p to the x times one minus p
to the n minus x. Bernoulli distributions have
successes and failures and x takes on a value either zero
or one, success or failure. Our first probability
density function is a uniform distribution
within the bounds a and b. The PDF for uniform
distribution is the function one over b minus a. For the values from negative
infinity less than a, less than or equal to x, less than b less than infinity. Our second mass function
is the binomial function, the PMF of the binomial function
is equal to n_C_x p to the x one minus p_to the n minus x. Binomial distributions change based on the values of n and p, but the highest mass will always be around the value n times p. Our next density function
is a normal distribution, with parameters Mu and Sigma. Changing values of Mu or
Sigma will change the peak or the standard deviation
of the distribution. The probability
density function of normal distribution is equal to one over the square root of two Pi Sigma squared
times e to the negative 1.5 times x minus Mu
over Sigma squared. Normal distributions
exist for values of x between negative
infinity and infinity. A Poisson distribution has a probability mass function equal to Lambda to the k times e to the negative Lambda
all over k factorial, where Lambda is any value
over the number zero. Poisson distributions are
often used for rare counts, for values of x equal to zero, one, two up to infinity. Our last PDF is for an exponential distribution
which has the function, Lambda_e to the negative
lambda times x. In an exponential distribution xs can take on values
from zero to infinity. Why would we fit
a distribution to our data? First it's easier to work with. Distributions have
known parameters and any solutions you find will apply to all experiments
with the same type. Let's take a quick look
at an example. What is the distribution
of tweets or number of tweets
around a news event? Here we have event time zero, and a decreasing number of tweets after that event at
each subsequent time. In this case, it might be
an exponential distribution. But how would we tell? One of the ways to tell
would be through doing a maximum likelihood estimation. Let's say we have a dataset
and we're trying to figure out what distribution it is
likely to have come from. If it were a normal distribution, then the mean would be
close to the center of the distribution
and we would expect most of the measurements
to be close to that mean. We would also expect relative
symmetry around the mean, it looks promising, this data might have come from a
normal distribution. However, if we move
to that center of the distribution to the left, then there would be
a low probability or likelihood of seeing
these data points. If we centered
the distribution elsewhere, in this case high, then again, the mass
of the data is not likely to have come from
this parent distribution. Here, if we center the distribution around
the mean of the data, the likelihood of seeing
these values is quite high. Let's plot the likelihood
of observing the data based on the location of
the center of the distribution. In maximum likelihood,
we're trying to maximize the likelihood of
observing the values we saw given a location
for the mean. We can do the same for other parameters such as
the standard deviation, where we have
the maximum likelihood, that is the ideal
standard deviation for the parent population from
which these data came. Now, we have the maximum
likelihood estimates for the mean and the standard
deviation or something else depending on which distribution
you're working with that maximize the likelihood of
seeing the data you saw. A few words of warning. Likelihood and probability are two different things though they look and behave similarly. We talk about
probability when we know the model parameters
and when we're predicting a value
from that model. Generally, likelihood
is expressed in the form parameters
given data, whereas probability is expressed as data given parameters. We tend to use
the words probability and likelihood interchangeably
in English. This is not the case in
statistics or data science. Remember that
likelihood refers to the optimal value of the mean
or standard deviation, or whatever parameter
for a distribution, given observed measures,
where probability starts with known parameters and we're predicting a value.