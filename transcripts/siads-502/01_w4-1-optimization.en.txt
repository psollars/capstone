Math methods for
Data Science, Optimization. What is optimization? It's making the best decision,
given the data. Choosing our best route home
to avoid traffic, scheduling your day
to make sure you have the most time for an nap, figuring out which
emails to answer first. Optimization is common
in statistics and machine learning
algorithms for big data. We can apply
optimization techniques to social network analysis, identifying risk factors in
biology and bioinformatics, speech recognition in
text, speech and language, identifying geographical and astronomical features in images, advertising and
consumer analysis, medical imaging,
deblurring/denoising, computer vision in image and video processing amongst
other applications. Let's look at classification
as an optimization problem. Classification is
a form of optimization. We have data, some two
variables x and y, and we're trying to
find a rule that will classify points into
their actual categories. Where exactly would
we draw that line, or curve, or plane? There are several basic elements
of optimization. First variables are inputs. These are free parameters
which an algorithm can tune. Constraints, boundaries
within which the parameters must fall, there can be multiple constraints
on different variables. Objective functions,
this is a set of goals the algorithm uses
to pursue a solution. Are we minimizing error
or maximizing utility, or some other form of
minimization or maximization? Some definitions before we start in the next set
of lectures. Data analysis, using knowledge from data
to extract features. Machine learning, using data to make predictions
about similar data, kernel, taking data in one form and applying a type
of transformation to aid in classification. Deep learning, taking data and passing it through
a neural network, to aid and classification or find the best weights
for a neural network. Convex functions, a function in the set of real values on
an n-dimension interval is convex if the line
segment between any two points on the graph with the function lies
above the graph. Gradient, gradient is a method used to find the minimum
point of a function. Think of this as the speed
at which you are sliding down a mountain at certain
points to reach the bottom. Thinking ahead with optimization, you will likely have
further classes on this subject. This course is designed
to give you a taste of what is happening
with optimization. We'd highly recommend
multivariable calculus and advanced statistics to be able to fully apply these methods. Keep in mind the course
objectives for this course. Describe how optimization
works in everyday life, mathematics, and data science, express the intuition of optimization techniques
for first order methods, including gradient descent, express the intuition of
constrained optimization.