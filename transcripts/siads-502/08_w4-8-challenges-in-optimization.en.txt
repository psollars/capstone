Math methods for
data science optimization, challenges in optimization. In this lecture,
I want to discuss some challenges to optimization. First, we start with
theoretical challenges that may affect the techniques
chosen to find a solution. These include, changing
our weights too fast by adding or
subtracting too much. Changing our weights too little, which might lead to convergence
on a local minimum. Getting stuck on a local minimum. Objectives or performance
measures that are non-linear, non-smooth, non-convex, and no- decomposable
over samples. Some practical challenges include computational efficiency, data storage and processing. The volume aspect of
data is an issue now, largely through
the rapid availability of datasets that exceed
terabytes and even petabytes, whether it's through simulations
and experiments business transactional data or digital
footprints of individuals. Complex optimization is often subject to a large amount of uncertainties such as varying
environmental conditions, system degeneration or
changing customer demand. Two basic ideas can be adopted to address the uncertainties
and optimization. One is to find solutions that are
relatively insensitive to small changes in
decision variables known as robust
optimal solutions, another is to use
dynamic optimization, to track the optimal
whenever it changes. This sounds like a good idea but tracking a movement optimum is computationally intensive and changing a solution
may be expensive, and frequent changes are always
acceptable in many cases. Regardless, the main idea is to reach a realistic
trade off between finding a robust optimal solution and tracking the moving optimum. We also run into overfitting
in optimization. We could use regularization
or adding a term to a function to reduce
the amount of overfitting. Overfitting just
means that your model predicts well on the data
that you use to train it, but performs poorly in the real-world on new data
hasn't seen before. This can happen if
one parameter is weighted too heavily and ends up
dominating the formula. Regularization is a term added to the optimization process
that helps avoid this. We also need to be aware of when we are discovering
correlation, rather than causation
and also a fishing. Big Data has a lot of information
and we may be able to manipulate the data to find whatever it is we
are looking for. To summarize some of
the challenges in optimization, we can think of volume, veracity, variability,
and variety. For volume, we're
talking data and high dimensions with
multiple objectives. Veracity, we may have
noisy fitness evaluations. We're aiming for robustness and could use surrogate-assisted
optimization. Variability. Sometimes
we have moving optimums. Our goal is to be robust
over time and deal with constant data updates and make solutions that are sustainable. Variety. Our data is often heterogeneous in
content and source. We bring knowledge
from other fields into optimization constantly and are generally modeling very complex and
heterogeneous data. Lucky for us, optimization has been programmed
into a number of computer programs and packages including several packages
in Python like, SciPy and pyOpt, as well as in the R programming language. These computer programs
can take user input and do the difficult work of splitting the sample into
training, validation, and testing samples,
processing the optimization, calculating gradients or using other optimization methods, and producing optimized outputs. Future courses in data science, including data mining I and II, supervised and
unsupervised learning, deep learning and machine
learning pipelines, will build on all of the concepts you have
learned here and will expand on the theory and
applications of optimization.