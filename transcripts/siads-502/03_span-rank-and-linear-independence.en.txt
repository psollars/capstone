Welcome back to Math
Methods for Data Science. In this video, we're
going to talk about span, rank, and linear independence. Previously we saw that a matrix can be interpreted as a
linear transformation. Here we're going to see how
some of the properties of linear transformation relate back to properties of the matrix. First, we're going
to start off with a definition that is the
span of a set of vectors. The span of the set of vectors is equal to the set
of all points that can be reached by a linear
combination of these vectors. This is just all the
points that you can reach by putting these
vectors together. If you're taking the span
of a set with one vector, this is equal to the
set just c times v_1, or C belongs to R. We know basically what
happens with that. You start with some vector v_1 and you can stretch it out to be longer or multiply it
by negative numbers and get it to go backwards
but it traces out a line. If you take the span
of two vectors, then one of two things happens. You start off with the span of the first vector, this v_1, and either the second vector is a linear transformation of the first meaning it
equals c_1 times v_1. It's already contained
in this line here. If it's already
contained to the line, we don't get any more points. Or v_2 is not in the
line here in which case, we now cover a plane of points. A surface that's similar to R2. If we're in R2 we'll cover all the points
because what we can do is using v_2 we can
shift this whole line. If we v_2 here we get
shift the line down here and we can get any
point in between by multiplying v_2 by
a smaller number. We can just shift this line
down by multiplying v_2 by a bigger number and
shift this line up by multiplying v_2 by
a negative number. We'll cover the entire
surface of a plane. Let's look at a
particular example. Span of (0,1). (0,1) is just this point here. If we multiply this
by a big number, we get something up here and if we multiply
a small number, we get down here. The span of (0,1) is just
equal to the y-axis. Similarly, if we take the span of (1,2) it traces out
this line here. If we multiply a positive number, it extends the vector longer. Negative, it goes down here
so we get this line here. If we take the span of (0,1) and (1,2) this
equals all of R_2. Why? Because to get any point (x,1) (x,2)
we can just take, well let's make the
x vector correct. You get the x vector correct, we're going to take
c_2 equal to x_1. Then the x vector
would be correct. Then if we take c_1 equal to, well we need that two
times c_2 plus one times c_1 equals x_2. We know that c_2
equals x_1 so we get that c_1 equals x_2 minus 2x_1. If we plug these in here, then for any x_1, x_2, we just plug c_1 equals x_2
minus 2x_1, c_2 equals x_1. We will hit that vector x_1, x_2 so we can show it is
actually in this span here. You can also see it pictorially. What we're doing here
is we're using v_2, which is this vector here. Using the one two vector to
shift to the correct x value, say x equals 5 or
something up here, and then we can just
use the first vector 0, 1 or v_1 to shift to
the correct y value. That spans the whole plane. One important thing to know is that the space you span
is always a subspace. It always looks like R^k for some k. When
there is one vector, we got something look like R, it was just a line. When we added a second vector, either we got a line back, the same thing if
they were co-linear, or if we added another
vector that was not in the span of v_1 we got something looked
a lot like R^2 back. In general, if these vectors
are two dimensional, if they have m dimensions
where m is five, it could span a two-dimensional
subspace of R^5, but nonetheless it'll
look a lot like a plane. The basis vectors of RN are e_1 equals 1, 0, 0, 0. E_2 equals 0, 1, 0, 0 and to e_m equals 0, 0, 0, 1. These are vectors that have one in exactly one coordinate, zero everywhere else, and the subscript tells
you where the one is. E_2 the one is in the
second coordinate, e_m the one is in mth coordinate. Now if we look at a matrix times one of these
e vectors what happens? Let's say we have a matrix A, we'll make it very
clear right now. A equals 1, 2, 3, 4. If we take A times e_1, this is equal to 1, 3, 2, 4 times 1, 0. You take the dot product of the first row of this vector you get 1, second row 2. You can see that the result is merely equal to the
first row of the vector. This isn't an accident. If we take this matrix times e_2, we'll just get the second
column of the matrix. If you multiply this
matrix times e_i, it'll pull out the ith
column of the matrix. This is true in general. If you have a set of vectors v_1, v_n that belong to R^m, so these are m vectors
and we put them into a matrix as the columns v_1, v_2, v_n so now we have a matrix
with m rows and n columns, then if we multiply this by e_i, it just gets us back
the ith vector. What that means is that if we think of
this as a matrix A, the span of these vectors is going to be equal to the image of this
linear transformation. What do I mean by that? If you take the span of these vectors, that's just Alpha_1, all the points you get
Alpha_1 times v_1, Alpha_2 times v_2,
Alpha_m times v_n, but this here equals
v_1 v_2 this matrix, v_n times Alpha_1
Alpha_2...Alpha_n. Anything in the span
of these vectors is something you get in the image of this linear transformation by using instead of
the coefficients, you just stack the
coefficients up into a vector. The span of v_1, v_n is equal to this image. We'll just call it A of R^n. This is our set S here which is all of the possible inputs is equal to the result of
all of the possible inputs. Now let's talk about
linear independence. We said that vectors
were linearly independent if the
only way to get zero when adding together was if all their
coefficients are zero. For any set of vectors, there's a maximum number of them that are
linearly independent. This turns out to be a fact. There's some maximum number that can be linearly independent. Note that you can never have more than m linearly
independent vectors. Why is that? Well, we'll
see it on the next slide. The column rank of an n by m matrix A is the maximum number of
linearly independent columns. We said there exists
a maximum number. An n by m matrix has full column rank if
its column rank is m. That it has m columns and they're all
linearly independent. The column rank is also equal to the dimension of
A applied to R^m. This is the dimension
of the image. Remember A, if it's an n by
m matrix it maps R^m to R^n, A maps R^m to R^n. The dimension of the output is at most the number
of columns and input, because each one can
simply add up a line, a dimension to the output, and it will have
full column rank if the output dimension
is as big as possible. Similarly, the row
rank of an n by m matrix A is the number of
linearly independent rows. An n by m matrix has full row
rank if its row rank is n, and the row rank is equal to the dimension of A
transpose applied to R^n. A maps R^m to R^ns
when you transpose it, it maps R^n to R^m. It's equal to this dimension, so it's full if the
image of R^n is actually equal to an n dimensional image
from the transpose. The column rank is always
equal to the row rank. This is maybe somewhat
surprising property. The rank of the matrix
is then defined as either the column
rank or the row rank. Again, one way to
think of this is A is a linear transform, and it's the dimension of the resulting image of
that linear transform. One result of this is that because the column rank is always less than
the number of columns, the row rank is always less
than the number of rows. The rank of a matrix is always the maximum of the number of rows and the number of columns. The rank is always less than the smaller of the rows
and the columns. This is easy to see in
this transform idea. If your matrix A maps, say R^m to R^n, the image can't have more than m dimensions
because it only has m matrices and vectors
that are spanning this space. But the image cannot
have more than n dimension because once
you have all of R^n, you only have n dimensions. The rank is equal to
the minimum of m and n. Let A be an n by m matrix. If we think of A as a
linear transformation, A maps R^m to R^n. The first claim is that if
A has full column rank, then A is injective. Recall that injective
just means that A of x equals A of y implies
that x equals y. The only way to get the same value is to start
off with the same columns. We already saw that A
of 0 has to equal 0, and so if there's
another way to get zero, if A of x equals 0, but x does not equal 0, that implies that
the columns of A are not linearly independent. This is a sort of
generalization to that. It says that if there is no way to get
zero more than one way, you can't get any number
more than one way. A here, it has full column rank, the number of columns is M.
The output has dimension m, and so you're mapping R^m to
another m-dimensional space, and it is injective. The second claim is
that a matrix with full row rank is surjective. Full row rank will
mean row rank n, because there's n rows in A. Remember surjective
means that for every point in the
target space R^n, there exists some
point that maps there. For y belonging to R_n, there exists x belonging to R_m, and so A of x equals y. This is pretty clear because
if we have full row rank, we have row rank n, and we said that means
that the dimension of the image is dimension n, which means it's all of our n, which means everything is n. Last matrix has
both full row rank and full column
rank is bijective, so recall that bijective just means it's injective
and surjective, but this also
implies that there's a one-to-one correspondence
between the image, the pre-image, and
the post image. If it's both, A has to
be an n by n matrix. Must map R_n to R_n, and it must be that
all the rows and all the columns are
linearly independent, and then it's just pairing up R_n to R_n in a way that rotates, and stretches, and
potentially flips. Let's do some examples. Let's say A equals 1, 2, 2, 3, and 3, 4. This is a three by two matrix. The first thing to notice is
that A has full column rank. If I think of the first
one as a vector to get the second vector while the first coordinate I have
to multiply by two, but if I multiply one
by two, I get two, but if I multiply two by
two, I don't get three. These two, the two columns
are not co-linear, and therefore it has
full column rank, and therefore A is injective. But let's look at the rows. It has three different rows here, and we know that
the row rank can be at most two by what
we said before. We could in fact find this, and we will here. We want x_1 plus 2x_2 equal 3, and 2x_1 plus 3x_2 equal 4, so that we're going to equal our third row by some multiple
of the first two rows, and we can multiply both by two, and then subtract the
second from the first, so that this will be zero here, we'll have 4 minus 3, which is x_2 and we will
have 6 minus 4 which is 2, so x_2 equals 2. Then we know that
x_1 equals minus 1 to make the first one work out. We can check that both
of these work out. If we take minus 1 times
1,2 plus 2 times 2,3, this equals the third
vector so we take minus 1 times 3,4 this equals
the zero vector, and so these are
linearly dependent. Therefore we know that
A is not surjective. We can also figure this out
because A has two columns, the span of two columns is
at most two dimensions, but this A maps into R_3, so of course it's not surjective. For example, you
cannot reach one, zero, one, and this is
not too hard to check. Let's look at another example. Here, we're going to
have A equal to 1 2 3, and 2 3 4. We just took the transpose
of the previous matrix. Now A has full row rank,
so it's surjective. But it does not have full column, so A is not injective, which makes sense
because you're shoving three-dimensions down to two-dimensions in this transform, and so you're going
to get some overlap. An example would
be X equals 1, 1, 1 maps to 6, 9, and similarly, maybe X
prime equals 2 minus 1, 2, and A of X prime is
also equal to 6, 9. This shows that A is
in fact not injective. Let's do another example. A is a 3 by 3 matrix, 1, 2, 3 2, 3, 4 3, 2, 1. The third row is equal to four times
the second row minus five times the first row. What that means is that A
does not have full rank, because it doesn't
have full row rank, it cannot have full column rank. The columns are
going to here span a dimension of two
instead of three, so A is neither injective nor is A surjective, which makes sense. A is going to map r3 to r3, but it's really only covering
a two-dimensional subspace. It's mapping to a
two-dimensional subspace in r3. Once you have the span of, we'll call this V_1 and V_2
is going to equal to 2, 3, 2, and the span of V_1 and V_2 is equal to the span of V_1, V_2, and V_3. We're not getting anything extra by including the last coordinate, and we're shoving three
dimensions down and this two-dimensional
subspace of R^2, and so it's neither
surjective because only covers a two-dimensional
subspace of R^3, nor is it injective
because you're pressing three dimensions
into two dimensions. Some examples are that 1, 1, 1 equals A times 2 minus 1, 2. You can check this out, these are in fact the
same result here. Let's see another example
of a matrix that's neither injective or surjective. You can see here
that in this A, V_2, that V_2 just equals 2 times V_1. V_1 and V_2 are co-linear, and so this A only
has row rank 1, and maps R^2 to R^2, but really it maps
R^2 to a subset, a one-dimensional subset of R^1. What happens here is if you
take this before vector, the red one, it maps
to this blue vector. What it's doing is it's
projecting down onto this subspace and then
stretching it out a little bit. Similarly, before I startup
this yellow vector, projected down to the
subspace and stretch it out and I'll get this
green vector back again. All the results
are going to be in this one-dimensional
subspace here. You're can see this
is clearly not surjective nor is it
injective because anything along this line here is projected down and then stretched out
to the same point. What we've seen is that we can look at the
rank of a set of vectors, which is equal to the
dimension of the vector span. This is equal to the
dimension of the image of a linear transformation
corresponding to that vector. The vectors are
independent if and only if the image has the same dimension as the number of vectors.