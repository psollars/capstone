Math methods for Data Science
Matrix Operations two. In this lecture, we'll discuss
outer and inner products, matrix inverse and transpose, and the trace and
determinant of a matrix. A transpose matrix denoted AT, is essentially a flip
of the matrix. It's written in
terms of components as aij transpose is equal to aji. A transpose matrix will result in a matrix that has the opposite dimensions
as the initial matrix. You can see in this example
of a matrix and its transpose that the diagonal
elements don't change. This is because when
we do a transpose, we're using the diagonal
as an anchor point in which to flip the matrix symmetrically across
that diagonal line. The operation of transposing a matrix has multiple properties. First of which, if you add two matrices together
and then transpose, the result is the same
as if you transpose each individual matrix
before adding together. If you multiply two matrices
and then transpose, it is equal to the product of the transpose of each matrix, but in reverse order, this applies to
more than two matrices. Also, if you find the inverse
of a matrix transpose, it's the same as the transpose
of the matrix inverse. We'll talk more about this when we talk about
matrix inverses. Let's look at
the practical application of applying a transpose. Here I have a defined matrix A, which is a four by two matrix. I know that my transpose matrix should have the
reverse dimensions, I'm expecting
a two by four matrix. The diagonal elements
will not change, so I can place those in their
positions in my new matrix. Then I simply flip
all of the elements. I take the first column and
that becomes the first row. The second column which then
becomes the second row. In order to talk about
the inner product, we need to revisit matrix
vector multiplication. It's a quick reminder
when we talked about matrix vector multiplication
in the last lecture, I mentioned that this
can be thought of as a dot product of
each of the rows. A little more formal, recall that a dot product of
two vectors is equivalent to the sum of each of the
matching components products. If you think of each vector
is an n by one matrix or a column vector and we rewrite our dot product in
terms of matrices, we get the dot product
of the two vectors is equivalent to
the transpose of one of those vectors times
the other vector. Since we're thinking of
these in terms of matrices, here I have a one by n matrix and an n by one matrix that I'm
multiplying together. My result is a
one-by-one matrix or a single number that is the sum of the products of each of the
matching components. So really, the dot product is matrix multiplication and
matrix multiplication is defined as dot products. Let's calculate an inner product using these two defined vectors. Since we're thinking of
these vectors as matrices, the first thing we have
to do is transpose the first vector and multiply
it by the second vector. So I can rewrite this
as the one by n vector 2,1,3 and multiply this by the column vector 6,5,4
which is an n by 1 matrix, which I can then find
the dot product of. My first value will be 2 times 6, plus 1 times 5, plus 3 times 4, which should give me the value 12 plus s plus 12 equal to 29. This makes sense,
because I should have a one vector or even a one matrix which is
equivalent to a single number. When we calculate
the outer product, we're multiplying two
column vectors and applying the transpose
to the second vector. This should result
in an n by n matrix, because we're taking the first
vector which is n by one, and multiplying it by the second factor
which is one by n, the ones will cancel
out and we will create an n by n matrix as our results. Here we have defined
our two vectors again and we're applying the transpose to
the second vector. I can rewrite this
as our first vector 2,1,3 or our column matrix
as we're calling it, times our second
vector transposed 6,5,4 and now I can
create our outer product. I'll take the first elements
from each vector, 2 times 6. That becomes the first component. Our first element from the first vector and
the second element from the second
vector, 2 times 5. Finally, our first element
from the first vector, times our third element
from the second vector. I'll continue to do this for each element in the first vector. This will give me a resulting
matrix of n by n, where my matrix is equal to 12, 6, 18, 10, 5, 15 and 8, 4, 12. This is our outer product. What is a matrix inverse? When you multiply an invertible
matrix A by its inverse, A to the negative one, you will produce
an identity matrix. An invertible matrix is
one that has an inverse, that is, the determinant
is not equal to zero. An identity matrix is
a square matrix that has one on the diagonal and
zeros on the off diagonal. In other words, an identity
matrix is equal to a matrix that has ones on the diagonal and zeroes
on the off diagonals. In this case, this is an identity matrix of
a three by three matrix. Square matrix means
that m is equal to n. All of these definitions might feel very circular
since we're using new words in almost every
one of these statements. Let's break it apart and pull these statements out and see
how they all work together. Here's our first statement. When you multiply an invertible
matrix A by its inverse, you will produce
an identity matrix. We write this as A inverse times A is equal to an identity matrix. Also, if we multiply a vector
by the identity matrix, the vector will remain unchanged. Mathematically, this is written showing several
properties of matrices. First communicability
of parentheses. The P\parentheses
are interchangeable. Here we have A inverse
times A times a vector is equivalent to A inverse times
A times that vector. Since A inverse A is
an identity matrix, multiplying an identity matrix by a vector does not change
the resulting vector. We typically don't calculate
the matrix inverse by hand, this is something that
computers will do, but it's a useful property
that we'll see in the future. We'll talk more
about determinants, identities and
invertible matrices as we discuss eigenvalues and
eigenvectors in the next week. Also, as we talk about determinants and
traces of matrices. The trace of a matrix is the sum of the n values
on its diagonal. Note we're defining
a matrix is n by n, meaning this is a square matrix. We say, the trace of an n by
n matrix is a single number. We also write the trace
of matrix A as the sum from i equals 1 to the number of rows or
number of columns, since they're equal, of all
of the diagonal elements. So let's calculate
the trace of this matrix. Identify all of the diagonal
elements and sum them, 12 plus 5 plus 12 equals 29. Traces have several properties. First, the trace of
two scaled matrices summed together is equivalent to the scaled Trace of
each of those matrices. This is called a linear property. The trace of matrix
a times matrix B is equal to the trace of
matrix B times matrix A. The trace of matrix
a times B times C, is equal to the trace
of C times B times A, which is also equal to
the trace of B times C times A. This is called a cyclic property. The trace of matrix a transpose is equal to
the trace of matrix A. This should be intuitive
since the diagonals will not change when we
do a transpose. Finally, the trace
of a is equal to the sum of the eigenvalues of A. This will make more sense
when we talk about eigenvalues and
eigenvectors in week two. The determinant of a matrix is a calculation that
involves all of the coefficients or components of the matrix and whose output
is a single number. In other words,
the determinant of an n by n matrix is equal
to a single number. A determinant is an indicator of the relative geometry
of the row vectors. It tells you the volume of a box with sides given by the rows. What do I mean by this? Let's take a two-by-two matrix. The determinant of
a two-by-two matrix, if we label each of
the values a, b, c, and d, is equivalent to a, d minus b, c. a, d minus b, c is the area of a parallelogram formed by the coordinates
of vectors a and b, and c and d. In other words, if I start at the origin zero, zero, and I create a vector a, b, and I create a vector c, d, completing a parallelogram,
I have a plus c, b plus d. The area inside this parallelogram
is ad minus bc. How do I calculate
the determinant using a three-by-three matrix? What we saw with
a two-by-two matrix, I could take ab times bc, but that isn't the case with
this three-by-three matrix. However, if I select
an element like a, here I can multiply a by the determinant created
if we knock out the rest of the values in the same row as a and in the same column as a and create
a smaller matrix where I can calculate the determinant a times the determinant
ei minus fh. Or in other words, I'm going to take
a times ei minus fh. I'm going to subtract the next elements
and the determinant created by knocking out the next
elements row and column. Let's take a look at this. So if I select the next element and knockout the row and column, I'll create f times
the determinant d, f, g, i where I'm subtracting
off b times di minus fg, and then adding the next element times the determinant created by knocking out that
elements row and column. In other words, c times the determinant
created by d, e, g, h, or in other words, c times dh minus eg and that will give me
my final determinant value. That seems very complicated, but once you see the pattern and figure out how to do
the calculations, it becomes a lot simpler. So let's actually apply this
to a three-by-three matrix. Let's start with
the first element here, one, and multiply it by the determinant given by
the matrix five, two, eight, one, which we simply got by creating this as
our new two-by-two matrix, and then we're subtracting
the next element, two times the determinant created by knocking
out the rows and columns of that element,
six, seven, two, one and then adding the third element times the determinant created by knocking out that elements
rows and columns, six, seven, five, eight. Let's simplify some
of this algebra. One times 5 times 1, minus 2 times 8, minus 2 times 6 times
1 minus 2 times 7, plus 3 times 6 times 8 minus 5 times 7. This should equal negative
11, minus negative 16, plus negative nine, which
is equal to negative four, which is the determinant of
this three by three matrix. Now, one thing you might
notice is that we have a minus sign here and
a plus sign here. This alternates. So the first set of two elements times
their determinants will be a minus sign, the next set will be
a plus sign/if we keep doing this in
bigger and bigger matrices, we'll alternate between
minus and plus and minus and plus for all of
the rest of the calculations. Now, this is definitely not something that we
do by hand often. It's very easy to
make mistakes and we typically will let
the computer do this, but I do want to show
you one more example. So this is a matrix where the determinant is
equal to a value. Let's take a look
at another matrix. This is very similar to
the previous one but we changed a couple of the values of a
few of the elements here. So the four, six, nine are new, let's calculate
this matrix determinants. So again, I'm going to
take this first element, one times the determinant
of five, six, eight, nine, minus two times the determinant
of four, seven, six, nine plus three times
the determinant four, seven, five, eight, which if we do
all the calculations here, gives me the value zero. This matrix is something
that we call non-invertible, or in other words
the determinant of the matrix is equal to zero. This means that that matrix
does not have an inverse. We also call this
a singular matrix. Determinants have
several properties that might be useful
in the future. The determinant of matrix
A times matrix B is equal to the determinant of matrix A times the determinant
of matrix B. The determinant of A is equal to the product of
the eigenvalues of matrix A. Again, this will make
more sense when we talk about eigenvalues and
eigenvectors next week. The determinant of A transpose is equal to the determinant of A. Finally, the determinant
of A inverse is equal to one over
the determinant of matrix A. Well, we've made it through week one material and I'd like to place this in the bigger
context of the course. Our topics may look
somewhat disjoint, but they're truly all connected. Next week, we'll be heading
towards eigenvalues, eigenvectors, and
matrix decomposition, and the following
weeks we'll talk about optimization and
probability and statistics.