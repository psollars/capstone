A widely used an effective method in
machine learning involves creating learning models known as ensembles. An ensembles takes multiple
individual learning models and combines them to produce an aggregate
model that is more powerful than any of its individual learning models alone. Why our ensembles effective? Well one reason is that if we
have different learning models, although each of them might
perform well individually, they'll tend to make different
kinds of mistakes on a data set. And typically this happens because
each individual model might overfit to a different part of the data. By combining different individual
models into an ensemble, we can average out their individual
mistakes to reduce the risk of overfitting while maintaining
strong prediction performance. Random forests are an example of the
ensemble idea applied to decision trees. Random forests are widely
used in practice and achieve very good results on
a wide variety of problems. They can be used as classifiers
via the scikit learn random forest classifier class or for
regression using the random forest regressor class both in
the sklearn ensemble module. As we saw earlier at one disadvantage
of using a single decision tree was the decision trees tend to be prone
to overfitting the training data. As its name would suggest,
a random forest creates lots of individual decision trees in the training set off and
on the order of tens or hundreds of trees. The idea is that each of the individual
trees and a random forest should do reasonably well at predicting
the target values in the training set. But should also be constructed
to be different in some way from the other trees in the forest. Again, as the name would suggest, this
difference is accomplished by introducing random variation into the process
of building each decision tree. This random variation during tree
building happens in two ways. First, the data used to build
each tree is selected randomly. And second, the features chosen in each
split test are also randomly selected. To create a random forest model, you
first decide on how many trees to build. This is set using
the n_estimator parameter for both random forest classifier and
random forest regressor. Each tree we built from a different random
sample of the day to call the bootstrap sample. Bootstrap samples are commonly used
in statistics and machine learning. If you're training set has n instances or
samples in total, a bootstrap sample of size n is
created by just repeatedly picking one of the n data set rows at
random with replacement. That is, allowing for the possibility of picking
the same row again at each selection. You repeat this random
selection process n times. The resulting bootstrap sample has n rows
just like the original training set, but with possibly some rows from the original
data set missing and others occurring multiple times, just due to the nature of
the random selection with replacement. When building a decision tree for a random
forest, the process is almost the same as for a standard decision tree, but
with one important difference. When picking the best split for a node,
instead of finding the best split across all possible features a random
subset of features is chosen. And the best split is found within
that smaller subset of features. The number of features in this subset that
are randomly considered at each stage is controlled by the max_features parameter. This randomness in selecting the bootstrap
sample to train an individual tree in a forest ensembles combined with the fact
that splitting a node in the tree is restricted to random subsets of
the features at each split, virtually guarantees that all of the decision trees
in the random forest will be different. The random forest model is quite
sensitive to the max_features parameter. max_features is set to 1, the random
forest is limited to performing a split on the single feature that was selected
randomly instead of being able to take the best split
over several variables. This means the trees in the forest will
likely be very different from each other and possibly with many levels in order
to produce a good fit to the data. On the other hand, if max features is high
close to the total number of features that each instance has the trees in the forest
will tend to be similar and probably will require fewer levels to fit the data
using the most informative features. Once a random forest model is trained,
it predicts the target value for new instances by first making a prediction
for every tree in the random forest. For regression tasks, the overall
prediction is then typically the mean of the individual tree predictions. For classification, the overall
prediction is based on a weighted vote. Each tree gives a probability for
each possible target class label. Then the probabilities for each class
are averaged across all the trees and the class with the highest probability
is the final predicted class. Here's an example of learning a random
forest on the example fruit data set using 2 features, height and width. Here we're showing the training data
plotted in terms of two feature values, with height on the X axis and
width on the Y axis. As usual, there are four categories
of fruit to be predicted. Because the number of features is
restricted to just two in this very simple example, the randomness in creating
the tree ensemble is coming mostly from the bootstrap sampling
of the training data. You can see that the decision boundaries
overall have the box-like shape that we associate with decision trees, but
with some additional detailed variation to accommodate specific local
changes in the training data. Overall, you can get an impression of the
increased complexity of this random forest model in capturing both the global and
local patterns in the training data compared to the single decision
tree model we saw earlier. Let's take a look at the notebook
code that created and visualized this random forest
on the fruit data set. This code also plots
the decision boundaries for the other five possible feature pairs. Again, to use the random
forest classifier, we import the random forest classifier
class from the sklearn.ensemble library. After doing the usual train test,
split and setting up the pyplot figure for plotting, we iterate through pairs
of feature columns in the data set. For each pair of features, we call the fit method on that subset of
the training data x using the labels y. We then use the utility function
plot_class_regions_for_classifier that's available in the shared module for
this course. To visualize the training data and
the random forest decision boundaries. Let's apply random forests to
a larger data set with more features. For comparison with other
supervised learning methods, we'll use the breast
cancer data set again. We create a new random forest classifier,
and since there about 30 features, we'll set max_features to 8 to give
a diverse set of trees that also fit the data reasonably well. We can see that random forests with
no feature scaling or extensive parameter tuning achieve very good
test set performance on this data set. In fact, it's as good or better than all
the other supervised methods we've seen so far, including kernelized
support vector machines and neural networks that require
more careful tuning. Notice that we did not have to perform
scaling or other preprocessing as we did with a number of other
supervised learning methods. This is one advantage of
using random forests. Also note that we passed
in a fixed value for the random state parameter in order
to make the results reproducible. If we didn't set the random state
parameter, the model would likely be different each time due to the randomized
nature of the random forest algorithm. So on the positive side, random forests are widely used
because they're very powerful. They give excellent prediction performance
on a wide variety of problems. And they don't require careful
scaling of the feature data or extensive parameter tuning. And even though building many different
trees requires a corresponding increase in computation. Building random forests is easily
parallelized across multiple CPUs. On the negative side,
while random forests do inherit many of the benefits of decision trees,
one big difference is that random forest models can be very difficult for
people to interpret. Making it difficult to see the predictive
structure of the features, or to know why a particular
prediction was made. In addition,
random forests are not a good choice for tasks that have very high dimensional
sparse features like text classification. Where linear models can provide efficient
training and fast, accurate prediction. So to recap, here are some of the key
parameters that you'll need for using random forests. n_estimators sets
the number of trees to use. The default value for n_estimators is 10,
and increasing this number for larger datasets is almost
certainly a good idea. Since ensembles that can average over
more trees will reduce overfitting. Just bear in mind that increasing
the number of trees in the model will also increase the computational
cost of training. You use more time and more memory. So in practice you'll want to choose
the parameters that make best use of the resources
available on your system. As we saw earlier, the max_features parameter has
a strong effect on performance. It has a large influence on how diverse
the random trees in the forest. Typically the default setting
of max features, which for classification is the square root
of the total number of features. And for regression is the log base
two of the total number of features. Works quite well in practice,
although explicitly adjusting max features may give you some
additional performance gain, with smaller values of max features
tending to reduce overfitting. The max depth parameter controls
the depth of each tree in the ensemble. The default setting for this is none. In other words, the nodes in a tree will
continue to be split until all leaves contain the same class or have fewer
samples than the minimum sample split parameter value, which is two by default. Most systems now have multicore processor,
and so you can use the end
jobs parameter to tell the random forest algorithm how many cores
to use in parallel to train the model. Generally, you can expect something
close to a linear speedup. So for example, if you have four cores, the training will be four times as
fast as if you just choose one. If you set end jobs to negative one,
it will use all the cores on your system. And setting end jobs to a number
that's more than the number of cores in your system won't
have any additional effect. Finally, given the random nature of random
forests, if you want reproducible results, it's especially important to
choose a fixed setting for the random state parameter. In the examples we've shown here,
we typically set random state to zero, but any fixed number will work just as well.