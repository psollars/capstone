In the examples we've used
so far for classification, we've primarily focused
on binary classification, where the target value
to be predicted was a binary value that was either positive or
negative class. In a lot of real-world datasets, the target value to be predicted
is actually a category. For example, in
our fruit dataset, there were four
different categories of fruit to be
predicted, not just two. How do we deal with
this multi-class classification situation
with scikit-learn? Well, fortunately, scikit-learn
makes it very easy to learn multi-class
classification models. Essentially, it does
this by converting a multi-class classification
problem into a series of binary problems.
What do I mean by that? Well, essentially
when you pass in a dataset that has a categorical variable
for the target value, scikit-learn detects
this automatically. Then for each class
to be predicted, scikit-learn creates
one binary classifier that predicts that class
against all the other classes. For example, in
the fruit dataset, there are four
categories of fruit. Scikit-learn learns four
different binary classifiers. To predict a new data instance, what it then does is takes that data instance whose
labels to be predicted, and runs it against each of the binary classifiers in
turn and the classifier that has the highest score is the one that whose class it uses
as the prediction value. Let's look at a
specific example of multi-class classification
with this fruit dataset. Here we simply pass
in the normal dataset that has the value from
1 - 4 as the category of fruit and to be predicted and we fit it exactly
the same way that we would fit the model
it as if it were binary problem and in general, if we're just fitting
and then predicting, all this would be
completely transparent. Scikit-learn would simply
do the right thing and would learn multiple classes, and it would predict
multiple classes and we didn't really
have to do much else. However, we can get access to what's happening under
the hood as it were. If we look at the coefficients
and the intercepts of the linear models that result from fitting
to the training data. This is what this example shows. What we're doing here is fitting a linear support
vector machine to the fruit training data and if we look at the
coefficient values, we'll see that instead
of just one pair of coefficients for a single
linear model classifier, we actually get four
values and these values correspond to the four classes of fruit in the training set. What scikit-learn has
done here is it's created four binary classifiers, one for each class. You can see there
are four pairs of coefficients here and there also for four intercept values. In this case, the first pair of coefficients corresponds
to a classifier that classifies apples versus
the rest of the fruit and so these pair of coefficients and this intercept
define a straight line. In this case it's this. The apples in this visual are the red points and so the
coefficients of the apple model define a linear decision boundary that's marked by
this red line here. If you plot it out,
you'll see that it indeed it has an intercept of negative 3 and you can actually compute for any data instance
using this linear formula, what it will predict either
apple or not an apple. If we take a specific example, something that has a height
of two and a width of six. Some query point
here, for example, this is a linear classifier. We can take the coefficients, this first pair of
coefficients here, and this first intercept
value here and these form a linear classifier for
apples versus not apples. We can take the height feature, multiply it by the
first coefficient, take the width feature, multiply it by the
second coefficient, and then add in the
third intercept feature, this third bias term. To predict whether
this object that has height of two and width
of six is an apple, we simply compute this linear
formula it turns out that the value is positive 0.59. Because that value is greater
than or equal to zero, that indicates that the model is predicting that the object
is indeed an apple and that makes sense because it's
on this side of the apple versus not apple binary
classifier decision boundary. Similarly, we can
take another object, say one who has a height
of two and a width of two. In this part of the space
over here and we can plug in those two feature values into the same apple classifier
and when we do that, it turns out that
the prediction value of the linear model
in that case is negative 2.3 and
that's less than zero, which is on this side of the decision boundary
and so the linear model for apple versus not
apple here is predicting that this object is not an apple. Again, when scikit-learn
has to predict the class of a new object with potentially
multiple classes it will go through each of these binary
classifiers in turn and it will predict the
class whose classifier has the highest score for that instance.