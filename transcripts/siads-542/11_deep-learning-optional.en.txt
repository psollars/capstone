As we discussed in the
first week of the course, one of the key challenges in machine learning is
finding the right features to use as input to a learning model for
a particular problem. This is called feature
engineering and can be part art and part science. It can also be the single
most important factor in doing well on a learning task. Sometimes, in fact more often, more important than the
choice of the model itself. Because of the difficulty
of feature engineering, there's been a lot of
research on what's called feature learning or feature
extraction algorithms, that can find good
features automatically. This brings us to deep learning. At a high level, one
of the advantages of deep learning is that it includes a sophisticated automatic
feature learning phase as part of its
supervised training. Moreover, deep learning
is called deep because this feature extraction typically doesn't use just one
feature learning step, but a hierarchy of multiple
feature learning layers, each feeding into the next. Here's one simplified
example of what a deep learning architecture
might look like in practice, for an image recognition task. In this case, digit recognition: recognizing a handwritten digit from zero to nine, for example. You can see the automatic
feature extraction step, made up of a hierarchy
of feature layers, each of which is based on a network that does convolution, which can be thought
of as a filter for a specific pattern, followed by a sub-sampling
step also known as pooling, that can detect a
translated or rotated version of that feature
anywhere in the image, so that features are detected properly for the final
classification step, which is implemented as a
fully connected network. The sub-sampling step
also has the effect of reducing the computational
complexity of the network. Depending on the properties of the object we want to
predict for example, if we care only about
the presence of an object in an image
compared to its say, specific location, the sub-sampling part of the architecture may or
may not be included. This is only one example of a
deep learning architecture. The size, structure, and other properties may
look very different, depending on the specific
learning problem. This image from a paper by
Honglak Lee and colleagues at the University of Michigan
shows an illustration of multilayer feature learning
for a face recognition. Here there are three
groups from left to right, corresponding to first, second, and third stages of
feature learning. The matrix at each stage shows
a set of image features, with one feature per square. Each feature can be thought
of as a detector or filter that lights up when
that pattern is present in the underlying image. The first layer of their
deep learning architecture, extracts the most primitive
low-level features such as edges and
different kinds of blobs. The second layer creates
new features from combinations of those
first layer features. For faces, this might correspond to key elements that capture shapes of higher-level
features like noses or eyes. The third layer in turn
creates new features from combinations of the
second layer features, forming still higher-level
features that capture typical face types and
facial expressions. Finally, all of these
features are used as input to the final
supervised learning step, namely the face classifier. Here are the feature
layers that result from training on different
types of objects, cars, elephants, chairs,
and a mixture of objects. These kinds of complex
features can't be learned from a small
number of layers. Advances in both algorithms
and computing power, allow current deep
learning systems to train architectures that can have dozens of layers of nonlinear,
hierarchical features. It turns out that the human
brain does something quite related to this when
processing visual information. There are specific
neural circuits that first do low level
feature extraction, such as edge detection, and finding the frequency
of repeated patterns, which are then used to compute more sophisticated features, to help estimate things
like simple shapes and their orientation or whether a shape is in the
foreground or background. Followed by further layers of higher level visual processing, that support more
complex tasks such as face recognition and interpreting the motion of multiple
moving objects. On the positive side,
deep learning systems have achieved impressive gains, and have achieved state of the art performance on
many difficult tasks. Deep learning's automatic
feature extraction mechanisms also reduce the need for human guesswork in
finding good features. Finally, with current software, deep learning architectures
are quite flexible and can be adapted for different
tasks and domains. On the negative side however, deep learning can require very large training sets
and computing power, and that can limit its
practicality in some scenarios. The complexity of
implementation could be considered as one of the
negatives of deep learning. This is the reason
that a number of sophisticated high level
software packages have been developed to assist in the development of deep
learning architectures. Also, despite the faces example we saw earlier, which gave clear, easy-to-interpret
features in most cases, often the features and weights of typical deep learning systems are not nearly so easy to interpret. That is, it's not clear
why or what features led a deep learning system to make a particular prediction. While Scikit-learn with the MLPClassifier
and MLPRegressor classes provides a
useful environment to learn about and apply
simple neural networks, if you're interested in getting
a deep understanding of deep learning and the software
tools required to use it, we've provided some links
to additional resources. In particular, software
packages usable with Python include
Keras and Lasagne, which in turn use libraries that include
TensorFlow and Theano. Deep learning typically
requires not only significant volumes
of data for training but also significant computation. Turns out that the processors
inside video cards called GPUs or high performance
graphical processing units, are well suited to large
scale, highly parallelized, high performance computing
because they can do key underlying operations like matrix multiplication
very quickly. This is because
they're designed to process large volumes
of data from memory, as you might do for streaming
video, for example. They have many high
speed registers that can operate in
parallel on this data. Unlike Scikit-learn, which
cannot currently exploit GPUs, these deep learning libraries
can make full use of GPU clusters for
large scale learning of deep learning architectures.