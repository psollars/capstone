Another family of supervised learning
models that's related to linear classification models is the Naive Bayes family
of classifiers, which are based on simple
probabilistic models of how the data in each class
might have been generated. Naive Bayes classifiers are called naive because informally, they make the simplifying
assumption that each feature of an instance is independent of all the others given the class. In practice of course this
is not often the case. Features often are
somewhat correlated. For example in predicting
whether a house is likely to sell above the
owners asking price, some features such as the area of the interior rooms are
likely to be correlated with other features such
as the size of the land that the house is built on or the
number of bedrooms. These features in turn might be correlated with the location
of the property and so on. This naive simplifying assumption
means on the one hand, that learning a Naive
Bayes classifier is very fast because only
simple per class statistics need to
be estimated for each feature and applied for
each feature independently. On the other hand the penalty for this efficiency is that the
generalization performance of Naive Bayes classifiers
can often be a bit worse than other more
sophisticated methods, or even linear models
for classification. Even so, especially for
high dimensional datasets, Naive Bayes classifiers can achieve performance that's often competitive to other more
sophisticated methods like support vector
machines for some tasks. There are three flavors of Naive Bayes classifier that are available in scikit-learn. The Bernoulli Naive Bayes model uses a set of binary
occurrence features. When classifying text
documents for example, the Bernoulli Naive Bayes model is quite handy because we can represent the presence or absence of a given word in the text with a binary feature. Of course this doesn't
take into account how often the word
occurs in the text. The Multinomial Naive
Bayes model uses a set of count-based features
each of which does account for how many times
a particular feature, such as a word, is observed in training example
like a document. In this lecture we won't
have time to cover the Bernoulli or Multinomial
Naive Bayes models. However, those models are
particularly well suited to textual data where each feature corresponds to an observation
for particular word. This lecture will focus on Gaussian Naive Bayes classifiers, which assume features that are
continuous or real valued. During training, the Gaussian
Naive Bayes Classifier estimates for each feature, the mean and standard deviation of the feature value
for each class. For prediction the
classifier compares the features of the
example data point to be predicted with the
features statistics for each class and selects the class that best
matches the data point. More specifically, the Gaussian Naive Bayes classifier
assumes that the data for each
class was generated by a simple class specific
Gaussian distribution. Predicting the class
of a new data point corresponds mathematically
to estimating the probability
that each class is Gaussian distribution was most likely to have generated
the data point. Classifier then picks the class that has highest probability. Without going into the
mathematics involved, it can be shown that
the decision boundary between classes in the two-class Gaussian
Naive Bayes classifier. In general is a parabolic
curve between the classes. In the special case, where the variance
of each feature is the same for both classes, the decision boundary
will be linear. Here's what that looks
like typically on a simple binary
classification dataset. The gray ellipses give
an idea of the shape of the Gaussian distribution for each class as if we were
looking down from above. You can see the centers of
the Gaussian correspond to the mean value of each
feature for each class. Where specifically the
gray ellipses show the contour line of the
Gaussian distribution for each class, that corresponds to about two standard deviations
from the mean. The line between the yellow
and gray background areas represents the decision boundary, and we can see that this
is indeed parabolic. To use the Gaussian Naive
Bayes classifier in Python, we just instantiate
an instance of the GaussianNB class and call the fit method
on the training data, just as we would with
any other classifier. It's worth noting that
the Naive Bayes models are among a few classifiers in scikit-learn that support
a method called partial fit. Which can be used instead of
fit to train the classifier incrementally in case
you're working with a huge data set that
doesn't fit into memory. More details on that
are available in the scikit-learn documentation
for Naive Bayes. For the GaussianNB class, there are no special
parameters to control the model's complexity. Looking at one example in the notebook from our
synthetic two-class data set, we can see that in
fact the Gaussian Naive Bayes Classifier achieves quite good performance on this simple
classification example. When the classes are no
longer as easily separable as with this second more
difficult binary example here, like linear models, Naive Bayes
does not perform as well. On a real world example using
the breast cancer data set, the Gaussian Naive Bayes
classifier also does quite well, being quite competitive with other methods such as
support vector classifiers. Typically, Gaussian
Naive Bayes is used for high-dimensional data when each data
instance has hundreds, thousands, or maybe
even more features. Likewise the Bernoulli and
Multinomial flavors of Naive Bayes are used for
text classification, where there are very
large number of distinct words as features and where the feature vectors are sparse because any
given document uses only a small fraction of
the overall vocabulary. There's more in-depth
material on the Bernoulli and Multinomial Naive
Bayes classifiers in the text-mining portion
of this specialization. It can be shown that
Naive Bayes classifiers are related mathematically
to linear models and so many of the
pros and cons of linear models also
apply to Naive Bayes. On the positive side, Naive Bayes classifiers are
fast to train and use for prediction and thus are well-suited to high-dimensional
data including text, and to applications involving very large datasets
where efficiency is critical and computational costs rule out other
classification approaches. On the negative side when the conditional
independence assumption about features doesn't hold. In other words, for a
given class there's significant covariance
among features, as is the case with many
real-world datasets. Other more sophisticated
classification methods that can account for these
dependencies are likely to outperform Naive Bayes. On a side note when getting confidence or
probability estimates associated with predictions, Naive Bayes classifiers produce unreliable
estimates typically. Still, Naive Bayes
classifiers can perform very competitively
on some tasks and are also often very useful as baseline models against which more sophisticated
models can be compared.