Precision recall curves are very
widely used evaluation method for machine learning. As we just saw an example,
the X axis shows precision, and the Y axis shows recall. Now, an ideal classifier would be able
to achieve perfect precision of 1.0, and perfect recall of 1.0. So the optimal point would
be up here in the top right. And in general, with precision recall
curves, the closer in some sense the curve is to the top right corner,
the more preferable it is, the more beneficial the trade-off it
gives between precision and recall. And we saw some examples already of
how there is a trade-off between those two quantities, between precision and
recall with many classifiers. This example here, is an actual
precision recall curve that we generated using the following notebook code. The red circle indicates the precision and recall that's achieved when
the decision threshold is zero. So, I created this curve using
exactly the same method, as we saw in the previous example. By looking at the decision function
output from support vector classifier, applying varying decision boundary, and looking at how the precision recall
change as the decision boundary change. Now fortunately, Scikit-learn
has a function that's built in, that does all of that. That can compute precision recall curve. And that's what we've been
using in the notebook here. So, you can see that in this
particular application, there is a general downward trend. So as the precision of the classifier
goes up, the recall tends to go down. In this particular case, you'll see also
that it's not exactly a smooth curve there, these sort of jaggy areas here. And in fact, it gets sort of the jumps
tend to get a little bigger, as we approach maximum precision. And this is a consequence of how
the formulas for precision and recall are computed. They use discrete counts that include
the number of true positives. And so,
as the decision threshold increases, there are fewer and fewer points
that remain as positive predictions. So the fractions that are computed for
these smaller numbers can change pretty dramatically, with small changes
in the decision threshold. That's why these sort of trailing
edges of the precision recall curve can appear a bit jagged
when you plot them. ROC curves, or receiver operating
characteristic curves, are very widely used visualization method that illustrate
the performance of a binary classifier. ROC curves on the X axis show
a classifiers false positive rate, so that would go from zero to 1.0. And, on the Y axis they show
a classifiers true positive rate, so that would also go from zero to 1.0. The ideal point in ROC space,
is one where the classifier achieves false positive rate of zero, and
a true positive rate of one. So that would be the upper left corner. So curves in ROC space represent different
trade-offs as the decision boundary. The threshold is varied for the
classifier, so just as in the precision recall case, as we vary decision
threshold, we'll get different numbers of false positives and true positives
that we can plot on the chart. So the dotted line here that I'm showing,
is the classifier curve that results the ROC curve,
that results from a classifier that simply randomly guesses the label for
a binary class. So it's basically like flipping a coin. So if you have two classes with
equal numbers of positive and negative instances, then flipping
a coin will get you randomly equal numbers of false positives and
true positives for large datasets. So the dotted line here
is used as a baseline. So bad classifier will have
performance that is random or maybe even worse than random. Or, maybe slightly better than random. Reasonably good classifier will
give an ROC curve that is more, that is consistently better than random,
across all decision threshold choices. And then an excellent classifier
would be one like I've shown here, which is way up, and to the left. This particular example is an example of
a logistic regression classifier using the notebook example that you've seen. So, the shape of the curve
can be important as well. The steepness of the curve,
we want classifiers that maximize the true positive rate,
while minimizing the false positive rate. Now as we'll see next, we can quantify the
goodness of a classifier in some sense, but looking at how much area
there is underneath the curve? So the area underneath the random
classifier is going to be 0.5. But then the area as we, as you can
see the size of the bumpiness of the classifier as it approaches
the top left corner, well, the area underneath the curve will get
larger and larger it'll approach one. And so, as we'll see in the next slide, we
use something called area under the curve. AUC, that's a single number that measures
this total area underneath the ROC curve, as a way to summarize
a classifier's performance. So, an AUC of zero represents
a very bad classifier, and an AUC of one,
will represent an optimal classifier.