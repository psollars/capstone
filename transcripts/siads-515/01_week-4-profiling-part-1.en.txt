So the next segment here is
about Timing and Profiling. I want to cover two things. So I want to make sure
you know what profiling is and I also want you to know why we are
going to profile. So profiling, simply put, is trying to figure out
where your algorithms, or your scripts, or your cells in Jupyter are
spending most of their time. What's taking the resources? Why do we do that? Because we want to be able to identify places where you might want to focus your efforts
to improve efficiency. When we do timing on code, we're trying to identify time inefficiency which will then point us to places where we might want to
focus our efforts. For timing code, we can do it in three or four
different ways. There's basic Python time, there's %timeit, and %%timeit. Now, you should recognize at
this point that a percent at the beginning of a command
is a Jupyter magic command. These were probably introduced
in the last segment, but I want to go over them in a little bit more detail now. Finally, we'll go
to a simpler form of timing your code called %time. Most of your experience with
timing code should come from that second option
%timeit and %%timeit. Let me talk a little
bit about Python time. So Python time is
generic to Python, and there are multiple
ways to do this. I want to mention up
front that this is not the recommended way
to time your program. What you can do is take a look
at the datetime library or the datetime module in Python
and look at how time works. So the general idea here is
that you're going to record the time immediately
before your code runs. You're going to run some code, and then you're going to
record the time when it ends. Finally, you're going
to take the Delta or the difference
between those two times, and represent that as the
amount of time taken. So this is somewhat fragile, it's subject to weird
things around time. Your time resolution
might not be to your liking, but it's possible. You'll certainly see
this in a number of different settings
like machine learning, where we're going to take
a measure of how long it takes to train a model
or to test a model. So you'll see this. It also works outside of Jupyter. So if you find yourself doing scripts instead of
Jupyter Notebooks, you will have to use Python time because you won't have your magic commands available to you. So let's talk about the main way to measure
time in Jupyter. So that's %timeit and %%timeit. One thing about
Jupyter commands is that the single percent lines, so %timeit is line-oriented. So you can time a
particular line in a cell. If you want to time
the entire cell, you can use the double
percent notation. So what's interesting
about timeit, both the single and double
percent forms of it, is that it will perform
multiple runs of your code with reasonable
dynamic parameters. Now, you can specify those
parameters explicitly, but it does a pretty good
job of selecting them. So it's going to run that
code multiple times, and it's going to give
you an estimate in terms of the average or mean
time taken to complete, as well as some measure
of spread of that time. So usually, a standard deviation. So it'll run your code in multiple loops and it'll
run it with multiple runs. We'll go over that in
the Jupyter Notebook. So here we are doing some timing and
profiling in Jupyter. I'm going to walk
through some cells here just to get us onto
the same page. We're going to do our
standard importing of Pandas, NumPy, and we're also for some work a little bit later on
in this notebook we're going to
import a whole bunch of stuff from the math module. So let's go ahead and run that. We're going to read in some data. This is about some information
about New York hotels. It contains information
like latitude, longitude, the cheapest rate, or the lowest rate to get
a room in that hotel, and the most expensive rate or the highest rate
for that room. So let's go ahead
and read that in and let's take a look at what
this dataframe looks like. So here we are with our hotel ID, the name, the address, so on. The columns that I'm interested
in for the purposes of this week are the latitude
and longitude lines, and also, the high
rate and the low rate, that's an average measure. So that's what the
dataframe looks like. I want to remind you that if
we are ever interested in finding out about the columns
that are available to you, you can use df.columns, that is, look at the columns
attribute of that dataframe. Now, what I want to
do next is define a function that will normalize
some variables for us. So normalizing means, in
general, re-scaling your data. Now, what I want to
do here is I want to exclude extreme values,
that is, outliers. So what I'm going to do to do that is I'm going to
define a function normalized that takes a dataframe and a series within
that dataframe. What I'll do next is
convert that dataframe, make sure that it is a
floating point value. So I'm going to take
that series and re-assign it as a float. The next thing I'm going to
do is calculate the mean, put it in a variable called avg, calculate the standard deviation, and assign that to
a variable called sd for standard deviation. Notice here I've used
some NumPy functionality, I want to mention that NumPy is very, very highly optimized. So when possible, I
would recommend using NumPy methods and
routines over other ones, say, that you write yourself. What I'm then going to do
is define a lower bound, which is the mean or the average minus two
standard deviations. We're going to define an
upper bound similarly with the average plus two
standard deviations. Now hopefully, you
remember from your stats prep that plus or minus
two standard deviations will contain most of the values in a normally
distributed variable. What we're going to do
next is we're going to collapse in the outliers. So we're going to do a df.loc
using the loc attribute. We're going to set up
the condition where that series is less
than the lower bound. If that happens, we're
going to assign the lower bound to that
particular number. So in other words, we're going to walk
through this series. We're going to
examine each value. If the value is less
than the lower bound, then we're going to re-assign
that value as lower bound. So we're going to
clip that range, and we're going to operate
for the purposes of this demonstration on
the high rate column. We're going to do a similar
operation on the upper bound. So exactly the same thing, but we're going to
look for values that are greater than
the upper bound. If we find something
that's an extreme value, we're just going to replace
it with the upper bound. Why are we doing this? Well, we want to get rid of
the outliers because they're going to skew our results. There are multiple ways
of dealing with outliers. I'm not saying that this is something you always want to do, but it's what we want to do here. To be honest about it, I'm defining a function
that has a bunch of different steps in it that we
can examine for efficiency. We can try to figure out
where the time is being taken in this particular routine. Finally, I'm going to
take the logarithm. I'm going to log transform
my prices. Why is that? Well, if you take a look at the distribution of
those high rates, you'll see that it's a
skewed distribution. One way to make that
skewed distribution more normal or more normally distributed is to take
the logarithm of it. We're going to add
one to each price. Why do I do that? Well, if
you take the log of zero, it'll throw an error. So we just want to offset
that by one just to make sure that if we ever
take the log of a zero value, we're actually taking
the log of one, which turns out to be zero. Finally, we're going to
return that normalized price. So let's run this just to get that function
definition here. So we've defined normalize. Then if we call normalize, we get a series returned That's that
series normalized price, and it looks like this. So our high rate are here, not our lowest price,
our highest price. The logarithm of that is 5.04. So remember, these are all
log transformed values, and here's a value down here
where we actually wound up with the logarithm of
one or logarithm of zero, which we said was
going to be zero. So that's our normalized
function, that's how that works. I wanted to show you that. Let's go ahead and now
start timing this. So if we wanted to
time a whole cell, where we're going to
assign an output series to DF high rate normalized based on the output of
that normalized function, which is what we just saw here. We can go ahead and do that. So let's run that and
see what happens. You'll notice it
takes a few seconds because it's running
multiple runs. Let's take a look
at the output here. So when I timed this entire cell, I found out that overall
it took on average 3.54 milliseconds for each run plus
or minus 696 microseconds. So yes, we do have that
high of a resolution on the timing of our function. This is per loop and it ran
seven runs of 100 loops each. Where did those
numbers come from? Well, timeit does a pretty
good job of figuring out how long it thinks you're
willing to wait for the result. In other words, if you have a function that you're timing that takes
a very long time, it will run 100 loops each, it might run 10 loops. In terms of number of runs, it might decrease the number of runs from seven down to three. As long as we can do some reasonable calculation of the mean and standard deviation, we'll go ahead and use timeit and trust timeit with its
default parameters. If you don't like the default parameters or you're worried about running something
multiple times, for example, if you're loading a database with data and you don't want to
erase the data each time, clearly we don't want to do this. We'll talk about a
way around that as we close out the timing section
of this week's lecture. So here you saw that I use the double percent
form of timeit, it timed the entire cell. I didn't have to do
that here because my cell only contains one line and that is df high_rate_
normalized equals normalize. This doesn't make a lot of sense, but let's say I wanted
to do something twice. Let's go ahead and run that. So now we're timing
the entire cell, you can probably predict
what it's going to say. It roughly doubles
the amount of time, so that takes 6.26 seconds to run this particular cell that
contains two operations. Again, I know it's a
fictitious weird example, but you get the idea. Now, if we just wanted to
time one of those calls, we could use percent timeit
as a prefix to the line. So here we're going
to do percent timeit with a space and
there's the line. This should give us
very similar results to what we saw just a minute ago. So here's our 3.36 seconds
again plus or minus 364. Now, you'll notice that
every time you run this, it's a little bit different. So there's 3.26 instead of 3.36, there's a certain degree of stochasticity or
unpredictability here, things just take different
amounts of time and when you think about how fine
grained these timings are, it's not really surprising. Now, I mentioned the
situation where we might not want to
run multiple loops. So here we've run seven
runs of 100 loops each, that is, we've executed
this 700 times. You can imagine a
situation where, and I mentioned
this a minute ago, we're loading up
a database and we don't want to reload
the database 700 times, that would be ridiculous, we'd be looking at a
lot of transfer rates, we'd be looking a lot of indexing on the server
side of things. So we might want to
in that case use just percent time instead
of percent timeit. Percent time will run through that particular line
once and what we're interested in here is this particular number
of 3.74 milliseconds. You can look up the
documentation for what these other values mean so you see that there are
system times and user times. We're interested in the
user time for this. So let's turn our attention
now to something that build on timing and
that is profiling. Profiling can be done in
Jupyter Notebooks using another series of magic
commands in particular, percent prun and percent lprun. Percent prun will
show the time in each function and that's useful, but what I want to focus on here is the next command lprun. So percent lprun shows the
line-by-line profiling. Percent prun is
generally available, you don't have to do
anything special with it. Percent lprun is available with the kernel extension and I'll show you what that
means in a minute. So here we are back
in our notebook and when I talked about loading
a kernel extension, we have to use a
different magic command, load ext which is load
extension for line profiler. There are a lot of different extensions
that are available for Jupyter or IPython actually. So percent load
extension will load up any given extension
that you have to install this using Pip or Conda. We've done that for you in the environment that you're
using in this course. So we've done the installation at the system level
of line profiler, now we want to make that
available to Jupyter. There's a little bit of
overhead involved in using the line
profiler so we don't always want to load
that extension, but let's go ahead
and run that line. Now, the next piece that
I'm going to do is lprun, and this is a strange format, so I have to give it the name of the function that
I want to profile. So that's an indication
that I want to do a line profiler of normalize and the line that I'm going to execute is exactly
the same as before. So I'm going to assign to a new column or actually the
column is this so I'm going to reassign it df high_rate_ normalized is going to
be set to normalize, the normalized call that
we used just a minute ago. So here's normalize over
here and we have to reiterate that over here
with minus f normalize. Let's run this, this will make a little bit more
sense in a minute. So here's our output of
our line profiling run. Again, remember with profiling, we're trying to dig into a particular function
and find out where the majority of
our time is being spent. The total time spent
here is 0.01753 seconds. So you can compare that to
the previous run where we had an estimate for how long
this took using timeit, so it's a little bit different
and you can compare those. The function that we're
looking at is echoed back to us so we're looking at normalize and what line profiler does, is it breaks down
every single line, hence line profiler,
so we have lines 1, 2, 3, 4, et cetera, all the way up to 16 and
you'll see that this corresponds over on
the right-hand side to the line contents that
are reported to you. So line 1 is the definition line that starts the definition
of our function. Then we have the line where we are converting
it to a float. We have a couple of
commented outlines here. Remember we talked about this, looking at the mean and
standard deviation. So it's exactly
the same function, we haven't changed normalized at all and you can see
that this continues on, comments are not recorded
in terms of their times. The other columns that I want to show you here are
the number of hits, that is the number of times
each of these is hit. You'll see that this is one in all of these
cases because we don't have any loops
in our function. If your function has loops, it will report multiple hits every time you go
through that loop. So for example, you might loop through something 1,000 times, the hits count will go up. It also reports the time, I believe this is in
milliseconds or microseconds. So this is the time taken, so it takes 239 time units, it divides that by
the number of hits. Again, we are dealing
with one-hit lines here. So per hit will
always be the same as time and it does that for every line which allows
us then to report the percentage of time spent
on each of these commands. So for example here
converting the series to afloat takes two percent of the total time spent
in this function. Similarly, 4.4 and 4.9 percent respectively are spent on taking the mean and
standard deviation. If you think about that, that makes sense
because you know that standard deviation needs to
calculate the mean first. So standard deviation will always take a little
bit longer than mean, but it doesn't take a
significant amount of time. You'll notice that a
simple mathematical operation like taking
one value ABG and subtracting two times
some other value takes virtually no time at all
which is good news for us. Remember this column here is
the percentage of time spent as a fraction of the total
time spent in this function. So 42 percent of the time
is spent walking through these series and
comparing the values to the lower bound and if it is
less than the lower bound, assigning a new value to that particular
element in the series. So 42 percent of the
time is spent here, 32 percent of the time is spent looking at the other
end of things, that's the upper
bound, 32 percent, 13 percent of the
time is taken doing a mathematical operation or
transformation of a log, but you'll notice
here that we're also calling as float again
or as type float. So perhaps there's
something there that we're repeating that
we did further up. So pd_series up there, here we have to call as type
float just to make sure, we might want to take
that out to see if we can improve efficiency and again, the return takes
virtually no time at all. So my question to you is, where is most of the
time being spent? Now, we know from
what we just talked about that the majority
of the time is spent in df.loc looking
for this condition, and that's not surprising if you think about how loc works, which leads me to
my next question. What order is the
normalized function and how can you figure this out? So if you think about what df.loc with the
condition is doing, it has to walk through
each element in that series and there's
no shortcuts here, it's going to have to examine every single element to
see if it's less than the lower_bound and conversely greater than the upper_bound
in the second one. So because we have
to walk through each element in a series, that's going to scale
with linear time. So if we add one more
element it's going to take another time unit to walk
through that last element, there's no shortcuts here. So this is an order
and operation. You might ask the question, well hang on a sec, there are two df.loc, so wouldn't this be ordered 2n? It turns out that
with big O notation, we are not concerned with
constant multipliers. So we're not
interested in knowing something is 2n or 3n or 4n, we're interested in
knowing about that n, so we drop the constant. So the answer to the question is, this is an order n function because we're
walking through the series. It happens to be twice but again, we drop the constant values.