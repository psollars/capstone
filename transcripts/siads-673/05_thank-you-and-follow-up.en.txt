This is getting close to
the end of the course. We want to thank you for taking this course and
going through this with us. We hope you gained some insight on how the
Cloud can be beneficial. The elasticity of the Cloud that allows you to do
your work as you do without upfront
capital investment is really useful for
many businesses. Before we conclude
the course though, we want to take a little
bit of time and talk about our favorite thing
to do in the Cloud. Ultimately, the Cloud
is just a set of tools. If you've taken
our other classes, which I think are prereqs, you know that we'd like
to talk about tools, and so for me, the Cloud is a set of tools that allow us to do
our work better. For me, I really
liked the aspect of like taking a process that's been done the
old-fashioned way within server hardware. Managed VMs are on that, or even on physical hardware. Moving to the Cloud, one of the examples is
we had a process that we had a bunch of VMs for that did a bunch
of calculations. We move those to spot instances, and rather than
running hardware 24/7, we now pay a lot less, get a lot more high
quality machine, and are even more eco-friendly. We moved an entire building automation system to the Cloud. If you've ever been
in a building, you've held up a key
card to unlock a door, that became a Lambda
function where the reader would actually reach
out to the cloud and say, here's the person
here's the door, and the Lambda
function would look at a database and come
back and say yes, no. In those cases, what are your feelings around
resiliency and control around
physical security? Like what tradeoffs do you make and what things are
you letting them out there? The downside here is obviously
if the Internet goes down, all the doors are locked. When you think through that,
It's like, well, okay. But in this instance
that wasn't a problem, because if the Internet was
down, we don't really want. That's a much larger problem. This facility had inbound dual redundant
internet connection. The Internet being
down isn't the end of the world if they can't get in because they
probably don't need to do any work in these labs anyhow, because you're not
going to be able to log into the computers anyhow, and people who need to get
in have physical keys. That system actually was
built with dual zones. It was in was in US
East 1 and US East 2, so in Ohio and in Virginia. The likelihood that both
were down at the same time. Multi-regioned it. We multi-regioned it
and multi-zoned it. It was in the region, sure. It's nice because we had
this replicated database that pulled data from HR, also again with AWS batch. It's nice to be able
to apply CloudLab. Speaking of AWS
batch for those of you who are in machine
learning pipelines, AWS batch is the
auto-grader there. When you submit your URL, we're actually firing off a batch job, of
course, that I wrote, that download your
Git repository and start doing things
on it for grading, which beats the having a VM that just sits there
and closes things. Of course, when we're
doing it that way, we get a fresh
container every time. As a result, one student can't add requirements for other students, it'll
cause headaches. What's your favorite
Cloud workflow like Fox. I really like combining
different services. A really common workflow
is putting data into S3, and then taking action
once it's there. With S3, you can associate events that get triggered when
something hits S3, but you can also get
more complex and have those events touch a SNS queue, a Simple Notification
Service topic, which then you can attach
a simple queue service Q2, and then you can use other software to pull
things off of that queue. The nice thing about
combining those two things, and it sounds complex
with SNS and SQS, but they're doing two
different things. SNS is providing a
pub/sub service where I can publish something and have other things
subscribe to it, and SQS is giving me a
way where I can have multiple queues connected
to that one topic in SNS. I can have different pieces of software doing
different things depending on what
those events are, and maybe my software wants
to do something different. This is a really common
workflow when you're dealing with large
amounts of data, because then you can
also federate and spread out the
responsibility for doing that among
different teams. You can just have a team that's curious about events
that are happening and can subscribe to
them and take action when those things get triggered. Can you give me an
example of when you might use that
with your day job, like with that workflow, because it sounds like
it's a workflow that can be reused for
multiple use cases. Definitely. That's
a workflow that I think you're going to
see a lot in industry. I think a concrete example
would be adding some data, like let's say, a data that's generated off of
an instrument or a piece of lab equipment. Being automatically
uploaded to S3, all that workflow has to worry about is just
getting the data up to S3. You can use this architecture to decouple the
responsibility of getting the data up and then having the data to be processed
in some way in the Cloud. Notice the processing doesn't necessarily even have
to be in the Cloud. You could also have
a Python script running somewhere that's
listening to those events and taking action
when they come in, running on your
laptop or wherever. That's not a common use case, but the point is that, that code can really run
anywhere as long as it can connect and hear those
events as they're happening. There's a fun. We should say that this was
unrehearsed and unplanned, so we are riffing this. But there's a fun
livestreaming thing that I put together where the person doing the
presenting needs to be able to switch between three
cameras within a room. They're alone, there's
not a camera operator. Normally, you have
a camera operator sitting and pressing the
buttons to change it, and so we gave them an
Inovonics key remote. Inovonics, you'll see them in like building
security-type places. This is a key remote, unlocking a door is normally
how it'd be used. But the key, the
Inovonics, it's a 4,500. I forget the name of the actual part but it's
4,500, somewhere there. Actually uses the
pub/sub model in the Cloud and sends
the events to AWS. What we did is we basically let the Inovonics thing send
those events up to the Cloud. Then there's a
computer in the room that subscribe to those events. It looks for an
event and changes the camera and put what
it sees in the Cloud. It sounds like that slow, the latency is under a second. You can see it because you press the button there is a
little bit of a delay, but it's still under a second. Right. Back to the tying
things together, so you've got S3. I'm assuming there's
a Lambda function tied to that S3 input, or is it direct to SNS? The way I like to do things
is to connect things to SNS, because you can
connect to a Lambda directly to those events in S3, and that can be, that
can be really useful. The benefit of going
to SNS is that then you can fan out to
multiple different things. You can have a lot of control about how that fan-out happens, how the resiliency of those
different things are set. If you have multiple
tasks that need to happen for a
given piece of data, like say, video that needs
to be processed in some way, that is you could have
different tasks handling that depending on
those criteria, and being able to fan it out. Basically that pub/sub, the publish-subscribe
model separates the concerns between
having the data arrive, and then taking action
in different ways. You can have multiple things
subscribe to those events. The power of that starts to
become apparent very fast. I had a project
where we were taking medical videos that were recorded from an encounter between a doctor and a patient. We put cameras in
the room and gave the the clinician the ability to start and stop the camera. But at the end of the recording, the cameras dumped the data
programmatically in S3, and then we had a
Lambda function, but it's the same concept
that would come in and say, oh, there's new data. It would then kick off a recognition job that
would start going through and actually removing
identifiers from the video. It wasn't perfect
but was pretty good. It would go through
and transcribe the audio at the same time, and then drop that off for
people to further review. There was actually SNS
afterwards, it came through. You could see having
other processing to downsample the video
or do other things. Maybe not useful in
that particular case, but those are the things where different things
come into play. We want to thank you
for taking this class. Feel free to reach out to us if you've got questions or you need assistance with your Cloud
workloads, but thank you. Yeah, thanks.