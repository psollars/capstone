Good morning, afternoon, or
evening, depending on where you are currently located, I want to start
off and talk a little about AWS Glue. AWS Glue is a really useful service
that I can't give you a great demo of, because it requires a bunch of
custom setup for your environments. But what AWS Glue is,
it lets you set up complex database transformation jobs, and
what are called ETL jobs. You deal with these when you
take data from one location, make changes to it, and
move it into another location. I'll give an example of this, let's assume
you've got a point of sales system, and your point of sales system
is very normalized data. And you want to put that into a data lake,
or some other type of less normalized environment,
to help you write an algorithm around it. So, that might have a code for
status, where you've got one for the item was sold, two for
the items in cart, three for the order was canceled, four for
the order was shipped, whatever it may be. And you want to change that, so that you've got a real-time status
word in there as opposed to the number. Now you could do this in Python, you could
just take the raw data, and go through and do a bunch of replaces with it in Python,
and that would work for you. But maybe there's another group of people
that want to build tableau dashboards with it, and they don't want to
have to worry about that, so you handle this in one place. What data grip lets you do,
is you start off by defining a table, and you'll go through this. And I'll just give a quick demo
of what this looks like, and you're going to basically say, where
are you going to get your table from? And you can specify that you're
going to go through an AWS space for it, you can upload a CSV file,
whatever it may be. And then you actually start writing
your ETL jobs, and your ETL jobs, you can use a visual interface to actually
write these, which is pretty cool. And so you'll start through writing your
ETL job with, I typically do visual with source and target, but
you can start completely blank. You can even actually do this in spark,
and AWS will manage the spark environment for
you, or you can run it through notebook. So we'll go ahead and say create here,
and basically what this is doing, is it's showing you the steps
that are going to be involved. You're going to start with data
that exists in an S3 bucket, we'll learn more about those later on. You're going to apply
a transformation to it, and then you're going to output
the data back into an S3bucket. Now, you can actually change
where these things go, let's assume you want to drop it off in
MySQL, and you want to take it from MySQL. So in this environment,
you can actually configure this, that you're going to take
data from MySQL and S3. You're going to do some transformations
on it, and when you're done, you're going to put it back in S3 and
MySQL. Now, once you configure this and
you write your transformation processes, you can actually have this
run completely automated. And you can tell AWS to run this daily,
nightly, whatever it may be, and of course like everything else in AWS, you only get
charged for the resources, thank you.