Okay, good afternoon, hang on one second,
we're going to start that again. I can't do good afternoon because
you don't know what time zone it is. So hello everybody, we are here today
to get a demo on AWS SageMaker and Amazon's kind of machine
learning products. And we've got a really interesting
person from Amazon with us today, and we're going to learn
a little bit about him and then we're going to kind
of go right into the demo. So can you introduce yourself to us and
kind of what your job title is? And how you got to Amazon? What was your career path from
graduating to employment at Amazon? >> Boy, how much time do you have? Do you have a whole day? We can talk about that. But yeah, let me just do
a highlight of how I get here. My name is [FOREIGN] I'm lead
solutions architect with AWS research. What that means is I lead a lot of
discussions with our researchers within research institutions. How I got here,
I started my career as astrophysicist, did numerical simulations of computational
fluid dynamics with radiations. Did some very cool simulations of
we call produced stellar jets. So very high speed outflow
coming out of a produced are and then because of the some of
the instabilities that the outflow dissipate and
has some very interesting structures. The time I did my research have
Hubble happened to find some very interesting observations
of those stellar jets. We were able to compare some
of our numerical simulation results with the observation
from Hubble space telescope. That was a very interesting time. So throughout that whole process,
what I did was I did a lot of programming, parallel computing with Fortran 90,
believe it or not, it's like 20 something years ago. And then through that process I actually
find out that I love the programming part a lot more than the science part. Even the science part is really
interesting but I find writing code and try to debug and then try to get the results to
compare with some real world scenario. I was more interested in that part. So I moved into the software
development after I graduated. My first job out of college
was to build a software for a NASA Goddard Space flight center
it's called Fist viewer that does processing the fist files
coming out of satellites. And then because of those files
have very specific format and then there are a lot of images
metadata embedded into the file. So we built a tool to help astronomers
to process a large amount of data and then be able to visualize those
to simulations, calculations. And then after that a good
friend of my worked for a startup company in DC,
you guys probably know that company named Blackboard which is
e-learning platform provider. So when they were still a very small
startup company I joined them in 2000. And then I was one of the first
Java developers they hired so I helped them to develop
their version five which is the enterprise version of
the learning management system. And then two years into three years
into that I start interacting with other group of people at Blackboard
outside the development organization. They are thinking about expanding
into international market. So I got involved with
the business development side of the software development. So then they were expanding in Europe and
in Asia, China specifically because I was
born in china, I speak Chinese. So I naturally got involved
into that business development. I went there to become their
first CTO of the joint venture with the Chinese company. We actually introduced e-learning
to the entire Chinese market. >> Amazing. >> It was it was one of
the first in china and then we were able to introduced
Blackboard as an e-learning platform to over 250
universities in china. And then I become the CEO of the joint
venture for a couple of years. So I was doing the commute
between Washington DC and Beijing every other week. So I I got over a million
miles on my united flights. But that also took a toll,
it just got really tired of traveling because my kids were young at that time,
they need some ample time. So I decided to move back to the US, so even I was in management of CTO and CEO I still keep my technical skills. I was leading a team of developers. I never wanted to lose
my technical skills. So I came back and joined join
Blackboard for a year to to help them to build the international support team,
technical support team. And then I Blackboard left briefly
to join a startup that provides call center support for all the higher
education institutions in the US. And a year later Blackboard
bought that company back. So I went back as a VP of software
development to Blackboard, but I was still within that start up, we changed the name to
Blackboard student services. So what we did was because of
the challenges for large institutions to provide technical and phone support
to all their student populations. Some schools,
especially this Community colleges, they have over 100,000
students throughout the years. And then lot of those students are first
year college students within their family. The entire process of going into
college and then get financial aid, that process is really complicated. You guys probably know better than I do,
right? So for a first generation of
college students to be able to navigate that complex
situation is stressful and still schools try their best to
support phone support for them. To walk them through the whole process. At what time you need to submit,
what documentations and then if you're not familiar with the software system,
how do you reset password and all this? They need to have a number to call. So the Blackboard student services provide
outsourced services to universities. So we'll pick up the call for you,
we'll answer this call for you. There is a need to build a system for
call center, so if the call center business,
it's a pretty low tech about 10 years ago. It's a phone and a person and that person
we trained them to answer questions. And then they will go through
five different systems to look at information up. Right, for
example if you want to reset password. Blackboard will have to go logging
the blackboard to click on the button to reset the password. And also looking or to look into
Peoplesoft to find your application status where you are in the process, how much money you owe to the library for
example from last semester. That's why your process is not being
improved like that kind of challenges. It takes a long time and
very frustrating for the students. So what we start thinking about is
why don't we build a platform that integrates with all
those systems directly. And then aggregate those data in a single
place and do some predictive modeling. Do some analytics on the data for
you for these students. And then based on the attribute of
this data will provide your next step, so you can move on to
your next thing to do. So we start building this platform
integrated with our call phone systems. That's when I really got
interested into machine learning. So my background was high
performance computing, didn't have much to do with the data. But dealing with building this
interaction management system for the call center really got me interested
into AI and machine learning. I used to spend a lot of time learning
the basics of Ai and machine learning, fortunately I still remember
a little bit about the math. So give me a little bit leverage there. So the couple of things we tried to do
there is that well, first of all we, we did the natural language
processing NLPs to build chatbots. At that time they are the commercial
chat about platform are still not really many out there. So we build our own chatbots using
natural language processing. And also integrated that with the data
we aggregate from PeopleSoft data tail banners, learning management system,
Blackboard canvas. And then a lot of the times when we
are answering a technical question for a user, let's say Michael call. And then we put up all the information and
then there's an indication that says, well looks like Michael hasn't
been to class for two weeks. There might be issues of him
dropping out or something like that. Can we actually provide
some advisory guidance or something or ask some advisors or to call to connect with the students
to prevent them from dropping out. So one of the biggest issues the colleges
are facing is that people won't be able to, students won't be able to finish
in four or five years, right? So we try to provide additional
services to prevent. Predictive model is something that we also
build into the interaction management, so we can raise some red flags. Yeah, that's how I got
into the machine learning, I had to find out that's
really interesting to me. I spent a lot of time doing
a lot of the learning, a lot of the machine learning stuff. And then, yeah, after 18 years
with Blackboard, I heard everybody talking about Because blackboard at
that time was migrating to cloud, I was like claws, sounds like pretty cool,
and what is this thing? So I said, well let's let me try it out. So I started looking for
opportunities at AWS and at that time I know very
little about Cloud computing. But I have been higher education
business for awhile and unfortunately people who interview me
look past my lack of Cloud knowledge. [LAUGH] really appreciate my
experiencing in the higher education and they gave me an opportunity. So yeah, this is actually a really
interesting opportunity for me. I used to be a manager and
then manage a team of developers. Now I'm actually just focus on technology. So let me share with you what
solutions architect at AWS. So I spent my first six months
doing nothing but learning. So if you know a little bit about AWS,
the Cloud technology, AWS has over 200 services,
different kind of services. Machine learning is one of those, quantum
computing's high performance computing, storage, compute,
networking, security, right? There's a lot to learn and
then AWS give all the solutions architect opportunities to learn
those new things to develop skills. We have environment, if you find something
that you are really interested into, you can just start building,
that's the culture here. We are builders culture,
strongly encourage all of you when you are finished your study here. I really take a look at
the opportunities that AW S. So what my daily life
look like in general, is that it used to be a account. Solutions architect,
solutions architect means we are between the our services and
the customers. So the customers when they have
a technical challenge in my case, a lot of our researchers, they start
with a large amount of data for example. They want to do genomics research and
or they want to do machine learning. We will help them to find
the right storage solution. So as three for example,
if they want to do have file systems instead of object storage,
they want to have file systems. So because they're familiar
with the file system, they know Linux folder look like,
they know what a drive look like. So we have that kind of solutions, will
help them to pick up the right storage and then help them to identify
how big your data is, right? What kind of security measure
you need to have in place? Very important. So when you guys are dealing with
data a lot, the security of the data, the safety of the data,
security of the data. These are different aspects
of the data in the Cloud that how often do you want to back up or
how many copies do you want, right? And then what kind of encryption
do you need to have in place? And there are a lot of workloads,
especially sensitive data that has a lot of regulations,
compliance standards, things like those. You need to consider doing those
themselves as an individual researcher or in a research institution takes a really
really long time and really expensive. It's not just a bunch of data on your
hard disk on your laptop anymore. It's encryptions around it
who can have access to that. How many copies do you want in what place? Those are the things people
don't usually think about. But when they get asked to do that
because of the compliance they usually Cloud usually become a really viable
solution because a lot of those things are taken care of by
the Cloud service provider. And then start with the data,
what kind of computing you want to do? Then if you have just megabytes of data,
you might not need to have high performance computing or
high performance storage but. If you do need to read and
write a lot of small files for example, or your data package for your model training
is terabytes or hundreds of terabytes. Then you need to think about what kind of
storage big data framework for example, will need to be considered. How do you do that? Those are the questions as a solutions
architect is what we work on every day. We look at your workloads,
we help you to analyze and then we present you with a couple of options,
we'll find you based on your requirements. Do you want to do this
thing really quickly or do you want to do this
thing really cheaply? Right, so
we'll get based on what you need, I will provide the options for
you to t those workloads in the Cloud. And then a lot of times we also spend
a lot of time running workshops, for example, different aspects,
anything that related to Cloud. If you want to learn to talk to us, we
will come in, we'll have a couple of days. I just finished yesterday, just finished
a two day workshop at Princeton for machine learning. Let's start with the very basics
of the Cloud and then well, if you want to do deep dive, we can
actually do hands on workshops as well. Those are a lot of times if there's
existing workshop, we'll use that one and then we also spend a lot of time
building customized workshops. Something that for
example internet to come to me and says, well, can you help me do,
like a lot of people want to learn HPC, high performance computing,
parallel cluster federation. Can you help me to do that? That's not a really popular topic. So I have to spend time
building solutions to. >> [INAUDIBLE]
>> Yeah. >> And I know you've got a demo for
us that we want to get to, but I'll ask one last question first. What's the newest interesting
thing you've learned recently? >> So, the most recent one is out is the Cryo The Cryo electoral my crosscut. So I have to practice
how to pronounce that. All right, so here's what it is. And I'll tell you why it's very
interesting related to machine learning. So, when the structure biologists want
to study the structure of molecules. So you can't just say, I'm not going to
buy this really high powerful microscope, because optical microscope
can't see that small. So we're talking about hundreds of or
1000 times the size of the molecules. So, the optical Microsoft microscope
just can't go down that small. So, actually that invention
won the Nobel prize. I think it's 2007, three of those who invented
the Cryo Actually won the Nobel prize. So what they did was
they freeze the sample. Let's say if you have
a sample of coronavirus, coronavirus is macro molecule structure. The freeze them really fast to
a really low temperature and that's where the word a Cryo came from. [COUGH] So
because if you freeze it really slowly, then the liquid will become ice. Ice is a crystal,
that's really hard to see through. But if you freeze it really quickly, that will become certain states
that you can still see through. But nothing can move around it, right? So, and then you take pictures, so that way you take pictures instead
of using optical pictures, you shoot atoms or you shoot electrons. That's why where the E
come from the electrons. And you bombard this example with
electrons, and the electrons will interact with the sample and the scatter and
then create an image, right? Because of the very difficult,
the signal to noise ratio is really small. We're talking about zero point something. A very good picture has the signal
to noise ratio about 40, right? You can imagine how hard it is. It's basically just a blurred but
fortunately some very smart people use math
to develop an algorithm. If you add hundreds of
thousands of those together, you can actually start seeing
the signal coming out, right? Because all the other
noises start canceling out. So unfortunately,
that is something that Cryo Does, is that it take thousands of pictures. So, the challenge there is
after you take those pictures, you get a huge amount of data every day. All right, one microscope can
produce over 10 terabytes of data. And those 10 terabytes of data to process
those 10 terabytes of data because those are all images. Image processing is really
computer intensive and requires accelerated GPUs
to process those data. And then traditionally and
you will process those and then go eyeball those pictures to
identify the structure of the molecules. Because you can only take a two
dimensional picture, right? There is no three dimensional but
because you have so many different viruses in that view of
picture with different orientations, you're kind of taking a snapshot of all
the this one particular virus in different states, all at the same time. Then you can use the algorithm
to reconstruct that 3D model all of those two dimensional slices,
right? Because you have thousands of
those things in the same picture. So that basically you need
a very large storage and very high performance compute to do that. So what people start
doing that with machine learning is in machine learning
there's object detection, right? And also image identification,
object identification. And then you can actually use machine
learning deep learning neural networks. And then to look at those pictures
instead of you eyeballing those things. Those machine learning algorithms can
actually pick out those particles the images of the particles and
then use that to reconstruct 3 D images. So a lot of the coronavirus models
you see in the news and stuff, those are probably
reconstructed by Cryo E M. So what we did for
them is instead of spending millions of dollars by those storage, very
expensive storage, do this in the Cloud, then spin up parallel clusters
to process those things. You can cut down the cost and
the time to research really quickly. So that's one of the latest very
interesting machine learning related examples I can give you. >> Awesome. So you've got a demo on kind of AWS
like machine learning space and some of the automatic tools or
auto processing tools. And so I think are the students in this
class would be really interested in seeing that demo? So why don't we flip over to that. >> Sure.
>> And we'll share your screen and
see how it goes. >> Yeah, before I leave that, so
I'll just leave you with two things. I know my career is probably
not that very common. I have done a lot of different things. There are two things I always like
to tell students, that one is to never stop learning new things that
will open a lot of doors for you. And then the second would be,
be helpful to everybody around you. So the reason I got a lot of
opportunities within my career path is that I try to help everybody I know,
I try to do things for them. And then every time there's
an opportunity with other people, they always think of me as someone who
can help them, they always come to me. So learning new things will open doors. Those people who you help will
open a lot more doors for you. And then you can advance your
career a lot easier that way. So let me again start sharing my screen. [COUGH] You see the screen okay? >> I do, it looks great. >> All right, awesome. So I will spend the first 20, 25 minutes to give you a very high
level overview of [COUGH] the services. What is machine learning in the cloud? I know you guys are all close to
being experts in machine learning. But how to actually practice machine
learning in the cloud environment, that's what I will highlight. And then introduce you to
the SageMaker framework, and also the tools,
You're going to see in the cloud. And then I will, because one of the focus
you guys are interested in is the AutoMLs, I will show you a couple of examples
how AWS does the AutoML through the SageMaker Studio Autopilot and
also SageMaker Canvas. And then, if we have time at the end, I like to show you a couple of other
examples in the SageMaker Studio. Different industries,
how does Jupyter Notebooks, how those models are being built, and
how the data are being processed. And how those models are being
deployed in the cloud environment. So this is a really high level overview
of the whole machine learning stack. So there are three different levels. If you're a machine learning practitioner,
data scientist, right? You're building your own model, and then
even you're building your own framework. So the three most popular frameworks for machine learning is TensorFlow,
MXNet and PyTorch. Even if you are developing
your own model and not using any of those frameworks,
things like the GPUs and CPUs, and also different special chips,
I'll talk about that a little bit later. This is the services that you
can use if you're doing this really ground works in machine learning. So what about those
chips here I mentioned? So, initially,
when we do machine learning and most of the models being
built are on CPUs. Those CPUs start evolving
into really high clock speed with a lot more
memories on the CPU units. And then people started thinking
about using GPUs for linear algebra, for example. That part of the training runs
really efficiently on GPUs. So NVIDIA GPUs initially designed for
graphics, and then gradually being
used in machine learning. Again, a lot of speed up for a lot of
the model trainings and inferencing. But we start finding out that
those accelerations are not using the full potential of those GPUs. Because they initially designed for
graphics, right? And also they are really expensive, because they're building those special
capabilities to process graphics. So service providers start thinking about building those machine
learning specific chips. In our case, in AWS,
we start building our own chips. One is called Trainium,
one is called Inferentia. From the name, you probably can tell that
Trainium is a chip specially designed for training a machine learning model. And Inferentia is a chip that we
built specifically for inference. So inference is the process that once you
build the model you make it available to provide predictions,
that's what called inferences. So the Trainium and Inferentia. And recently Intel start
building purpose-built chips for machine learning as well,
the Habana Gaudi is one of those in AWS. So all those chips,
with the GPUs, CPUs, and also specialized machine learning chips, are all provided to you as the virtual
machines or container instances. So if you are developing your own
algorithm with those frameworks, or even higher level frameworks,
like Keras and Gluon, you can run those things on this
environment provided by AWS. That part of the the workload
is a lot easier, you don't have to worry
about the infrastructure. So a few years back, and
we start thinking about, well, maybe some people with the coding skills
want to do machine learning, right? So not just the data scientists
who wants to do machine learning. So we started thinking about this idea of
encapsulating some of the complexities, heavy liftings frameworks included. And then keep that away
from the developers. We want to make machine learning
available to a larger audience, including developers who can write code. So Amazon SageMaker is the framework and
the service that we provide to do that. So it includes a whole series
of different tools and functionalities, all packaged
within the SageMaker framework. And then the interface to that SageMaker
framework is the SageMaker Studio. Inside the SageMaker Studio,
I will show you the SageMaker Studio later on in our demo, we provide tools for
you to aggregate and prepare the data. So the data processing part, and then we
also provide you with a much easier way to pick an algorithm and
then train your model. So the main three steps of
the machine learning training is, if you process your data,
you train, and tune your data, and then you've got the model, and
then you deploy it, right? So the SageMaker Studio
environment provide a interactive development environment for
you to do all those within SageMaker. And also recently we start thinking about
deploying models to the edge devices. For example, smaller IoT devices,
and also cell phones and smartphones, things like those. You can run those models
on those devices directly without going back to the Internet. Go ahead. >> because I think a great example there,
and if students have the time,
you sell a race car. >> Yes, yeah, [LAUGH] Deep Racer. Yeah, that's a really very
good example of HCompute. Do you want me to talk
about maybe on that? >> You can build a model for it, and
the model runs entirely on the car. Because the car has to make decisions, and
it doesn't have the time for the latency to hit the Internet, go up to the cloud,
and make a decision, come back down. And it's just, I think it's like $300 US,
and we have a couple here. And it's just a really
fun tool to play with, where you can actually see these
things happen in real time. Where you can program the car to
stop if it detects an obstacle or go faster if it detects a green sign. There's all sorts of
fun things you can do, don't put it in your
room with a green screen. >> [LAUGH] Well, so actually, we recently add another sensor
on top of that car later. So laser range finder, so you can
actually asset the letter to map out the entire room first, and then to
avoid obstacles and stuff like that. So that might work in
a [LAUGH] green room. So I've never tried that, but anyway,
yeah, that's a watch [LAUGH]. >> One of what we were doing an intro for
not machine learning people just to show that, hey,
you can do some programming over here. And we were having, I think they were
ninth graders where you could hold up a red sign and the car would stop,
and you could hold up a green sign and the car would-
>> That's really cool. >> But it was so cool for
them to be able to see the code. Here's the X code and you could change the
lighting in the room and it wouldn't work. It was, anyhow, sorry to interrupt. >> Yeah, no, no, so that's very cool. So we actually have more than just
a DeepRacer, there's a DeepLens as well. Somebody built a DeepLens to open
the door for a cat to his house. When you use the image detection,
like say object detection and image recognition, right? Here's a cat, here's my cat, open it up. Don't open the door for any other cats. DeepComposer is is another one. So you can actually write your own music,
use DeepComposer, those are all edge devices. There's a lot of usage of each device
with machine learning in manufacturing, for example, right? So if you run a large oil refinery, for
example, you've got all those pipes, and valves and stuff that you need
to have is instead of asking people to go around to check the status
of those curricular components. You can actually have those
IoT devices detecting, trying to detect the anomalies
of those operations conditions, so it's just very cool. So I talked about the the bottom layer
which is for the data scientist, and then the middle layer for
developers or people who can write code. Let me just, And then we also build, let's say,
if you're a business analyst, you have a lot of data,
then you have some images. A really good example is that for medical in medical professions,
medical institutions. They have a lot of docker nodes, for
example, they are all hand written. There are a lot of those and
do you want to process that? You don't necessarily want to hire a whole
bunch of developers to write machine learning models. We start building services and
we call AI ervices, those are the package of services. Machine learning Children's services for practitioners for nurses, for business managers, for business analysts. So they can submit the documents, for example, we can extract those
documents into keywords. Or we can extract those documents and
turn those forms into a stretcher data. So a lot of those are turning
unstructured data into structured data. Image recognition, for example,
we can identify who's in the picture, and then maybe that even give you
the boundary boxes of those objects. And then a lot of the one of the most
popular one is Alexa, right? Alexa is a higher level AI services we
provide, its natural language processing. We train Alexa constantly
on very large datasets, and then so they can become better and
better every day. There are also things
like the the lookout, those are the anomaly detection
runs on IoT devices, I mentioned. So those are a whole bunch of,
a lot of those vision, text, search, chatbots,
personalization and forecasting. For example, fraud detection,
even for different industries, those are AI services that
are readily available in AWS. In different ways, you can log into AWS
council, and then upload the document. And then you'll get the results back in
JSON format or you can just run APIs. API calls to the services, and
then do batch processing or you can develop your own program. And then make a SDK API costs, and then integrate all those services
into your own application. If you're writing a mobile app
application or a web application, you want to introduce machine
learning capabilities in there. Tone of this call center platform IO,
I was talking about. You can build the next generation of
your call center business instead of building your own predictive models. And you can actually make API calls to
a model that's hosted in aid of this Sagemaker. So that makes the machine
learning really available to different levels of people who
wants to do machine learning. So the heavy lifting as I talked about,
all those machines are done. We provide a wide range
of different instances. Those are just a few examples
of the instance families from general purpose M5s
to compute intensive. Start with the C and R the memory
intensive, that start with Rs. So based on your workloads, what you need, you can choose different
types of families. We also build our own special
chips arm based architecture. So that any of this instance end with
the g, that's a graviton chips that does the scientific computing floating
point operation really, really well. And it's a lot cheaper than Intel chips. Then we have a really large, those large
GPU based instances, for example the P4D. This is the piece of
a machine learning machine. It has eight A100 GPUs on board, and then through the interconnected
is 300 gigabytes per second. The graphic memory alone has over
400 gigabytes of graphic memory and has over 1 terabytes of memory on the box. So those are the really most popular
machine learning instances and we'll also have the instance that just for
influencing and training. So I see, Michael, you popped up and
you have a question? >> My video cut out, I wasn't supposed to. >> Okay, all right, so this is one of
my favorite examples Next Gen Stats. So I'm not sure if you have noticed, I noticed this is just my guess,
I don't have any proof. So this year's NFL,
especially close in the in the playoffs. Have you noticed that a lot
of games are really close? There are three points win by
a field goal and stuff like that? Actually more than normal, right? It's almost all the games
are really close. And also, there's a lot of calls
by the coach that they don't want to go on fourth dance even
when they're 3, 5 yards away. So this Next Gen Stats,
so all the players, they have chips embedded into their shoes,
and helmets, and gloves. And every tenth of a second those
IoT devices were sent data into a data center inside the stadium. And then that stadium will go
through direct next to AWS cloud and then that data will be sent to the cloud. And then within one second
the prediction of the next play or the outcome of the next play based
on how the players are lined up, who's on the field, and
then what's the previous formation was. That prediction will be sent
back to the teleprompters of those Tony Romos, and
Chris, the sort, right? So they can start making predictions. Initially, I was, man, these guys
are really, really good at their work. Well, they are good, but they got
some help from this Next Gen Stats. [LAUGH] It's interesting, you said they
go to a data center in the stadium. I would have never thought that there
would be data centers in stadiums. Yeah, every single one of them. So all the stadiums in NFL has this
direct net connected to AWS cloud. So that's really sometimes a couple of ten
gigs lines directly to our data centers. So they want to reduce the latency. They don't want to go through the public
Internet and all this, right? So they have a direct connect. And then it's not possible for
us to ask the little device in the shoes to send the data
directly to AWS, right? So they're not powerful enough. So the IoT devices will aggregate
data into local data centers and the data center would send it. Yeah, so this is one of the things
I keep noticing is that those data-driven decision makings on
the field as more and more now, and you can see people looking at their
tablets all the time on the sideline. This is one of the really good examples. Those models are trained
on large amount of data. They're getting trained every week with
the new data coming in from previous week. And then they provided to the decision
makers on the field or the broadcasters. [COUGH] So the the basic workflow of a machine
learning model is at three stages, build, train, and deploy, right? So SageMaker,
those are all of the line items. I'm not going to go
through each one of them. Those are the functions that
provide those tools for you to develop models for
example, and train, and attune them, and deploying them. More importantly,
before you even get there, so some people always say 80% of
the time you spend doing machine learning is actually in data processing,
data preparation. So we also provide tools for
you to make the data analytics and data processing a lot easier. So in this case for example,
if you don't have labeled data to build a supervised learning, we actually
have a service called Ground Truth to help you to label those data using
a combination of automated tools and human behind the scenes for
you to identify. For example, draw boxes around objects and
images in self-driving scenario or help you to label certain things
based on the common knowledge, right? So Amazon SageMaker does all this for you. So let's just dive a little
bit deeper into that, the build part of the build, right? So SageMaker Studio is the tool that
you used to interact with SageMaker. There are other options. Once you develop your model you
can automate the data processing, training, deployment through APIs. Then you don't need to
use SageMaker Studio. But SageMaker Studio is the first
step that you're probably going to, if you are developing a model
that's what you need to use, it is a Jupyter Notebook environment,
they our own flavor of JupyterLab. We add a lot of functionalities. We created a lot more different
default kernels in there for you to run different frameworks. So your students are familiar
with the Jupyter Notebook? Yeah, absolutely. Okay.
Yeah, so each of the blocks have different things, I will go through some of
the examples in greater depth later on. But in addition to the notebooks,
we also add other tools. For example, if you're building
a machine learning pipeline, then we have a visualization for
you to look at the pipeline, give you an option to
execute that pipeline. We also have data flues. And when you're doing the data
preparation, first step, next step, you want to drop a column, you
want to do a transformation for example, these are all visually available for you. And then you can manage different
experiments within the same environment. [COUGH] So we provide this IDE for you. So you don't have to worry about
the machine that runs underneath it. So today you're probably do
everything in your own laptop or you get a virtual machine from the school,
and then you install all your packages,
and then you run the Jupyter Notebook, you do your training and then they
deployed somewhere to run, right? But all those hardwares
are taken care of you by us. So you don't have to worry
about these computers anymore. Focus on your notebook,
focus on your algorithm. Then this SageMaker Notebook can
also be shared among other people. So for example, if you're running
a class then you create one domain, all the students or all the researchers of
women, a research project can share that environment and
then share the instance underneath it. So they can share the research data for
example, they can share the results. The research in research world,
the sharing your result, being able to let other people
validate your experiment or result is really important, in SageMaker
that makes it really, really easy. You can create a Jupyter Notebook,
package it with your model and your data, send it to someone else. And then they will run it,
then they can validate what you have done. So when you create a Jupyter Notebook and start running a notebook would ask you
what type of machines you want to run. So here is where you can specify. So one thing I want to keep emphasizing
is that this is the instance that only runs your Jupiter Notebook, right? Because those large GPU
instances are really expensive, you don't want to have that instance. And then just run your Jupyter Notebook
for ten hours and then only train it for two hours. Train your neural network on it for
two hours. So we separate the notebook instance
from the training, from the deployment. So you only work on that smaller
instance and then working in your Jupyter notebook and then the other heavy
liftings are actually done somewhere else. I'll show you through architecture
framework diagram how that's done. So in this case, you pick instance. If you want to do local GPU, you can select the GPU instance
accelerated computers. So the good thing is that,
once you claim one of those instances, any of the other notebooks is
going to run on the same instance, so you don't have to pay for
multiple ones, right? So very important with that. And then with one click, if you're done
with your work you want to share your notebook, [LAUGH] a lot of instructors
actually use this for homeworks. So you can just say, hey here's Jupyter
notebook, good work on your homework and then once you're done send it to me and
then the instructor or the TA will be able to run through that entire
notebook and then check the results. And then with the SageMaker notebook, we
provide a lot of default building kernels. Kernels is the [INAUDIBLE] run
in your interactive python or sometimes URL or Julia as well. With those, you have to create
your own customized kernels. It's a runtime environment with a lot
of the packages already installed, conduct environment you install MXNet for
example in there already. When you pick that kernel,
you will be able to import those MXNet packages directly,
you don't have to go install them. Same thing [COUGH] with the GPU kernels,
that means those kernels already have CUDA libraries and all those
graphic libraries installed already, so you don't have to go
fight with those things. Sometimes, that's a pretty hard battle
to fight with the drivers [LAUGH] so you don't have to worry about that. >> [INAUDIBLE] trying to set this up by hand
>> [LAUGH] >> Getting all the libraries to install, compile, getting python to
recognize the GPUs, that is not a >> [LAUGH] >> All library name, that is an afternoon of work at a minimum. >> Yeah, that's why I used the word fight,
[LAUGH] I have to fight with them. So with those kernels makes
really easy to get started. So, this is what the SageMaker
studio look like. All the sessions,
all the notebooks open are listed. See this,
I have a lot of notebooks open already but I only have these two instances running. So you are basically only paying for those two instances even if you're
running a lot of notebooks. And so one other thing I want to
mention that in SageMaker, in addition to all these algorithms,
we provide things like parallelism. So if you have a large
model you want to train, you want to run on distributed
environment or distributed data, we have two different ways for you to run
this in parallel, in multiple instances. You can divide your neural network up
into ten different chunks, for example. Each one of them will run
on a separate GPU instance. And then at the end of each iteration,
it will communicate with each other, aggregate the loss functions, for example, and then you'll move on
to the next iteration. And then if you are just running
a decent size of the neural network but your data is too big to be
processed in a single instance, you can use data parallelism to separate
the data into ten different chunks. Then you can train into ten
different machines that way, so model parallelism and data parallelism. Yeah, so I-
>> You're about to get into this I think. One of the, let me fix my volume. One of the things that we run into is,
it's the same cost ultimately. The cost difference isn't that high
because what you're talking about here is either 20 hours on a single instance-
>> Yeah. >> Or one hour on ten instances. Or if you're billed by the hour,
it's the same cost except- >> Exactly. Because you can move forward a lot faster
because you've got the hour as opposed to the 20 hours which means when you make
that line of code change you've got to wait for the results, you have less
time to drink coffee unfortunately. >> [LAUGH] Exactly. So, we also talked about the speed
to science, speed to research. That's what we're talking about, right? Instead of waiting for 10, 20 hours,
you get your result in one hour or two hours and then you pay for
the same, all right. So, you only pay for the time you're
using the computer and the storage. So, let's move a little quicker unless
I want to show you some examples. So there are 17 building algorithms
at an Amazon SageMaker already. Pretty much anything that you
can think about regressions and classification for example, for
tabular data and then image recognition or even with the regression with the neural
networks, you can do that as well. So those are the building algorithms you
can use immediately in your training and then you only need to provide your data,
then your model will be ready for you to run. [COUGH] For clustering,
unsupervised learning, forecasting, time series and then dimensionality
reduction, the PCAs, and all this,
those are all building algorithms. So I will just show you quickly what
you can do with those algorithms. Right, so that's one option,
use your building algorithm. A couple of lines of code point
to where your input data is, the training data or
your validation data or test data is. And then start the training,
you get the model. Or you can write your own script, so
if you're using one of the framework tensorFlow, for example,
you can use the tensorFlow, the container. Then provide your own python, and
then we can train that model that way. Or if someone else's already built a
container, like docker container that has all the algorithms in it, you can hand it
over to us, we can train that as well or you can train it locally on
that Jupyter notebook instance. Once all those things you're done, you
want to run large production environment with machine learning, you can create
a pipeline that can be automated. So those are all different
ways to run machine learning. Here is a really simple example,
in this case with your own script. So you are, I'm sorry, with the building
algorithm called XGBoost within SageMaker, this is the algorithm that gets used for
regression and classification. First line is, you create an XGBoost
with your own script, and then you provide your hyperparameters,
and then you point where the training data is your validation
data is, and this is the one line of code actually train that model
is called estimator.fit, right? So once you do that And
then your model will start training and then a couple of minutes later will be
available depending on the size, right. So this is another example of
using someone else's PyTorch. You provide a script that
does the PyTorch, and then this is the instant type. So this is what I mean, you're running
your notebook in a smaller instance, but you want a larger instance to train,
this is where you specify. And then we'll ship this whole thing,
we'll package it up including this container, and then to run on
this instance with this special size. Alright.
So it's a it's an environment that's managed by AWS. You don't have to worry
about the machine itself and then you run the fat same thing. Or with the container,
you just point to where your images and then you run the training there. So here's what that
architecture looks like. Here is your AWS environment. You create jupiter notebooks and
you run your jupiter notebooks there. You go look for your data and
your S3 bucket. Once you start training is going to
the container down one line of code of you specify in there is actually
a docker container. Got shipped to an environment
controlled by AWS is fully managed. We'll run it for you and then we'll
give you back the model at the end. Right?
So that's how it's done at a very high level. And then for deployment. Once you have the model not, doesn't
have just to have to be your own model, you can be someone else model. Hugging face, Mxnet,
someone already trained something. You can deploy them. So what the deployment process looks like? It sounds really simple. Yeah the deployment right? [LAUGH] So but it has a lot involved. What it does is actually take
your model usually is a jar file. And then create a web server around it and then make your model
available through API coast. Within the API coast If you're
running a production environment. You want to be able to handle thousands or
tens of thousands or even millions of requests
per second like Alexa. Alexa is the deployment of
a machine learning model. Everybody else is making requests to that. So that's the end point,
we're talking about. The model endpoint, this whole thing
takes a lot of effort creating instance, create a web server and
then make the API available. You need to secure the API as well. And then you need to put it behind
the low balance or things like those just make it really it's really hard
to to get started to get doing. But in SageMaker it's
a couple of lines of code and we take care of this entire stack for you. Okay, that's something that you don't
usually deal with until you have to run a really large influence endpoints. So that's important to know. And then enable this auto
scaling is straightforward. You can specify the minimum capacity. I want to start with the y instance,
I want to cap at k instance for example. So that's how you handle large
request on your end point. So we managed the scaling for you. When you have more people using
your service will go scale up. We'll add a couple and
more machines in there. So you don't have to pay for
the 10 machines all the time, you just pay for whatever you use. So that's autoscaling is a really important characteristic
of a cloud computing. So there are those are the train and deploy steps in how you do that
in SageMaker really quick. So there are other options. There are other features we talked
about that you might be able to use in the one is the autopilot. So I'll show you an example. What the autopilot is want to automate
the machine learning process. Start from data processing and
then build a model and train the model that
those all those stages. There are different options for you. For example when you're
running the data processing. You look at a bunch of data and then you want to have a strategy of how
you deal with missing data for example. How do you do the imputation when you
have data missing from that Excel spreadsheet you get? Right?
So those are different strategies you
can use using different strategies. Could particularly impact your results,
your model precision for example. And then how do you want to do
the encoding of text fields for example. Those are all different strategies. You need to try different ways. And then when you are building a model
you want to say maybe I want to use XG boost for my regression. Or maybe I want to try leaning
learner another algorithm, different ways of chinese
different algorithms. And also each algorithm will can
have a different set of hyper parameters that you want to experimental. Because all those things will impact the
result of the performance of your model. So imagine doing all
those things yourself. Five steps here. Five steps here. And then five steps here. You're talking about five
times five times five. That's a lot of different
combinations you have to do. So auto ml just take that away and
take that complexity away from you. You just tell me give me some guidelines
how many different models you want. And then I will just run all
this experiment for you. And then at the end I would
recommend this is the best model so far based on the Matrix you tell me. Let's say if you want to
focus on f one scores or if you want to focus on the precision or
total recall, the recalls. Those are the parameters we're going
to use to measure the performance of the model. And then I'll run the auto ml for you and then at the end I'll just tell you which
candidate is the best one for you. So that's what the auto ml does
in Sagemaker is called autopilot. So I'll show you later on how we do that. So. >> It's effectively brute force. >> It's a brute force. You don't have to be done by you. [LAUGH].
So someone else will work it on. Which is yeah, which is nice. [LAUGH].
>> But you're automating the brute force of,
let's try all these different things. See which one does the best and keep going until we get
a solution that we're happy with. >> Yeah exactly. Yeah automating of those,
that's why it's called auto mls [LAUGH]. If you look at breaking it down,
it's not much to it. But it makes things a lot easier for
some models. So this StageMaker the studio has
the building tools for you to do auto ml. And then we also, so
some of the things I mentioned, we call experiments different twice. Right?
So before if you want to do different trials, you want to have
an spreadsheet ready. Every time you do something
you want to record it. So here's my hyper parameter this
here is the result I want to do. So keeping track of those are manual but in SageMaker studio keeping track of
all the different experiments you do. It's a lot easier. So we actually provide this
environment for you to visualize. For you to look at the metrics and
integrate with our back end loggins. And then then you can just focus
on trying different models. So experiments is something
that you can be tracked down, worked on very easily in
SageMaker studio as well. So they have visualizations for
you to take a look track of where you are. It's also deep burger. Machine learning is has been a lot of
the process has been a black box for a lot of the people. Right?
So you create trainer model and then you wanted. Your goal is to get your loss function to
converge a lot of, in a lot of the cases. But how does that model or how is your
loss function behaving and stuff? You might be able to
know through the output. But you don't know exactly how these
neural networks are being triggered. For example, if they are fully connected
with thousands of nose in there. Right? So that kind of things,
we introduced the SageMaker debugger. That you can't help you to start making
sense out of what's going on during the training itself. So debugger is help you to capture
the relevant data during the training process and then help give you some
alerts based on some of the rules. If, let's say,
my the loss function is diverging, I might want to kill this training
process and try something else, right? So those are the capabilities that
are introduced by the SageMaker Debuggers. And then,
we also have a model of monitoring. This is not something that people talk
about a lot but really important. So [COUGH] when your model
is being trained and you train on a certain datasets. That datasets you collect from last year,
for example. Let's say if we have a model like tracking
the delta varying of COVID, right? So you want to predict the spread and
stuff like this. But starting early, recently,
there is omicron variant, those things will behave differently. So your model might work really well
with delta variant but when the new data coming in, you're going to start seeing
the performance of your model degrading. So that's what you need to keep
an eye on your model all the time. Is that, is your model behaving
the way you want it based on the new data that's coming in? If it's not, you might want to go back and
include your data into your training and then redo the whole training,
redo the whole model again, right? So keeping an eye on the model performance
is something that's really important after your model is trained. So we provide a model monitor capability
in SageMaker Studio as well for you to automatically collect the data and
continuously monitoring those data and provide you with the data visualization. And then even trigger out
the entire rebuild pipeline to rebuild your model
including the new data. So that's something that's already
practical in production environment. And those are the visualizations in
the studio that can help you to do that. So Canvas is one of our newest
services that we provide in SageMaker. So AutoML, you still have to do
it in a Jupiter Notebook, so you have to know some of the code. That way, we made it a lot easier. On top of the AutoMK functionality,
we extracted a little bit further. If you start with a CSV file, you upload
to the environment and then you select on it and we give you the visualization
of all the data in your package. And you tell me which column you want to
drop, which roles you want to exclude and then click on the button will train it for
you. So we'll basically do the AutoML
with the button clicks, so those two options
are going to show you later on. That makes a lot of the business
analysts happier, right? Because you ask them to go along into the
Jupiter Notebook and then run the code. They might not be familiar
with that environment. They don't know what
the colonel is probably, but they can upload a CSV file and
then they know the trend. They know what's more important to them. They can help select the features and
then they can train the model and more importantly,
the model is ready for you to use. So the influencing is ready. You can do bachelor referencing. You can just type in
individual records and then you will get the result
of the probabilities. Okay, so
that's a very nice UI autopilot or AutoML, so, yeah,
that's enough for the talk. Let me just switch to a demo. I'll start with the Canvas. So in this case, you log into SageMaker,
this is our AWS SageMaker console. The way you get to this
SageMaker console is always by just typing the name of the service. Then you get this link, you click and
you will go to that console. So when I say SageMaker console,
this is where we are. And then you can start
the Canvas by just launching an application, so I already did that. This is what the Canvas look like. So as someone coming in, I can just go create a new
model with the default and then I already uploaded some
of the data in there, right? So there are two datasets. One is the very famous Wisconsin
breast cancer datasets. I'll show you a notebook, looking at
the data center later on if we have time. And so you select that and
then by selecting the dataset, you pre upload it or
you can just upload it directly. So can see there's an overview
of all the features. Each one of the columns is a feature,
right? We call it feature. So on the top,
you want to tell me what your target which is the labeled column is. In this case, it's diagnosis. So this dataset is created
by researchers in Wisconsin. The data is really,
really high quality, really clean. So that's why the precisions or
the performance is really good. What they did was they take samples
of breast cancer tissues and then they look into a microscope and
then measure the size of those cells. And then look at the features,
how regular those cells are there? How many concave points, irregularities, the size of the radius
of those irregularity? And then they make those numbers
into two dimensional dataset. So those are the issue of the columns and then they know based on the diagnosed,
is that benign or malignant, right? So this would B and M,
those are the two class labels. You can tell that it's not too different
and there's a little bit of imbalance. The benign is more but fairly decent,
better than a lot of other models. And so in this case, you can just
do some analysis on the data to see the distributions, for
example, of each column. And then, once you look at the data and
say that data is pretty good. So you can actually, whoops,
I build in the data. If you want to say drop certain columns,
you can just select from here, right? Those are column is going to be dropped. What's interesting is that when
you click on the preview model, it will take a few seconds. This one is fast but
based on the size of the data, it will run a really really
fast model building. It will build a model that gives you
a really, really rough estimate. This looks like the data is pretty good. This is going to give you
a 97% of the accuracy, right? So once you are satisfied with this, so do a quick build or you can do
a quick build or standard build. Standard build takes a lot longer because
it's trying a lot more different models and hyper parameter combinations. A quick build takes a few minutes, so
you can build a model right there, right? So I already have one built for
you for today. So this is what what the model looks like, because I already built this model and
then I can do some analysis, right? So I can tell you which feature
actually contributed to the performance of the model the most. This concave point means
is contributing to 16%. So those are the top five columns
that can tell you, right? Which features, normally those
feature contribution is a pretty hard thing to do unless you do things like
the PCAs, dimensionality reduction. Then you'll have to guess what column
is going to be important to you. But this tool does this for you
automatically, does this kind of analysis. All right, once you're done, so this
model is built pretty good 97.4, right? So the endpoint, the whole thing I was
mentioned about, you get the model, you create a web server,
you create a load balancer stuff. That's already done for
you after the training is finished. So in this case you can just
either do a single prediction, which means you can manually import input. Those features in there will give
you a prediction right here. Say based on those numbers, this is
a benign breast cancer center, right? Yeah, so that's a really high level, very simple demo of the canvas,
is how you do AutoML, very easily without even
looking at the code. So next time I'll show you
how to do actually do that in SageMaker with the code. So one of the main things I want to, have you guys worked with
the SageMaker examples? >> Yeah they've got an assignment prior to
this video that goes over the examples. One of the examples where
they're going into the model. >> Okay, yeah, so the Github, definitely
there's a AWS SageMaker examples. Hundreds of those examples, different
kinds of algorithms, different verticals, different industries, best place to
learn machine learning hands on. The way you get access to
that is using this building. Git integration so
you can create a new get repo by just pulling cloning on this git repo and then all the codewill be
brought into the SageMaker. In my case I have a SageMaker
example already pulled down so all the examples are in here. So I am going to walk you through,
the first thing is that we have this
data analytics tool or processing tool called data wrangler. So you this is another plugin
into SageMaker is that using data wrangler you can input data and
visually look at data. And then try to look at the statistics,
for example, do analysis that's the first stage,
really useful. So let's look at that autopilot,
how you do that in Jupyter notebook. So those are basic preparations
how to get this data. In this case we're using
customer turn data. Customer turn is a really
popular problem in marketing. What it does is if you want to run
a campaign try to convince people to buy your product, you want to call them,
you want to talk to them. And then based on the what they did
before or some of the features, bio demo data for example how,
where do they live? Do they own a house? Do they own something, right? In this case it's a bank trying
to sell somebody a city. It's a product of a bank but
they look at different features. What their employment history? Do they own any houses? Are they married? Are they students or
do they have profession? That kind of stuff. Those are those features. So they want to build
a predictive model to see if most likely this person is
going to buy the cd or not. They don't want to waste too much time
if the possibility is really low because calling people is pretty expensive. So this model is used a lot in our
examples of building simple two dimensional datasets. So if you look at the dataset,
their age, their job, their marital status, their education
levels, those are the features. And then there is a feature at the end,
yes or no? Did this person actually end
up buying the product, right? So those are your feature. So in my case I don't want to
manually build this process. I want to do AutoML, so what you're doing
is first of all get the data in place. So here you split the data into
training and test dataset, and then just create an input configuration,
so here's my data. So there's the code for
creating AutoMLs is that SageMaker go create an auto
machine learning job. I give you the name,
here's my input data, here's where my output data which is where I'm going to
put machine learning, the model itself. Here is the order of the configuration,
and then just run it. All right so train it that so
if you look at what it does, actually, it's doing
a lot of data analyzing. So like I mentioned you want to figure out
which column to use, which column drop? How do you actually fill in those missing
data for example different strategies. And then you do feature
engineering as well. You want to maybe combine two columns or
you want to drop one of the columns because two of them
are really highly correlated. And then you want to train a model
with the different hyper parameters, called tuning, model tuning and then at
the end we'll generate those reports. So after about, like in this case about 20 minutes, you'll get those reports. So this is the entire AutoML process,
now, what you get at the end is this. I can see that these predictions
the matrix of the different things that is trying, its giving me the output, right? I can monitor the process there. And then more importantly
with this AutoML training, it provides me at the end, let me
just go all the way down to the end. It's, actually tells me I created
two additional Jupyter notebooks for you automatically, why? Is to for for you to look at how I
actually did the data analytics and feature engineering. The other one is how I did the training. So you have visibility into how
the Autopilot works, right? So let me just go to the first one,
the the data exploration report that's automatically generated
by AutoML in SageMaker. You will tell me by looking at
your target, your yes and no. There's a pretty big class imbalance,
that a lot of majority of people said no, there's only 11% of people said yes,
right? And then here's the sample of that data. Does it look the right? Does it look right to you? And then, we did a look at the correlation
of some of the some of the columns, some of the features for
example, this employee and this is the interest rate in Europe,
right? And also the employment rate. They are highly correlated, because it tends to be that way when
you the employment rate is high, your inflation rate is kind of high and
then your interest rate will be high. Maybe I don't need to use those three
features all at the same time because they're kind of like the same. And then more feature you have the more
likely you're going to over fit and then more likely you're going to run
this training a little longer, right? So I might want to drop this
to one of those two columns. And then,
are there any anomalous in the Sarah? Because this data is already cleaned. Really well, there's nothing numb anomaly. There is no anomaly in there. There's no missing data because
the data is a really high quality and then we want to look at certain columns. This cardinality is that you have
text based columns in there. Are they actually yes or no's or
just five different colors. If that's the case, I might be
able to code them into 0-5, right? But if it's just a description of text
they're probably going to be thousands of different values. It's not going to be a categorical so
they won't do that. So those are the strategies you
have to employ when you're doing the data analytics before
the machine learning itself. So autopilot does that automatically for
you. And then, also does some descriptive
analysis, statistics, the age and what's the mean? The median and all this? Very cool stuff. And then, also yeah that's
what the data analytics part. So the the model part of the this
is the second Jupiter notebook. The auto ml generates is for you to look
at first of all, look at the set ups. And then the candidate pipeline. So you gave me because I told
them that only because I want this thing to train really fast. I said just do five different things. Try five different combinations. So you automatically create,
generate five different strategies. The first one is they use a DPP
means data process zero and then I use XG boost for
this classification problem. And then, in this case I trained them
on different sizes of the machines because their data processing and
also machine learning training. So they use two different
instances to do this. And then this is the combination
I use the XP boost. Those are some of
the training configurations. And then, the second strategy is
called DPP one is also extra boost. But with a little bit different
data strategy in this case I use a different strategy for
the categorical feature for example, very similar configurations. This is the third one. Third one has a different
machine learning model. It has a different data processing
strategy but with the leader and learner instead of XP boost. So this is our combination,
all total of five of this. And then at the end I wanted to
just do select the candidate and go run that out of autopilot. So here is the list of
the piper parameters, right? So, XG boost have a different set of
hyper parameters than the linear learner. Linear learner have different
set of hyper parameters. So those are configured in this
hyper parameter configuration. And then at the end I'll just go run it. So this is what a very
high level what that you can look at into what this autopilot did. So any questions or do we have any
more time for a couple more examples? >> We probably have about
ten more minutes or so. But what's cool is how you've basically
taken all of this complexity and abstracted into canvas. >> Exactly.
>> So that business can actually go through and run these and
get some value out of it without having any understanding
about the complexity beneath it. >> That's right. >> Really awesome feature that's
moving machine learning out of this. It used to be a math realm only then
it became a programmer realm and now it's going to be the case. >> Yeah, you got it, exactly. So, next it's that same breast cancer. Some of the data we were looking at this
is how you actually do it in Sagemaker. Is that first of all,
of course get your data into shape and then look at the data and do some
processing and the next it would be too. So this is one item code, is how you get a building a lot of
the Sagemaker building algorithm, right? So, I'll go back to the architecture
is that everything we do in Sagemaker runs in a container. Pretty much even your notebook
that runs on the container. And then when you get this algorithm,
it's in the container. So, what's in that container is all
the framework that's required by that particular algorithm. If that's built on top of tensorflow or build on top of pytorch those frameworks
are already installed in that container. And also the algorithm itself, the code of a linear learner
training is also in that container. All you need to do is to
provide me with where your data is there data stores, right? And then, your output, where you want
the model to be stored after I finished training and then you're hyper parameter. In this case,
there is a feature dimension. Mini batch size, air packs and
a number of models. Those are all hyper parameters you passed
to me and then you probably guessed it. Next time, we'll just to do a create
a training job later train, right? If you want to do interactively, that will
be the estimator fit method you call. In this case,
we ship it over in a batch fashion and let the container run
in somewhere else too. And then,
the next step will be deploy it, right? Once you get the jar file,
the tar file and then how do you set up the end endpoint and
then you start doing the predictions. In this case, create that we send
a batch of data through this, we estimate the accuracies and
compare with the baseline. Next up we'll move to something else,
which is really cool. We were introduced recently as well. It's called Sagemaker jumpstart. So initially doing machine learning
is just like what I show you start everything with Jupiter notebook,
you write your own code, pull down the models during influencing. So, the typically this
the jumpstart which, The automate some of the process for
you, even further. For example if you want to
do image classification, instead of asking you to start
with a bunch of data and the trainer model we actually put
on some pretrained model for you. In this case exceptionally threes or resin
yet those things just click on those and then click on the deploy will
actually deploy the model. We pull down the model from
somewhere from a repo and then deployed it as an endpoint, and then
you can just start doing the inferencing. For example in this case I have this, once you deploy that model it will
give you a jupyter notebook, right. In this case,
give me a jupyter notebook that has some example in there and
then already deploy that end points. And then you can just say here's my image,
go identify where my chairs are, where the tables are. There's any flowers in there, or just say well what's the internet
without a cat, right. So there's my animal,
can identify a cat 100%. And there's a dog identified as well. So making teaching,
things like those a lot easier if you want to get started with industry for
example. In this jump start you can find
different industry examples. A different solutions for
different things, document understanding for
example fraud detections. You can just click on those and then
launch that solution will provide you with all the things that you need to learn,
and then you can just kind of pay some of the code from jupyter notebook and
work on your own model, right? So let me show you another one. So corporate rating for example. So this is a very practical
example in the jump start, is that once you launch you
get jupyter notebook model. So what it does, the difference between
this one is also using a tabular data but the difference between this and the other tabular data is that there is
one column that has free text in there. So all the other public
trade companies need to submit their the 1K, 10K, 2SEC, right? So we can validate that the SEC
can invalidate their risk and stuff to decide if there
is any risk in their stock. So using NLP and natural language
processing we can actually turn that really boring long
document into something really important and
then we can include that important data. Let me just scroll down,
it was really important things. So by reading this that text
column we can actually extract cause visit the document is positive or
negative. Are there any certainties or uncertainties
involved into the description? Sometimes the company will describe
their immediate risk into certain or something, right? Or they're under litigation or something
or if there's any fraud involved. So this NLPs will actually extract
those features from the free text and add it to the data points. So then you can use a very
simple regression model or a classification model to build on
those two dimensional typically sets. So this is one of the very
practically examples. Yeah, there are lots of
other examples we don't have time to go through but yeah,
this is well and here. >> Okay, well thank you very very much for
your time. This is really enlightening and
a great demo of amazon's tools and your background from astrophysicists. Machine learning scientist
is a very unique path. If you had one last piece of advice
other than to never stop learning, what in your background has like best
prepared you for your work today? >> So always do a deep dive on
something that you don't know, right? So that goes back to never stop learning, not just a very broad things,
do a deep dive to hands on. Don't give up your scale so that the hands on the scale is something
that keep up throughout the year. When there's something
that you don't know and then always ask around to see if there
are any people that he can mentor you, you will be surprised at how
willing people around you want to help you to be successful, right. So as around reach out too deep dives into
things you don't know, don't give up. I don't believe in geniuses
that they might be geniuses, but through hard work you can
always learn things you don't know. So that's something that really
helped me throughout my career. >> Well, thank you so much for your time. This was great. >> All right, thank you.