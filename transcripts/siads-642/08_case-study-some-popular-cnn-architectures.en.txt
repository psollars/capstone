Hi everyone, this is
Paramveer Dhillon. In this video we will study some of the popular
architectures of Convolutional Neural Networks and how they exploit some of the design principles that we have learned in
the previous videos. Perhaps one of the first successful
application of CNNs to classification task was done way back in 1998 by Yamli Kuhn, and colleagues at NYU. They proposed a CNN
architecture called LeNet for the task of
document recognition. In particular the task they considered was online
handwriting recognition. The architecture of the
CNN as shown on the slide contains two convolutional layers separated by two pooling layers, and then finally two
fully connected layers to perform the eventual
classification. The convolutional
filters that were used were of size 5 by 5, and they used a stride of size 1, whereas the pooling layers were 2 by 2 with a stride of 2. Next let's look at AlexNet, which we also discussed
briefly in the last video. AlexNet was the first successful
application of CNNs to the ImageNet dataset
and is considered as a major breakthrough in application of deep learning
to computer vision. It achieved an error rate of 15.3 percent on ImageNet in 2012, which was state of
the art at that time. The architecture of
AlexNet contained five convolutional
layers with max pooling, and then finally three fully
connected layers before making 1,000 class prediction problem via the softmax function. Let's zoom into the architecture
of AlexNet further. It turns out that AlexNet
contained eight layers and was the first CNN-based winner
of the ImageNet challenge. It was also the first one to use the fast and efficient ReLU, or Rectified Linear Unit
activation functions, and used extensive
data augmentation. The original AlexNet uses 11 by 11 convolutional filters
with a stride of size 4. Several follow-up CNN
architectures improved on AlexNet by using even smaller
convolutional filters and even deeper networks. A not worthy successor
to AlexNet was the architecture called VGGNet
from Oxford University. VGG stands for the name of the Research group,
Visual Geometry Group. It cut the error rate of
AlexNet on ImageNet into half as it got an error
rate of just 7.3 percent. VGGNet was twice as deep
as the AlexNet as it had 16 layers and used smaller convolutional
filters of size 3 by 3. In total VGGNet had a staggering 138 million
model parameters. The main rationale for using smaller filters and
more layers is that the stack of smaller filters has the same receptive field
as some larger ones, but more layers allow us to incorporate more
non-linearities, and potentially fewer
model parameters overall. Another notable advance
in the use of CNNs for image classification is due to an architecture called ResNet
or a Residual Network. As we just saw for VGGNet, deeper neural networks
allow us to learn more flexible non-linear
representations and therefore achieve
significant reduction in error. But as the networks get deeper they become
harder to train. For example the ResNet paper showed that a 56 layer
neural network has both higher training as well as a higher testing error compared
to a 20 layer network. One of the reasons behind this apparently
surprising finding is that there's attenuation of the valuable predictive
signal as it passes through so many layers and the
associated activation functions. The solution to this problem and the key idea behind
a ResNet is to fit the residual
value of the signal as opposed to the
actual desired mapping. Doing so allows us to train a staggering 152 layer
residual network which obtains an error rate of just
3.57 percent on ImageNet, which is better than human
level accuracy on this task. Let's see how a residual layer is actually operationalized
by the ResNets. The figure on the left on the slide shows a
standard layer in a CNN where they tried to fit the actual desired
mapping H of x. A residual layer in contrast and shown
in the middle figure on the slide fits the
residual H of x minus x, and architecturally it
is obtained by using a residual or a
short-circuit connection as shown in the figure. It is common to use
residual connections after every couple of
convolutional layers, as can be seen on the
architectural diagram of ResNet on the far
right on the slide. Now that we have seen
several state of the art architectures for CNNs that are in use these days, let's see what's the current
state of CNN research. In recent years the trend
has been to go deeper as extra layers of non-linearities give significant accuracy boosts, and the trend has
also been to use smaller filters as they can have similar receptive fields as
some of the larger filters, but at the same time are parsimonious in terms
of model parameters. Further, it is also becoming
increasingly common to residual connections in state of the art CNN architectures
these days. As you can see from the
plot shown on the slide, which shows the accuracies of various models on the
ImageNet dataset. It is a very fast-moving area of research with continuous
innovation and most modern-day
architectures achieve accuracies better than humans
on the ImageNet dataset.