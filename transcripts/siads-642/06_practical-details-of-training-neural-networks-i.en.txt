Hi everyone. This
is Paramveer Dhillon. In this video, we will study
about how we can bring some of the ideas that we studied
in previous videos to life. In particular, we
will study about some practical details of
training neural networks. As we have seen, deep neural networks
typically have seven hidden layers of non-linearities stucked
on top of each other. The model training in order to find the model
parameters that minimize the loss function is performed by an algorithm called
back-propagation. However, it turns out that
we might not be able to find the model parameters
corresponding to the unique minimum value of
the loss function achievable, but is also known
as a global minima. This is so because the terrain of the loss
function might have several local minima and our optimization procedure could get stuck in one of them. Only a convex function, which by definition is
shaped like a bowl, has a unique global minima
and no local minima. Adding several hidden
layers to neural networks, makes the optimization of the corresponding loss
function a non-convex problem, with severe local minima, and as a result,
tough to optimize. Hence, the reserves of neural
network training are often very sensitive to
the initialization of the weight parameters, as there is a risk of getting
stuck in local minima. Luckily, the deep learning
community has developed a set of robust
practical techniques over the last decade or so, that let us escape
bad local minima. Recall that the neural
network parameters are optimized by
gradient descent, which involves
computing the gradient of the loss function
with respect to the model parameters and taking a small step of size
Theta in that direction. This tunable parameter
that controls the size of the jump is
known as the learning rate. Also, as shown in the
figure on the slide, the optimization of
the loss function of deep neural networks is
a non-convex problem, and as a result, has more than one local minima. It is important to choose
the learning rate for neural network learning
with great care, as the results are
highly sensitive to it. We neither want the learning
rate too small or too large. If the learning
rate is too small, our learning procedure will
take very long to converge, and still may converge
to a bad local minima. On the other hand, too big a learning rate
is also not desirable, as we may overshoot the optimal solution or we
might jump around too much, hence leading to
unstable solutions. Let's see what are some of the potential strategies for choosing a good learning rate. The first strategy is more
of a brute-force solution. We can try a bunch of different learning rates and see what gives the lowest loss. A second more principled strategy is to choose adaptive
learning rates. That is, learning rates that adapt to the underlying
terrain of the loss function. We automatically
increase or decrease the learning rate based on how large the
current gradient is, how fast it is changing, and what's the magnitude
of the weights. Adaptive learning rates
are the norm these days, and there're several
different algorithms which can help us be adaptive in selecting
learning rates, example, Adam, Adagrad, RMSProp, etc, to name a few. A second practical issue with gradient descent
based optimization is that it can be awfully slow for really large datasets
since we have to iterate over all the observations in the dataset in order to
compute the gradient. It turns out that we can overcome this problem by using stochastic gradient
descent, or SGD. Stochastic gradient
descent computes the loss function for one randomly chosen observation in each iteration of
gradient descent. SGD provides the solution to the computational problem as it's fast and easy to compute. But since it chooses one observation in each
iteration randomly, the results could be
noisy and unstable. It turns out that
it's possible to attain best of both the worlds. That is, compute
the gradients fast, but also have a more
stable solution by using the technique
called mini-batching. Mini-batching essentially
involves creating small sized batches
of data of size n called mini-batches and computing the loss function
and gradients for those. Note that, the stochastic
gradient descent, SGD, is an extreme case
of mini-batching, with n is equal to one. So it's easy to see how
mini-batching provides more stable solutions as it averages the estimates
over the mini-batch. Further, different
mini-batches can be processed in parallel now, which opens the doors for significant speed-ups
using GPU hardware. Weight initialization is also an important design consideration
for neural networks, and it can significantly
impact their performance. Bad weight initializations
can lead to the optimization getting
stuck in local minima, hence giving us
sub-optimal solutions. It is a common practice to
initialize weight parameters randomly while optimizing standard machine learning models. This could though be
highly sub-optimal for deep neural networks due to the highly non-convex nature
of the loss function. A common strategy for
weight initialization in neural networks is via what is known as the Xavier
initialization, which is named after the authors who proposed
this technique. It involves dividing the randomly initialized
weights with the square root of the sum of the fan-in and fan-out of the layer. Fan-in and fan-out of a given neural network
layer is the number of incoming and the number of outgoing network
connections respectively. Another commonly used technique to stabilize the training of neural networks is what's
known as batch normalization. Batch normalization essentially
re-centers and re-scales the inputs to each layer of the neural network to mitigate
internal covariate shift. Internal covariate shift can interfere with the speed
of learning the model. Batch normalization is performed per channel of the input. So for a three-dimensional
red, green, blue image, batch normalization
will be performed to re-center and re-scale
each dimension of the image separately. I highly encourage
you all to read the article linked
to on this slide. It provides a really
nice overview of the various gradient
descent algorithms, including the adaptive learning
ones that we just saw.