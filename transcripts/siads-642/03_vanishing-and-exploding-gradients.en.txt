Hi everyone. This is
Paramveer Dhillon. In this video, we will
continue our discussion about the Recurrent
Neural Networks or RNNs, which were introduced
in the previous video. In particular, in this video, we will see some of the
difficulties that arise while craning RNNs and how we deal
with such difficulties. Just to recap, in the last video, we saw that we optimize
weight parameters of recurrent neural networks via backpropagation through time. The forward pass over the data involves computing
the loss function, and in the backward pass, we compute the gradient of the loss function and
propagate the information all the way back from the entire sequence to
update the model parameters. It turns out that
backpropagation for RNNs does perform lots of
repeated multiplications. For example, in order to
compute the gradient of the loss function with respect
to the hidden state h_0, we use the chain rule
of differentiation, which involves computing
the gradient of loss function with respect
to hidden state at time 0.4, then we multiply it by
successive derivatives of hidden states all the way to the beginning
of the sequence. These derivatives are
represented by the terms partial derivative of
h_4 with respect to h_3, then h_3 with respect to h_2, and then h_2 with respect to h_1, and then finally,
h_1 with respect to h_0 as we can
see on the slide. Now, let's see how these successive
multiplications look like for a Recurrent
Neural Networks. As we know, RNNs update
the hidden state h_t, as a weighted non-linear
combination of the previous hidden state
h_t minus 1 and the current input x_ t. When
we compute the derivative of the hidden state at time t with respect to hidden state
at time t minus 1, we can see that it involves
a multiplication with the model parameter Theta h_h. Now, for a really long sequence, we need several successive
multiplications with Theta h_h in the backward pass of the backpropagation algorithm. If Theta h_h is too small
or too big to begin with, we can get to a situation
where the gradient can be vanishingly
small or too large. With little math, it can be shown that if the largest eigenvalue of the Theta h_h model parameter
matrix is less than 1, then the gradients will
shrink exponentially, leading to vanishing gradients. Similarly, if the
largest eigenvalue of Theta h_h is larger than 1, then the gradients will explode. Both of these situations
that is vanishing or exploding gradients are a problem for stable training of RNNs. A natural question arises, why are vanishing or exploding
gradients a problem? First, let us look at
vanishing gradients. The vanishing gradient problem entails that the
gradient signal from faraway gets lost
as it becomes much smaller in magnitude
compared to nearby signal. This is problematic as that
means that model weights are only updated based on nearby information
in the sequence. Hence, RNNs are unable to model long-range
dependencies in language, which are essential to model for many language modeling tasks. Now, let's see why exploding
gradients can be a problem. If the gradients are too big, then the updates in the stochastic gradient
descent can become too big, which can lead to instability
in model training. System parameter estimates
are jumping around a lot due to large updates
to the model parameters. There is a risk of reaching a bad local optima due to a
bad parameter configuration. Let's look at solutions to the vanishing and exploding
gradient problems. First, let's look at a potential solution to the
exploding gradient problem. The exploding gradient
problem can be solved effectively by a technique
known as gradient clipping. Basically, it means
that if the gradient gets larger than some threshold, then scale it down before
the gradient update. The slide shows the simple and straightforward
algorithmic solution to clipping the large gradients. At a high level, gradient clipping entails taking a step in the
direction of gradient, but a smaller clipped one. The problem of vanishing gradients has a couple of
solutions and they are more nuanced than the
gradient clipping solution for solving exploding gradients. The first and rather
straightforward solution to vanishing
gradients is by using the ReLU activation function as the non-linear activation
function in the RNN. Since ReLU keeps
all the information in the input that
is greater than 0, the gradients don't get
progressively smaller. The second solution to vanishing gradient calls for a
new modeling approach. In fact, a new set of modeling
approaches that control what information passes through at each step in the sequence. These models are collectively called gated recurrent cells, since they gate or control
the information flow. Some examples of such models
are Gated Recurrent Units, GRUs, and LSTMs, Long
Short-Term Memory units. We will study these models next.