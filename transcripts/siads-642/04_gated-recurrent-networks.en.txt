Hi everyone, this is Paramveer Dhillon. In this video, we will study gated
recurrent networks in general, and we would dive deep into the details
of two specific types of gated recurrent networks,
in particular, LSTMs and GRUs. As we will see, gated recurrent
networks help us alleviate some of the difficulties in training RNN
that we discussed in the previous video. Let's recap what we
learned in the last video. The main bottleneck with a vanilla RNNs
that leads to a vanishing gradient problem is the inability to
filter the gradient signal as it passes through several time steps. Gated recurrent cells or gated recurrent
neural networks that we will study next, explicitly control the information that
flows between different time steps. Which allows us to effectively
model the all important long range dependencies in language. Next, let's look at an LSTM or
a long short term memory network, which are a type of RNN
proposed by Hochreiter and Schmidhuber in 1997, to deal with
the vanishing gradient problem. They are perhaps the oldest and the most
commonly used gated recurrent network. The ability to model long
range dependencies, though, comes at an additional
cost of model complexity. LSTMs, it turns out, have way more
parameters than a vanilla RNN. As you can see from
the diagrams on the slide, LSTMs are way more complex
than standard RNNs. LSTMs process the hidden
state in three steps. This hidden state is similar to the hidden
state that is maintained by RNNs and that we saw earlier. So in the three steps in which
the LSTMs process the hidden state are first the LSTMs forget
some part of the current state. Next, they update the hidden state and finally, they output
the final hidden state. Next, let's look at some of
the architectural details of an LSTM in detail. At each time step,
the LSTM maintains two states, a hidden state, ht and a cell state, ct. The hidden state serves
the same purpose as in the case of a vanilla RNN and
is updated in a similar fashion. The cell state, ct, on the other
hand stores long term information. The LSTM can perform three
operations on this cell state. Erase, read or
write information from the cell. At a high level, this flexibility is what gives LSTMs the
ability to model long range dependencies. LSTMs have three gates corresponding
to these three operations, which control what information is erased,
read or written. At each time step, each of the three
gates can be in one of the three states, open, closed or somewhere in between. The gates are dynamic, so their value is computed based on
the context of the current input. The three gates that the LSTMs
have are called the forget gate, the input gate and the output gate. Forget gate controls what information
is forgotten from the last step. It is computed as a weighted nonlinear
combination of the previous hidden state, ht minus 1 and the current input,
xt, and a bias toward bf. Note that Wf, Uf and bf are the learnable
parameters of the forget gate. Throughout this course we have denoted
learnable parameters using theta. The reason for this departure from the convention is to
be consistent with the LSTM literature. Hence, we represent the model
parameters by W, U and b matrices for the case of LSTMs. The second gate of the LSTM
is the input gate. It controls what part of the new cell
contents are written to the cell. Just like the forget cell, it is also
computed as a weighted non linear combination of the previous hidden state,
ht minus 1 and the current input, xt,
and a bias term, but with a different set of parameters Wi,
Ui and bi. Similarly, the output gate,
which is the third gate, controls what part of cell contents
are output to the hidden state. The learnable parameters of
this gate are Wo, Uo, and bo. Now that we have seen how the different
gates of LSTMs are parameterized, let's see how the hidden and
cell states are updated. The new cell content to be written
to the cell is also computed as a weighted nonlinear combination of
the previous hidden state, ht minus 1 and the current input, xt, and a bias term b. But with a different set of parameters Wc,
Uc and bc. The updated cell state is computed by using the forget gate to forget some
part of the previous cell state. And by writing some new cell state, which is computed using the input gate and
the updated cell state. The hidden state is obtained by outputting
some content from the cell state by element wise multiplication of
the output gate with the cell content. The diagram on this slide summarizes
the details of the LSTM model. To summarize, first we compute the forget, input, output gates, and
also compute new cell content. Then we forget some cell content
from the previous state and write some new cell content. And finally, we output from cell
content to the hidden state. Each of the operations
that we just described has its own set of
learnable model parameters. This is what makes LSTM a significantly
more complex model than a vanilla RNN. But as a result, also LSTMs also have
more representational power than RNNs. This ability to filter the cell state and
hidden state from one time step to next time step is what allows LSTM to
model long range dependencies. After we have a close look at LSTMs, it seems that there is a bit
of repetition in LSTM. For example, the input,
forget, output gates and the new cell state computations have
the exact same parametric forms but just different sets of parameters. So it might be possible to make
the model more parsimonious and get rid of some of these
redundant parameters. There have been various
parsimonious alternatives to LSTMs that have been proposed,
but perhaps GRUs or gated recurrent units are the ones
that have gotten the most attention. Perhaps due to their simplicity. GRUs have far fewer parameters than LSTMs, are faster to train and
give comparable accuracies. The key difference between a GRU and
a LSTM is that unlike LSTM, a GRU does not maintain a separate
cell state and the hidden state. And further, it only has two gates
to control the information flow, a reset gate and an update gate. Since much motivation and several details
of the GRUs are similar to LSTMs, we won't discuss GRUs in greater detail. Those of you who are interested
in learning more about GRUs can read the research paper that
proposed GRU and is linked at the slide. Now that we have studied two of
the popular gated recurrent networks, it is natural to ask as to how do LSTMs or its variants such as GRUs solve
the vanishing gradient problem. The truth is that LSTMs do not guarantee that there is no
vanishing gradient problem. They just provide
a convenient way to model long term dependencies we
are getting mechanisms. In an LSTM, if the forget gate
is said to remember everything, then the information in the cell
is preserved indefinitely. On the contrary, there is no
corresponding mechanism in a vanilla RNN to preserve information
in the hidden state. You might be wondering if vanishing or exploding gradient problem
is just unique to RNNs. As we saw, the sole cause of these
problems is the repeated multiplications in the backward pass of
the backpropagation algorithm. The answer to that is no. These problems are not unique to RNNs. They can manifest in other
neural network architectures such as feed forward neural networks or
convolutional neural networks as well. Especially when those
architectures are deep. Since then, there will be lots
of repeated multiplications. And the signal will attenuate
over longer distances. In fact, another neural network
architecture that we saw last week, when we were studying
convolution neural networks. Residual networks are ResNets,
which add residual connections to fit the model residual rather
than fitting the desired mapping. Another way to preserve information over
longer distances in deeper architectures.