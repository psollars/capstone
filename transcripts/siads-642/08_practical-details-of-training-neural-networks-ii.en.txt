Hi everyone, this is Paramveer Dhillon. Next we will continue our
discussion from the last video and study some more practical details
of training neural networks. In order to ensure that our trained
neural network generalizes well, to unseen test data, we need to control the
complexity of the model that we learned. If the model is too complex or too simple, it could lead to overfitting and
underfitting, respectively. Thereby reducing the performance of
the model on the held out test data. Just as with standard
machine learning methods, we control the complexity of the neural
network models via regularization. As we all know, regularization is very
important to model generalizability. There are three main regularization
approaches that are specifically tuned to neural networks: dropout,
early stopping and data augmentation. Next, we will study these approaches. First, let's study dropout. 
Dropout in its simplest form entails dropping a small fraction of the model
activations by setting them to zero. As we can see in the figures shown
on the slide, the neural network on the right is essentially the same
neural network as the one on left, but with two of its activations,
z 2 and z 3 dropped. The rationale behind dropping
these two activations is to make the neural
network more robust so that it doesn't solely rely on any one
connection while making predictions. Drop out can also be seen as a way
of combining different models by performing model averaging, akin to
ensemble methods in machine learning. Early stopping is also
a type of regularization that is commonly used to prevent
overfitting in neural networks. It has been shown to provide
significant improvement in the generalization
capabilities of the train model. Early stopping just entails stopping
the training procedure a little early as the name suggests, so
as to not allow the model to overfit and hence have a higher test error
compared to the training error. As you can see in the plot on the slide,
during the training procedure, there is initially an underfitting regime
where our trained model is too simple and has high test error. And much later during
the training procedure, we reach an overfitting regime where
again, the test error is high. The key behind early stopping is to stop the model training
between these two regimes. Finally, a regularization technique,
which has become very popular in neural network training
these days is data augmentation. For certain kinds of data
domains such as image data, augmentation makes a lot of sense. Data augmentation entails
increasing the robustness of the learned model by increasing
the diversity of the training data. So essentially, we augment the training
data with fake data that is generated by transforming the input data while
preserving its output class label. Some of the commonly used transformations
for data augmentation are rotation, cropping, mirroring,
jittering of the input image. As we can see in the example below,
we have augmented our training data by mirroring and rotating cat images
while preserving the cat label. It is important to note that all
the transformations that we performed in particular mirroring and
rotating the images, they preserved the actual final classification label,
which was in this case the cat. So any transformation, which would change the output label will
not be a good data augmentation strategy. Data augmentation has mostly been used for
computer vision and imaging applications. But it is conceivable to employ it for other domains also,
example text or speech.