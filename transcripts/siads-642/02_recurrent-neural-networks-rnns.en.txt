Hi everyone. This is
Paramveer Dhillon. In this video, we will build on the language modeling task that was described in
the previous video. We will introduce Recurrent
Neural Networks, or RNNs, as a way of modeling such kind of sequential data as language. To recap, we saw that RNNs or
Recurrent Neural Networks, are a popular deep-learning
model for modeling sequence data that we come across in domains such as
text or speech. As we saw in the last video, RNNs can model
long-range dependencies that can handle variable
length sequences, and they can preserve the
ordering information in text. Furthermore, we saw that RNNs can be used in several
configurations, such as to learn
many to one mapping, as is the case with
sentiment classification, or a one-to-many mapping
as an image captioning, or maybe a many-to-many mapping as in machine translation. Next, let's see how our
RNN is different than the other neural
network architectures that we have seen till now, that is, the feed-forward neural networks or the multilayer perceptrons and the convolutional
neural networks, CNNs. RNNs as we know, process data that is a sequence. RNN does that by
maintaining a hidden state, h_t, that is updated as
the sequence is processed. The update to the hidden state is performed by applying a
simple recurrence relation. The updated hidden state
depends non-linearly on the previous hidden state and current input vector x_t, and it has learnable
parameters Theta. Let's zoom in on how the RNN
updates its hidden state. In the simplest form, the updated state is a weighted linear combination of the previous hidden state, h_t minus 1, and the
current input x_t. The resulting signal is
passed through a tan h, or hyperbolic tangent
nonlinear activation function. Finally, the output state
at a given point in the sequence can be predicted from the hidden state
at that time point, all the thetas,
that is, theta hh, theta xh, and theta yh, are modeled parameters that
are learned from data. On this slide we can see
the computational graph of RNN unrolled over
several time steps. As we can see, there are several advantages of using RNNs. First, RNNs use the
same weight parameters at each time step, so we can process sequences
of varying length. It doesn't matter how
long the sequence is, the parameter that maps the input to the hidden state
is always theta xh, and the parameter that
maps the hidden state from one time period to
the next time period is always the same, theta hh. Similarly, theta yh parameters map the hidden state
to the output. What this means is that the
model size or the number of parameters do not increase with the length of
the input sequence. Finally, the computation
at a given time step t can use information
from several steps back, which is helpful for modeling
long-range dependencies. Next, let's look at some
shortcomings of RNNs. First, though the
recurrent computation is useful for modeling
sequence data, it is also slow and
difficult to parallelize. Second, it is hard to explicitly access information
from several steps back. RNNs only indirectly model
long-range dependencies. For example, the
hidden stated time t is dependent on state
at time t minus 1, which in turn is
dependent on state at time t minus 2 and so on. There is no direct way of
modeling dependencies, let's say n steps back. We will soon see some neural network
architectures that would help us explicitly model
long-range dependencies. Now that we have seen the architectural
details of vanilla RNNs, let's see how RNNs are trained. It turns out that the recipe for training RNNs is the same as we saw in the first week of this course for
multilayer perceptron. We minimize the loss
function, for example, a cross-entropy loss function over the entire
training data set. The operation is done by
gradient descent with mini-batching and
adaptive learning rates. Since the data has
sequential nature, the backpropagation
algorithm for weight optimization in this case is called backpropagation
through time. The name backpropagation
through time, or BPTT for short, might sound like a mouthful, but it is essentially the backpropagation algorithm
that we have seen earlier, but we just sum the gradient
over the entire sequence. So in the forward
pass over the data, we compute the loss function, and then in the backward pass, we compute the gradient of the loss function and
propagate the information all the way back over the entire sequence to
update the model parameters. It turns out that
backpropagation for RNNs does perform lots of
repeated multiplications.