Hi everyone, this is Paramveer Dhillon,
in this video we will use the convolutional operator that we just
learned about in the previous video, and extend the idea to build
convolutional neural networks. That is CNN, just to recap, here's the example of convolution that
we've worked out in the previous video. The input image was represented
by a binary matrix and the convolutional filter was of size 3
by3 with the weight specified as shown. The output future map was also of size
3 by 3, and was gotten by sliding the convolutional filter over the input
image starting in the top left corner. The key idea behind
convolutional neural networks or CNN's is that the weight of
the convolutional filter are unknown and they are the model parameters
that are learned from the data. Now we are in a position to define
a convolutional layer in neural networks, similar to the fully connected
layer that we saw for multi layer perceptron in week 1. For each neuron in the hidden layer
we take a patch from the input image with pixel values X. And we compute a weighted sum by
multiplying the pixel values with the corresponding entries of
the convolutional filter and add them up. We further add up blas
term to the objective. Finally, we apply a nonlinear activation
function to the weighted sum. So, at a high level, all the details of
the computation of a convolutional layer are the same as for a fully connected
layer in a multi layer perceptron. With the only key difference that
the convolutional layer share their parameters spetially. Let's visualize the operations
of convolutional layer. Our input image here is a 3 dimensional
image with separate channels for red, green and blue colors. And our conventional filter
is also 3 dimensional and is obtained by stacking the same set of
model parameters across the 3 dimensions. So our input image is of size 32 x 32 x 3. And our convolutional
filter is of size 5 x 5 x3. Next, as we have seen earlier, we slide the filter over the entire
image to computer future map. Which is then transformed by a nonlinear
activation function to generate an activation map, also known as
a feature map or an output map. In the example shown, the resulting
activation map is of size 28 x 28. It is common to apply
several convolutional filters in a typical convolutional neural network. In the end, we just stacked together
the activation maps resulting from the application of the different
convolutional filters, to get an image of activation maps. So, for our example,
we used five different conversational filters of size 5x5 x3. So finally we get an image
of size 28x28x5 and this is the image of activation
maps as shown on the slide. Next we assemble a CNN from
the various convolutional layers. Essentially a CNN is a sequence of
convolutional layers interspersed with nonlinear activation functions,
which in this case is the ReLU or the rectified linear unit function. In our example, we first apply
5x5x3 filters to the input image, followed by 10, 5x5x5 filters. It is common to zero-pad the input
image by adding additional rows and columns of 0 pixels to the image
before applying convolutional filters. Zero-padding is performed essentially
to preserve the size of the input image spatially. So that we get an output activation map
of the same size as the input image. It is common to zero-pad with
f-1 divided by 2 pixels. Where f is the size of
the convolutional filter. Hence, for a 3x3 combination filter
we will have a zero padding of 1 and similarly a zero-padding of 2,
for 5x5 convolutional filters. Another important design parameter of
a CNN is the stride of the convolutional filter. That is, how many pixels does it move
in one step from left to right and from top to bottom. The slide shows a 2x2
filter with stride 1. Since it moves by 1 pixel
between consecutive applications applied to
an input image of size 6 by 6. On the slide, we see the exact same
filter applied to the same image, but with a stride size of 2. The trend over the last several years
has been to make CNN architectures really deep,
containing many convolutional layers. While this makes our model highly rich and
expressive in representation, it also blows up the computational requirements
required for the model to run. This is so because the activation maps in
deep CNN's can get really large spatially. This also leads to a significant increase
in the number of model parameters. A popular solution to this problem
is to down sample the activation maps while preserving
the spatial structure. This operation is called pooling and is
implemented where we have pooling layer. The slide shows are 224x224x64 image down sampled to
112x112x 64 image by pooling. There are two common ways of pooling
the activation maps of CNN's, max pooling and average pooling. Out of these,
max pooling is the one most commonly used. An average pooling has fallen
out of favor recently. Recall that each convolutional filter
provides us an activation map. The polling operation is applied
to each activation map separately. For example, if we have an activation
map as shown on the slide and we perform max pooling on it, by using
a 2x2 filter with stride size of 2. Then essentially our output will
be the max activation value out of each of the 4,
2x2 patches shown in the slide, that is, out of the patch on top left,
containing activations 1, 2, 4, 6. The pooling filter picks 6 since
it's the maximum of those values. Similarly from the patch on the top right,
it picks 8 and so on. As you can see, pooling has reduced
the size of our activation map by 75%, while preserving the spatial
structure of the activations. Now that we have learned
the entire recipe for building convolutional neural networks for
imaging data. CNN's are essentially constituted of
convolutional layers with non linearity, ease and an occasional pooling. If our final goal is
classification example, whether the image contains a car or
a truck or a plane. Then, we flatten the final activation map
of our CNN from a matrix to a long vector, which is then input to a fully connected
MLP or a multi layer perceptron layer.