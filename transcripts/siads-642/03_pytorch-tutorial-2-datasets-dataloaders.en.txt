Hi everyone. This is the second tutorial in our
PyTorch tutorial series. This will be about data sets
and data loaders in PyTorch. Again, this notebook
is adapted from the official PyTorch tutorial on data sets and data loaders, you can follow this
link here and look at their very comprehensive
tutorials there. Code for processing data samples can get very messy and hard to maintain as you go
on with your project. Ideally, we want our
code for processing data to be separated from other code like
model training, and maybe inference
code, testing code. For better readability
and modularity. PyTorch provide two utilities that you can use
to deal with data. One is data loader and the
other is called data set. They are both part of the
torch UQ data package. data set stores samples and
their corresponding labels. Data loader actually
is a Python iterabl, that build on top of
the data set that allows us to easily access
those samples in batches. Actually there are
many other libraries that are closely
associated with PyTorch. They provide a number of
pre-loaded data set such as, the very famous [inaudible]
or fashion-mnist. Those data set are all
sub classes of this torch.utils.data.data set
class, and of course, they have those functions
implemented for you for those particular
data set.They can be just use red out of the box and you can find them
following those links. Provided you with a wide
range of data sets, image data sets, text data sets, and some
audio data sets there. Well, if you're interested, you can explore more there. Actually in the
official tutorial, they use the
fashion-mnist data set, but I think in this tutorial, we'll actually learn how
to lowly your own data, that's actually not part of the Pytorch official data set. Because I feel this is
actually a more common workflow that you actually encounter a lot in your project. Because most of the
time we're basically in working with our own
data sets anyway. It's important to
know how we can load our own data for training. Actually, the fashion-mnist
data is an image data set, which we will talk more
actually in week two. But here, right now, we're going to use a non-image
data set as an example, is actually very classic
California Housing data set, as our running example
for this tutorial. Later on in the series, we will learn how to deal with image data set or
texts data set. But for now, we'll
just stick with a very nice supervised
learning data set. Without further ado, let's see how we can work
with this data set. This data set is actually, while it's not part of
Pytorch is part of sklearn, so I'm going to import this
data set from sklearn, in this case, for visualization. I actually asked sklearn to return them as
Data frames first, so that we can look into the data set and see
what this data is about is part of the housing
price in California. You can actually read
and more documentation following this link here. But here is basically saying, it shows you a attribute about the house or in
numerical values. That's the x variable, that's the features
for the target. I think it's just the
housing price then. Basically is a supervised
learning data set, and your job is to
Make a prediction of the house price using those
eight features of a house. You can read more in
the documentation. But that's not very important
for this specific tutorial. Because our interest now is to actually
load this data set into PyTorch so that we can
use that for model training. That's the data exploration. Because now we're
in deep learning, so let's try to convert
everything into PyTorch Tensors. We're going to import
PyTorch, of course. For a nice print, I'm just going to disable
the scientific mode here and set a white
line width here. It's awesome, print options
here. It doesn't matter. Now, I'm going to convert
the data frame into Torch Tensors. How
can we do that? Well, the data frame has
a method called to NumPy, that allows you to convert a data frame
into a NumPy array. Then, as we learned
from last tutorial, there is form NumPy
function that allows us to cover a NumPy array in
the interior towards tensor. We can just use those. Now, you can see that also double-check
the size of x and y. It has a size of this much, and the number of rows in x matches the number
of elements in Y. It looks like we didn't have any loss of data here,
during the conversion. A little bit node
on this conversion, this don't float here. In NumPy, the
default data type is actually numpy float 64, also known as the
double datatype. For real numbers,
we usually have two data types.
Float and double. Double is basically the
64 bit float numbers, and that's the
default for NumPy. But in PyTorch, the default is usually float32. This is known as the float type. Computation is performed
in a double-precision, are often actually much slower. Even though of course
there are more accurate, but it actually, to achieve that accuracy, we actually need more memory
to store the numbers. Also, the computation
itself will also be much slower compared
to single precision, even we run the code on the GPU. Moreover, the additional
precision offered by double, usually it doesn't matter
because compute accuracy, for example, on the data set, we usually only care about maybe animals like five digits, after the decimal point. Even though doubled hyperwise, maybe up to 16, on data very precisely, but we don't actually
care that much, that's why PyTorch
naturally chooses float32, the flow type as its
default datatype. That's why when we want to convert a NumPy array
to PyTorch Tensor, often you have to explicitly do this conversion
into a float tensors so that all your
computations can be done in float at a high-speed
compared to double. Is just a little note
about this conversion. Better pay attention
to this later on difference here
between double and float. Now, that we have our
data in a tensor format, now we can create a data
set out of data tensor. Actually our data is
already a tensor, PyTorch provide this
special data set called tensor data set, that allows you to create a
data set out of the tensors. It's actually very easy to use, you're basically just
pass in your tensors, in this case X on Y to
the constructor here, and you will create
a data set for you. It looks easy, but you
may ask you a question, we have been talking
about data set. Why do we actually
need a dataset? Why can't I just
work with tensors? They're already tensors,
I can just use them in my computation in the models. Well, because the dataset actually allows us to index into tensors and retrieve
a feature label pair , at the same time. What I mean by that
is for example, this cal housing
is now a dataset, it shows like TensorDataset,
with those elements. You can just index into this dataset just like
you index into a list. As you can see, it
returns a tuple where the first element is actually the first row from your dataset, from the x variable, and the second element is the
y label, the housing price. You can naturally
do more than that. You can actually do a
slicing over this dataset. In this case, if you
have colon five, so that means you're
going to grab the first five rows
from your x-variable, from your features dataset, and then also the
corresponding y labels from your dataset as well. It's a list, but if you index or
slice into the list, you will get the corresponding
portion of your data, which is very nice
because it saves you the trouble of indexing
into the tensor yourself, which is very error-prone. But here in this case, the dataset object will
handle that all for you. Another feature of a dataset is that you can
check the length of the dataset using
the len function, just like I'll check
the length of a list. This tells you how many examples that we have in our dataset. We can also split our
dataset into training and testing as in the standard
practice using PyTorch's, this random split function. Of course you may also
split a data beforehand, before you create this dataset using whatever your
favorite package is, and then create a separate
dataset for each split. But when in Rome, when we're already
using PyTorch, then we should use as much
as PyTorch as possible. Here let's split our data. I'm going to do a 70, 30 split up training
and testing. The training fraction is 0.7. Here, because of the PyTorch
version we are using, we do have to
calculate the size of the training set and the
size of the test set. But I believe in the most
recent and original PyTorch, maybe you can just pass
in the training fraction. You don't have to
actually compute the size of the
training and testing. But here we just do
a computation here. The size of the
training set is 0.7 times the total length
of the dataset, and taking a floor here because the size has
to be an integer. The size of the test
is basically whatever left after we separate
all the training set. The random split function
accept a dataset, also what tuple that indicates
the size of each portion, size of each split, basically. We can run this code, and we'll now get actually
two datasets now. We split the dataset
into two sub datasets, and we can again check
the length of those two. One is training, one is
testing, and of course, you can check the
lens does add up to the total length of
our original dataset. Now we have seen what a dataset is for and how we can split a dataset into
training and testing. Now it's time for us to combine dataset
with a data loader. Well, data set can retrieve your features and labels
one sample at a time. But when training a model, we typically want to pass samples in so-called
mini-batches. You may also want to reshuffle the data at the end of the every [inaudible] to make sure your
data comes in random order. This can reduce
overfitting of your model. Of course, there's something happening behind the scene
which is Python also uses the multiprocessing to
speed up your data retrieval. All that stuff is hidden
behind this data loader. This data loader abstracts away all this
complexity for you, and is a great tool, is complimentary to
the data set objects. How can we use this data loader? It's actually fairly easy. We import the data loader from the package and use
this constructor here. This constructor accepts a data set at the first argument. You have to tell
it the batch size, the size of your mini-batches. This data loader is
going to retrieve you, in this case, 64
examples at a time. One invocation of this data
loader will return to you 64 examples from
your whatever data set you pass into
this data loader. There is option that you can
specify which is for telling the data loader
whether to reshuffle the data after all the
data has been loaded. After you go through
all the mini-batches, then data loader goes back to the beginning
and it reshuffles data. We usually reshuffle
the data for training data set, but usually, there is little use of
shuffling the test is that because we're going to
test on all data anyway, so it doesn't matter in what
order we test on those data. We usually don't
shuffle the test set, but we do shuffle
the training set. Here, I have pass in a
training data set for train data loader and passing the test set for
the test data loader. Usually, you would have separate data loaders for
separate data set you have. Now we have the data loader, how we can use them. Well, there are actually two
ways of using a data loader. One way is that you can use these Python
built-in functions, next and either in combination to retrieve
data from a data loader. In this case, the error
function will create an iterator on top of
this trend data loader, and the next function
will actually retrieve the next item from this iterator created on top
of this trend data loader. That item will actually
come in a tuple of training features
and train labels. That's something we
have seen earlier. Let's just print out
this stemming here. You can see that we
have got two tensors, train features and train labels, and train features
at the size of 64,8 because the
batch size is 64. Every time the train
data loader will return us 64 examples and eight
because we have eight features, that's the feature dimension. The size of the train
label tensor is 64 because we have 64 examples and one label
for each example 64. The next plus either, so this way of retrieving
data from data loader, it's very useful
when you want to just examine a batch of data. For example, after you have implemented your data
loading functions, you may want to
check whether you have the correct
implementation or not. This will be very handy because it just gives you
a batch of data. For example, you can examine
the size of the data. If the data is maybe
an image data, you can even show
that image to make sure all the pre-processing that you have designed are correctly applied
to those images, and you can basically double-check if
everything looks okay, then you can start training. Next plus data is very
useful for that case. But actually, more
common paradigm here, is to use your data
loader within the for-loop so that you can continuously step
into your batches, because next plus here
only gives you one batch. But in training or in testing, we have to keep stabbing into our training data until we
exhaust all the batches. In that case, a for-loop will actually be
more convenient. A basic structure is like this. We don't need anymore
either or next, you'll basically set up a
for-loop for all labels. For features and labels
in this test data loader, then it's going to execute the
loop for you is very easy. Here, features and labels
are just the two tensors, containing the
features and labels. We can try to run this, as you can see here, we have lots of prints
out here because it's iterate through all the
batches anymore dataset. As you can see at
each iteration, the shape of feature
is, again 64 8. There's lots of prints out here. At each iteration, the size of feature is 64 8
and the size of labels is 64. That's not surprising. But I think it's interesting to see what do we have for the
very last batch actually. If we go all the way down here, because the batch
size is so small, and we have so many examples, so we have so many batches. If you use a larger batch size and definitely you're going to have fewer batches. It's interesting to see the
size of the last batch. As we see here, is
no longer a 64, is actually 49 the
size of the batch. Because, well, the size of our test dataset is
not divisible by 64. The last batch is usually
whatever is left, after all the previous batches. Sometimes you may have to pay
special attention to this. The last batch may not, of the size of your batch size, it could be shorter than that. You have something to
pay attention to here. In this tutorial,
we have seen how we can create a dataset out
of some tensor data. In this case, we
have some features and labels unsupervised
learning dataset. We create a dataset,
all of that, and we have seen how we
can use data loaders, how we compare a
data loader with a dataset so that
we can step into those batches using a for loop or using the next
Plus either mechanism. For other datasets you may have, you can follow the
same procedures to set up those datasets
and data loaders, which is an important
step before we can actually train a model. In the following tutorials, we will learn how we can set up a model and how we can
train those models using the dataset
and data loaders we set up in this tutorial.
See you there.