Hi everyone. This is
Paramveer Dhillon. Now that we have studied the anatomy of a specific
type of neural network, that is the multilayer
perceptron, we will study about some general principles of training neural networks that are applicable to much
more broader class of methods other than just the
simple multilayer perceptron. Once we have decided on an architecture for a
multilayer perceptron, then we need to train the model. Training of neural
networks proceeds in a similar fashion as for a standard machine
learning model, such as an SVM or a
logistic regression. That is, we choose a loss
function that quantifies how we should penalize deviations from the correct label on the training data set, and then we minimize that loss function over
the training dataset. The goal is to find
a set of weights or model parameters Thetas that gives the smallest loss
over the training dataset. The loss minimization over the training dataset is
performed by gradient descent. That is, we iteratively
take steps in the direction of decreasing loss function while changing the
model parameters. We do so until we reach
converge that is a point after which a change in weight parameters no longer
decreases the loss function. Next, let's see how the gradient descent
algorithm actually operates. First, we initialize the model parameters
Thetas randomly. Then we compute the gradient of the loss function with respect to the model parameters to
find the direction of steepest descent along the
contour of the loss function. Once we have found
such a direction, we take a small
step of size Theta along that direction by
changing the model parameters. Now, we repeat this
process again with a new parameters and do so
till we get convergence. Once convergence
has been achieved, we get the final set of
weight parameters that minimize the loss function
over the training dataset. Things get a little dicey for neural networks as there
are several layers of model parameters and
computing gradient seems a little messy
at the first site. However, gradient
descent can be performed efficiently in neural networks
via back propagation. Essentially, back propagation is just a fancy name for
the chain rule of differentiation with caching or storing of intermediate results. For example, the gradient of loss function with respect to the parameters of first layer Theta one can be
computed via chain rule. First, we compute the gradient of loss function with
respect to y hat, then if we multiply
it by the gradient of y hat with respect to
the second layer, Z_2, and so on, and finally, we compute the gradient of
the first hidden layer, Z_1, with respect to Theta_1. Back propagation
for neural networks is done layer by layer. The process that I just described might seem cumbersome and
tedious to do by hand. Luckily for us, it is performed
automatically for us by standard deep-learning
frameworks such as PyTorch or TensorFlow. Finally, I encourage
you all to read this linked article
which demonstrates step-by-step details of the
back propagation algorithm. As they say, the devil
is in the details. So I encourage you to
work out the details of the back propagation algorithm by following this tutorial closely. This is a required reading to be completed by all the students.