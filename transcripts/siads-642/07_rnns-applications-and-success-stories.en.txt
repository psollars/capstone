Hi, everyone, this is Paramveer Dhillon. In this week's module, we have studied the architectural details
of Recurrent Neural Networks, RNN's. As well as we have seen several
architectural variations on the basic RNN architecture. Next, let us look at some
of the success stories, and some of the successful applications
of recurrent neural networks. Some of the sample
sequence modeling tasks, where RNNs have given significant boosts
in performance our music generation. In this task,
the input is music in sheet notation, and the output is the next
corrector in sheet music. The second such task is
sentence classification. That is, we want to assign
a single label to a sentence. For example,
the sentiment expressed in that sentence. Another application of RNNs, and one which has gathered significant
attention is text generation. That is, green RNNs on some text and
then generate some new text in that style. Let's see some examples
of this application. Someone trained RNN on the text
speeches of former President Obama and generated new text from it. The output text as you
can read on this slide, it reads awfully like a real
President Obama's speech. The promise of the men and women who were still going to take
out the fact that the American people have fought to make sure that they
have to be able to protect our part. It was a chance to stand
together to completely look for the commitment to borrow
from the American people. So, as you can see, President Obama used
to use terms like American people or stand together a lot in his speeches. And that's what also the text
generated by RNN also contains. Though the text generated by
RNN is mostly coherent, but semantically, it doesn't make much sense. In fact,
this is why RNN based text generation is a very active area of research
in natural language processing. Next, here is some text generated
in the style of Harry Potter. And the text goes, I thought you're all
right, he said, Harry grinned at Harry. Why should she be cheerful so
while you gave detentions, Moody. Or give them a hang of the fires and tell
me, it will come and finish me in this Quidditch Diggory, all been an Animagus
like a moment, Dumbledore snapped. Sorry, I think you please,
if it will be waiting for you after this, and it's like if
Dobby has learned to register this. Once again,
the text generated is coherent, but it lacks much semantics to it. For those of you who are interested
in reading more about this RNN based text generation, I encourage you
to go to the link at the bottom of this, as well as the previous slide. Which contains all the details of how
the RNN model was trained, to generate these texts in the styles of former
President Obama as well as Harry Potter. That said, the largest success
story of RNNs, and deep learning in natural language processing is
perhaps that of machine translation. Just to recall,
we studied machine translation in detail along with an encoder, decoder
architecture for it in the last video. Neural machine translation gave
the most significant improvement, in translation quality with the least
amount of hand engineering. Which was common in the previous
generation MT systems. This improvement in translation quality
was so staggering that in 2016, Google Translate switched to
Neural Machine Translation. The two previous MT systems
were syntax based, and phrase-based machine translation systems, which involved a lot of hand
engineered modules cobbled together. And as you can see in this plot,
starting 2016 neural machine translation started significantly outperforming
the phrase and syntax based systems. And after that there was no looking back. Even today, Google Translate
uses neural Machine Translation. But the architecture of the model
used is based on a successor of the encoder-decoder architecture
that we saw last time. Time and this new architecture is
known as the transformer model.