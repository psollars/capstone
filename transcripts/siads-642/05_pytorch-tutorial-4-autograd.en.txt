Hi everyone. This
is a Tutorial 4, Autograd in PyTorch
tutorial series. If you still remember, in the very first
tutorial in this series, we mentioned some very important features
that PyTorch provide. Actually, those features are
what make PyTorch useful. One of those features
is the ability to compute the gradient of the loss function
at the very end of a neural network with respect to all the parameters
in the neural network. In the last tutorial, we learned how to build
a neural network and how to get all the parameters
of a neural network. In this tutorial, we'll
look into this ability, which is also known as automatic differentiation
in PyTorch, that computes the
gradient with respect to all those parameters
we can get by calling dot parameter function
of area and dot module. Automatic differentiation
is implemented with this PyTorch package
known as torch.autograd. When training neural
networks, of course, the most frequently used
algorithm is backpropagation. You probably learned
this in the lecture. That algorithm is, basically, to compute the gradient of the loss function with respect to all the parameters
of a model. In PyTorch, in order to implement the
backpropagation algorithm, PyTorch have this so-called
differentiation engine, which is the torch
Autograd package, and it supports this
automatic differentiation of any so-called
computational graph, which we'll explain later. Without further ado, let's
quickly jump into an example here to see the
Autograd in action. Consider the following very simple one layer neural network, well, actually it's not
quite a neural network. Just it's basically two
parameters w and b, with some input x, and we'll try to define
some loss function. As you can see in this case, we don't even define a module, but instead we just define
those as tensors here. As you can see, the input is x, is basically a tensor of ones, and y is the output of
our so-called module. I think the most important
ones are the w and b, those are the parameters
of our module. Actually, as you can see here, those tensors are defined
using those random functions. We were seeing in
the first tutorial, torch.rand n that draws numbers from the
standard deviation, and five and three are
the shape of the tensor. But what's unusual here is we specify a argument called
requires gradient. We set that to true
for w and also for b. We'll actually explain
those arguments in more detail in later, but basically those arguments
are what distinguishes ordinary tensors
from those tensors for which we can
compute a gradient. Those arguments being
true is also what makes those tensors parameters
in this case. With those parameters
and input, output, we can compute an
intermediate value z, which is just a matrix
multiplication between x and w. Remember the function
matmul, m plus b. What we're doing here is
basically a linear layer. What a linear layer
does is basically a matrix multiplication with the weight and plus the bias. Z is actually the output
of our so-called model. Finally, we need to
compute the loss function. In this case, we assume we're dealing with
the classification problem. The loss function we're using is the binary cross-entropy
with logics function. For the purpose
of this tutorial, you don't need to know the
details about this function. All I need to know is
that it's a function of the output of our neural network
z and the true labels y. That function is
going to produce a scalar value as
the loss value. That's all we need to
know about this function. Let's run the code there. Here we have a very nice
computational graph that's actually from the
official PyTorch tutorial. This graph describes
what's happening here. Basically we have
x as our input, w and b are our parameters, and this graph
basically shows you how those tensors are interacting
with one another. We first have x and w. We
have those two vectors. We performed a matrix
multiplication on those two tensors, and the result is
being added to b. B is another parameter, so b is added to the result
of this multiplication. Then the result is actually z. Z and y are inputs to this
cross entropy loss function. Finally, the output from
this function is our loss, which is a scalar value. Actually behind the scene, for every computation that
requires autograd PyTorch, we actually build such
a computational graph. Here are some more description
w and b parameters, which we need to optimize. Because we need to
optimize those parameters, we need to compute the
gradient of those before we can take an optimization step. In order to do that,
we had to set that requires grad property
of those tensors. That's why in the
definition we have requires_grad equal to
true for both w and b. That actually tells
the autograd engine that we're interested in the
gradient of those tensors, w and b, and please help
me compute those gradient. Here's a little note.
You can set the value of the requires_grad either
when you create a tensor, pass a value of requires_grad to the tensor equation function. Or if you already have
a tensor, you can do, remember this is an
in-place operation, because it has a suffix
underscore here. So x.requires_grad underscore, that's an in-place method. True means we're going to set the requires_grad attribute
to true using this method. We can do either way. Actually later I'm
going to show you an example of this
in-place method. Here is just a paragraph
in the official tutorial that tells more
information behind. Basically those computations you performed in this case is the mainstream multiplication is actually an object of a class, also called a function. This is actually all
behind the scenes. PyTorch will build this object. The use of this object
is that it knows how to do the forward computation. It also knows how to compute the gradient or
the derivative in the backward propagation step. In fact, for all the intermediate tensors involved in this
computational graph, it's like PyTorch
is taking a note of your computations so
that in the forward pass it takes notes
along the way by recording the
operations that create those tensors as a grad function fn attribute in those tensors. If you just print out this
we can get some information. The gradient function for z is an object so-called
AddBackward. That makes sense
because, what is z? Z is the result of
some kind of addition. It makes sense to have a grad function,
so-called AddBackward. But you definitely don't need to worry about
all those details. Basically the tutorial here is trying to tell you just a little bit more
behind the scene. You almost never need to track the grad function of any of the tensors
in your graph. This is all left to
Patchwork to handle that. But basically you can imagine those grad functions as PyTorch taking notes
along the way, try to record those
operations in the graph. Of course, if you're
interested you can see more information
following this link there. That's our forward computation, in a forward computation we
do measure modification, we add a bias, compute some loss and now it's time to
compute the gradient. In this case, we want to compute the gradient of the loss
function with respect to w and b because w and
b are our parameters. Actually PyTorch has
done a lot of work to make the computation
of gradient very easy, in this case, all we need to do is to
give PyTorch a signal that tells petrol that we're going to need the
gradient of w and b. In this case, the way
we give that signal is just by calling the loss, remember the loss is actually a tensor at the very end of
our computational graph. Let's go back to the graph here. The loss is actually the tensor at the very
end of the graph. All you need to do is
to grab that tensor at the very end and call
backward on that tensor. That's all you need to do. Pytorch will actually go
through all your graph backwards to compute
all the gradients when it's appropriate, in this case, because in the graph we only require
gradient for w and b, so after this backward path will get a gradient for w and b, so we can actually try this out. We call backward and you'll
see that the gradient will actually it's going to be stored in an attribute of that tensor is
called dot grad, w dot grad will retrieves the gradient for
w and similarly for b, so those are the
gradients for w and b. The gradient is of course, of the same shape as the tensor and that is
actually always true. Here are some more nodes, we can actually only obtain
the gradient poverty only for those leaf nodes in the computational graph where the required square root
property is set to true. For all other nodes
in the graph, the gradient won't be available, so what are those leaf nodes? The leaf nodes are typically
tensors created by the user, which is you, the user PyTorch. Some examples of
the leaf node would be the weights in
a neural network. If you follow the procedures in the last tutorial
to define a module, to find new network, then all those layers
are actually created by you and so are the weights
contained in those modules. Those tensors, and by default, PyTorch will have the
requires grad property for those tensors to
be true by default, so we never need to
worry about Santos, properties because
if you've defined, let's say linear layer,
then by default, the parameters are going to be a tensor that requires grad. Those tensors will be leaf nodes or another way
of thinking about this is those leaf nodes
are not derived from other nodes in the computational graph
or in other words, they're not the result of some
other tensor computations. They are the initial tensors
in the computational graph. It is for those tensors that we can compute
the gradient, but for any intermediate
nodes, for example, in this case, the variable, the tensor z is actually an intermediate node because
let's go back to the graph. Z is actually somewhere in the middle of this
graph, it's actually, it's a result of some
other tensor computation, is a result of some addition. In that case, it's gradient won't be
available is not because we can now compute the gradient
of z with respect to z, but it's just not
very useful because z is not a parameter because all we care in a new network is the parameters and we wonder
compute the gradient of the parameters so that we can
optimize these parameters. If you try to print
out the gradient of z, your shows you is none, there's nothing for its gradient and it will actually
give you a warning. It basically says that
this attribute of a tensor that is
not a leaf tensor is not available actually. It won't be populated during
the backward process. However, it does tell you if you really want
this the gradient of z you had to use retain_grad
method and so on. But we won't get there. Another note is that
usually we can only perform the gradient
computations using this backward once
on a given graph. But if you need to do several backward calls
on the same graph, you will have to pass this retained graph attribute and this parameter to
the backward call. Actually in practical cases, it's very rarely that you will actually need this
unless you are really doing some fancy project
that you need to backward pass the same
graph multiple times. Especially if you're
wondering, maybe compute the, maybe the higher-order
derivatives. Maybe the second-order
derivative. In that case, probably you need to have this retained
graph to be true. But usually in
most of the cases, you don't need to
worry about this. The reason you can only backward once for
a given graph is that the computation
graph will be destructive naturally
after you call backward. Because to actually
retain this graph, it consumes memory because all the history must be stored. If you were on a GPU, then it has to be stored
on a GPU as well. All of those histories and all those connections
on the graph, and they do, and they'll
take out some spaces. I think Patrick here and make a, is making a very wise decision to delete the graph after
every backward call. That's about the forward
pass and backward pass. By default, all the tensors with the requires grad to
be true will receive their gradient after
a backward pass. But sometimes we may only want the forward path
now the backward path. For example, when we were
already have a model well-trained and we may
want to apply this model to some test data to make some
predictions for example. In that case, all we need
is the forward path, not the backward path
because we're no longer training a model. We don't need a gradient. In that case we can
save some resources because actually
constructing that graph actually do consume
some resources. We can stop tracking
the history, tracking the forward computation by using this
torch.no grad block. Here for example, we have
this computation as before, z equal to matmul
between x and w plus b. Let's just run this. In the first print, we know that z is expecting a gradient even
though it's not a leaf node, but it's always its a tensor
that expecting a gradient. But if we wrap this, the same statement inside no_grad block using the
width function here. The width Python keyword is basically to create
a block of statements. Within this block if we
do this computation, and then if we print out the required attribute of
z is actually going to be false because once you have
this torch no_grad block, then everything within
the block will be ignored by auto grade so your
history won't be recorded. In this case, all of the resulting tensors would
not require a gradient. That's why they're required
squared attribute is false. You could imagine that you
could replace this statement with your models forward path where you make a
prediction or something. In that case, all of
the history won't be recorded and it will
save a lot of resources. Another way of achieving
the same result is to use this function called
detach or a tensor Previously we
created a code block that does not require gradient. In this case we can also detach an individual tensor
from the computational graph. Here, let's redefine w and b for reasons I'm going
to show you later. But they do requires
gradient initially. Here we do the same
computation to compute the z, matmul between x, w plus b. So far, z is still inside
the computational graph. But here, if we do a z.detach, actually in this case is
not an in-place operation. There's actually an
in-place version that will actually did detach z itself from the
computational graph. But here we're not using
the in-place version, we're just using a detach. This actually will
return a copy of z that's detached
from the graph. Z detach or z_dat will be a copy of z that's
detached from the graph. If we print that out the
required squared attribute of z_dat will actually be false because it's out of the graph. Once you've detached
from the graph, you no longer require gradient. A detached tensor doesn't know
the history of how it was created because it is
detached from a graph. You won't actually know what's happened before it's creation. Even though in this case
it's pretty obvious that the tensor z-dat contains the value of this statement
because it's a copy of z. But from the point
of view of z_dat, you will see yourself as a fresh new tensor
field with some values. You won't know how
it was created. You won't know
their word w and x, multiplying together plus
b that create z_dat. Z_dat will only see itself
as a fresh new tensor. Therefore, if you try to do a backward path through z_dat, we actually won't get
the gradient for w and b because z_dat is already
detached from the graph, it has no connection
with w and b. We can't expect the gradient
to be populated for w and b. Here we use the
in-place version of requeres_grad to turn on the gradient
computation for z_dat, otherwise, we will be able
to do a backward pass. As usual, we compute
the loss but here in this case is between z_dat and y and we're trying
to do a backward pass. But if you print out the
gradient of w and b, they're both none because, as I said, there's no
connection between w, b and z_dat so you won't
get a gradient for w and b. There are some use cases for these enable the
gradient tracking. You can mark some parameters in a neural network as
frozen parameters. This is a very
common practice if you're fine tuning a
pre-trained neural network. In fine tuning a
pre-trained network, you probably only need retrain maybe the last layer
of the neural network and freeze all the parameters
of the previous layers. In that case, you could set those parameters that
requires grad attribute of those parameters to
be false and only turn it on that attribute
for maybe the last layer. You can follow this link
for more information. Of course like I said to
speed up computation, we only need to do
the forward path. Here are some long
paragraphs from the official tutorial that gives you more information
about the computational graph. I think I'm going to
leave it to you to read. But basically it conveys
the message that the PyTorch will
basically keep track of your forward computation and
then once you call backward, you compute all the
gradients with respect to all the leaf nodes
that requires grad. Perhaps I think one key message
from those paragraph is that PyTorch actually
accumulates the gradient. Accumulates means if you
call backward several times, the gradient will
be accumulated. For those leaf node, instead of the new value maybe overwrite the old
value. That won't happen. The new gradient
will actually be accumulated in the leaf node. What follows, actually I'm going to show you an example of this gradient accumulation
because this is a very important idea to know. In fact, gradient
accumulation is actually important technique when you
are resource constraint. When you don't have
a very powerful GPU, then gradient accumulation
will be effective technique to overcome that constraint. Let's see. When doing a
backward propagation, PyTorch actually accumulates
gradient where the value of the computed gradients is added to the grad property of all leaf nodes of a
computational graph. Let's see a concrete example. Let's just redefine everything. Let's just redefine w
and b, requires grad. This is basically the same cell as the one you saw in
the very beginning. Here we just do one backward. We successfully got some
gradient for w and b. Now, let's save those gradient. Let's save some copies of the first-time gradient because they want to see this
gradient accumulation effect. The way I'm saving the
gradient here is by just calling the clone function. Clone is also a tensor
function that lets you copy a tensor. We can
call it w.grad.clone. That's going to copy the
value of that tensor. I'm going to store that in
this variable called grad_w, and similarly for the gradient b. I have saved the
first-time gradient. Now, let's do something
that's interesting. We're going to do a loop. We're going to loop five times. Each time, we're
going to execute the same operation.
It's rather boring. Matmul for x, w plus b five times, and we compute the loss for
each of those computation, and then we do a backward. Essentially, we're doing
a backward five times, all on the same
computation. Let's do it. If we believe that
the gradients are being accumulated under
the grad attribute, then what should we
see in this case? Well, we already have the first-time gradient or they store it in
a grad attribute. Then we have done five more
gradient computations. If the gradients are
to be accumulated, then there should be actually six such copies of the
first-time gradient. The gradient won't
change because the operation we do in each
iteration are all the same. What we should have is that what's in the
grad attribute now? It's actually six time
the first time gradient. What do we have saved
in the grad_w tensor? Those two should be
close to each other, and similarly for
the gradient of b. We can check. Of
course they're close. This actually indicates
that the gradients are indeed being accumulated
to this attribute. The current gradient
is six times larger than the first-time gradient. We would have them
equal to each other, not six times larger. That's the default behavior of PyTorch gradient
accumulation. If you don't want the
gradient to be accumulated, you will have to zero out the gradient before
the next backward. You basically had to
set the gradient to zero before the next backward. In most of the cases, you
don't need to do that manually once you
have an optimizer. An optimizer will
actually help us do that. That will be a topic
of the next tutorials. With that, we'll
conclude our tutorial on automatic differentiation,
autograd. Thank you.