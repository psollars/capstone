Hi, everyone. This
is Paramveer Dhillon. I'm an assistant professor in the School of Information at
the University of Michigan. I would like to welcome
you all to the fourth and the final module of this
course on deep learning. Just to recap, in the
first week of this course, we learned about the
basic foundational blocks of deep learning, that is feedforward
neural networks and the simple
multi-layer perceptrons. We also learned about how to train neural networks
efficiently, as well as some of
the practical details of training neural
networks in first week. Then in week 2, we learned about a specific
neural network architecture, which is well-suited for
the task of image modeling. We learned about convolutional
neural networks or CNNs. Then in week 3, we learned about another
popular class of neural network
architecture called RNNs or recurring
neural networks, which are used for
modeling sequential data, such as the one that
arises in text, speech, or other similar
kinds of sequential data. In this week, we will focus on a new and fascinating area of generative models
in deep learning. Then we will study some of the shortcomings and
limitations of deep-learning. Finally, we will wrap off this module by
studying how do we deploy deep learning models at scale using techniques
such as auto ML. First of all, let's recap the two most common machine
learning paradigms. All the models that
you have seen in the first three weeks
of this course, including feedforward
neural networks, convolutional neural
networks or CNNs and RNNs, as well as several
others that you might have seen in a
machine learning course, example: linear or
logistic regression, support vector machines, etc, are all examples of
supervised learning algorithms. In all supervised
learning algorithms, we are given features
x and labels y. Our goal is to learn a mapping from the
features to the labels. This is in contrast to
unsupervised learning, where we are just
given the features x and there are no labels y. The goal in unsupervised
learning is to uncover some latent structure in the data or estimates some underlying
density function. The various clustering and dimensionality reduction methods fall under this category of unsupervised
learning algorithms. A central problem in unsupervised learning is of finding the latent
structure in the data, and generative models are a
common way of doing that. The goal is that we are
given training examples that is features x from
some distribution, and we want to learn a model that represents
their distribution. In other words, we want to learn a probability distribution Pmodel_X that is approximately
equal to Ptraining-data_X. Once we have found such a model that represents the
data distribution, then among other things, we can draw samples
from that distribution. As we shall see soon, this can have lots of important
practical applications. The samples from the underlying data distribution that we have learned by fitting a model can either be used for
density estimation, that is estimate the probability density
function or the PDF of an unobservable
underlying distribution from just the data samples. All the samples can be used to generate new samples from
the same distribution. For example, given images of faces from an underlying
distribution, generate new face images
from their distribution. A natural question arises. What can we do with a new samples generated from an underlying
data distribution? One application of such
a generative model and one that has received
lots of attention these days, is to reduce the bias in the datasets by
creating fair datasets. That is, datasets that are more diverse in demographics,
such as age, gender, or race, and more diverse in lighting, pose,
or illumination. Such diverse datasets
when used by supervised machine learning
or deep learning algorithms for classification
tasks can learn more fairer and less
discriminatory models. In addition to generating fair datasets for use by
supervised learning models, generative models can learn useful latent
representations that can be used as features
in supervised models. The generated data
samples can also be used to augment the data
for use in simulation, and planning of robotic
and autonomous vehicles. Last but definitely
not the least, there is a relatively
novel application of generative models and one that has gathered much
attention of late, it is to generate new
samples of art or faces. In fact, we will next study one such family of
models called gans.