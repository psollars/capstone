Hi everyone. This
is Paramveer Dhillon. Last time, we saw an overview of the
field of deep learning, as well as, we studied how deep learning is different than traditional
machine learning. We also learned about several success stories
of deep learning. In today's lecture,
we will learn about the basic building block of deep learning, the
multi-layer perception. First of all, let's recap the standard machine
learning pipeline. Assume we are given
n data points, and our goal is to learn a
decision function that maps the features x_i to
the target labels y_i. Different classifiers such
as logistic regression, support vector machines, and
SVM or linear regression, correspond to different
decision functions. Further, we are provided
a loss function, which measures how good our classifier is in predicting
the correct target label. Finally, we want to minimize this loss function over
our training dataset. Let's consider a simple
linear binary classification problem, where we have to
classify each data point into one of two classes:
plus one and minus one. Let's build a simple
linear classifier for this classification task. Our classifier, which we call c of x combines the features x_1 and x_2 in a linear fashion and has three
parameters: theta naught, theta one, and theta two. As we all know, the linear classifier draws a linear hyperplane to
separate the two classes. The separating hyperplane can
be seen as the dotted line, c_x is equal to zero in the plot on
the left in the slide. We can devise a simple
prediction rule based on our linear classifier. We just use the sign of c_x, that is whether it
is greater than zero or lesser than zero, and output it as the
predicted label. Now let's consider the logistic
regression classifier. It keeps our problem set
up the same as earlier but changes the prediction rule that we use to make prediction. Unlike the linear classifier, it outputs the probability of a data point belonging
to the positive class. A logistic regression also
combines the features x_1 and x_2 linearly but squashes
them using a link function, which in this case is a
Sigmoid function denoted by Sigma to lie between
the zero and one range. I am sure that all of
you had previously seen these two simple
linear and logistic classifiers that
we just discussed. Now let's see if our data is not as nice and clean
as we just saw. The data that you saw
plotted on this slide cannot be correctly classified by a single linear
decision function. In other words, you cannot cut the data with a
single straight line. A natural question arises as to how we can correctly
classify this type of data. The key idea here is to combine several
linear classifiers to form a non-linear
decision boundary to correctly classify
the data points. For this problem, we use three linear classifiers
represented by z_1, z_2, z_3 to classify
these data points. Each of them combines
the input features x_1 and x_2 linearly before
squashing them via the Sigmoid, the Sigma x function. Note that each of the three decision
functions, z_1, z_2, z_3, do not solve the original
problem ideally, but tries to solve a sub-problem. For example, the decision
function z_1 solves the problem of separating the red points to its left from the
remaining data points. Similarly, z_2 separates
the red points on top, and z_3 separates the red
data points to its right. Now when all these decision
functions are combined, they are able to separate all the red data points
from the blue data points. The three linear classifiers, z_1, z_2, and z_3, can be seen as a new set of features that we have computed, starting from our input
features x_1 and x_2. For our toy example, these new features in code, the valuable information
that whether the red data class is to the
left or right or on the top. Finally, we combine these
three newly computed features, z_1, z_2, and z_3, weighted linear
fashion once again, and we squash the
final output using a Sigmoid function to make the final model prediction y hat. The model that we just saw
on the last slide can be graphically represented
as shown on this slide. The graph on the slide
shows a number of nodes that represent
a variable to be computed and the
edges representing a dependency between the two
nodes that it's connecting. For example, an edge
from x_1 to z_1 represents the fact that we need the value of x_1 to compute z_1. So to summarize, x_1, x_2 represent the input
features in graph. Z_1, z_2, z_3 represent the new
features that we have learned by computing weighted
linear combination of the input features, and y hat represents
the output prediction. All the patterns shown
in the graph are model parameters that need
to be learned from data. The computation graph that
we show on this slide, it's what is called a multi-layer
perceptron or an MLP. Each node in the
graph is a neuron, as an analogy with neurons
in the human brain. Essentially, a neuron takes
a linear combination of its inputs and applies a non-linear activation
function Sigma x, which in this case is
the sigmoid function. Each set of new features
that we compute, such as z_1, z_2, z_3, constitute a hidden layer. In the MLP shown on the slide, there is only one hidden layer, but in general, they
could have more. The multi-layer perceptron is the simplest artificial
neural network and can have many hidden layers. Each hidden layer
allows us to learn a set of new non-linear features. In fact, stacking together several hidden layers to learn really complex
non-linear features, is what has allowed us to
get improved predictive accuracies on several domains
such as computer vision, natural language
processing and speech, and ushered in the
era of deep learning. Needless to say, a multi-layer perceptron is the building block of
all deep learning. More precisely, deep
learning is the study of neural networks with at
least two hidden layers. Neural networks in general, and multilayer perceptrons
in particular, have several important
design parameters. These design parameters
control the shape of non-linear features
that our models learn, and is an important determiner
of the performance of an MLP on some predictive task. First and foremost, the
number of hidden layers of MLP is an important
tunable parameters. That is, how deep is
our neural network? Each extra layer
makes the model more complex and increases
the parameter count, but allows us to learn incrementally more flexible
non-linear features. Next, the width of a
neural network, that is, how many neurons in a given layer is also an important
tunable parameter. In the toy MLP that we saw
on the previous slide, it has a width of three. As a rule of thumb, it has been shown that deeper and less wide
neural networks perform better at various
prediction tasks. It is also important
whether we want to connect all the hidden layers
with each other or not. Making a neural network fully-connected makes
the model more complex, but gives it more
representational power. But it fails to incorporate any domain knowledge
that the researcher has. Last and definitely
not the least, the choice of the non-linear
activation function also strongly determines
its performance. In our toy example, we used the sigmoid function
as the non-linearity, but there are several
other possibilities. Now, let's look at the importance of activation functions
in deep learning. As can be seen on the
plot on the left, linear activation
functions induce a linear decision boundary. So it will not be able
to correctly classify the data if the data is
not linearly separable. On the other hand, non-linear activation
functions allow us to learn highly flexible non-linear
decision boundaries, which allow us to
fit the data better, and get fewer
classification errors. Different non-linear
functions allow us to approximate various complex
decision boundaries. There are several commonly used non-linear
activation functions. The sigmoid function, which we also used in our toy example, is perhaps the oldest one and is inspired by
logistic regression. It squashes a real valued
number to the 0-1 range. Another popular activation
function is tanh, or the hyperbolic
tangent function. It squashes real valued numbers to the range minus
one to plus one. Tanh is also more steeper
than the sigmoid, as can be seen from the
plots on the slide. Last, one of the simplest
activation function, which has grown in popularity in recent years is
what's called ReLU, or the rectified linear unit. It is the simplest and most computationally efficient
activation function. ReLU, just zeros out
the negative part of its input and outputs
the positive part as is. It is in fact, inspired by half-wave
rectifier circuits commonly used in
electrical engineering.