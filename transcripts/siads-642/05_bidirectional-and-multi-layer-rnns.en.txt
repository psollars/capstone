Hi everyone. This is
Paramveer Dhillon. Till now, we have studied
several variations on the basic recurrent
neural network architecture. Next, we will study two more important variations on the basic RNN architecture, which gives them more representational
power and make them fit for several kinds of speech
and text modeling tasks. In particular, we will study bidirectional and multi layer
recurrent neural networks. Let's consider the problem
of sentiment classification, where we are given a sentence
and we want to classify the sentiment expressed by it as either positive or negative. This task can be
modeled using RNN in its many-to-one
modeling configuration. There might be something
strange that you might observe at the RNN that processes this text to output the sentiment label and
is shown on the slide. The RNN reads or processes text left to right sequentially. So a word to the
right of a given word has no effect on its
hidden state estimate. For example, if a
restaurant review, who's sentiment we
intend to classify, contains the words
not bad but good, then the hidden state
of the word, but, only contains contributions
by the words from its left. That is the words, not bad. But it contains more information from the words to its right. In this case, the word good. In many language tasks, we might get good
performance boosts if we model both the left and
right contexts of a word. This brings us to
bidirectional RNNs. Bidirectional RNNs
are a variant of the standard RNN that is information flow from both
left and right sides. Bidirectional RNNs, essentially
combine forward and backward RNNs that process
information left to right, and right to left respectively. Finally, the composite
hidden state is the concatenation of
the hidden states from the forward and
the backward RNNs, as can be seen in the
figure on the slide. Mathematically, the
bidirectional RNNs just concatenate the hidden state from the forward and backward RNNs, denoted by h_t, with a forward
or backward arrow on top. This composite
hidden state is what gets passed on to the next stage. All other modeling details of the bidirectional RNNs are the same as a unidirectional RNN. Also, is worth
mentioning that we can construct a bidirectional variant of not just the vanilla RNN, but also its gated variants
such as an LSTM or a GRU. It's easy to extend the graphical representation
of a unidirectional RNN to a bidirectional RNN
by simply drawing bi-directional arrows among the hidden state
variables of the RNN, which are shown in green
color on the slide. The second variation on the
standard RNN architecture that we will study in this
video are multi-layer RNNs. RNNs are already deep
in a way as they're unrolled over several time
steps or sequence length. That said, we can make them deep over the
second dimension also by stacking
together multiple RNNs, which are known as
multi-layer RNNs, or just stacked RNNs. Multi-layer RNNs allow us to compute more complex
representations of sequential data with lower layers capturing more generic set
of features of the language, and higher layers capturing more specific features related to the prediction task at hand. This is similar to what deep CNNs or convolutional neural
networks do for image data. That is, lower layers captured more broad and genetic features and upper layers are
more specialized. The figure on this slide
shows a multi-layer RNN. As we can see, essentially, the hidden state from
layer i is input to the hidden state at the same
time step in layer i plus 1. To summarize, bidirectional or
multi-layer RNNs can be built on top of
any RNN variant, including LSTMs, GRUs, etc. It turns out that
most of the state of the art natural language
processing models these days are deep, that is multi-layer
and bidirectional. One such architecture, which is really popular these
days and has given state of the art accuracy on several language
tasks is called BERT. BERT is a bidirectional and Deep Learning model
for language modeling, and it has 24 hidden layers. The architecture of BERT is beyond the scope of this class, but for those interested in
learning more about BERT, can read the paper that
is linked from the slide.