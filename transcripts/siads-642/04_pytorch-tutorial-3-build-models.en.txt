Hi everyone. This
is a Tutorial 3 of the PyTorch tutorial
series about building models. In the past two tutorials, we have seen the
fundamental data structure in PyTorch tensors and also how we can load some tensor data into a
data set and data loaders. In this tutorial, we'll
actually learn how to build a neural network that
interacts with our data. Again, this notebook
is adapted from the official PyTorch
tutorial on build model. As you probably have
learned from the lecture, neural network compresses layers that perform operations on data. Usually a very deep
neural network will have lots of layers. You may ask, how does layers
represented in PyTorch? In PyTorch there is
a torch.nnpackage, so nn is short for
neural network, so torch.nn that package will provide you with
all the building blocks that you need to build
your neural network. In fact, a layer of a neural network is usually known as a module in PyTorch. A module could be a single
fully connected layer, or it could be a
convolution layer that you will learn in
Week 2 of the class. A research module in
PyTorch is actually a subclass of this base
class called nn.module, so nn.module is a template that every neural network layer
has to follow in PyTorch. A neural network is
module itself that consists of other such modules. A neural network itself is
also a subclass of nn.module, but this neural
network could contain other layers that are also
subclasses of nn.module. As noted by the
official tutorial here, this nested structure
allows for building and managing complex
architectures easily, because there is really
no limit of how you can nest those structures. You can have a giant
neural network that consists of
several modules, and each module itself could also be a giant neural network that consists of some
children modules. There's really no
limit except for your memory that stops you from having a very
giant neural network. In the following sections, we're actually going to build a very simple neural network that can make predictions on the median house values in the California
Housing data set we have seen in Tutorial 2. As a first step,
let's try to import all the relevant packages. The usual way we import
the torch nn package is by this line here,
torch.nn as nn. Just like import NumPy
as np and others, just like data set
and data loader. As a review of our
previous tutorials, let's try to load data
into a data loader so that we can pass it on to
neural network later. As usual, first import the data, California housing
from scikit-learn, and here we actually just use the utilities from scikit-learn
to do a train test split. You can also do it in PyTorch. Have shown that to you
in the last tutorial. Here, once we have the data, which is in NumPy arrays, and here we can convert those
data into PyTorch tensors. In the last video we used this function towards
dot from NumPy, but here I'm actually showing another alternative,
which is torched. as tensor. As tensor will basically convert
this input here, no matter what type that
input is into a tensor. It will actually try to figure
out what this object is and convert it to a tensor
in an appropriate way. If there's a NumPy array, it will do its job and convert it into a tensor and could be
some other data type. It could already be a tensor. In that case, no conversion
will be performed. In this case, remember, we have to also cover
the datatype into torch float for the reasons I've explained in
the last tutorial. We'll just run this
block of code. All the rest is
just like viewing a tensor data set and
pass it to a data loader. Here we're using a
batch size of 64. We have covered everything
there in the last tutorial. If those still look a little
bit unfamiliar to you, you should definitely
re-watch the last tutorial. So far, we have our data ready. Now let's try to define
our neural network. We'll define our neural
network or actually create a class and that subclass will inherit the
nn.module based class. This is a standard way of
defining a subclass in Python. You'll pass the parent class here in the preface to this
definition of your class. Then we'll initialize
all the layers in the init function. Init is also known as the
constructor of this class. This function allows you to actually define all the
attributes of this class. In fact, each layer in this neural network will be defined as an attribute
of this class. It's a very Pythonic
way of defining a neural network because the whole neural network is
naturally the whole class, and each layer, each component
of this neural network is naturally an
attribute of this class. Everything is very natural here. That's the structure
of neural network. It's the static part
of neural network. But what about the dataflow? What about the computations
of your layers on those data? How can we define
those computations? Well, everything like that, the dynamic behavior of
a neural network will be defined in a function
called forward. I think the name is
pretty self-suggestive. It basically means if
we have some data, how this input data
will be passed through your layers and how well that data will
interact with your layers. That's what the
forward method does. Let's take a look now into how we can define
this neural network here. The name of this class
in neural network v1, because we are going to have a little bit more
versions to this. What are the layers of
this neural network? We have fc1, so fc is a short for
fully connected. Well, linear layer
has also other names. It's known as MLP, it's a multilayer perceptron, or sometimes it's known as
a fully connected layer, or actually in the
language of PyTorch, that's called a linear layer. A linear layer is basically a fully connected layer because we have lots
of linear layers here. Here I just call it fc1. That's the first linear layer. You should definitely look
at this documentation to understand its parameters. But here, two
important parameters are the in features
and out features. In features is like the dimension of the
input to this layer. In this case, it's
eight because this is the very first layer that will actually directly
interact with your data. Our data dimension is eight, so the in features in
this case must be eight. The out features is the dimension of the
output from this layer. In this case, we do
have some choices because the input has
a dimension of eight, but we have some flexibility of the output of the dimension. This dimension is also known
as the hidden dimension because it's not
outside of the network, it's already within the network. The output from the first layer will be the input to
the second layer. In some sense, the output is already hidden inside
the neural network. It's also known as the hidden
layer or hidden dimension, but here we just made a
choice that we set it to 16, just double the input dimension. That's the first layer. The second layer is
actually not quite a layer. Well here, it looks like it's being
represented as a layer, but it's actually just
a activation function. It's the ReLU function, rectified linear unit function. In this case, because ReLu
doesn't have any parameters, we don't need to specify
any parameters here. It's just a simple definition. After ReLu 1 comes FC2, That's the second
fully-connected layer. Here, I have a note here. In the middle layers, the in features must match
the last out features. Because the last all features is really the input
dimension to this layer. Because the second layer, we will accept the input from the output from the first
layer as its input. This in fishers must match the art features
from the first layer. And again, we do
have some choices for the out features
for the second layer. In this case, it's 16 again, I mean it could be 32, it
could be any number you want. Then after this is
another ReLU layer, and then finally we have a final linear layer that's going to produce
our predictions. So the in features
of that layer is 16 because they are features of
the previous layer is 16, but the out features
in the last layer must be whatever that dimension you want the output to be. Because that's going
to be the output. That's going to be
the dimension of your final output of
your neural network. In this case, because
we are trying to predict the
median house values, and those house values are just scalar values
with one dimension, so the Out features has
to be one in this case, and if that is the
label dimension, just a single dimension. That's the static structure
of the neural network, and now let's look
at the data flow, the interaction between
your layers and data. The forward function will
be the description of that. The four-function
actually accept arbitrary number of parameters. In this case, we only have the parameter here is
usually your input. In this case, we just
have a single input X. This X will first be sent to FC1 because that's
the first layer, and we got an output. We also call it X, because otherwise, we have to define five variables here. I guess the cleanest way
would be to reuse this X. So after the first statement, X will denote the output
from the first layer, and that X is going
to be passed to the ReLu1 and produce a new X, and so on and so forth. As you can see, actually, if we define our neural
network in this way, like we have each layer as a separate attribute
in our class, then you will have
lots of layers, then, of course, we have to probably write lots of
statements describing how the data is flowing
through those layers. It could be tedious sometimes. Of course, finally,
the output of FC3 will be returned as the final output of
this neural network. Okay. Let's try to run the cell, and let's try to create an instance of this
neural network. In fact, we can also move it
to a device of our choice. Remember how we
define a device in PyTorch code exactly
using this line here, and here, we just define an instance of this
neural network V1 and move it to device
using the two method. We have seen in previous
tutorials that we can move a tensor to a
device using two, but here I'm just
showing you that the two method also works
with a neural network. In that case, PyTorch
will automatically move everything in a neural
network to the device. In fact, we can print out the structure of
this neural network. It's a very nice print-out of the structure of the
neural network here. As you can see, it
basically exactly matches the definition
of the attribute here. That's the structure
of the model. Now, to use this model, we have to pass some
input data to it. This will have to call
this forward function. But as suggested by the
official tutorial here, do not call model dot
forward directly but rather, we should treat our model just
as if it were a function. Here as you can see, we grab the first batch right
from the trend data loader and move those
batches to the device because data must be on the
same device as your model. Otherwise computation
won't happen. Now, all we need
to do is to pass, so we invoke our model, model V1, that's our instance. We'll basically treat
it as a function call here would basically just passing the inputs
and there we go. There we will have the outputs. If we print out this, the output size
would be 64 by 1. That's exactly what I want
because the batch size is 64, we have 64 examples
and one just indicates that our predictions are
just scalars of dimension 1. That's pretty good. New network looks like we
have built it correctly and if we pass over
inputs to our model, we do gather, I'll post a
list of the correct shape. That's usually the first check you can do once you have built a neural network
to basically check whether you can really
pass through your data. Then you will start
to worry about their training loop and so on but at least at the first step, we have to make sure that our
model is built correctly. We have so many layers
in our new network. Now let's try to break
down those layers and understand each
layer individually. To illustrate that, we'll
use a smaller data, was a smaller batch
of data here. We're just going to grab
this first three examples from the first batch, actually call it x. This, of course it
has a shape of three, eight and eight is
the data dimension. The linear layer, also known as the fully connected layer is a module that applies a
linear transformation on the input using these
weights and biases. You can take a look at the documentation by
following this link here. Here, we can see the effect
of applying this module. If you want to invoke individual modules
in your new network, you can just invoke it using the usual way of evoking an
attribute of other class. We'll just do model v1.fc1 because fc1 is just an
attribute of our class and that class is also what we call a callable attributes. We can just pass the input to that attribute I
will gather results. Here this common is just
to help you remember the definition of
that attribute is linear layer with in features
8 and out features 16. Because of that, we can see that the size of the output is 3,16. Note that here the
batch size 3 doesn't change because the linear
layer won't affect the batch dimension but the size of the
data dimension has changed from 8-16 because
the out features is 16. That's the effect
of the first layer. After that is a ReLU layer. ReLUs or other non-linear
activations are what grade those complex mappings between
your inputs and outputs. They're usually applied after the linear transformation to introduce non-linearity so that your new network can create a very complex function to
model complex phenomenon. In this tutorial,
we actually use the ReLU activation function between our linear
layers but of course, there are many other more activation functions
you can choose. To introduce a
non-linearity model, we can refer to the link here to a full list of
activation functions in PyTorch. Note that no
activation functions will actually change the
shape of their input. Any activation function is basically a
element-wise function. It will not change the shape of the input like what
a linear layer does. In this case, the output naturally has a different
shape than the input, but that's not the
case for any of the activation functions because those functions are
element-wise functions. We can see the effect
of the ReLu here. This is the tensor before
applying the ReLu function, and this is a tensor after
applying the ReLu function. Note that a lot of values
in the output are zero, that's the characteristic
of the ReLu function. It keeps the positive values but squash all the
negative values to zero. That's the effect of
the ReLu function. Now, we're going to
introduce a new module. It's called the nn.Sequential. nn.Sequential is not a
neural network layer per se, but it's like a container, it's a older container of
neural network layers. The data will be passed through all the modules contained
in this sequential, in the same order as defined. Therefore, we can define a single nn.Sequential
container in place of those five individual
layers as we have seen in the neural network V1 model. What does it mean? Let's look at this version 2 of
the neural network. Note, it's also a class, that is a subclass of nn.Module. The first main difference
here is that we no longer have those five
attributes but instead, we just have a single attribute here that's called a
linear ReLu stack. That attribute is an
nn.Sequential object. It's like a stack of modules. Instead of defining
individual attributes for those five layers, we put those five layers inside this sequential
container so that those layers will be automatically executed in
the order they are defined. They're the same layers
as we saw in V1, but it's that they're no longer
attributes of this class. All the other rules for
defining in features, out features are all the same. Another main difference here
is in the forward function. That's the structure
of a neural network. Now, we also have to change
the forward in this case. Because in this case, we don't have those five
individual attributes or layers anymore, we have a single attribute. In this case, we
don't need to pass x around through those five layers because the sequential
will help us handle those. All we need to do is to pass the initial input to
your sequential module, and this module will execute all the
contained modules in this order correctly. You will handle all
the passing around of the input and pass the output from a previous
layer to the next layer. The sequential will
handle all of those for you and those modules will
be called sequentially, that's why it's
called a sequential. That's a very neat
tool you can use, especially when you have
a linear set of modules, you can definitely
consider using nn.Sequential to save you
a lot of lines of code. In this case, we
can see an example here of applying this neural
network to some input. As usual, we can define V2, define a new instance of this
and move it to the device. In this case, if you pass the
input through the module, as usual, we'll get an output
of the same size as before. It looks like we have implemented
this class correctly. This nn sequential is
indeed equivalent to having those five individual
layers and passing around the input data because we get an output are the same size. That's a great time saver. Here's another, probably
a time saver as well, or less space saver. Previously, the ReLu
activation function has been defined as a module because we're invoking
ReLu from the nn package, the torch.nn package, it's like an module, but actually, most
activation functions do not have any
learnable parameters. They are just a function with
no learnable parameters, but we do have some exceptions. One of those is the nn.PReLu. Or parameterized ReLu, you
can see its documentation and maybe also see the paper that proposed this
activation function. That activation
function does have some learnable parameters, but for most of those
activation function, we don't have any learnable
parameters, and therefore, those functions can
be applied just like an ordinary torch. The name of the function just
can be applied in that way. Let's see a concrete
example here. Let's define version 3
of this neural network. As you can see here
in the init function, we no longer have
five attributes, but we only keep
those linear layers as the attributes because they really contain
learnable parameters, and actually, that's
the whole point of defining this
neural network because we want to learn those parameters so that we can make an accurate prediction. They do contain
learnable parameters, and it will still as
attributes FC1, two, and three and all
the definitions are the same as V1 and V2. A little bit of homework here actually apart
from nn.sequential. There is another container
called end.modulelist, which is just like an
ordinary Python list, but it's used to hold modules. It used to hold neural
network layers. Actually, an alternative
way of defining would be to put those layers into a module list container. I will leave it to
you to explore there, and you can try to
rewrite this code in terms of nn.modulus container. Anyway, in this case, we don't see the
definitions of ReLu anymore because we're going
to apply ReLu as a function, in this case, so that will actually go into
the forward function here. In the forward function, we have, as usual, the input x, but here, as usual, we just passes x
through because we're not using a sequential here, so we had to manually
pass this input around. Once again, and output FC1. We can see here, instead of passing the
output directly to FC2, we have an additional step here, which is applying towards
the ReLu to this x. This x is actually the upward
form, the first layer. ReLu now is no longer a layer, but it's just a function. It's just like torch or sun, or towards the mean function is just an additional
function we apply to the input before
it's sent to the next layer. Similarly, for the last layer, we have additional
step of torch. ReLu. That's replaces the ReLu2 module in version 1, so can define this
class, and similarly, we can instantiate a class. We can get the outputs. Again, the same shape. Looks we have everything
correct here. If your activation
function is not listed as a torch.relu or something like that you should be
able to find it under this package it's called
torch.nn.functional. Functional is basically
a package where lots of useful operations are
provided as a single function. The usual way of using
that package is by importing this import
torch.nn.functional.sf. That's, a PyTorch convention. Then here we're trying to
compare because relu is also provided under functional. To invoke relu, we can either do
F.relu or torch.relu. Just through all close to see. Those two ways to
give the same result. But you can try other functions and they're functional as well. That's about defining
the models and also how to pass data around those models
and how to get an output. Sometimes instead of
those individual modules, we may be interested in all
the parameters of a model. What is said here, many layers inside neural network
are parameterized, meaning they have
lots of parameters. They have weights and biases that we need to optimize
during our training. Actually, if you define a
neural network in this way, by subclassing the nn.module, you can access all of
your parameters using either this method
called.parameters, or name the parameters. There are two methods provided
at PyTorch that allow you to access those parameters. Here let's see example. In this example, iterate
over each parameter and print its size and its name. Basically, what we
do here is a for loop into this method, because the method
like parameters or named parameters
will actually return an iterable of all
the parameters. In this case, named parameters
would actually return a pair where is a
name parameter pair. Let's just see the result here. This is a structure of actually
version 1 of the network. You can see here the
relu is actually part of this structure because it's
defined as an attributes. When we print out its structure,
it'll be shown there, but if you examine the
name parameters here, you won't see
anything related to relu because relu doesn't
have any parameters. All you can see is that
something like FC1.weight. That's the weight vector of FC1 first layer and
its size is 16, 8. That's the old features and
in features of that layer, and looks like that layer
also has got a bias term, so bias has a size of 16 and
that's its value and so on. We do have FC2.weight
FC2.bias and so on, so forth. As you can see in this summary
of all the parameters, we can only see those parameters from those linear layers, but we didn't see anything
from the relu layers. The parameters function or the name parameter function are very convenient way of grabbing all the parameters
inside a neural network, and that's going to be
tremendously useful when we set up the optimizer that's
going to be optimize all the parameters
of a neural network. With that, we'll conclude this tutorial and see
you in the next one.