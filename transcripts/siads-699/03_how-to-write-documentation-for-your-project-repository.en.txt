As part of turning in
your final project, you're going to have
to provide us with a GitHub repository
of your code. Sharing your code
usually through a tool like GitHub
is increasingly a standard of what it means to finish and publish a
data science results. If you're not too familiar
with GitHub before, then let me tell you that when you first go
to a GitHub page, you see something
called a README. A README is an overview of your project
and what's inside. A good README matters
a lot in the world of data science as well as software
engineering generally. When people produce projects
that they put on GitHub, it can almost function
as a little bit of an advertisement or
a social network. It's very common
for employers to want to see your
GitHub repository. I remember sharing my
GitHub repository with quite a lot of employers when
I was doing a job search. What does the README do? It provides a brief
overview of your project. It gives the reader what they need to start using your code. Any instructions about things
that you'd have to set up, things you need to install, and where to find the code
that they need to get started, it'll lay that out for them very clearly and it will explain
how your code is organized. I'm going to show you a couple examples of
READMEs that I think work. Here, we're looking at a very popular image
segmentation library. This is someone who's created
some pre-trained units for image segmentation
and package them into a library and made them
available to the public. In addition to the code here, they have a really nice README. They start with this logo. Logos are certainly
not required, but they're really cool. Then they start with
a bulleted list of some of the main
features of the library. This list here tells
you what's contained, but also, what is the value of the project and who
might be a user. If you have some of these needs, then this library
might be for you. There's a table of contents. Because if this has pretty
extensive documentation, this degree of documentation
is certainly not required, but I just want to highlight
that they have made use of some of the markdown formatting of the GitHub README document. They have a quick start, so it tells you what you would need to do if you have this
installed, so how to run it. A very basic example, a show off a little bit of
what you can do with it. They have a basic example
of loading a model and then changing one of the
parameters of the model. They include a code snippet
for a basic use case. Then they have some examples. These are Jupyter notebooks
that are working. They also include some
documentation about the model architectures that
are used in the library. This README combines some
basic installation notes, some details about
the architecture of the models that are used. It's really nicely formatted in a readable and
approachable way. It provides a lot of detail, but it organizes it in a
way so that it doesn't overload you with
unnecessary detail. For example, if you
were to start with these figures at
the very beginning, that might be pretty
overwhelming. But instead, by giving
you a Quick-start, it helps you just jump in without getting
completely bogged down in the details and you can start interacting with their
code very quickly. This repository contains
all of the code and data needed to repeat a LSTM, so long short-term
memory type of neural network sentiment
analysis on some text. I actually have no
idea what the text is. It's not important. It's
just a good README. What we have here is a title, a brief description
of what this is for, so what are some of the
other resources this is connected to and why
do you care about it? It gives you instructions for
how to download the data. This is an example of some place where the data is not part of the GitHub repository
because the dataset is quite large and GitHub repositories
have file size limits. I'll talk a little bit about this use case in
a couple minutes. But they tell you
how to get the data, they include the code for it. They give you some requirements, so they tell you precisely
what you will need. This is one way to do it. You can also include a
requirements.txt file, is a way that some
people do this, but you can also list them here. These are dependencies, they're making sure you have
everything you need. They talk about a
Docker container. Docker, some of you might
be interested in Docker. Docker is giving
you a container and a little environment that has everything installed
and ready to go. It's much easier than having to install all the
dependencies yourself. Again, not required that
you use Docker containers, but if you're somebody
who likes them, this might be an example
of how it works, and there installation notes. This is basically telling you everything that you would need to work with their code and how to get the
dataset that they used. What they're turning in here is actually just a
Jupyter Notebook. They don't have lots and
lots of Python scripts, they don't have
anything very crazy. It's a Jupyter Notebook,
and that's okay. This is well-presented
because basically, all you have to
do is navigate to the Jupyter Notebook
and they're helping you get all the
dependencies so that it actually works when
you open it up. Here we have the code for a
web app and an API that uses a machine learning model
to take pictures of basketball shots and estimate
the pose of the shooter. They've got information
here all about the code and the library and
this is in their Read Me, they've actually
got a little demo. This is an amazing way to get across without reading
a single thing. Why you care about this library? Why is this cool? It's just immensely neat to see this when you first
get to a page. Sometimes a picture or I'm
not sure if this is a GIF, a move or whatever
video file it is, but this is really valuable. Then they give a
little bit of detail about what exactly what it is. Here we go right into
getting started. Pre-requisites, they're doing
a pip install requirements, this repository
includes a text file that has all the
requirements in it. It mentions that you're
going to need a GPU. It's okay if your
project requires a GPU, but it's nice to note
that in the Read Me. Then if you were
to run their app, this is some instructions
about how you would do it. They also included a
Google Colab notebook. A Google Colab
notebook is basically a Jupyter environment
that has a GPU attached, they are free, it's a
wonderful resource. Creating a Colab notebook
can be an excellent way to provide access to your work to people who don't have a GPU. Then they go into some of
the project structure. There project has
quite a few parts because it not only has a model, but it also has an app to it. This diagram is a bit intense, but it does help for somebody who wanted to recreate this. Then it tells a little
bit more about how the analysis works and
more cool pictures. You've certainly
do not have to do this level of detail, but man, this is really cool
to look at and I suspect that this level of explanation here has
a lot to do with why so many people
like this repository. Here we have a repository for a time series
prediction problem. What we have here, in
addition to the code, we've got a link to the dataset. Then we have a description about how the dataset was created. A bit of documentation and also some details about some of the cleaning that's
happened in the dataset. Then we have some
documentation about what are the files that
are inside this project. These are listed,
they're.PY files, not a Jupyter Notebook. Then there's some description
of what each one is, what's contained in it. This is pretty useful. As you might notice here,
there's not any sub-folders, these are just what
the files are. This person is just helping us understand basically where
we would want to go. Now we know, okay, I was interested in
the XGBoost model, so this is my file. ARIMA, that's my file. She also tells us which ones are executable files,
which is handy. Some notes about
the environment, and then some modeling
results are pictured. Here the use of picture
shows us basically what kind of output should we
expect from the model. What's really
useful here is that she's telling us,
without writing it, this image is telling us this is the output that the
model produces. This is also roughly about
where the performance is. To me, this is a
pretty effective use of graphics as documentation. Then there's quite a
few. Then I also really like that they've
included a future work. Saying what are
some things that we want to do in the
future opens this up for people who
are interested in contributing down
the road to say, hey, I'd like to
contribute in this way. That is one of the
cool things about doing a GitHub repository. If you choose to
make yours public, then you might find
that there are people around the world who are interested in exactly what you've done and
want to help out. Now that we've seen a
few different examples of GitHub repositories
and their READMEs, you can see that there's
a lot of variability, but there's also a lot of
different ways to be effective. What I care about
is not the length, not if it seems simple or contains a lot of
images or movies, What I'm really looking for is, is it easy to dive
into your work and does it show why
your project is cool? I'm looking for you to
lower the barrier for me as a stranger to
jump into your work, understand what your code does and how I would
navigate your project, and also to understand why this is awesome and
deserves my attention. How you do that might be different than the examples
that I've shown you. It's up to you. If you go on GitHub and you browse around at
some cool projects, you'll see all sorts
of different examples. This is the bottom line,
what I care about. However you answer
these questions, just make sure that
you address them. One final caveat
that I want to get to is what if your
dataset is really big? You need your dataset
to reproduce your work. If you don't have a dataset
in your GitHub repository, it's hard to jump in and start
matching what you've done. But as I alluded to earlier, GitHub has some
file sized limits. You cannot put a 1
gigabyte dataset in your GitHub repository. What do you do if this happens? This happens a lot. There are a few ways
to handle this. One example is that
you could take a very tiny version
of your dataset, so you could just randomly
sample a 100 lines or a 100 images or whatever from your dataset and create
a little sample, so that that sample can
be run through your code. It means that people
won't be able to perfectly replicate
your results, but they will at
least be able to understand what
input is expected, what is the schema and
the data types that are expected as input and what
are the outputs of it, and that they can
make sure that all your code actually runs. Even doing this is useful. You can put your
dataset in storage. Just put it in the
storage platform of your choice, Google Drive, S3, whatever you like, and then share a code
snippet to download it. Assuming, of course, that
it is publicly accessible, you would have to
provide permissions but this is quite common. Another tool that you can use is something like Data
version control, which helps you create a link to your dataset that you store
in your GitHub repository. This is one example
of a tool for that. The last thing is
you don't have to publish your dataset
for the whole world, but it does need
to be available to the instructional
team if we ask. In other words, you don't
have to commit to hosting your possibly very
large dataset for the whole public forever
as part of this project. It's really up to you if you want to make it
widely available. But the dataset would need
to be able to be shared with the instructional team
like me if I were to ask.