We're going to be
talking a little bit about hyperparameter tuning. I'm going to be doing
a little bit of live white-boarding and then
a little bit of coding. Let's talk about what I mean
by hyperparameter tuning. Hyperparameter
tuning is a part of the modeling stage of the machine learning or
data science process. To put it in both the most simplest
and vague terms I can, hyperparameters are
anything that is not learned by a
Machine Learning model. For example, the central task of Machine Learning or data science is that
we're going to be fitting some model and we
don't know what it is. Maybe I'll say, I'm going
to fit a linear model. When we do this, the
parameters a and b are what is
learned by my model. But there are some
parameters here that are not learned by the model
and that's for example, the choice that we're going
to use a linear model. An alternative model could be that we're going
to add another term. It could be something like, let's make this a
quadratic function. The choice of model is in
a sense a hyperparameter. Additionally, things like how flexible is the model
training going to be? Do we want to be resistant to learning
these terms a and b? That's something called
regularization and I'll talk a little bit
more about that as well. There are some models that have really interesting
hyperparameters. For example, when we
think of neural networks, neural networks are
nested functions. What we do is we've got
these layers of neurons, let me go like this. I'll just draw a little demo, a little practice neural network. This is like the canonical
neural network diagram. But this is a neural network
here that is going to have one hidden layer right here. But, the important choice with neural networks is how many
hidden layers do I want? I could in fact make this
have two hidden layers. That tends to do pretty well. This is a hidden layer too and imagine that these
are all connected up. Neural networks can have as many hidden layers as we want. The number of layers as well as how many nodes per layer there
are, are hyperparameters. That won't change
throughout the course of an individual run of the
model training session. If I try to train
a neural network, it's not going to decide, I add some more nodes to it. You set that ahead of time. This choice is important
because of the more hyperparameters have a lot of control over how
complex a model is. In neural networks, when you make a neural network very deep, that's something that is very big in computer vision science where you use a
lot of parameters. You get a model that has a huge number of parameters
that can be learned, which is like in
this example here, how I go from having two parameters that
are possible to learn, to three parameters, the A, B, and the C. Then neural networks tend to be at the other end of this extreme, it's like "Okay, you're going to learn
a million parameters or 20 million parameters, probably more than too." If you're not totally sure about neural networks or
what they are, that's okay. The main point that
I want to make here is that hyperparameters are something that you'll
have to explore a bit. Often what's regulated
by a hyperparameter is how complicated
can my model get. I want to draw up a
picture that I think is helpful when thinking
about hyperparameters. A good way to think
of hyperparameters is that they very
frequently have to do with how flexible
our model is. Let's do hyperparameter. I'm just going to put
some parameters here. We don't know what
it is, it could be how many parameters
do we have in the model, it could be how many layers
in our neural network, it could be something
completely different, but we'll just imagine some kind of
hyperparameter right here. Then on this axis, let's do a very vague quantity of how much do I like my model. Without going into too
much rigorous detail here, let's just say that
liking your model is a balancing act
between my model learns as much as possible
from the training data and my model still performs
well on new data. We can imagine that
if you're model learns everything about
your training data and really sticks to it, really commits to it, that then you're going to not do well when you
generalize to new data, but also a model that doesn't learn very much about
the training data might not contain any
useful relationships to carry over to new data. We've got this function
that might look like this, that there's some spot where on one end of the
hyperparameter range, our model doesn't, it's not interesting enough to
learn anything cool. This might be like I just picked a linear model and
this is garbage, it can't learn anything
interesting because my data is not well
described by a line. On the other end of it, you're model is crazy complicated and can learn any
relationships that you might need but it doesn't
generalize well to new data because its so over-learned to what
you trained it on. There's going to be some
sweet spot and of course, I've oversimplified this and I'm drawing this in a
single dimension. Now you have to imagine
that there could be many of these dimensions
because there might be many possible hyperparameters. Just to make this a
little more interesting, sometimes models
don't look like this, but sometimes the model
happiness space can look like this and it can
get a little bit weird. A lot of times, and most of the times
in machine learning, figuring out how do I tune
these hyperparameters, how do I know where the
best possible value is, is not something that you can do. You can't solve it like algebra, it's very rare that you'll
be able to do what's called an analytical solution
where you can solve it. A lot of times you really
just have to get into your code and try
a lot of things, you just have to experiment. I'm going to show you a
little code example that I made to try to explore this. I'll share this code. What we're going to be doing
is a Scikit-learn problem. I'm making some synthetic data. This is a classification
problem that has two classes and they are separated but
it's synthetic data. Two classes, some noise, doesn't correspond
to anything real. I'm going to split
the data up so this is a Scikit-learn
function just to divide and to train in test sets. What I'm going to be fitting is a logistic regression model and that's one of our default, one of our simplest
kinds of classifiers. You might not know this or
you might but by default, in Scikit-learn
logistic regression comes with a parameter C, and C is a constant so it's not learned
during model training. When I'm running this method here, logistic regression fit, you're not fitting
C, you declare C ahead of time and that's what
makes it a hyperparameter. In this case C is a constant that controls
regularization. If you're not familiar
with regularization, what it means
rigorously, that's okay. The big picture
view of it is that when the number is really, really small, the coefficients, the numbers that are learned by our logistic regression are
extremely free to vary, they are very
sensitive to the data. On the other end,
when C is very high, we're saying no, no, no, no. We really want to be careful about if we're going to
update our coefficients. Should we be learning
anything from the data? Let's be cautious and we're
not going to budge that much so on one end it tends
towards it will learn, it's very flexible to
your training data. On the other end, this is a number that
if C is really high, your model's not going to
want to learn anything Let's see how I did this. I want to try to find what's the best
value of C. Scikit-learn, they'll give it to
you automatically. It will give you a value
of C automatically, but maybe I want to know what's
the best possible value. I am going to just make an
array of possible values. This is in log space
per [inaudible]. One to the minus five to
one to the second power. I'm just doing a 100 of those. I'm just making an array that is linearly spaced at log space. Then I'm declaring
an empty array for the purpose of keeping scores. I'm going to loop through every possible value of
the hyper-parameter. I'm going to loop over them. I'm going to fit the
logistic regression model with that value of
hyper-parameter, on our training data. Then I'm going to test
it on our new data, our held out or test data, and I'm going to
report the score. Then I'm going to
find the value of C that has the best
score and print it out. Hang on one second. If I run this, then it will find for
me the best value over the grid that I specify
here for our model. By best, I mean, it's performing the best
on my test to data-set. Something to note here
is that if you're going to be looping over, or considering many
possible hyper-parameters, and you're evaluating
them on your test set, in a way you are fitting
to your test data now. This test data that you
do this looping on, cannot be used anymore as your ultimate view of how well my model do you
want completely fresh data. You will want to actually add a third set of data that is not used at all in your hyper-parameter search
to evaluate your best model. Just a tip that is a way
that you can actually have some leakage and you can overestimate how well
your best model does. Now, writing a four loop like this is not desirable always. In fact, I would say it's
probably rarely desirable. I have done this here
for educational reasons, but you'll probably
want to look at, if we're doing this in
production or in a real project, you might want to
look at some methods. Scikit-learn has a lot
of tools for doing hyper-parameters search
in a pretty rigorous way, but also a very contained way where you don't have to write
the four loops yourself. It can do that under
the hood for you. There's also a lot
of software tools for doing hyper-parameter
searches. I have a suspicion that in
the next couple of years, hyper-parameter search
is maybe something that is largely automated. In fact, the more that you can be methodical and do
a rigorous search, whether that's a four loop
or a more advanced tool. I think you'll often find
that's a pretty good way to go. It's at least more scientific
than just doing it. Just putting in couple
parameters yourself. The just [inaudible]
hyper-parameter tuning is that you want to explore the space
of possible models. You want to explore something sufficiently
broad and you want to find where your model is doing the best between balancing, fitting and learning the most interesting and
informative details about the training data with generalizing well
to held-out data.