We've done the
introduction to pipelines. We're now going to get into
ML Pipeline specifically. If you remember from the
lecture on pipelines, we have a set of steps. The steps are stand-alone and the steps may have
input and output. We're going to dig into what does this look like from
an ML perspective. Here at Bob and Digital,
the University of Michigan, surprise has come to us and wants us to build
a prediction tool. Given a list of courses
that a student has taken, what courses are they
likely to take next? You look at this and
you start thinking, "we can build that, that's
a fairly simple thing to go through and
figure out how to do. We need to get some datasets
and we need to slow down." As technologists, we often start thinking about the technical
problem in front of us. This is not just
an ML discussion. When technologists are presented with a technical problem, we start thinking through the technical solution
to the problem. With ML specifically,
we really have to have a deep understanding of where does this data
come from first. We really have to
understand the data. I've seen people who will
actually shadow people who work with the data so they get a good understanding of it, but we really have to understand
where it comes from and how it travels through whatever system
we're working with. But also as a technologist, we need to take a step back. What are the actual
business goals.. If I put this problem
in front of you and I gave you a set of data, you could pretty much sit down, build the model and throw
some predictions out there. One of the things you
need to ask is why. Why is this organization
doing this? What are the goals? Most importantly, how
is success measured? In classes, we measure
success on accuracy. Was your model able to accurately predict
the information? Accuracy is really important, but that's typically
not how success is measured from a
business standpoint. When you start with a problem, really have an understanding of the data you're
getting it from, the data you're working with. But also, what is the
measurement for success? Where does the measurement
for success come from? One of the things is this is how you are evaluated effectively. If you build an awesome model, but it takes forever to run a prediction so that
customers actually leave the website because
the model is taking too long to provide data back to the customer, that's
not successful. Your model may be perfect from our accuracy score
or near perfect, but if it doesn't work
in the business context, it provides no value
to the business. Really understanding how
this works, what's measured, oftentimes, businesses may say, "you know what, we want
to recommend things." Think about, let's say an auto parts store, and
it's the same thing. Customers who have
bought these things, what are they also likely to buy? Same type of problem. The auto parts store may not actually want an accurate model. They may want you to
come back and say, "Yes, predict what the
customer may need but sort it by the
highest priced items, or even better, the
highest margin items. Because that's where
we make our money. Really make sure when
you starting to work on a pipeline process that you understand what the
business goals are. In homework assignments,
it's pretty obvious what the assumption is and
what we're aiming for. We're looking at a
score in the end. But in the business, goals
may not line up that evenly. Let's go back and look at
our first problem though. Understanding where
the data comes from. You know, as students at the University of
Michigan that we have as the University
of Michigan, a large amount of data on you. That data comes from a
variety of different systems. In a organizational environment, we typically refer to
this as a data warehouse. We have all the data
that the business generates in a place
that's easily queriable. Let's put the University
aside for a second. At Borromini, for example, we have in our data warehouse, data that comes from all of these different
locations: the HR system, our inventory systems, the website accounting
data, marketing, e-mail. When we e-mail our customers, we know when they
click the e-mail, how long they spend
looking at the email, if they click links in there, all sorts of data on customers, keycard access to the building. Who came into the
building one and along with that, the visitor log? Borromean tracks
every slack message you sent and every
email message you send. We have customer
demographic data, we have customer order history. This list goes on and on and on. What we do is we take
all of this data from all these different
data systems and we put them in one system that
makes it easier to query. If you think about
this for a second. If at Borromean you
needed to query the key-card access
to the building, the HR system, and the
customer order history. That might be four or five, maybe more different systems that you would have
to get access to, learn how they work
and query them. If they're all
databases, that's easy. But what if they're all
different types of databases? Maybe HR is stored in SQL Server, the inventory system
is in Microsoft's, is in MySQL, the key card an Access database is in some system
called Ingress SQL. Like there could be
all different systems. Slack and e-mail
messages are stored as JSON files on the server. What a data warehouse
lets us do is that we assemble all these
different types of data from around the
organization and we drop them into one system
that's easy to query. Typically, this is done as what's called
an extraction job. We take the data out of
a system and we put it into the data warehouse in
a format that we expect. We also do things
like standardize. HR inventory may store
dates in one format, whereas the key card system
stores in a different format. The data warehouse
has a format that we move all of that data into. This is actually a
pipeline in itself. It's multiple
pipelines that pulled this data nightly into
the data warehouse. Most large corporations
have data warehouses. Why is this important? Why am I spending all this
time talking about this? Well, your data
warehouse will have way more data in it than your
pipeline should know about, and you will end up, if you're not careful,
with data leakage. One of the things to
think about when you were building your pipeline is, what data will you have
available to you at the time you want to
make your prediction? Think about that.
That's why it's really important to understand the flow of data through your system. At Michigan, this is a sample
of our data warehouse. Our data warehouse is
built into two parts. One shows how the data relates to itself and this is out of the student data warehouse here. There's actually four
different data warehouse actually there's more than four, but most the time
you're only using four of the data warehouses
here at Michigan. This is out of how the
data relates to itself. This is an ER diagram. Further down, we have every single field and a
description of what it is. Some of this is obvious. First name, middle
name, last name. Some of them is not obvious, and so there's a
description behind it. This information is
actually public. It's a 183 pages long for
the student warehouse. I don't encourage you to
read all a 183 pages, but take a look through
it and see where all we have all these different data and how it's been assembled. Now let's talk about
building our first pipeline. I'm going to go through this
again in a high-level steps. Really, there's two pipelines. We have a pipeline
that builds a model, and we have a pipeline where we run our model in production, and the goal here is to use as much code between these
two systems as possible. Up until now in classes, you've done a split on your Data where you
take a set of data out and you have your testing data and
your training data. You train your algorithm on the training data and
then you validate or you test the training of your algorithm using
the testing data. I recommend that we actually
have a third set of data for pipelines and
that's a validation set. Before you get into actually
building your model, split your data off, and take a chunk
aside for validation. You're going to build
multiple models as part of your pipeline process
and we want to make sure that we are
validating all of the models off the same
validation data set. That way we can
compare across model. Effectively here are two
different pipelines, and this is from your building machine learning
applications book. We've got our training pipeline and we've got our
production pipeline. If we break this
down a little bit, this is actually coming
from the ITIL framework, where most projects have three different distinct
phases: the planning aspect, the building aspect and
the running aspect. In large organizations,
these are actually different teams that may
be responsible for this. At Bodmin, for example,
our production team, our operations team, is
separated from our developers. Our developers build things, our operations team is
responsible for making it run on our servers and
production for our customers. Our first step in
any pipeline is to figure out where our
data is coming from. In our building phase, we typically are getting
a dataset handed to us, probably from a data warehouse. It may be that we execute a live SQL query against
the data warehouse. It may be that somebody
gives us a CSV file. But we're going to get our
data from the data warehouse. In the running aspect, when our model is in production, predicting student courses, data is likely coming
from a user interaction. A user goes to a web page, we quickly pull the history of the courses that that user took, we pass it to our model, and we get a
prediction out of it. There's also a way to run a model in production
in batch mode where we might pre-generate results every night at midnight, say. But for the most part, we're getting data from different sources and it may
be in different formats. So what's next? We
clean the data. Whatever we're doing
to clean the data, we want to make sure
we do it exactly the same or as close to
exactly the same, in both our build phase
and our run phase. We really want to
make sure that we're not introducing data errors by doing something differently. Now, having said that, if you're getting your
data in different formats, obviously you're going to have a slightly different
process here. If your data for your
build phase is coming from a SQL query and in your run phase it's
coming from an API, you're going to have a
slightly different mechanism to acquire the data. But you want to clean your data in the same manner
in both systems. Then we're going to preprocess
it and here's where we do our feature tuning,
extraction, and construction. We also might have to encode our features in a
different manner and the important thing here is use the same code in your run phase. Otherwise you will be
introducing errors or anomalies when you run your model in production than you had in your build phase. Then we're going to
build the model. In the build phase this is
significantly more work. You're probably going to have
to do this multiple times. You're going to get a model,
you're going to test it. You're going to see if you
can get the best results. You may use the validation data and then you're
going do it again, and again, and
again until you get the data or the scores
you're looking for. In the run phase,
this is pretty easy, load the model, run the data. Here they differ a little bit. Then there's this
post model phase. Once you've got your model and you're ready to go with it, what do you do next? Once you have your model
and you're ready to go for it, what do you do next? What's the next step? In the building phase, you're done, you save it
off and you're complete. In the run phase, you
may need to change the format of the data
before handing it back. If your data is exporting things, if your model exports things, as a variable you might
have to change it, it may be a categorically
encoded variable so you may need to make it a [inaudible]
and then return sum JSON. Ongoing, this is probably
one of the important things, is make sure that your model
is still performing well. You've got your validation data, test that validation data. But also take real data and make sure that you're
getting correct data. We're going to discuss this in a little bit more
detail later on. But trends model shift over time and so if you're
keeping your model static, what happens in a year
when trend shift, if there's a brand new
popular class that gets introduced and students start taking that but your
model wasn't aware of it, your model is no longer accurate. Make sure you ongoing test that your
validation is working, test that you're
getting good data.