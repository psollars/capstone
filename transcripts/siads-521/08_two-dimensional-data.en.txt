Let's use matplotlib to
explore a bit of data. I'm going to focus here on just using
the library to build basic charts that you're probably already familiar with
then, we'll go into some more in-depth examples of how to do visual
exploration of data and data analysis. So first, let's bring in matplotlib and turn off that Jupyter
display figure magic. So matplotlib inline is our
Jupyter magic function, so that will render the actual
images right into our notebook. Then we're going to import the pyplot
scripting layer as plt and then from IPython.display I'll import
the set matplotlib clause, and we'll call this function
to actually close it. And we won't always do this, but I want to
make sure that we're talking about the matplotlib architecture
as we go through. And of course, we're going to bring
in pandas and numpy as well here. So important pandas as pd and
import numpy as np. We're going to quit using just
some arrays of fake data. Now, the data I'm going to use is
something called Anscombe's quartet. It's an interesting historic data set that
was used to demonstrate the importance of visual exploration. You can read more about it at Wikipedia,
and I've left a code I use to get
the data from Wikipedia below, but I've commented out and just left it for
you to see if you're interested. So what I actually did when I was building
this lecture is I called pd.read_html and I put in the URL to the Wikipedia page,
and it actually has a table of the data, and I wanted to skip the header row and
just take the first table. And then I overwrote the columns, so I
just wanted to make the columns names for four different data sets. So x1, y1 is one, x2,
y2 is another one and so forth, and then I saved it actually as a CSV and
uploaded it to the Coursera system. All right, so let's read that in, so
df = pd.read_csv("quartet.csv"), and let's take a look at what
actually is in this data. So the point of Anscombe's quartet is
to demonstrate that certain summary statistics might look the same or
nearly the same between different values. But then when we graphically examine them,
they look quite different. For instance,
let's calculate the mean of each column. So we could do this,
if you remember your pandas as df.agg and then we'll just pass in the numpy
np.mean function. So we can see that the mean of all the X
values is all identical, 9.0, and that the mean of the Y values are all identical
as well, just a smidge over that, 7.5. So let's check how correlated the X and Y values are between our
four series of data. So for this we'll import
scipy.stats as stats, and then I'm just going to iterate through
all of our different values, and I'm going to print the Pearson R
statistic for this. So here I'll just have a string
with a couple of curly braces and I'll use the .format, and I'll pass in I,
which one we're at one, two, three, four, and
then I'll call the stats.pearsonr. Remember that we could do this by
passing in two columns of data that we want to compare from our data frame. Okay, so
even the correlation between the X and Y values across the series
is almost identical. And it turns out that there's a number
of other statistical properties such as variance or the fit of a regression
line that are all very similar as well. However, we can often visualize many
different kinds of variation at once, and plotting these points can
produce even more insight. Let's check that first series. I'm going to plot this as a scatter plot. To do so we just pass in our X and
Y values as the first two parameters. We could also add a third parameter for
the format of the points to use. This follows a sort of
mini programming language. Right now I'm just going to use g.,
which means a green dot. So first we tell pyplot we want to create
a new figure then we say plt.plot, and I'm going to add our X values and our Y
values, so two columns, one Xs and one Ys. And then I'm giving it this formatting
string saying I want these to show up as green dots, and now let's call plt.show. Okay, great,
a bunch of seemingly random dots. Now, I'm going to plot the next
three series as well, and I'm going to change the color and
the marker type then re-render the plot. And you can check the docs for
more details on color and marker shapes. So I'm just going to call plt.plot. Remember, once we've got a figure and
we've got data, subsequent calls to pyplot will just plot
the data on top of that as long as we've turned off that cell magic function
at the beginning of our notebook. So I'm going to plot X2 and Y2,
and I'm going to make this RO, and then I'm going to plot X3 and
Y3 and make this blue plus sign. And then I'm going to plot this last one,
X4 and Y4, and I'll use K, so
that's black from CMYK color space and a little down triangle thing,
and I'll go plt.show. All right, wow,
this looks quite different. So let's change the size of this figure
to get a better sense of what's actually happened. So to do that we call
plt.get current figure, plt.gcf(). Remember we don't have to keep this fig
object around because we can always call pyplot and just get the current figure, and I gotta set the size of it in inches
to 12 by 7, and then let's show that. Okay, so that's much easier to see,
and we can see some patterns. In particular,
we can see that the fourth series, the black triangles,
have one really strong outlier. The red circles, which were the second
series, I think, form a gentle curve, and the first series was
scattered all around. And the blue plus signs,
which were our third series, are mostly in a horizontal line. Despite the summary statistics
looking very similar, we see that there are very much different
relationships between data points. And that's one of the reasons we actually
engage in exploratory data analysis. Now, what does this mean for
a given analysis? Well, that really depends
on what the analysis is of. If the x-axis was the predicted grade of
a student in the y axis was the amount of time they've spent on a given task. I would probably come to different
conclusions if I were looking at the black triangles, which suggests the time on
task is pretty meaningless except for that one individual in the upper right. Than if I were looking at the blue
plus signs, which suggest that for the most part people benefit from even
a small increase in time on task. So here's a couple of things I might
want to do to improve this visual, if I were looking at it. First, let's clear the axis. So plt.cla, clear the current axis,
so that's going to wipe out our data. Then I'm going to plot my data, and
here I'm also going to add a label from my data, which is a lot more meaningful than
talking about X1 and Y1, and so forth. So I'm going to plot x 4 in particular and
Y4, this last data set, the block triangles, and
I'm going to set this to subject pool 4, let's imagine we've got
some experiment running. Then I'm going to plot X2, Y2, and I'm
going to make these blue pluses, and so I'll set that to subject pool 2. Then I would add some descriptive text, so I might add a title like relationship
between grades and effort. Then on the X label I might
set the student grade, in the Y label I might set
the effort in minutes. And notice how these are all calls to
that scripting layer interface, pyplot. So next, I might want to make
sure the legend is rendered, and in this case I'd set its location and
some graphical framing for the legend. A value of 4, which you can read about in
the docs means the legend should appear in the lower right-hand corner. So here again, I just call pyplot.legend,
I pass the location, I want the lower right. I say I want a little frame around it or
I don't in this case, and then I set the title
of it to a legend. Now let's render it and
see how that's changed our plot. Nice, so
this plot's looking meaningful and useful. You can see though that there's lots of
little options in matplotlib in order to build the kind of plots that
you might be interested in. Let's move on to discuss another kind of
two-dimensional data plot, the line plot. Now, in matplotlib, the pyplot scripting
interface, this is actually the same thing as a scatter plot, it's just the points
in your series are connected by lines. Let's close our previous figure and
create a new one. So we call plt.close to
close off the figure and plt.figure to create a new one. Now, let's bring in a set of data points. Here, I'm going to create the set using
the numpy distributions for my Y value. So I lied a little bit, we're going to
use a little bit of fake data. And first I'm going to bring
in some exponential values and I'll sort them from lowest to highest. So here for my Y values, I want to
bring in numpy.random.exponential. I want to grab 100 exponential values, and then I'm going to actually make this
a sorted list, and then I'm going to use the Poisson distribution for
my second set of Y values. So again, I'm going to sort it,
np.random.poisson, and I'll bring in 100 values and
sort those, and we're going to look at what
these distributions look like. Now, the X values are going to
be the same for the plots, and this is just a set of
linearly increasing values, so we could just call numpy.arange 100 values,
and now we'll just try and plot them. So first, I'm going to plot the first set,
so all of our X values versus Y1, and then plot our X values versus Y2, and these
will plot them on top of one another, and we'll see what the plot looks like. All right, see,
this is very easy with matplotlib. It's going to try and plot things automatically as a line
plot unless you tell it otherwise. want to turn this into
a scatter plot instead? Then you just add a marker type
as the third parameter on plot. So we'll clear that axis. I'm just going to make the exact same plot
call, but I'm going to add that marker type at the end for each, so dots and
and O's, and then plt.show that. All right,
let's look at a more realistic example. Let's say I wanted to compare
the temperature in January and February of 2018 and 2019 in Ann Arbor. So first we need to get the data, so
I headed over to the NOAA site and I downloaded it from there, and
here's a URL if you're interested. And next we need to bring
in these two data sets. So we're going to use pandas again. So I'm going to do pd.read_csv,
and this is the 1892, 728 was my data file for
2018 data and 1892, 713 was my data file for 2019 data. Let's just look at that 2019 data. All right, so these are just data frames
from a weather station at the airport. And we see there's a bunch of missing
data as well as our TMax and Tmin for maximum and minimum temperatures. So let's join these data frames together. So remember, it's just pd.concat, and we
give it a list of all the data frames, and that's just going to merge them all
together by appending the rows. And let's reset the index since concat's
going to use the original indices. And these are kind of meaningless now,
that is, we'll have two that have the same index
of zero, so we'll just reset that index. Now, let's pull the year out of the date. You might remember that
this is actually easy for us to do with the str.extract
function of the data frame, and this takes in a regex which
we can merge across these. So we'll do df = pd.merge,
we'll pass in the data frame, and then we're going to take
a df.[DATE].str.extract, and I'm going to pass in
a fairly large regex here. I mean, this class in particular
is not about doing the regex, but I want you to practice
those kinds of skills. Those skills are very easy to lose
if you don't practice them, and you'll be doing a lot of
this with data cleaning. So here I'll have a capture group for
year, and I just want the first four characters, then the next character,
I don't care about, so I won't capture it. Then the next five capture entries are for
the month and the day, and we're actually doing this merge on
left_index = True and right_index = True. So what I'm doing here is I'm taking
two columns out of the DF date, so they're being projected,
the year and the month, and then I'm merging them back
together with the DF. So I'm merging on ourselves, because remember we have the same index
when we do projection with extract. Okay, now, let's just keep our Max and
Min columns for temperature as well as our new data. So I'm just going to get rid of
a bunch of data that we don't need and I just want year, month,
day TMax and TMin. So let's take a look at what we have now. So maybe take a moment to go back and make sure you understand
how that merge is working. Your skills with pandas will
really help you clean data and visualize it fast and accurately. Okay, now let's set up our figure, and
we're going to do it one year at a time. So let's make a new function. So we'll def plot_temp and
we'll just take the year-end. So first we'll close the existing
figure if there is one. Since we're using the scripting
interface we just call plot.close. And then I want to call plt.plot and
I want to pass in the data frame where the year is equal to
whatever the year it was here. Then we're going to drop NA, remember
that when we call the where function this way as opposed to the indexing operator,
we get NA, so we want to drop the NA, and I just want the TMax value. And then I'm going to set the label
to be the maximum temperature and I'm going to add the year to that. And then right after that I'm going to
plot the minimum temperature value as well, and so
that code looks almost the same. We're going to add the legend, I'm just going to copy the legend
code from above and we'll just add it in the lower right at the same time,
and I'm going to add some axis labels. And again, I call these a write directly
on the pyplot scripting interface. But underneath, that's looking up
the current axis for the figure, and then for X label it's looking up the X
axis and setting the label there. So if you wanted to use the object
interface, you can definitely do that. As data scientists in these computational
narratives, these Jupyter notebooks, we often don't, and we often use
the scripting interface instead. Neither is right or wrong, they're just
two different ways to interact with your visuals, and let's call plt.show and
take a look at that. All right, now let's see what
this actually looks like for 2018 now that we've defined the function,
so I'll call plot_temp("2018"). All right, not bad. A couple of adjustments though,
let's make a few tweaks to the display. So I'm just going to copy
in that same def function. So first, we're still going to
close that first figure, but this time I'm going to manually create the
figure so that we can set the size of it. So when you create plot.figure, and you
don't have to do this, you could just go plot.plot right away and the scripting
interface will do it for you. But here I can set the fig size in inches,
so I'll set it to 10 and 6 just to make a little bit
better use of our screen. And then I'm going to still plot the data
on it just like I did before, so this is just copy and pasted from above. So now let's give matplotlibs a little
bit more freedom as to where to put that legend, and so we could tell it to put
it wherever it feels it's appropriate. So we'd call plt.legend and we'd set the
location to 0, turn that frame off still in a title, and 0 really just means
put it in the best possible place so you're not overwriting data. We'll add some axis labels,
so that's no different, but this time we're also going to
add a title as well. Now, there's a handy function on
the axis object which allows us to shade the area between two series of data,
and this will really help us see
the size of the daily min-max swing. The general function signature is
fill between followed by X values and Y1 and Y2. So to do this we need a list of the X-axis
values, which is our day in this case, the minimum value and our maximum value. And this is actually pretty easy since
we can use the data frame index for our X values. So I'm going to call the scripting
interface plt.gca to get the current axis, and
then call the fill_between function, and I'm going to tell it that I want to
do it where, just along our index, right? So df.where, I'm just going to grab
our data, drop our NA and just ask for the index out of that, and
then I'm going to do the same thing, but just for TMin and
another one just for TMax. And actually it probably would have made
more sense to filter this data frame first in the two years. Right now we're seeing that maybe my
decision to bring both of those data files together was kind of questionable,
and that often happens. And then I'm going to set the face color,
so what the actual rendering looks like
to blue, and set its alpha channel so that it's not a deep royal blue,
but it's semi-transparent. So there's a lot of things you can
do to control what it looks like. All right, now let's render it. So plt.show, and
let's give it a try with the 2018 and 19 data, so plot_temp ("2018") and
plot_temp("2019"). Nice, okay, so let's touch on one more
thing with matplotlib and Jupyter. Remember how we're starting the notebook
telling Jupyter to set matplotlib close to false? So what happens if we leave
it as the default, true? So let's turn it back on,
set_matplotlib_close(true), and this is the default,
this is what you'll normally see. And then we'll just plot the temperature
for 2018 and plot the temperature for 2019. All right, well,
in this case not much happened. So everything works well and
as expected, but underneath, Jupyter has closed off the figures and
they're no longer available for editing. For instance, if we look at the available
fignums we'll actually get an empty list. So if we go plt.fignums, now remember,
we did this in a previous lecture and it returned back one because we had a figure,
but now it just returns an empty list, because Jupyter has actually
closed our figures on us. Similarly, this means that the current
axis in the current figure now are no longer existing, so we can't
update the figure, and when we try and get the axis, actually,
pyplot creates a new plot for us instead. So when we call plot.gca, it actually
creates an empty axis and inserts it into a figure and tries to render it for us,
but then actually closes that figure. So this can actually all be a source of
frustration if you're iteratively trying to build up the display of a plot. But the upside is that it eliminates
the need to call show and then you can plot data in just one line. So for instance,
with this setting I can say plt.plot, give it some set of data that I
wanted to look at, some X values, some set of data,
we'll call these exponential Y values. And in just this line I've been able
to build a plot and render it in line. So this is something for you to be aware
of as you move forward with matplotlib, and it's really the difference
between scripting and trying to do more of software engineering. In this video, you've been given a brief
introduction to using matplotlib two-dimensional data using scatter plots. Now, we actually covered a lot, from the methodological showing why
you should engage in the visual exploration of data using Anscombe's
quartet as an example down to the brass tacks of how to engage in the exploration
using the matplotlib toolkit. As you've seen there's a lot of different
parameters that you can use with matplotlib to control
the way a figure is rendered. To explore this further I recommend
that you check out the docs or check out the reference textbook for
this and play around a little bit with how to
style and transform your graphics.