In this lecture, we'll be using another case study to help us learn about probability plots. In particular, we'll be
focusing on something called the Quantile
Quantile plot or the Q-Q plot which is a graphical
technique that allows us to see whether a particular dataset follows a given distribution. We'll also discuss how to use
statistical testing to make quantitative determination
as to whether or not a sample is indeed drawn from
a particular distribution. Now, one of the benefits of
learning data science from a large public
research university like the University of Michigan, is that we're not just able to share with you the
different techniques and approaches but to ground these techniques in our own work. As you might know,
my researches in area broadly known as
Learning Analytics, which is the study of learning, teaching and education through data and analytics process. So in this lecture, I'm
going to share with you some of our own
work about how we applied visual exploration
and statistical methods to understand and evaluate
educational predictive models. So one area of current
interest in education involves developing
predictive models for use in early warning system. So for instance, we
might want to predict how a student might perform in a class and identify
those who are at risk of dropping
out of that course. The hope is to use this
information in order to stage an appropriate intervention
and to provide that student with the necessary support
and resources to succeed. At the University of Michigan,
we actually have one of these systems called
Student Explorer which was created by a
colleague of mine Stephanie Teasley who's on the faculty here in the
School of Information. You can see from the
diagram above that it uses a number of different features
about the student grade, the course average and the
site page views to classify the student into one of three categories and it
uses the sort of red, yellow, green signal
light metaphor when communicating this to advisors depending upon the risk category
that student is in. Now, many of these early
warning systems used private and sensitive student
data such as your grades, demographics, online activity
logs and other details. As a result, there's a number of different concerns about
how this data is being used and the potential implications including questions about
personal privacy concerns, legislative oversight and risks and inequities that come
up in these models. So while universities have
many protocols procedures and structure to keep your
data safe and secure, numerous news stories on data
breaches certainly do not instill confidence in people regarding how data
is being managed. For education specifically, there also pieces of
legislation such as FERPA which restricts how personal identifying
information is to be used and shared. So this brings you to the research issue that I've
been studying along with my PhD student Warren
Li and collaborator Fluorine Schwab who's faculty here in the School
of Information. Specifically, if
students explicitly opt out of having their data
sharing for these purposes, how would that affect
the quality of predictive models we
were able to build? Would that be bias to these models across
different groups like different ethnic
groups are students in different years of
their programs? Will the models degrade to a point where
they're no longer usable and if so how fast
with this degradation happen? So we started to explore this and we collected data
on how students would opt out and how it might affect the quality of predictive models. Now, I can't share the
original data and so we won't implement the Machine
Learning model here. Instead I've provided
a select subgroup of grades and
predictions that are models output which I'll use to demonstrate our
analysis approach. So let's load that
into a DataFrame. So I'll bring in pandas, numpy, we'll bring in the
pyplot scripting layer, we'll setup Matplotlib to do inline plotting and we'll read from this pickle file
my DataFrame of data. So we're also going to
define two constant lists, one for the valid letter
grades and another for the corresponding grade points and these will be used later on. So we have valid grades. So that's an A down
through an E and then we've got numeric grades starting at 4.0 all the way down. Now, let's look at the DataFrame. So here's our DataFrame.head, and you'll see that the index is pretty meaningless and there's
a bunch of actual grades that a student received
and a bunch of predicted grades that
our machine model actually generated. Now, we need a way to organize this information and get
a visual sense of what our model is doing
especially because there's a lot of different
observations in this set. To do so, let's plot the actual grades students received against their
predicted grades. For example, how many times did our model correctly predicting A when the student
actually received an A versus a B and so forth? So to do this, I'm going
to build something called a confusion matrix. I'm just going to dump
a bunch of code here, talk through it but I
don't expect you to necessarily to understand
this right off the get-go. So build a function
plot confusion matrix. The first thing that
I'm going to take in is the actual matrix itself, CM. So this is actually going to be a square matrix so it'll have columns A through E and it'll have rows
A through E as well. I'll take in classes
as well and these are a list of appropriate
data values, essentially the row and
the column headers. We'll pass in a
normalization flag as well and by default
we'll set this to false. The first thing I did,
we actually changed our confusion matrix and
made sure everything was in floating point numbers and we didn't have any
odd objects there. So you can pretty
much ignore that. If the normalizing flag is set, we're going to change the
data values through scaling. So you saw this in a
previous lecture how to scale data and how
sometimes that makes sense. So here if the normalized set, we're going to take everything in our confusion matrix
and divide it by the sum across an axis
to scale all of that data. Now, the plot I'm
going to show is actually using a function called IM Show and it's actually used to show images
or pixel data. This basically is what's being passed in by CM since
it's a square matrix. So we can treat it as if it were image data or pixel data. We're essentially just
rendering a bunch of pixels to the screen
a grid of values. So use the pyplot
scripting layer.IM show. We pass in the
confusion matrix again, this is just a square matrix of floating-point
values if normalized. We're going to set
interpolation to nearest the just
cleans up some of what it looks like and we
set a color map as well and then I'm going
to add something called a color bar to the side
and you can read the docs on that if
you're interested. Notice the color map
that I've set, CM. So this is how we
actually want to show our values and while you can
read about this in the docs, my student Warren
actually likes blues and that's why we're using
these values in particular. Now, let's add a
title and some axes. So the title we're going to
call the confusion matrix. The y label is going
to be our true labels. So this is what we
actually observed in the wild and in our data and then the x label is what our predicted model
actually came out with. Again, these are going
to be our letter grades and we're going to update the x and y tick mark and we're again expecting these
grades to be passed in. So I just want to show
you how to do that, you've seen it before but it
doesn't hurt to reaffirm. So I make a bunch of
tick marks so there's just a giant numpy matrix and then I can actually plot on those tick marks the different
classes that are passed in. So that's A, A minus, B plus and so forth. I could set a rotation for the x label and then I'll
do this for the y as well. Okay. So we've got that
function already done. We have the mechanics
of plotting, the square matrix of values down. This is going to
come in handy not just here but anytime
you're interested in looking at a confusion
matrix which is essentially this square matrix of actual values versus
predicted values. This will help you
identify where error might lie in any
models you're making. So let's actually get around
to creating that matrix. We have a DataFrame of results but we now want to
show an aggregation into a list of the
true grades versus predicted grades and
the Library Sklearn which will use the
Machine Learning course, has a nice function to
create this kind of matrix. It's important this original
DataFrame that we have of actual versus predicted is
more than our square matrix. It's got a lot of data in
it like a 100,000 plus observations and so we need to summarize this down
into a square matrix. So from Sklearn.metrics,
we'll import the confusion matrix function. We'll create a
variable cm and we'll set it to the confusion matrix and we just pass it the two different columns
we're interested in, the actual column and
the predicted column and let's take a
look at what that matrix actually looks like. Okay. That's interesting. So we see big numbers
in some places and small numbers and
other places and it's really not super
meaningful to us. So let's dump this into our plotting function
and see what it looks like. So here I'll just call our function plot confusion matrix. I'm going to pass him
the confusion matrix and then valid grades, this isn't just
that list of labels essentially that we want
being passed in as well. All right. So that's interesting. We've got an image
actually being displayed. It's just a bunch of pixels
you can think about it. On the left-hand side
we see our true labels of A down to E and on
the bottom we can see our predicted labels A to E. There's the color bar
on the right hand side and that actually shows us how many observations there were per item. So the darker the blue
the closer they are to 50,000 and really light
blues are under 10,000. Notice that regardless of
the number of true labels, we almost always predict
that a student will receive either an
A- or an A. So what's up with that? So let's take a look at the
frequencies of our data. So if we take our
DataFrame and we group a by actual
so we're actually looking at all the
categories of actual and just apply the length
function to that. Okay. So we see that
in this dataset there's way more A
grade predictions, over a 100,000 when compared to any other
grid. So why is this? So to try and figure out
why this is happening, let's start running some
diagnostics and try to get a better understanding
regarding our underlying data. Specifically, we're going to make a Q-Q plot which stands for
a Quantile-Quantile plot. Now, from the box plots and violin plots discussions
you should be familiar now with quantiles which
are points separate out 25 percent or
one-quarter of the data. Quantiles also known as percentiles are just a
generalization of this idea. So the 0.5 quantile would
be the 50th percentile and so half of the data would live
below or above this point. The formula to create a
Q-Q plot is as follows. First, we order our n data
points in an ascending order. So we're making each
point its own quantile. Next, we evenly divide
a normal distribution into n plus one segments each with an equal
amount of area. Then we compute the
Z values for each of these cutoff points
and these are what we call the theoretical quantiles. Then we plot the actual
quantiles from step one against the theoretical
quantiles in step three. Now, the open textbook I've
shared has discussions of this under the term
quantile normal plot. Since they're specifically
interested in checking whether a
given set of data is normally distributed and
I call these QN plots. In the most general case however, you can use any distribution of data for the
theoretical quantiles and you can find this on page 83 and here's a link to
that textbook again. Just reflecting for a
moment on these four steps, it actually means
that you are creating a scatter plot where
one dimension is the normal distribution and the other is your actual
distribution. This means that a straight
line of points means that your data follows the
same distribution. If you want to see
this in more detail, here's a couple of
additional videos that describe this statistics. But for the rest of
this demonstration, I want to show you how
to write the code to do this in matplotlib. So here's two good YouTube
videos that go into more details on this statistics
part of the QQ plot. So you won't need to manually go through the process I
just described instead, we can use the probability
plot library from scipy.stats. So let's import stats. So import scipy.stats as stats. Now, to get plots like this, we want to convert
our letter grades into numeric equivalents. So let's just create
a mapping dictionary. So we're going to take
and create some new variable grade point dict and we're going to zip our valid grades and our numeric grades together and create a
dictionary out of it. This is just a short
form quick way to create a dictionary that maps our
letter grades to numbers. We're only doing that because
now we want to apply it to our dataframe actually replacing
values as appropriate. So we're going to take our
original dataframe and replace the actual grade point in there with this
new numeric value. Okay we're gonna create two plots here on two different axes. Don't worry about
this. Think of each of these as their own figure. So the first thing we
do is we're going to create a new figure
using the pyplot scripting library and we'll create a 12 inches
by five inches. Then I'm going to get one axis as one subplot and another
axis is another subplot. You could just ignore the
numbers path to subplot. Just think of these as
two figures side-by-side. Now we can pass this list of
grades into stats.probplot. This function takes
the distribution we want to compare against. So we're going to use
stats dot normal for the normal distribution and
a location for the plot. We'll indicate that we want
it on the first axis, ax1. So stats.probplot will
pass in our grade disk. So that's our set of grades, this whole giant dataset. Then we're going to
say we want to plot this against the stats.norm. So this is the
normal distribution. We didn't just sample
this distribution, we're passing in an object that's a theoretical
distributions. That's important only
in functionality here. You can think of it like
passing in samples. Then we're just going to say
plot it to that first axis. So let's plot all of this probability plot
to that first axis. Now, lets also plot the
histogram of the grades. So Panda's plotting also takes an axis to just drop the plot. This is why
understanding matplotlib is actually really important
in the Python world. Most libraries which
offer plotting support out of the box do so on matplotlib and so that includes not only Pandas, but
also stats.probplot. So we can just take our
dataframe grade disk, say we want to create
a histogram and just point it to the axis that
we're interested in. So we see that we have the
two plots side by side, the right-hand side is
our histogram and on the left-hand side
we have a QQ plot. So now, time for
some interpretation. First, in the histogram
on the right we, see this is very much not
a normal distribution. All of the values
skew to the right, very few people end up
getting a zero for instance. On the left, we see our QQ plot. The red line indicates the
theoretical quantiles and the blue dots show our actual values and these
clearly don't line up. So I wonder if our data here is actually an exponential
distribution instead. So let's just take
a look at this by running another prob plot. So we can take stats.probplot. I want the grade distribution
that same set of grades, but now I'm going to compare that to the stats.exponential
distribution. Remember, we don't have to set up the figure like we did before. If we're only plotting one thing, we can just tell the pyplot scripting layer that we want
to get the current axis. It'll automatically
create a new figure for us because we have the cell closing figures
flag already set by default. So no it doesn't really look like this is exponential either. So to demonstrate
what happens if we do have something that looks
more bell curved shape, here's an example
of some fake data. So I'm just going to
create some curved data. These are grades of students. Right? So I'm going
to take a 4.0. There's only one person
who got those, 3.7, two people got those,
3.3 what's that? A B plus maybe. A couple got those then a B, three people got those then
a lot of C's and so forth. I'll do that all the way down. I'll leave and what the heck, I'll even include zero
in there as well. Then let's call stats.probplot. We'll pass in this data, the set of curved grades. We expect this to
be aligned now with the normal distribution
and we'll plot that. Notice how the points
have fallen nicely along that theoretical
quantile line. So back to the problem
we actually faced. Now we could leave
the lecture here, but I want to dig
deeper and I want to show you our next steps. The issue was that there
were significantly more data for learners with high grades. So our model is unable to predict other types
of grades reliably. In other words, if
we just predicted that everybody would get an A, we'd actually have
pretty good accuracy. Even though that
wouldn't make for very intriguing or
useful predictive model. To combat this, we can
try and balance out data in our dataset and we do
this through under-sampling. Now, you're going
to learn more about different sampling techniques
in a later course. But this is
essentially just means that we're reducing the size of our dataset so that each class that we're
looking at to predict, the letter grade, has basically the same number
of students in it. Actually, it's not
quite this simple. We used a technique called SMOTE. You can read about this
if you're interested. The synthetic minority
oversampling technique, to generate a bunch of fake
data for model training. But let's leave all that for the future and let's just
stick with under-sampling. So let's read our new predictions after
building the model. So I saved that file in mads_data_post.pkl and we'll
read that into process df. Now let's just turn this
into a confusion matrix. So we can just call the
confusion matrix library, pass in the actuals unpredicted. So this is the exact same kind of data that we had before, but it's different data
because it's been tweaked. Now let's just plot it. So the benefit of this plot
confusion matrix function is we could just reuse it here. So we're going to pass
in that confusion matrix and pass in our list
of valid grades. So now we see we get a much
more broad range of values. We can now plot the QQ plot over predicted grades and take a look at how it stacks up to the normal distribution as well. So let's make sure
we're looking at the predicted grades
distribution this time. So I'm going to take grade_dist and I'm going to set it to our processed_df.replace and I'm going to
make sure that we're projecting the predicted column as opposed to the actual column. We're going to copy and
paste our plotting code too. So figure axis one, axis two and then let's put
up the probability plots. So again, that's stats.probplot. We pass it the data that
we're actually interested in showing and then we tell it the distribution that we
wanted to plot against. So we're plotting it against the normal distribution and again, we focuses on the first axis. Then for good measure,
will just add that histogram up which is normal Panda's plotting
on to the second axis. So we see that while not
exactly normally distributed, the set of grades does cover a much larger
and diverse area. If you're interested
more in what we did in this work to quantify the
quality of our model, you can check out
the paper linked in the course as an
optional reading. So in this class, we're focused on the visual
exploration of data, and I've showed you a
few ways to explore data in this particular
research work where the first was we built
a simple heat map of pixels representing
a confusion matrix, and then we built this
quantile-quantile plots and histograms to compare our distribution of data against theoretical
distributions, in this case, the
normal distribution. But because this is
a real analysis, I want to go beyond this
visual analysis and into follow up on the statistical analysis
that we did as well. I wanted to do this in
part because it relates a lot to our visual
analysis technique. So if we have a null hypothesis
that our distribution is normal and our alternative hypothesis that our
distribution is not normal, we can conduct a
Kolmogorov-Smirnov test or KS test to determine
whether or not we should reject the
null hypothesis. Similar to how we find a
t-statistic when we do t-tests, we can also calculate a KS test. So I'm going to paste in
here the formal definition, and it's given just
for reference. But you won't be expected
to understand the details, and the material in this
sub box is purely optional. Wikipedia has a great article on this project if
you're interested. I think the main
idea can actually be illustrated in this
diagram from Wikipedia. It shows both the one sample
and two sample KS test. So we see here on the one-sample
KS test that there is a comparison between a
nice smooth red curves, so that's our theoretical
distribution, and our actual stepped curve. So that's our actual
observed data. The two sample KS test is
actually just the same thing, but it's showing the
probabilities against two observations, two
different observations. So we can see that the
cumulative distribution function is comparable to a distribution
that we're interested in, such as this normal distribution. This is just like our QQ plot. However, in this case, the test statistic is
calculated by considering the largest distance between those two sets as shown by
the arrows in the figure. So the KS test measures
this distance. You could do this
investigation either versus theoretical
distribution data, the one-sample KS or between
two different samples, the two-sample KS test. So let's generate some
synthetic experiment data. So I'll take some
experiment data, and I'll just put from the
chi-squared distribution. I'll use a parameter maybe
six degrees of freedom, and we'll get about
a 1,000 different points of that, and
we'll sort them. Let's say we've got
some baseline that we want to compare to
it, so why not? We'll just use the
normal distribution, and we'll call this
our theoretical data. Then we can look at these
two plots overlaid. So we can just call plt.plot with the experimental data and then plt.plot with the
theoretical data, and this should plot
these as scatter graphs. So now let's assume the
experimental data is a population of students that we give some magic treatment to. The theoretical data is actually something we observed
in another classroom. Are the distributions of scores between these students different? Well, we don't have to
write the KS test ourself, of course, it exists within
the scipy.stats library. So here we can just call stats.ks_2samp because
we're comparing two distributions or two sets of data pulled from distributions, I should say of data
that we actually have, and we pass in the
experimental data, and we can pass in the theoretical data
and print that out. So the result of a
KS test is actually two values: the KS
statistic and the p-value. You can see here that
the statistic is pretty close to one and the p-value
is very very very small. So it looks like these are indeed from two different distributions. Recall that we talked
about some of the risk of bias in predictive
models early on. Here's an example of the actual predictive
models we've trained, and this shows the performance of our predictive models
when predicting on male and female students. Each point represents
different amount of simulated opt-out evenly
spaced from 0-99 percent. This is actually real
data from our study. So I'll read into a
DataFrame data on gender. So this is from a pickle file. Let's take a look at
the head of that. So we see that we've
got male values and we've got female values. So as an aside, the actual
metric that we're using for the quality of a model
is called Cohen's Kappa, and you'll learn about
this in the future. The one means that the
predictive model is good, and a zero means that
we're basically guessing. The scores that we
have above there are actually not all that great. So we want to see if
there's a difference between these columns of data, so we plotted it.
We labeled them. We built our linear space for dropping out data and
doing a simulated dropout. We made our plot, set our scores by gender, and then here was a little
iteration through our plots. We set our limits, we described
what the plot look like, and we set our legend. Okay. So looking at the plot, it seems that there's actually not much difference
in our ability to predict grades for male students
versus female students. In this case, that's
actually sign of a model that has
minimal gender bias. But we can use
statistical tools too. This plot comes from our paper, was right before our paper. Along the bottom of
this plot, the x-axis, this is the proportion of data dropped from the training
set for our model. So we actually retrained models with a percentage
dropped each time. As you get towards one, we've dropped all of the data. So that means we expect our
model to go down to nothing. At zero, we actually
kept all of our data, and so then 0.5 would
be half the data. The blue lines for men and
the orange lines for women. You can see the metric
we're using here on the y-axis is called
Kappa Cohen's Kappa. If it was one, that means it
would be perfect prediction. If it's zero, it's not
actually totally inaccurate, it's just as accurate
as chance would be. So that would be one
and however many grades given the distribution as well. You can see our values here
are actually pretty crummy. They are 0.25 or something
like that average values. There's a lot of
questions about what would be good values here, but that's definitely
another lecture. So we could actually use the Kolmogorov-Smirnov
test as well here, and so we can print
out the KS two-sample. We just pass in the men
and the women scores, and we see that we get the test statistic back
as well as the p-value. So then you can see a
pretty high p-values, suggesting that we can't really reject the null hypothesis. Okay. So this was a big lecture. We talked about
probability plots and statistical tests to help us compare different
distributions. Then we specifically mentioned
QQ plots and KS test. There's other graphical
tools such as PP plots, and tests like
Shapiro-Wilk or so forth. Each of these have their own advantages
and disadvantages. What I've tried to do
is to equip you with all the key ideas behind
these techniques. I encourage you to explore the additional techniques
if you would like to. In the full paper for this study, we found that there's actually quite a significant
range of opt-out that schools have to consider when developing these
kinds of systems. They vary depending on how you count people that don't
respond to your data sharing. For instance, do you count people who don't respond
at all to the question of opt-out as having opt-out or whether they're actually
opting in still? That can determine where
along that axis you're actually interested in
looking at your models. We do actually find some bias, and you can look at the
paper for that as well. We're better at predicting grades for students,
for instance, who are in their fourth
year of their study versus those who are
freshmen students. But this gap actually
quickly narrows as opt-out increases and models
do degrade over time. So if you're interested
in learning more, I provided a link to the paper online and in the
Jupiter notebook.