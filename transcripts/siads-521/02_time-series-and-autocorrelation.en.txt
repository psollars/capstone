In this video we'll be
talking about time series. And as the name implies, these are just simply a series of
data points that are ordered in time. We'll first plot a time series, and then I'll discuss some of the features
like periodicity and lag. And after that, we're going to use data to create
what's called an autocorrelation plot. This is a helpful tool for checking
whether or not we have randomness in our data, or if there are patterns or
correlations that might exist. As the image above shows, a time series
plot is similar to other graphs that you may have already encountered, only that each point along the x-axis
represents a point in time. So for instance, the vertical line
might represent the actual and five-day forecast in temperature for
August in the beginning of 2017. Throughout this lecture I'm going to be
using a historical data set that logs weekly case reports of the measles disease
from 1948 to 1967 in England and Wales. So let's go ahead and read in this
file and take a look at the data. So I'll bring in Pandas and
we'll bring in NumPy. We're going to read this data set in. And so, it's called ewcitmeas dot txt. And this is actually
a whitespace delimited. And let's look at the top of that. So there's several pre-processing
steps that we should conduct before moving on to
a more detailed analysis. For instance, there are these asterisks
where there's missing values, and the date, month, and year format
isn't particularly easy to work with. So let's address this by renaming
the date related column names so they're compatible with Pandas
two date time function, which will automatically generate a single
date time column and day time data types. And these are much more convenient for
us to work with. And the documentation on time
series data can be found here. And it's really quite essential when
dealing with time series in general. So I'm going to rename these
columns to day, month, year, and just change them in place. Then, I'm going to change
the year to a four-digit format. And then, I can turn this into
an actual date time field. And so, I could just call
pandas dot two date time. This is a helper function. Remember that Pandas was originally
written by Wes McKinney in part to work with date time data. As a consultant to hedge funds, he was doing a lot of work looking at
time series data and financial data. So here, we could just pass in this data
frame where we've projected three columns. Just going to try and
merge those into date time. And then,
we'll just set that to some new value. So let's take a look at
what that looks like. So as I said, Pandas has excellent time series
features because of how Wes created it. And I'll be honest, I don't actually use
time series analyses a lot in my work. So I often find I have to
reference the Pandas docs. But whenever I do, I'm consistently amazed at some of the
things you can do right out of the box. So let's replace asterisks with
these not-a-number values, and then we can use the drop-in-a function
to remove those from our data frame. Also, if you try to
immediately make a plot, you'll notice some weird behavior,
which is due to the fact that the data types of other columns
are actually all strings right now. So for now, since we're only going to
be looking at the cases in London, we'll cast that column to an end. So I'm going to replace everything with
an asterisk with np dot not a number. I'm going to drop all of our NAs. And then, we're just going to pull that
London data out as an int 64 value, the number of cases there. And we'll ignore the other columns for
now, and we'll just look at the head of this. And so that date time, okay. Okay, so data cleaning,
it's in every project that we have to do. So far, we haven't really
done much with time series. So let's bring in piplot and
set up matplotlib. So we're going to import matplotlib dot
piplot as plot, and matplotlib in line. So one little trick I'll show you here is
that you can set the default figure size using the rcParam setting. And this is a global matplotlib
dictionary of default values. And I often find I set a few things
at the top of my notebook, and then I can override them
if I need to throughout. So here, we're going to set the figure
size, but you can change different fonts, colors, etc. But it's important. This must be done after
the percent matplotlib. So after this magic function,
inline is called. And that's actually because there's
a bug in something, either Jupyter or the inline function or matplotlib. So we'll import maplotlib as mpl. And then, we can just go mpl dot rcparams. And here,
we'll just say figure dot figSize. And that sets the default figure size. We'll do it 16 inches by 4 inches. All right, so now,
we're ready to plot the time series. Let's look at the values for
London over time. So here, we just go plot, and
we just pass in our xy's. Okay, so you may notice that there seems
to be some spikes that happen every so often in this previous plot. And so, one of the questions one
might be interested in is whether or not there's some pattern to
these increases in reporting. Or in other words, is the time series
periodic in nature, similar to how the four seasons change in a generally
predictable pattern year after year? With the exceptions depending
on where you live, of course. So our investigation in
this lecture is to compare an existing time series of data with
a time lagged version of itself. And this is the auto part of
the autocorrelation plots that I spoke of. If the two time series are correlated,
then we'll see a positive value. And we'll see a negative value
if they're not correlated, or anti-correlated in particular,
negatively correlated. Otherwise, if there's no relationship,
we expect to see a value close to zero, sort of a flat line. And we can measure this correlation by
sliding the second function, that is, the second set of data values,
over the first function and measuring the amount of overlap. And so, here's an image. If you'd like to read more about
the details behind how this method works, I highly recommend the Wikipedia
article on the topic of convolution. And here's a great link to it. And that's where that image comes from. But let's do a quick
simple example of our own. So to help solidify the concept
of autocorrelation a bit, we're going to do
an autocorrelation by hand. The first thing we're going to do
is center our data by subtracting off the mean. So here, we're just going to scale,
essentially, or we'll center our data. So we're going to take the df London
value and we're going to broadcast the subtract operation, and
we're going to take df London dot mean. So we're just subtracting the mean
of this data set from itself. Now, we want to see the correlation
between two functions. And these functions,
these are our time waves, if you will, or what look to be waves in our graph. And they're actually
represented as arrays of data. So we're not talking functions
in the computational sense, we're talking functions
in the physics sense. And our arrays actually represent
those functions as collected data. And since this is autocorrelation, we're
comparing the data in x against itself, but we want to shift
the comparison of that data. So we can do this using the mode full
parameter of NumPy's correlate function. So NumPy, numerical Python, has
a correlate function built right in it, and we can shift it with mode full. So the details behind this are a little
outside the scope of this lecture, but I wanted to pull back some
of the layers of the onion so that you didn't just see the images but
you can actually dig in a little bit more. And a lot of this comes from signal
processing and signal comparison. And the SciPy signal docks
are actually pretty good about this. And so, if you get into this and you find yourself wanting to compare What
appear to be waves or functions of data. You might find that there's
a lot more information here. Okay, so
we'll just call the NumPy correlate. We'll pass in our two functions,
in this case our two data arrays, which is just the centered London data. And then, we say mode full, which
basically says take the one on the right, the second parameter, and
time shift if over the one on the left. So the result of this correlation
is a new ndarray, and it's actually twice the size of x. And this new ndarray is symmetric
in that the first half and the second half are mirror
images of one another. So for our analysis, we're just going to
look at the second half of this array, and this corresponds to a time lag of 0. So that starts a time lag 0, and so
we'll just take autocore sub x dot size, just to truncate,
get rid of the first half of the array. And finally, we're going to
normalize the values between 0 and 1 by dividing by the maximum value. And this just makes things
a lot more readable. So we'll just take autocore
div equals autocore dot max. Nice, handy, quick way to do it. Okay, so that's how we create
the autocorrelated data. Now, we want to actually plot it. So we just call pyplot
dot plot in autocor, and then add a reference line at 0. Remember, 0 is our flatline,
and that means no correlation. And so, we could just do that by
saying that we want a x axis line, horizontal line. And then,
we're going to toss in some labels. So I'll remind you that
the x-axis is the lag, and the y label is the normalized correlation. Okay, so there we go. So now that we have
an autocorrelation plot, how do we actually interpret this figure? Well, remember,
this is a plot of our signal, that is the function in a mathematics
sense represented by the data from our original figure where we compare
the signal to itself shifting over time. So there's an extremely high correlation
at the beginning of the time since the signals are identical. Then, as we shift, this rapidly
drops off as they become dissimilar. And then,
increases again as both signals spike. By the end of the time frame, there's essentially no
correlation between the signals. So what's a reasonable amount of lag? And that really depends on your question
and the data you're looking at. So let's dig into this a bit
more now that we have an idea of how an autocorrelation plot is generated. I'm going to pivot a bit and start using the autocorrelation function
that's actually built into matplotlib. Yeah, we actually did all of that
is a learning experience, but the library has the functionality
already there for you. To use it, we just generate
the new axis in figure, and then call acor as a function. So here, from pyplot I'll
just get the current axis. That's actually going to
create a new figure for us and return the default axis for it. And then, we just say dot acor. And I'm going to to pass in x,
use v lines as true. Max lag args is 104. Normed is true, and lw is 2. Now, acor has these couple of
parameters that I just tossed in. So first, we're going to pass in
the data that we want to plot. And next, this max lags value,
I want to set this to 104. Since our data is weekly,
I just wanted to zoom in and look at the autocorrelation for
the first two years of data. So that's a 104 weeks. And I also want the data to be normalized. And so,
I set the use v lines parameter to true. This changes the plot from a scatter
plot to a bar plot, or what's called a stem plot, which makes it easier to
see the area under the correlation. And lastly, I put a semicolon
up at the end of the line. So that's kind of weird. What's up with the semicolon? And this is actually not Python at all,
but it's a Jupyter notebook feature. By default, the Jupyter notebooks display
the last variable in a given cell. And that's why when we do something like
df dot head we get a rendered data frame. Because implicitly, the Jupyter system
is changing this to display df dot head. And this is normally super handy. But in this case,
acor not only plots our data, but it returns four different variables. Lags, which is a vector of lag periods. C, which is the autocorrelation vector. Line, which is the number
of lines to be plotted. And v,
which is a horizontal line to be plotted. And so,
this turns out to be a lot of data. So the default Jupyter notebook will
send it all back to the display. The semicolon is a handy trick that
tells Jupyter to change this default behavior and not display the value
of the last line of the notebook. But our plot should still come out
because matplotlib will render it. Right, remember
the autocorrelation is mirrored. So let's set our x-axis values to
look at the same time lags that we're interested in. And now, we don't need to use the
semicolon value because I'm going to set some things right after it that
don't have any meaningful returns. So I'm going to do our
autocorrelation again. And I'm going to turn that grid
on on the axis, which is useful. And then,
I'm just going to zoom in to that 0 and 104 week period by setting the x limits. Okay, so notice that for
the first four lags, the correlations remain fairly high,
above 0.8. And this means that
the measel cases reported for a given week are very similar to
those in the subsequent four weeks. About a month. However, if we look halfway through
the graph, say around week 52, the number of cases is quite dissimilar. And this suggests that there isn't
an annual trend in measles cases. In fact, it seems to be a little
bit of a negative correlation. January's number of cases this year
don't really reflect it in next year's January number of cases. But taking a bigger picture perspective
and viewing what's happening even larger lag shows that by week 104,
it seems to peak again. Which gives evidence that the number of
measles cases seems to have a biennial period, or that the data are strongly
correlated once every two years. We're going to have to come up
with another example because I think autocorrelations are interesting,
yet they can be a bit complex. So I decided to engage in
a little experiment, and we'll share that data with you. While I've been preparing some of
this lecture, I've been capturing my various systems statistics, such as
CPU usage and memory usage to a file. A lot of operating system work is
actually based around anticipating or predicting what a user will do, and then trying to intelligently cache data or
a computation to make it fast. Traditional hard disks
are a great example of this, where it's expensive to read
a small amount of data. But if you know the user is going
to want to read a lot of data, you can put it in the disc contiguously
to make good use of the space. So my question was could we
use autocorrelation plots to explore my system usage? So let's take a look at my data. And I captured this using
the command dstat minus tcm, and then I just output this to
a file called dstats sciads 521. And so, we'll just read that in, and the
first six rows are kind of junky comments. And so, we'll just skip those, and
let's look at the head of that. Okay, our first column is a time step. And in the next five are CPU statistics,
and the last four are memory statistics. So let's plot some CPU data. So I'm going to take this data frame,
and I'm going to set the index to time. And I'm actually going to apply
a lambda to that to clean that up and turn that into a date time. I actually only care about the time. I don't care about it that date. It's all the same because it's
from a very small window. So I'm going to trim that
with string splitting. And let's take a look at
what this data looks like. So I'm just going to
plot it time versus idle. Okay, so by eye, I can't see anything meaningful time
series related trends in that data. There's certainly some interesting bits. Like when I start Spotify towards
the beginning of my data collection and we see a dip in CPU availability. Now, let's look over five minutes of
data at the idle time of my CPU and show that is an autocorrelation. So I'm going to call piplot dot gca,
get the current axes, and they'll call the autocorrelation
to build the autocorrelation plot. Again, a reminder that we
pass in our data series. So in this case, it's our idle values. We want use v lines equals to true. The max lags I'm going to
set to five minutes. So 5 times 60 seconds because
this is second based data. I want this normalized, and
we'll set lw to 2, the line width. And then, I'm going to turn the grid on,
and I'm going to zoom in just so that we're looking at that
one chunk of the data. Okay, so this is pretty interesting, and it's pretty different from
our previous findings. This decreasing slope suggests
that the further you look, the greater the lag is,
the less the correlation is. So CPU availability is really
well predicted by the most recent CPU availability. But that doesn't describe well
what might happen in five minutes. Whereas with the measles case, we found
that the measles in London was well predictive of what happened in London
two years ago versus one year ago. So very different kinds of conclusions
that we can draw from using these autocorrelation plots. Okay, one more example. Here's some data I captured
on my running activity. Specifically, there's a power level in
watts that I captured from my power meter, and my heart rate in beats per minute. So let's take a look. So we'll read this in. I've saved this in a data file for
a single run called stride dot csv, and we'll read in the data file. So let's create our date time index and
do a bit of exploratory data analysis. So dataframe equals df dot
set_index df sub timestamp. And let's plot power. Remember, we could set the label for
a legend and format for the line to take a look at it. So I'm going to plot
timestamp versus power. And I want this to be red, and
I'll have a little dash there, and we'll set the label of this to be power. Now, we want to put both of these on
the same plot, but since they're different scales, power goes over 200 and
I certainly hope that heart rate doesn't, we want to lock the axes,
the x-axis in particular. But we want to let the y-axis for both float, and matplotlib has a handy
function for this called twinax. So we just do this plot dot gca,
get the current axis, and we call twinx. And so now, we're going to be able to plot two things
to that plot on separate axes, y axis. And now, we can plot the heart rate. So plot dot plot,
the timestamp, the heart rate. And we'll plot that is a blue line,
and we'll label that as heart rate. Okay, let's put a little labeling
on the plot, too, always good. So we're going to put a legend on there,
and a lower center. We'll set the one axis is heart rate and
the other one's power. And we'll say heart rate and
power versus time. Let's send that to matplotlib to render. Okay, so we can see that there's
some different ramp-ups for these two data points. While power can be achieved quickly and
seems to flux within a small range, heart rate takes a while to increase and
sort of hits a maximum. So how would these different
in autocorrelation plot? So let's create a new figure. I'm going to use matplotlib's
autocorrelation function for this, but I'm actually going to
plot the results myself. Let's create a new figure,
and this time around, I'm going to use 15 as my number of
observations to look for lag over. So I'm just going to look over
15 different observations. So we'll take power lags,
power auto, star under, and set this to autocorrelation. Power normed equals true. Max lags equals 15. And we'll do the same thing for
heart rate and get that. So what's all this asterisk
under that I just did? So I'm trying to layer in here a few
more advanced Python features. When you do tuple unpacking, that is,
when you're assigning multiple variables to elements in a list, like acor returns,
we talked about the four different pieces that it returns, you might not
actually want all of the elements. In this case, I actually only care
about the first two variables, and I don't care about the lines and
such that acor actually went and created. So I can just repack all of those up. And this is the astrisk notation. And then,
I can assign them to a new variable. And in this case,
I use the underscore as a variable name. And that just plain old looks weird,
and that's kind of the point. It's a convention for
a junk variable in Python. But actually, it's a valid variable name, so you can actually go get
the lines that were created. You just print out underscore in
the next cell, if you want to try it. Okay, back to the task at hand. Since we used matplotlib and piplot,
a new figure has been opened. So let's clear it here so
we can do our own thing. So we're going to call piplot
to clear the current figure. And now,
let's plot our comparisons as line graphs. Since this is mirrored, we'll start
in the middle of each nd array. So we'll find the middle point, and
then we'll plot the power lags value, and we'll plot the heart lags value. And then, let's put a little
labeling on the plot as well. So we'll set the legend, the x and
the y labels, and the title. And let's take a look at that. So we can see that the autocorrelation for
both of these curves follows a similar linearly decreasing trend that we
observed in our previous example. But the power actually has less
autocorrelation than the heart rate moment-to-moment, that is,
over small lag periods. And if you think about this, this seems
pretty reasonable physiologically. It's very hard for me to change my heart
rate from one moment to the next quickly. My body managers my heart rate for
me and scales it up and down as needed. But for power, I just stop,
sending this power to 0. Or I just start sprinting,
sending the power high. However, for both of these, there's a norm to a sort of
steady state while I'm running. But I wonder what if I engaged
in different running strategies, would that change how
the autocorrelation plots look? For instance,
when you engage in power training, you aim to keep your power at
a certain level, say 300 watts, and to let other variables change depending
upon the training conditions. It might be interesting to compare speed,
elevation, heart rate, and power together to explore
this relationship further. In this video I've only scratched the
surface when it comes to exploring time series data. For instance, other tools we might use
might include partial autocorrelation plots and autoregressive integrated
moving average models for forecasting. And this is certainly not the last time
that you're going to see these time series data, especially as we become
more adept at collecting time series information from our
increasingly sensor augmented world.