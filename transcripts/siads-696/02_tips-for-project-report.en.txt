Hi everyone. Today
I would like to just take a few minutes
to talk to you about the extreme importance of the analysis and evaluation part of your machine
learning projects. It'll be super important
basically to provide not just the accuracy
numbers for the models, but to make your results more insightful and also
more convincing. There are four key methods
that I'm going to talk about that you should
strongly consider, including in any kind of analysis of your machine
learning models. The whole point is that
not only do we get better insight into your own machine learning
models with these methods, but it will also
add credibility to your results by
showing the readers of your report or people who
attend your talk that you've really made an attempt to
understand why your model behaves the way it does and not just where it's successful, but also where it's failing. These results can also
point the way towards future improvements for your
machine learning models. I'm going to talk about that
and I'm also going to spend just a very brief few slides on other presentation issues
that I encounter in reports. So some quick tips and things to avoid or make sure that you include when you're presenting results as well.
Let's get started. The four methods
I'm going to talk about are things that you do in the evaluation
phase of your work. The 1st one is feature
importance and feature ablation. The aim of this
particular approach is to understand
what features are contributing most
to the accuracy or whatever metric you're using, and which features
are not contributing. These clusters of
features that are important to use to
gather and so forth. We'll talk about
that in a minute. The 2nd supercritical thing that very few students
do at their 1st pass in creating a machine
learning report analysis is, not just how the
classifier is doing well but also what kinds of
errors is it still making. How is the classifier or
the regressor failing? We're talking about
specific examples and by looking at these
specific examples of what types of failure
are most common. So that's a super important
analysis to provide as well. The third is
sensitivity analysis. How sensitive are your results to settings of key
hyperparameters? If you make a small change in some critical hyperparameter, does that completely change the results that
you're getting or not? Which hyperparameters are stable in that respect and
which ones are not? Finally, there's an analysis called learning curve analysis. How sensitive is
the performance of your method or methods to the amount of training
data you have, for example? I'm going to go through
each of these four. Typically, the first three
are the most important. But learning curves
are also super useful, especially if you have or if you're starting a project
with a small amount of data and you want to
figure out whether it's worth the cost of
gathering more data. Choose the ones of
these that work for your project and analysis goals. But typically, the
first three get complimentary aspects and super important for getting
better insight into models. The 1st critical insight method has to do with feature analysis, so using feature
importance scores and doing feature ablation analysis. Feature importance scores are numbers that give
the indicator for how much particular feature contributes to overall prediction accuracy
or prediction metric. In other words, for example, is there an increase in model error if you
remove a feature? If you take a feature out and the model that gets
much less accurate, you might consider that feature
to be more important than a feature model accuracy doesn't really decrease
much if you take it away. Now there are a lot of
different approaches to feature importance. Don't have time to
get into all of them, but I'll talk a bit about things like permutation
importance in a minute. But feature ablation
is the other part of feature analysis and
that's a very simple idea. Namely, take away
a set of features. What's the average
increase in model error or decrease in accuracy if you
take away a set of features? For example, if you
have a classifier that predicts the reading
difficulty of text, you might have features
that are based on statistics of a text like
the average sentence length, the number of syllables,
and so forth. You might also have
text-based features that capture those specific words that are used in a sentence, or some words are more
sophisticated than others. So you've got
text-based features, and then you got
numeric features. A feature ablation analysis
is meant to give insight into whether the text features
or the numeric features contribute more to the overall accuracy
of the classifier. The idea is that by
removing a set of features, you can see how much that
hurts the model's performance. That's feature ablation. Why is it important to do
this feature analysis? Well, it's the number 1 way you can understand more about
your prediction model, and about the task itself. It could be super
important to get some insight into what
types of features contribute most to
accuracy in terms of understanding
the problem itself. In the case of
reading difficulty, it's interesting to understand, do vocabulary
features contribute more than grammatical features? This could be important
if you are trying to understand what might
happen to the classifier, if you use it for a slightly different task
than it was designed for. Second reason why
these feature analysis are important is it
may be important to improve the efficiency of
your methods in a pipeline. You can do future analysis, and you can eliminate
ineffective features, things that don't contribute
to prediction performance. Third, a good reason is
to detect data leakage. If you do a feature
importance analysis and you get a super high feature the important score
for a feature that you would never suspect, something like a patient ID that really shouldn't have
much predictive value, you're going to want to
look very closely at those suspicious
features that come out of the feature
importance analysis. How do you do these
feature analyses? Well for feature ablation,
it's really simple. You just train one model first of all that has
all the features, and see what the
classification or regression metric
performance is. Then you pick sets of
features that you want to get some insight into about their
relative effectiveness. For example, you might pick the reading difficulty
classifier, the numeric features,
or the text features, or you might pick the vocabulary features and the grammatical
features and so on. But the point is that you then, after you train one model, with all the features and you
train another model, that leaves out each of the critical subsets of
features you're interested in, and you see how much
the performance is hurt by leaving out
each of those subsets. Remember that when you do this, you should do this using five or 10-fold
cross-validation. You want to average the results
of this feature ablation across 10 different random training test
splits for example. But this is a really good and
effective way of getting at the real effect that
each subset of feature has on the final prediction
accuracy or metric. The feature importance analysis, a lot of it is built into
sci-kit-learn already, and I'm not going to go into the details of how some
of these methods work. That's for a separate lecture. But I'll point out two methods in particular that you
can take a look at. One of them is the
output property called feature_importances, which comes with a
tree-based classifier. After you fit the classifier, you can get access
to this set of feature importance weights that is generated automatically. There's a more robust way to do feature importance called
permutation_importance. That's more costly but also
works on generic estimators. Take a good look at
those two approaches to do the feature importance
analysis for your models. The second analysis
technique that will add tremendous insight and credibility to your machine
learning results is failure analysis also sometimes called error analysis. Dr. Andrew Ng gave a graduation talk for the Masters of Applied Data
Science Program recently, and he's very big on doing
this error analysis. Frankly, there's nothing
magical about it. It's a manual process that inspects the mistakes that your prediction
model is making. His idea was that when you
do this error analysis, you often uncover details about how errors and inconsistencies and how the data were
labeled originally. You can often get a lot of
interesting performance gains, or at least insight
into the problem by fixing the data and
not necessarily optimizing just the model. But, failure analysis or error analysis starts with
making a spreadsheet. You want to characterize the different errors that occur, and how likely they are
to occur in the dataset, so some types of failures
or errors might occur. Ten percent of the
test examples and some might only occur 1 tenth
of one percent of the time. By understanding both the
nature of the errors, but also how often they occur, that'll help you prioritize
your next steps. You obviously would probably
want to spend your time on the errors that are
much more likely to be seen than the very rare ones. Why do this kind of analysis? Well, obviously it can give great insight into
what needs to be improved in the
classifier and how to prioritize the next steps. What kinds of new features
might you need to add? What are some
deficiencies that are in the model and might
be causing errors? This helps you estimate as well once you have enough
examples of errors, what's the best-case
improvement scenario? If you fixed all of
these particular errors, how much would that improve the performance of
your classifier? Then as I said, the
third reason to do this is you can uncover mistakes or inconsistencies in how the data were gathered or
how they were labeled, or maybe there's a problem between the training
and the test model. See you might discover
that you trained the model on the bitmaps
from the Internet, but you're testing
them on images from smartphones which can have very different characteristics. Lots of reasons to
do error analysis. Again, I emphasize that
this is looking at error analysis on
specific failures. You make a spreadsheet containing a habit
you're on the example. You want to basically compare the predicted label
with the true label and look at different
combinations of true versus
predicted mismatch. Typically, 100 examples is a good start where
you're looking at where the predicted label does not match the true label. It's nice to be able to find it. If you can't do a 100 examples, at least find three or four just to get a
sense of what's going on. I've included a link here to an Andrew Ng video on
error analysis just to re-emphasize the point
that this is a really important if the not terribly
glamorous type of activity. The other way
obviously you can do failure error analysis is
with confusion matrices. You can use confusion
matrices to characterize the
different kinds of errors and their
likely predominance. Which classes are typically
confused with each other? You can use confusion
matrices to help guide you in your manual
error analysis. You can see what are the
types of classes that are confused most often and
focus just on those. Again, this can give you
insight into what to improve and how to
prioritize next steps as well as what's the best-case scenario
if we were to fix all of these
kinds of errors. How good would our
classifier be? That could motivate
how much of a gap between current and optimal
performance might there be. This is covered in Week 3 of supervised learning
in 542 by the way. Another really important
type of analysis you can do is called learning curve analysis.
This is really simple. All it does is it
plots the amount of training data you use
in a model against the accuracy or the
evaluation metric that you get from using that
amount of training data. You try training and testing a particular model with
multiple training set sizes. You start with a small
number of examples like 100, maybe less and then you grow that typically by doubling the size of the data each time. This is often plotted, as I show here on the right, using a log X axis scale. That's a log scale
in powers of ten. The goal of this is to
just understand like how sensitive classifier is to the amount of training data. Especially if you're starting out a project with
not a lot of data, doing this kind of
learning curve analysis can help you understand, okay, what's the potential for further performance
improvements if I were to double the
amount of training data or quadruple the amount of
training data I have? Is it worth gathering
more data or can we see from the learning curve analysis that with our current amount of data there's a plateau in performance and
it really wouldn't improve a huge amount if you really doubled the
training set size? This kind of analysis
can also help if you got imbalanced classes
classification problems. It helps you decide
on the right amount of oversampling or fake
training data you want to create for the minority
class that shows you how much it can affect the overall
classification accuracy. You can use this
type of analysis to decide how much
oversampling to do. You can decide
that by looking at the validation
performance and when that stabilizes with this
kind of learning curve. Again, it's very simple
to do this analysis. You choose a set of training set sizes
that you want to plot. For each of those sizes, you do k-fold cross-validation
to get the mean and standard deviation or
a confidence interval around the performance at
each training set size, and then you report the average
performance of that size. Then you repeat that
for all the sizes that you're interested
in.. That will give you the plot on the
right and then you can analyze the
trend in the curve. You can extrapolate,
for example, to see what the likely
effect of doubling or multiplying by 10 the size of the current
training set is. Related to that learning
curve analysis, is a more general
sensitivity analysis. Instead of just
modifying training data, you're actually
playing around with the key hyperparameters in your model to see how
the overall performance, how the overall accuracy, or evaluation metric
is affected by small changes in important
hyperparameters. For example, in this
slide to the right, I'm showing how the accuracy of a support vector machine is affected when you change the two main
hyperparameters: the Gamma, the RBF kernel width, and then the parameter C, the complexity of
the soft tolerance of errors across the
decision boundary. The fact that you want
to get really fancy, you can plot both
hyperparameters together with the
z-axis representing the performance to
get a beautiful idea of how the sensitivity in that space affect
the parameters. Again, like with
learning curves, the x-axis scales
are often log scale because that's the
easiest way to show a wide range of
hyperparameter values. Why do this
sensitivity analysis? Well, it helps you
understand how likely it is that your
model will generalize. If it's super, super sensitive to very specific settings
of hyperparameters, it may be less robust when you use it on alternate
training test sets. So it's a measure of
model variability. It gives you some insight into where the model needs the most
flexibility and fittings. It may be that you see for two particular
hyperparameter combinations, there's a lot of
sensitivity that may indicate that you can tune
maybe to the third parameter, that you can tune to help improve fit in that area
where it's pretty sensitive. Finally, it's very
useful to understand the sensitivity of this
type of hyperparameter if you're deploying this model
in a real-world setting. When you deploy the model
for various reasons, it may be that there's some other thing
like efficiency that's affected by the choice
of hyperparameter model. A model might become
more inefficient in a pipeline for one set of
hyperparameters or another, but if the overall
performance isn't sensitive so much to
choices within that range, it may be okay to set
the hyperparameters to values that are
close to the optimal. You'll know because
of sensitive analysis that performance isn't
going to be hurt that much by slightly different
parameters and that choice may provide much greater
efficiency for technical reasons in a pipeline. Again, it's just getting
better insight into how stable and how
sensitive your models are to key parameters. Like with learning curves, this is easy even if
computationally intensive to do. You pick key hyperparameters, you choose a series of
values you want to probe, and then for each set of
key hyperparameter values, you do some k-fold
cross-validation. You average the results to get the standard deviation and the confidence interval across this k-fold, is typically 10, and you repeat this
for all values of the hyperparameters
that you want to test.. One message
I have for you is, make sure that you
set aside time in your projects because this is
computationally intensive. Make sure you set aside time as you get towards the
end of the project to do this analysis. Make sure you have the
computing resources you need because this is not the kind of analysis you can do necessarily the day before
your report is due. You want to build in several
days a week, if possible, or at least make sure you have the computing resources set
up because this is pretty computationally intensive
and the reward for that computational cost is much better insight
into the models. So make sure you leave
yourself time in the project. Put this analysis
explicitly into your schedule to
make sure you have the time and the
computing resources to do a good job at it. Those were for analysis
methods that you should apply in
your evaluations. I wanted to mention a couple of other things before I wrap up. The first is getting back to reporting the main
accuracy results. You don't want to report your main accuracy results on a single train test split
if at all possible. You want your results to
be statistically reliable. You want to be able to show that this particular choice
of model compared to a different model has there some statistical
reliability for making a comparison. What you really have
to do, for example, if you want to compare
Support Vector Machines to random forests, don't rely on a single
train test split to make that decision. You need to do 10-fold
cross-validation, stratified cross-validation for both the support
vector machine and the random forest models. Then based on that 10-fold, averaging across the 10 folds, I'm looking at the 95 percent confidence
interval for each of those, then you can make statistically more
reliable comparison about which type of
model is better. Anytime you're
comparing model types, support vector machines, random forests, gradient
boosted decision trees, Naive Bayes, anytime you're
making this model comparison. This is not the model selection processes is after you've
selected the best model. Now you want to compare
across different model types. This is where you
need to do like a 10-fold cross-validation for the best model of
each of these types. Make sure you stay
in the caption of your table exactly how you
did the cross validation. There are minor things, they're easy to take care of
and they'll greatly improve the ease with which
people can read your report and
understand the results. Number 1, please number
all tables and figures. That makes it
possible to actually easily reference a
table in texts if you want to write
about something that makes it much easier for you and for us as instructors. If you label all tables
and figures, number them. Make sure you label
the axes of all plots. I can't think of a
number of times that I looked at a
beautiful plot that didn't have any
explanation about what was on one particular
axis or another. Make sure you label
axes of the plots. When you're presenting
your main results, make sure that you present, not just visually in a plot, but actually give
this precise numbers in a table as well. The ideal knows that it's
nice to have both methods, both the visual summary but also the specific numbers
in a table and again. Finally, this is
real minor thing, but I just wanted to point
out that there's really no need to report results that have like no 10 or 15 insignificant
digits waste space. It's really a
meaningless precision. It's perfectly fine to report three or four digits of
significance in your results. Well, that's my summary of some really critical
things you need to think about doing in
your analyses and reports. For milestone
capstone and beyond. Anything you do
professionally where you want to communicate clear, convincing results from
machine learning model. It's not enough to report
just the accuracy. You have to show
some insight and some credibility as to what's going on and
what you're recording. Take a look at
these four methods. Feature importance analysis, ablation, and important scores. Do an error analysis of
your best classifier. Do a sensitivity analysis
to understand how sensitive your results are
the key hyperparameters. Consider using learning curves to understand how sensitive the performance to the amount of training data that you have. Again, choose these that work for your project
analysis goals. But I really highly recommend that you consider
at least the first three for any of the
analysis you do, especially during the Nazi Applied Data
Science Capstone and milestone courses. This is going to be super important and super
interesting I think to understand what is the nature of this
classification problem? What are the
interesting successes and failures of
particular methods? I think it's a really
interesting and fun aspect of machine learning
that this kind of analysis often gives you a profound insight
into what's going on. It may even lead you to reconsider your original methods
for feature engineering. This is part of an
iterative process. What you learned from
this analysis can easily inform the next iterations of your machine learning model. Anyway, thanks for
listening and remember, I don't train on your test data.