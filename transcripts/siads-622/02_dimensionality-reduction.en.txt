All right, so it's time to start tackling
big, big data sets with lots and lots of columns and lots and lots of rows. So, we're going to think about
a few different techniques and we're going to start with dimensionality
reduction as the first one. Basically, what happens if we just
eliminate a lot of the variables that we don't care about or try to reduce a bunch
of data set, a bunch of columns into one? So, let's say we have lots and lots of
data, lots and lots of properties for something that we've collected. So, we're going to start with cars and
we're going to find a way of reducing it into something that we
can plot on the screen. So, a car has all these
different properties, we're going to find a way of projecting
this into a simple XY coordinate as a representative of that individual car. And what this will do is preserve some of
the relationships that we care about in the data. So if we care about, for example, the
distance between two cars, that is, how similar two cars are to each other, this
kind of technique is going to work great. Because on the representation
on the screen or on paper, what we're going to see is cars that are
similar based on whatever metric we pick, will appear near each other. And so, we'll be able to use some
of the gestalt principles visually, in order to find clusters of cars or
distances between cars, they'll correspond to
some kind of reality. All right, there are many, many different
kinds of techniques for doing reduction. This is not a statistics class or
multidimensional data reduction class, there are those if you're interested. There are techniques ranging from
multi dimensional scaling and principal component analysis, linear
discriminant analysis, factor analysis. All these things are different names for
related ideas and unfortunately, I can't go through all of them. What you do need to know is
that picking the right one is kind of a statistics problem. So, you have to look at the data
distribution and you have to understand the kind of question that
you're asking about the data. So, depending on those things, you're
going to pick the right one and we'll give you some resources to look at if you
need to decide between different options. But you can start with very standard ones,
a lot of the packages like scikit learn and all these programs that you might use
in Python, implement a bunch of these. And you can start playing with them,
start plotting your data and seeing which one might work well. So, here's the basic idea for
multidimensional scaling. So, let's start with just two dimensions
just to make it easier for us. We have a bunch of data that we've
collected for, let's say people and we have weight and
height data collected for them. So we've put different circles
representing these different people on this plot on
the simple scatter plot. What we want to do now is project
this data onto one dimension, so we're going to start simple,
we're going to go from 2D to 1D. We've colored these groups based
on something that we know, so we have some ground truth, some knowledge
about the way these things are. What we'd ideally like is for all the blue points to sit near each
other on this one dimensional line and all the kind of reddish orange
points to also sit near each other. What we don't want to have
happen is the blue points and the orange points to sit near each other,
those should be far away. So, one way of doing this is we might
think of a line that we draw in the space and then we project all
the points onto that line, okay? This unfortunately does not
meet our criteria, okay? All the points that should be near each
other are still near each other, but points that should be far apart,
those orange and the blue ones are near each other as well. So, all the points have basically
been projected into the same kind of small spot. So this is a bad projection, this is a bad multi dimensional reduction
technique because it doesn't preserve the relationships that we
had in the original dataset. Instead, things like MDS and related
techniques try to find a better line, a better way of projecting
these points through space. So, what is a different way of projecting,
what is a different line that we can draw that optimizes those kinds of
relationships that we want to preserve? So, this line does support the kind of
thing that we've been talking about, which is that all the blue points
basically sit near each other, all the orange points sit near each other. And the cluster of blue and the cluster
of orange, sit far away from each other. So, this mechanism reflects the underlying
reality of the data in a much more realistic way. It allows us to see that there are two
different clusters in the data. So in this particular case,
we had some ground truth, in another case, we might not even have that, right? Like all these points might just be black, we haven't allocated
them to a specific class. We don't know that one of them might
reflect, I don't know, teens and the other one like older adults, right? Those might be two different groups
that we see and we don't know, sort of what group each point falls into. We just want to know if there
are groupings that naturally happen and so multi dimensional scaling, as
this kind of technique, will allow us to identify
those groups by just looking at this line. Now multidimensional scaling, of course, works better if you use more
dimensions than just one. It doesn't usually make sense to
project onto one dimensions if you have multiple and so we're going
to use largely two or three dimensions. What you should know, though, is multidimensional scaling
is an optimization problem. Imagine trying to find a line
basically by rotating it in space, so that you find the optimal projection
to preserve those kinds of distances in the original data set. You do have to define an equation, basically describing either the similarity
or dissimilarity between two points. So you as the designer of this
algorithm or the user of the algorithm, need to define these distances and we're going to talk about a few
different ways of doing that. And then the end result,
once you feed in this matrix of distances, you're going to get back
this two dimensional or three dimensional projection, which you
can plot out as a basic scatter plot. Okay, so the basic idea at a high level is this
is kind of how the algorithm works is. You have the data type of the variable, so you have that initial table
that we've started with, where each row represents an entity and
each column represents some aspect of it. Weight, height, year of birth, whatever
it is that we might have collected about that specific individual, or you'll
see it with cars about a specific car. We're going to then transform that into a
matrix that reflects the distances between each of those rows, so
that will be the data times the data. So, on the X axis and
the Y axis of that table, we're going to have the same information,
like the car or the person or whatever and then cell within that table,
we're going to calculate the distance. So, we have to have some
function that does that. To that table we can then apply
a multidimensional scaling or one of these other techniques
to get the two dimensional or three dimensional projection. So, the pairwise similarity calculation
is kind of the first place where you need to make some hard
decisions about your data. Depending on the data, you might pick very
different kinds of similarity measures. So, I'm going to show an example in
a second where we have data from fish. Like they've collected a bunch of genetic
material from fish and they want to basically do multi dimensional scaling,
that kind of projection on that data. So, a similarity function that we might
use is how many genetic markers overlap, so we can just count those or
find some other way of measuring how many of the jaccard or
dice as other measures. So basically, how many overlapping
genetic markers there are. We might have more sophisticated
data like the car data sets. So, if we want to determine how similar
two cars are, one to the other, we might have to invent some new function to allow
us to make that kind of comparison. So, we might compare how many
car doors they have in common. So, one car has two doors and the other car has two doors then
the distance between them might be 0. But if one car has four doors and
the other car has two doors, we might want to have a larger distance. We might want to add some factor for
the cylinders and factor for the year of manufacture and so on. And so we can define a different kind
of function that is used to represent the distance. In this particular example
that's on the screen now, we've just calculated this using
a modified Euclidean distance. So, if you remember how you measure
the distance between two points in space, it's the square root of X1 minus
X2 squared and so on, right? So, that's a different way of doing it and
supporting this. Okay, so here's an example of
multidimensional scaling with real data, as I promised this is fish data. And this happens to be fish that were
collected in the Mediterranean around Sicily and Corsica I think, so
those are the two islands being displayed. The data was collected for
a bunch of different fish species. They looked at the similarity
between genetic markers and then that middle plot is basically the
multidimensional scaling of the fish data. And immediately you can
start seeing groups, okay? So, they have some additional
knowledge of fish and their relationship to each other or where
they collected the fish in this case. So, you can see the red fish came from one
area of the island, there's the green, the yellow, the brown, and
purple and so on. These are different fish that were
collected in different places. And the nice thing about the
multidimensional scaling is it confirms for us, some of what we might expect. The fish that come from
places near each other, so like all the red fish that were collected
from that one area of the island, all have very similar genetic information. And that is very far apart from the purple
fish, the fish at the bottom of the other island, which are quite
different genetically from the red fish. So, you can see they're very far away in
this multi dimensional scaling plot, but very near each other, the purple to purple
fish sit near each other within that plot. The green is interesting,
it splits because some of the fish come from part of the island that is
on the one side of the tip and the other part of the fish come
from the other side of the tip. And so, the island at the top
is basically splitting that and you can see that a little
bit in the genetic profile. The green fish are a little bit similar to
the red and depending on where they are. So again, this is kind of a nice
confirmation of what we might expect, given the way the world naturally is. So, this is kind of a nice double check,
because in some situations, we're going to use multidimensional scaling and we're not
going to know if we did the right thing, like if it was doing the right thing. So, having that extra confirmation, extra
sanity check that it matches sort of our expectations of reality is a good thing. I will say that there are some
dangers to that kind of behavior and we will see that when we talk
about text visualization. Where if you do too much of that,
you read too much into the data, you're going to get the wrong thing. So, in a running example with our cars, this is a multidimensional scaling
projection of the car data. It's probably a little bit hard to read,
but each of these different labels represents
a car and in the space, you have very different cars that are clustering near
each other based on various properties. So, everything from like speedy
sports cars in the upper right, to kind of more heavy sedans in
the upper left, okay, to smaller cars, smaller compacts down at the bottom. So, this is a representation,
that again kind of makes sense, we get some confirmation that the
multidimensional scaling has performed, as we would have expected. Because the cars that we know are similar
to each other, sit near each other, on this multi dimensional map. And the cars that are very different
that we know should be very far apart, are similarly very far apart. So, as I mentioned, there are some
challenges to dimensionality reduction. One that we haven't talked about is that
the dimensions of the visualization, sort of what we have at the end over here,
the X axis and the Y axis don't mean anything anymore. Oftentimes, people have an expectation
that things on the X axis, things on the Y axis should
mean something, right? Like we project the height and the weight
onto the X axis and the Y axis and that goes away with
multidimensional scaling. In fact, you could basically
rotate the scatter plot and if you run the algorithm
with different parameters, you might very well get the entire
thing flipped or just slightly rotated. And it's still preserving all the
distances as we would have expected, but doesn't map into any sort of natural
dimensions that people like to read into. And so, convincing people to
understand that when they look at these visualizations is a particular challenge. You have to basically get them to not
think in terms of more on the X axis and less on the X axis or
more on the Y axis less on the Y axis. Those things, those concepts don't always
mean anything in this kind of projection. And the thing that I did mention, and we'll
come back to again when we talk about text because it's a particularly bad problem, Is that depending on how
you run the algorithm, you're going to get very
different kinds of projections. And there's a tendency because of
the randomization to over read into the visualization. So, we're going to see examples where
people are going to start telling stories about why the data is the way it is. When it's in fact just kind of
an aspect of the algorithm, the underlying sort of magic that went in, the random variable that was used
to initially seed this data. And so, we're going to have to be a little
bit cautious about our use of these kinds of techniques, in that they might show people something
that isn't actually there, okay? And because of the gestalt principles, like our desire to over interpret by
seeing groupings, because they're sort of located near each other in space,
that's going to be problematic. All right, so the takeaways,
when you have too many dimensions, you might choose to
basically just reduce them. So, this is our first technique for
the problem of having multi dimensional, multi variant data sets that are too big,
so doing some kind of data reduction. We talked about multidimensional
scaling largely in this. But there are many other alternative
algorithms and they all have sort of weird, different properties that
try to optimize different things. Some of them try to optimize variances, some of them try to
optimize the projections. Whatever it is, so
you have to be a little bit careful and have to learn how to use
these techniques effectively. All of them, though,
are optimizations, right? We're throwing away data, we had lots and
lots of different columns, we've reduced it down to two. So inevitably, we've thrown things away, we've tried to do that in some optimal
fashion, but it is an optimization. And so, it is an imperfect
representation of our original dataset. Your choices will control very much
how these things happen on the screen. So, you're going to have lots of
algorithms, lots of parameters for those algorithms that you're
going to have to pick between. And so you're going to have to do,
at least you're going to have to try a few different approaches when you get data
sets to sort of see which one works. But it's better to have an underlying
theoretical understanding of these things before doing it too much and over
explaining, based on what you're seeing. All right and with that,
thank you for listening.