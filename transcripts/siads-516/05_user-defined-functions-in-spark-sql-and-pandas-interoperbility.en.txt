Now we talked about user
defined functions last week. And this is exactly the same
idea using Spark SQL but we have to do one extra step. We need to register these UDFs in order to allow the Sparks SQL interpreter
to know that they exist. Because remember that Spark SQL
query is just a string and we're going to be referring to
the text version of the functions. So here's how this is going to work. So we're going to do up here
the same approach that we did of defining a plain old function. This is a plain old python function,
it's ridiculously simple. It takes a number and returns the square of that number
by multiplying it by itself. The next component is exactly
what we did last week, where we took that same function, square. We used a lambda operator
here to allow us to create a function that will take every value and
apply that square function to it. And remember the one
trick with using UDFs in Spark was that we had to
define the return type. So we had to say that this
is an integer fine, so we follow exactly those same steps. If we want to use user defined
functions in Sparks SQL except we have one more step. The extra step that we need to follow here
is to register using Spark UDF register. A name of that function and the type that particular
function itself so square. So we need to be able to register
that function that allows us to come down to the query statement
that you see in the next block. And use square udf int, here as that function that we registered up top. So we could use that within
a plain old string that we're passing as a query to the query
engine in Sparks SQL. Now, this is a somewhat
fictitious example, why would we want to square the counts,
who knows? But it shows the point that we can
register that function on the outside of the SQL query string. And then use it within
the SQL query string itself. So we're going to be using UDFs in
Sparks SQL and we'll see how those work. The last thing that I want to
mention is we can get our data out of Spark by using two
Pandas on a Spark Data Frame. Now, we'll create a Panda's Data Frame, we
talked about doing things like collect or show. This is another way to extract data
want you to be careful with this though. Because you don't want to be
in a situation where you have a massive pet a massive Spark Data Frame. And you try to convert that
to a Panda's Data Frame, you'll exhaust the memory on your machine. And note that you can always go the other
way by providing a Panda's Data Frame to spark through its initialization. So we initialize from lists or sets or
dictionaries or CSV files or JSON files. We can also initialize a Spark Data Frame
using a Panda's Data Frame. Let's go over and take a look at how some
of this works in a real Jupyter Notebook.