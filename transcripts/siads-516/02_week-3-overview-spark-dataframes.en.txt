Hi, and welcome to Week 3,
Spark DataFrames. In this week, we're gonan do a review of the Spark APIs that we covered
in previous weeks or last week. Then we're going to go on and
look at Spark DataFrames. We're going to look at three basic things
that are important, creation manipulation, and finally user-defined functions. So, you might remember from last week
that there are three APIs for Spark. They're RDDs,
which we did cover last week, and there are two additional APIs,
datasets and dataframes. RDDs are resilient distributed data sets,
RDDs. They're relatively low-level,
so you have to get in there and manipulate it the way you want it to. They support creation, transformation, and
actions, and we covered those last time. And you'll remember that for creation we
could use parallelize on an existing data structure or we could load a file
using the text file function. Transformations return a new RDD, and you'll remember those as being things like
map, reduce, reduce by key, sort by key. We also had actions on RDDs
that return non-RDDs, and a couple of examples of those
were count and collect. Now, I want to talk really
quickly about Spark Datasets. Spark Datasets represent
a distributed collection of data. And it turns out they're
only available via Scala and Java, that is,
there's no Python interface. Now, I want to use this opportunity
to talk a little bit about Spark and where it came from. So, Spark was written in Scala,
in the Scala language, and that's the language that you might
want to investigate as a data scientist. Scala runs on the Java Virtual Machine,
or JVM. So there's some functionality that
is hidden from Python because Scala is the native language for this. As I said, we're not going to
be looking at Scala, but there is some additional functionality
in Spark that's available to you. When you see error messages in Spark, sometimes you'll see
things that mention Java. Don't freak out, that's just a function of
the fact that Spark is written in Scala, which sits on top of Java. Now, I say this all really as
an aside because we're not going to be using datasets because there
is no Python interface to them. So, let's focus on Spark DataFrames. Now, DataFrames are Spark Datasets,
the things we're not going to look at, organized into named columns. And that should sound familiar
because they're tables, and conceptually they're very similar to the
DataFrames that you see in Pandas or R. So, here's how we get started
with Spark DataFrames. All of the interaction with DataFrames
is done via a SparkSession. And that's an entry point to programming
Spark with the DataFrame API. To create a SparkSession, we're going to
use the builder pattern shown above. And that should look familiar because we
used that last time, we had an additional line where we then extracted from
the SparkSession a SparkContext. And remember, the SparkContexts
are how we interact with our RDDs. SparkSessions, which
contain SparkContexts, are how we interact with Spark DataFrames. I know these terms are very similar,
they can get very confusing. SparkSessions are something we create
at the beginning of our script or our notebook, and for
the most part we just leave them alone. So you'll see that builder pattern when
we get to working in the notebook. Now, to create a DataFrame,
you need a SparkSession. And once you have that SparkSession by
using that builder pattern we just saw, you can create a DataFrame. And a DataFrame can be
created from a list, an RDD, or usually a specially
formatted JSON file. Although recent versions of Spark do a much better job of handling
almost any type of JSON file. And when I said a list, I really
mean any simple structure in Python. So then we're gonan create
a data frame from a list. We can do this either from a list of
tuples, which would include a list of column names or a list of values where
we're going to specify the value type. This is where things get a little
confusing and a little overwhelming, so we're going to walk through
these step by step. If you look at the box on the lower
left-hand side of your screen, you'll see that I have a variable
called df from other list. And I'm assigning to that the output
from a spark.createDataFrame function. You'll see the argument that I'm
passing to that consists of two lists. The first list is a list of tuples,
so I have a name and a value. So I have Chris 67, Frank 70,
that's the content of the first list. The second list is a list of the names of
the columns, so you see name and score. Right below that, you see another line,
df_from_other_list.show. We'll talk about show and a bunch of
other commands to extract the contents of a data frame in just a few minutes. But for now, look at the output. So the output from show
gives us these two columns. First column is named name,
second column is named score, and the contents are hopefully what
you expect, Chris 67 and Frank 70. So that's the simplest way to create
a DataFrame with meaningful column names. Let's take a look at the box on
the right-hand side of the screen. And I want you to focus
on the large text first, df_from_list = spark.createDataFrame. And now I've wrapped this around,
I have a list of floats from 1 to 5. I also have something there that says
FloatType with parentheses after that. It turns out that Spark needs a lot
of help of figuring out what type numbers are, or for
that matter, almost any object. That's part of, again,
that legacy of coming from Scala or Java where we have
strongly typed variables. In other words, a float is a float, an integer is
an integer, a string is a string. We get a little loose with that in Python,
sometimes we can automatically promote an integer to a float, we can get
a little bit sloppy in our practices. In Spark we have to be very strict
about what type of number this is. So what I'm instructing Spark to do
by using createDataFrame on a list of what appears to be floats and then giving it a float type is ensuring
that that column has a type of float. So these are floating point numbers. To access FloatType, I need to import that particular function
from pyspark.sql.types. So that's the line that you see just above
the df_from_list where I do that import, so from pyspark.sql.types
import FloatType. And again,, just below our assignment,
we have df_from_list.show, and that's how we're going to
take a look at our values. And that will output that table
in a way that we can read. So, before I turn over to the notebook, I
want to talk a little bit about displaying and extracting DataFrame contents. A you're relatively limited in
what you can do within Spark. And there are reasons for that too,
because we want to make sure that we're processing this large data,
this big data in a very efficient way. So, the most common way of extracting
data from a DataFrame is to use the ,show function. And by default,
that shows the first 20 entries. In fact, It's almost always
just the first 20 entries. Df.first will show just the first entry. So just the first girl that comes back. The next two things that we can
do are df.head and df.take or dataframe head, dataframe take and
the default is 5 for that number. If you don't specify a number,
it'll take the first five, head and take are exactly the same. Head is actually a recent implementation,
it used to be called take. Finally, we have the collect function. And this will return a Python list
of all the data frame rows and I have their danger. Because again,
we're dealing with big data, we can have potentially millions,
billions, trillions of rows, when you call collect,
it will return all of those items. So thinking back to our mapreduce
framework, we want to make sure that we have reduced the data down to as
summarized as possible before we effectively pull that out of spark for
processing in something else. So let's take a look at
this in our notebook. And I thought we'd do a quick
review of RDDs from last week. Remember our pattern here for working with
Spark is to use this builder pattern. So I'm going to run that and
just to review what we're doing here. We're importing Spark session
from pie Spark's SQL and then we're creating a new variable Spark,
which is the return value. And now at this is all chained together,
but I've split it in two separate
lines just for readability. That's what the backslashes are there for. I could concatenate all
those things onto one line. I would have it scroll off
to the right hand side. It would be hard to read. So I've used those backslashes
to improve legibility. So SparkSession.Builder.MasterLocal [*. What does that mean? So local[* means I want to use
the local processing backend. That is in your case, the instance
that we're running on Jupiter and * means use all the processors. So here you're getting some hints
about large-scale computing. Let's say that I have
a 1000 core machine and I only wanted to use
a hundred of those cores. I could use local[*100 in that case. I think in our case the star
corresponds to 4 cores. There are two cores that
are multi-threaded. App name, you can use anything
here as long as it's unique for all of the concurrent
jobs that you're running. Now, you'll probably see me just
use my first Spark application throughout all my work here. I'm never going to be running
two Spark jobs at the same time. So again, you need something there,
it doesn't really matter what. Then you'll see get or create. That's kind of interesting if you think
about it because why isn't it just get? It turns out you can only
have one Spark session active at one time for
a particular script or notebook. So get or create will get the Spark
session if it already exists or create a new one. So the first time through,
it will create one. Second time through, it will just
get the one it already created. That all happens under the hood. You don't have to worry about it. And just to drive the relationship
between spark and Spark and context home, you'll see that next line where I'm
assigning to a variable called SC, the attributes Spark context
from our Spark object. So those two things are highly related and this is the part that's
a review from last week. So we have this text file data
/ blurb that we're reading in. We have data lines as our next
one is going to be a filter for all the lines that contain data. So let's take a look at what those are and
we'll take a look at datalines.take. This is working from RDDs remember so
we have all the lines from this blurb. If you read that blurb, you might
recognize where it comes from and because these have been filtered on data, we have all of the lines
that contain the word data. Showing you again how we use first, we can extract the first line of
our filtered lines using first. And reminding you,
once again that we can use parallelize to take an existing data
structure like this one. 1, 2, 3, 4, that list of integer
values and we're going to use the map function with an anonymous function to
square those and then collect them. So what does that give us,
that'll give us 1, 4, 9 and 16. So we're linking RDDs here
from Spark with plain old Python in a for
loop of those squared values. So remember, when we do collect in RDDs, we get a list of probably plain old Python
objects like numericals or strings. Carrying on with our review,
we can also use flat map and and mapping to give us
something like word counts. Here, we have a word count reduced by key. You can study this code or
we have an accumulator and a value and we're adding that
value to the accumulator. Take the first top 10 words and
we find out or the first 10 words rather and
we find out that's what we've parsed out. We can sort those by key and
add them up, collect them. We find out the word counts for
each of those words. You can see here,
it's a pretty long list and then we can take the first 10 of a sort
by the second value of the tupple. And we get the most
common words as being and data of to a blank now,
that's interesting. We might want to go back in our code and
see where that came from. But that's a quick review of some of
the work we did with RDDs last week. So here we are with week three and data
frames and I'm going to run this again. Of course, I can re-import things and you'll notice that it didn't complain
about Spark being created again. We used get or create. It actually just got the Spark
session that was already running. So here's the code from our slides where
we have a data frame from list and that's going to do create data frame
on Chris and Frank with name and score. So we can change this for
example, from Frank to Matt and that will change the output. Okay, so pretty straightforward stuff. A little bit of a strange syntax
where we have the index names over as a separate list,
but you get the idea. Now here's the example where
we used floating point values to create a one-column
data frame called value and it's called value by default. And we'll just leave that as
the default value for the column name. Here are some of the extraction
routines that we looked at, there's show,
which will give us the entire data frame. That first, if we call first,
we actually get something back, that's a data frame row. And we'll talk a little bit later about
how we can manipulate that object, which is a specific type of
object that comes from Spark. It converted into
something like a dictionary which might be more helpful for us. So here we can't really do anything
with that until we convert it to another Python data type. Carrying on our ways of extracting things,
we have head, Now in this case I've asked for two so
we get two rows back as a list. That means we can iterate
through that list. Take is exactly the same. It gives us the same output and
finally collect, in this particular case gives us
all the same thing as the take and the head because there are only two
elements in that particular data frame. So let's talk about a slightly different
way of creating a data frame and this is creating a data frame from an RDD. So you're going to promote
an RDD into a data frame. And it turns out this is pretty simple if
you're okay with the default column names, which you'll see aren't really great. Otherwise, you'll need to
create a row in which you will communicate what column names you want. So let's see what that looks like. Down below on the left
hand side of the screen, you'll see I'm just creating
a simple RDD called lot_rdd. Lot stands for list of tuple. So lot_rdd is going to be
a parallelization of a very simple list of Chris ,67 Frank, 70. So exactly the thing we saw before, but we're going to parallelize
that into an RDD. Now, of course if you were just
starting from that list and going right to a data frame,
you wouldn't need this but I'm saying let's just generate an RDD
because you might have an RDD lying around from somewhere else that you want
to promote up into that data frame. So over on the right hand side here,
you'll see I'm creating a data frame from an RDD using Spark create
data frame as simple as just supplying the name of the variable containing
the data frame over here in lot_rdd. And then I'm going to call show
to take a look at our work. Now let me give you another
way of doing this and then we'll turn over to the notebook to
look at how we did this in real time. So, to access rows, I'm going to have to
import that from the Pyspark SQL Library. So I need to access that
particular object type. And then what I'm going to do is I'm
going to take in my next line where I say lot_rdd name columns is going to be that
lot_rdd so that very simple list of tuples RDD that we created on the left hand
side call the map function on that RDD. We should be getting more and
more comfortable with that notion of calling map with a function and
this is an anonymous function. So we're using Lambda. We could define the function elsewhere and
then call the function but right now, it's pretty simple. So I'm going to leave it here where I'm
going to take for every element for every row in the original RDD,
I'm going to create a spark row. So that's what's over
here after the x: Row and I'm going to pass to that
row a pair of things. I'm going to say name equals x zero and score equals- now this time I'm going to
convert to an integer just to show you how we can convert to an integer
of the second element in that tuple. So I have name equals
the first element and score equals an integer
of the second element. So I'm going to create that lot_rdd
name columns just to have it there so we can look at it if we want to. And then I'm going to create a new
data frame dfPeople_named_columns is going to be a spark create data frame
just like we did with the other RDD but this RDD looks a little bit different and
then I'm going to show you the results. So, the outputs of those two
shows are given to you below and what I'm getting at here is you see
how those first column names in that first data frame are underscore
one and underscore two? Again, if you're happy with that and you
can keep it straight ,if it's a relatively simple process that you're doing,
that's fine. But if you want to indicate what
the names are, remember in an RDD, you can't name the columns. That's why we need to use this
special form of name equals value in our mapping from the RDD to give us
the names of the columns that we want. So let's take a look at that
in the jupyter notebook. So let's go ahead and
look at that in the notebook itself. So here we're going to create an RDD,
which is what we did, what I showed you in the slides that
has a list of tuples right over here. So we have Chris and Frank with their
scores associated with them and I'm going to create lot
underscore rdd with that. The next thing I'm going to do is go ahead
and create that data frame from that RDD. In fact, I'm going to go ahead and
split this for you. There's a split cell function here so we can take a look at the output
just of this cell, right? So we have the underscore one and
underscore two. They're not really helpful. We want to get a little
bit better than that. So here's what I propose we do. We'll just leave that on the screen for
you. So we have the underscore one, underscore two from that very simple way
of creating that data frame from the RDD. Here we're going to get
a little bit more complex. I'm going to use this
map function here and call name equals the first
value of the tuple and score equals an integer of
the second value of the tuple. I'm going to create a data frame
just like I did before and then I'm going to show you the output. So there we go. There's the integer form of the values. Let's say I wanted to change this
to something like first_name. I could do that as simply as that, you'll
notice that there are no quotation marks. These are key value pairs where the key
is picked up as a keyword argument. So that's a little bit different
than what you might expect. Let's change that back to name. And if we wanted to make
sure we had a float in there, We could use float there as well. And I'm going to again change that back
to integer just so we have that straight. So another way to create a data
frame is from a JSON file and the syntax for this is pretty easy. It's df equals spark.read.json and then the name of the file
that you want to read. So we'll look at this in
a minute in the notebook, but it's as simple as calling show
on the resulting data frame and you can see this is getting
a little bit more complex. And we have some shortening
of field values and we'll take a look at that in
more detail in just a bit. The nice thing about loading from a JSON
file is that it will infer the schema for you. Now, if you've ever looked at a JSON file,
you'll know that it's got basically two or three structures and it's got lists,
dictionaries and values. So, inferring the schema, trying to figure out what each record has,
can be a bit of a pain sometimes. So we have a handy thing in Spark where it
will try to figure out the schema of each row, of each element in the list,
the big list that makes up that JSON file. And to look at that schema, we're
going to use the printSchema function. So for example in the file I just loaded,
business.json, at the very root of the file,
we have things like addresses, attributes, skipping way down now to is_open,
latitude, longitude and so on. And you'll notice too that within
attributes, we have nested values as well. So we have AcceptsInsurance,
AgesAllowed, Alcohol and Ambience and then for example within Ambience,
we have casual, classy, divey, hipster, intimate, romantic,
touristy, up trendy and upscale. So, we have all of a sudden some hint
that our JSON can be quite complex and Spark is pretty happy handling that,
so that's kind of helpful. In general, we can create a data frame
from any one of a number of files. Now I've shown you JSON, we can use the
generic spark.read.load on some file name. In this case, I've called it foo.json and
tell it what format it is. And we have things like JSON, Parquet is
something that you'll encounter in Spark, JBDC is a connector to a database. We have orc, svm, CSV and text, orc and libsvm are compressed file formats
you'll see in a lot of scientific work. CSV and text files are things
that you should be familiar with.