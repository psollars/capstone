Welcome to Week 4, in which we're going to cap off
everything by talking about Spark SQL. Here's what I want to do in this week. I want to give a little bit of
a review for Spark DataFrames. We talked about those last week, but they're important for the work that
we're going to be learning about today. I want to spend the bulk of this
week talking about Sparks SQL and in particular, things like registering
temporary views, working with SQL, and then moving on to some advanced topics. I'm going to spend some
time doing a SQL review. Now, I know you've just done
SQL in a previous course, but I want to make sure that those concepts
are really fresh in your mind. They're key to understanding
how Spark SQL works. So remember that Spark DataFrames
are the main focus of what we work on in Spark in general. DataFrames are those Spark data sets that
are organized into named columns and therefore they make tables. Conceptually, they're exactly
the same as Pandas, R DataFrames, Excel tables, anything that's a table
conceptually is very similar. And we talked about the ability to for
example, select a column using the select function. We also talked a little
bit about filtering and you probably had a chance to work
through some of this with the homework. So here for example, we have a data frame, in which we're taking the column stars and
looking for entries or rows where the value of
stars is greater than or equal to 4. And remember, we had a number of
different ways of showing our results. .show gave us a human-readable,
.collect, for example gave us a list of rows that we
could further manipulate programmatically. We also talked about groupBy and that allowed us to use summary or
aggregation functions like counting. We also talked about the ability
to sort our results. So here for example, rather than
sorting in some random order or sorting by count,
we could sort descendingly by stars. We also talked about
user defined functions. And I mentioned that these were similar
to the map and apply functions in Panda. Now we had to wrap plain old Python
functions and specify the output type. Now I want to be clear that
user defined functions may have sounded like something that
made it been advanced or optional. But really they're the core of what
allows you as a data scientist to write expressive functions in something
like the Spark ecosystem. So they're important and we're going to
build on those this week as well. So that's great, we talked about
selecting, filtering, sorting, and grouping, but what if you need
something more a powerful? What if you needed to do
more complex filtering? Well, you could chain those filters
together, but then you're sort of stuck with this idea about well, I want this and
that and I want this or that. So the Boolean operators between
filters sometimes gets a bit confusing. What if you just needed some more
expressive language options? There are certain things
you just can't do using the functional approaches to Spark SQL or
to Spark. So how about if I offered you
a SQL interface to your data, without actually having to
load the data into a database. What would that look like?