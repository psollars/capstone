Welcome to week two Introduction to Spark
and Resilient Distributed Datasets. The overview for this week is as follows. First we're going to
talk about what Spark is. I'm going to introduce
you to the Spark stack. We're going to talk about some basic
Spark functionality including something called the SparkContext and
Resilient Distributed Datasets or RDDs. With respect to RDDs, I'm going to talk
about three things you can do with them. You can create them, transform them,
or perform actions on them. And then we're going to build on our
understanding of our RDS by looking at something called PairRDDs. So what is Spark? Well, Spark is a cluster
computing platform. It's generally considered fast and
multi-purpose and it extends MapReduce to add
things like interactive queries, really good for exploratory data analysis,
and stream processing. Spark also supports complex
workflows which allows you to combine different
types of analyses. So you might for, example, need to
manipulate some data into an appropriate form and
then do some machine learning on that. So how does it do this? Well, let's take a look
at the Spark stack, and today we're going to
focus on the Spark Core. Now, you'll see some things
that might look familiar. You'll see something down here YARN,
so that's yet another resource navigator the YARN piece. We're going to be looking at Spark Core
today which gives us that basic functionality. The Spark Core module provides as you
would expect basic Spark functionality. So Spark Core is responsible for
things like task scheduling, for memory management, storage interaction,
and fault recovery. So all the pieces that could
interact potentially with Hadoop are part of the basic Spark functionality. The Spark Core also defines
something that's very important for us called
Resilient Distributed Datasets or RDDs. The relationship between Spark and Hadoop can be a little hard
to get your head around. Now Spark does not require Hadoop but
it can leverage at least it's storage functionality, so there's a good
interaction between Spark and HDFS. Now Spark can also leverage
other types of storage format. You'll hear things like parquet,
for example. You can also use Spark to
read things like CSV files. Now Spark can also leverage Hadoop's YARN
yet another resource navigator as I mentioned in a previous slide
to manage compute resources. It has other pieces available to it,
for example, in the system that you'll be using for your interactive work in the course
we'll be using something called Mesos. A couple of important concepts from Spark
are the SparkSession and SparkContext. Every Spark application has something
that's called a driver program that's responsible for running operations
on a cluster of computing devices. That's important because that
driver program represents an abstract connection to
some computing resources. It doesn't matter if
it's your local laptop, if it's access to an AWS EMR cluster,
or say a super computing cluster similar to that we have
here at the University of Michigan. The driver programs access Spark via
something called a SparkSession. So I want you to be able to distinguish
between a SparkSession object and a SparkContext object. A SparkContext is a component
of a SparkSession. So the SparkSession is
a more umbrella-like term that encompasses the SparkContext. The SparkContext though is what we're
going to be using for this week. To get set up for using that SparkContext
in our particular Spark environment that we have available to us in the course
we're going to use some what I call boilerplate code that I simply provide
to you to create our SparkSession. Now our SparkContext object is
automatically created from our SparkSession. And for now we're going to
focus on the SparkContext, we'll return to SparkSession next week. Here's what the code looks
like to set up a SparkContext. That first line that you
see from pyspark.sql import SparkSession is how we make
the SparkSession available to us. The next line is wrapped around using
backslashes so that you can read it. This could be extended out as one long
line but that makes it hard to read and I'd have to scroll off my
screen to show you everything. So I've used the backslashes
to allow myself to continue the line on subsequent lines
without having to widen my screen. So if we read through that,
I'll read through it and then I'll explain each piece. So I said here spark =
SparkSession .builder .master with some arguments .appName .getOrCreate. So what does that do? Well, SparkSession is that object
that we import from pyspark.sql. It has in it something called
a builder which is something that allows us to create
a SparkSession for use right now. Our master node is going to use all of the
local processors that are available to us. That's what the argument
of local [ * ] means. Our app name is a completely optional
parameter but it's useful in case you find yourself running multiple applications at
the same time on the same Spark cluster. For our purposes were probably ever
going to be using one app at a time but if you have multiple apps we're going to
need to give those apps a unique identifier so
Spark can keep them straight. Finally, we're going to
invoke getOrCreate and that's going to return our SparkSession. Now, it turns out that for a particular
instance we're only ever going to have one SparkSession created,
so that's why we have getOrCreate. If a SparkSession already exists we
can reassign it to a variable, but we can't create a second SparkSession. So that's what's going to set up our
SparkSession in a variable called Spark, so Spark is just a plain old variable
that contains a SparkSession. As I mentioned a SparkContext is
a component of a SparkSession and it's accessed using the Spark
contracts attribute. So the last line on the screen
here sc = spark.sparkContext is simply assigning to
the variable sc a pointer to the SparkContext that is part
of our SparkSession object. Diagrammatically, here's how it works. So on the left-hand side there you
have the Spark driver which is part of the SparkSession,
part of the Spark shell, and that is what allows us
to distribute the work. It manages the running of the components
of our program on the different executors. Now the executors can be these different
nodes in our cluster of computers. They could refer to different
processors on a local machine. This is all abstracted out for
us by Spark. So we as data scientists don't necessarily
want to get involved with the fine-grained details of how we're interacting with
the hardware or the infrastructure. What I want you to take away from
this is that the SparkSession manages the distribution of computing and storage
resources across a cluster of computers.