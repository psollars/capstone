So now that we have some idea about
how to load data into a DataFrame, let's see what we can do
with the DataFrame itself. So one thing we can do is describe it. We can look at, for example, the column
names by looking at the columns attribute and I'm going to make a distinction
between attributes and functions. Functions have parentheses after them so
you'll see describe and show down there have parentheses. Count has parentheses. Columns is an attribute. It's just a variable within that DataFrame
so we don't have the parentheses there. So df.columns has or
contains the column names. df.dtypes shows the data types or
the data types for each of the columns. df.describe will calculate some simple
statistics on the entire contents of the DataFrame and it will return
a DataFrame of those simple statistics. So we're going to call show on
the results of describe, got that? So df.describe gives us
a DataFrame which we're going to call show on to actually look at. And df.count will count
the number of entries. That's something you'll often want to do
because we're trying to answer questions like how many of something exists? Okay, one of the things we can do given a
DataFrame with a whole bunch of different columns is select one or more columns. And we do that using df.select. Now, next week, we're going to be
talking about select from SQL. This is different. This is selecting a column name. I know it's a bit confusing but
select will extract just one or more columns,
depending on what argument you pass to it. That will give us back a DataFrame. So we're using df.select on a name
which will give us a DataFrame. So we're going to call show on it. That's the output that you see here. We can create a new column
by using withColumn. Now, withColumn is built into a DataFrame,
but what we're going to have
to do is give it a name. So in this particular case, I'm asking you to look at the second
line where I say df.withColumn. And then I have a new column name and then I'm going to use the col,
the col operator. So when I say col, I'm referring to col
as in column of the old DataFrame name. So it's a very strange convention
is something that again, getting into Spark takes
a little bit of effort. This is one of those places. So after bringing that col function
from pyspark.sql.functions with that import line, so
from pyspark.sql.functions import call. And then I'm going to create that
new column new by calling col on the name of the old column. So we're going to have to supply that new
column name as well as the data source, optionally with some transformations and
we'll take a look at that as well. Deleting a column is
pretty straightforward. We call df.drop with a DataFrame dot drop
function and we give it the column name. Remember all of these things return a new
DataFrame because they're built on RDDs. RDDs are Resilient Distributed Datasets
that are immutable. You can't change them. You can just return a new one. Now, you can overwrite the same
variable name with that result. So it looks like you've done it in place. But in fact,
you are creating a new DataFrame. Filtering is done in the same way that
we see filtering in things like pandas. So we're going to call
the dot filter function. And then we're going to
pass it an expression similar to the boolean masking
that you see in pandas. So here I'm going to filter that DataFrame
for those cases where the stars column, so I have a stars column in that
DataFrame, are greater than or equal to 4. We'll get into why we're talking
about stars in a minute. That again will return a DataFrame. So we're going to call show
on it to see the results. Now, I've hinted at DataFrame rows
before and they're called rows but there's still an obstruction of
the concept of a row from a table. And so when we call something like
collect, so I've got that DataFrame filter and I've given it some
condition in the previous slide. We saw that that was the number of stars
being greater than or equal to 4 and then I call collect on that. What that's going to do is it's going
to return a list of DataFrame rows. So to pull out a row, I'll just look at
the first element there, result 0, so that's a plain old list. I'm just pulling the first element,
the zeroth element, and then I'm going to take a look at that row. We'll see that in a second. I can call the asDict function and
then perhaps look at the values there. But asDict will convert that row,
that Spark row into a dictionary and then I can do the usual
things with that dictionary. I can iterate through
the key value pairs in there. I can iterate through the keys of that and
so on. So we'll take a look at that in a minute,
but I wanted to get that out there because we're also going to be doing
things like groupBy and sorting. So we can group by any column. This is an aggregation like what
we've seen in pandas with groupBy. We've also seen aggregation in SQL
that you've done in previous courses. So that first and that first line there, df.groupBy("stars") will
group by our values of stars. It will then require us to call some
sort of summarization function. Count is very common. And then I'm going to show the results. So that's what gives us that DataFrame
that's represented on the left hand side of the screen where we have stars and
counts. But you'll notice that that's not
in a particularly useful order. It's actually in the order in which
those values of stars is encountered. So we're going to do something a little
bit fancier in that second line. We're just going to add
an element to that. We're going to do df.groupBy("stars"). We're going to count it,
which is exactly what we've done before. But now I've inserted another
step here to sort on the stars column in a descending matter, that is
ascending=False and show the results. So there you see we're getting something
a little bit more interesting. We're getting the rankings. Let's say that these
are rankings of reviews, the number of stars from 5 to 1 and
then we have the counts as well. And you'll see that
the counts are not in order. The stars are in order. You can probably figure
out how to sort by counts. We can also create a new dataframe with
a subset of columns from our dataframe. So if we wanted to create a new dataframe,
we could call toDF on our original dataframe and
specify the columns that we want. So this is similar to select. We can, of course, rename columns and the
rename function here is withColumnRenamed. Give it the old column name and the new column name that will return a new
dataframe, which we can call show on. Now, here's where things get interesting. And I need to give you this piece of
information because it'll make sense, I hope, both now and
when we start talking about some of the more advanced things we can
do with user-defined functions. What explode does is it will create a row
for each value in a list or array or complex or compound data structure
in a column in our Spark DataFrame. Okay, so what does that mean? Let's go over and take a look on
the right hand side here where we say df_from_other_list2 =
spark.createDataFrame. We've seen that pattern now. But what's a little bit different here
is I have again, this list of tuples and I have a list of Names for the columns. But what I have here is instead of Chris
67 Frank 70, I now have a list of scores. So I have Chris 67, 42, and Frank 70, 72. When I go ahead and
create a dataframe from that and look at the output you'll
see it right there. So it actually represents
the scores as a list. So yes, you can have lists in
your values of your columns. That's interesting. That's a little bit different than
what we've seen in a lot of cases. You can actually do
that in Pandas as well. The Explode function will take
whatever column you give Explode. It will repeat that row over and
over again exploding out that list. So you have an entry for each of those
values, so let's see what that looks like. On the right hand-side there we're
going to have to import the Explode function from pyspark.sql.functions. And then we're going to
create a new column. Remember we did that with
the withColumn function. So we're going to create
a new column called Score. And notice that's singular compared
to the plural value of scores. And that column is going to be populated
by the return value of Explode called on the scores column. And if we take a look at that,
here's what we get. So we get Chris, you see 67, 42 for
scores and then my score value is 67. I get another entry Chris 67,
42 and then 42. So you see how the 67 and 42 have basically been turned
into vertically oriented values. And then we do the same thing for Frank. It's a little weird but super useful for reasons that you'll see when we
get to user-defined functions. Another thing that we're going to
do is introduce a condition. So when some condition exists,
we're going to set a value. Otherwise, we're going to
set it to some other value. So it's like a built-in if else statement
that we're going to evaluate per row. So to do that we're going
to use the when operator. And that's part of pyspark.sql.functions,
which I like to import as a capital F. Again, it makes it a little bit
easier to see what the code says without having to type
pyspark.sql.functions.when. I'm also importing COL because
we'll need it a little bit later. So operating on that exploded
data frame that we just saw in the previous slide, I'm going to create
still one more column with withColumn. And we're going to call
that new column good. Then what we're going to do
is use the when function. And that's in pyspark.sql.functions,
which we've imported as capital F. So I'm calling capital F.when. The condition that I'm setting is saying,
from the exploded data frame when the score value is greater
than 50, I'm going to set it, the value that is, of good to 1. And then otherwise, so
we often use F.when.otherwise. So if else,
that's the mapping when, otherwise. So again backing up F.when,
the score's greater than 50. We're going to set this value to 1. Otherwise, we're going to set it to 0,
and I'm going to show you the data frame. So there's the output and
if you walk down the score column, 67 yields a value in the good column of 1. 42 gives us 0, 70 gives us 1,
72 gives us 1. So that's a built-in if else statement. So let's take a look at this
in the notebook itself. In this cell, I'm going to o ahead and
load a JSON file. I'm going to load that into
a variable called business. And it's going to be
the contents of this file called data/yelp_academic_dataset_business. We're going to be using
the yelp dataset set a fair bit in this segment of the course. So it's worth getting used to
what this dataset looks like and examining some of it. So we just loaded that into business and
we can take a look at the contents of it. Now, this looks a little bit messy. It's wrapped around. I think we have one, two, three lines. This is a big table that's
been wrapped around. We see address, attributes, business ID,
and so on and these entries below. So here we have address. Here's the address for the first
business here, something on Camino. We have a bunch of empty values here,
which is for attributes. We have a business ID,
which is again truncated. We have categories these would be cut. So here we have Golf and Active Life. In the second row,
we have Specialty Food and then something that's truncated and so on. So show is a human-readable-ish
sort of output that we get. Okay, so that's show. Let's go ahead and take a look at
what the schema of this looks like. So the schema here gives us a listing
of the fields and their subfields. So these are columns and columns within
those columns not necessarily a list of values like we saw with the scores. But these are actually
columns within columns. So we have address and attributes and
these are the different attributes. As we look further down this,
we have things like business ID, categories, city, hours where you get
an idea what these things can be. You also get an indication, for example here,
that is open is a long value latitude. Looks like it's a double precision float. Postal code for example is a string. So when spark loads a JSON file, it infers not just the names
of the columns, but also takes its best guess
at the column data type. We can extract just the column names
using the column's attribute, so that's a little simpler and
you see it gives us a list. The reason we're interested in doing this
is you might want to programmatically do things with the columns. So you don't want to have to use show and
then try to examine the output of show. If you have columns here, you can iterate through that list
just like you would any other list. So here's an example of
what that might look like. I'm just going to open up a new
cell here and then say for, B in business.columns: print("Column is ", And then b, so you see how I have access
to the individual column names here. Another thing we might want to look at
in that data frame are the data types. So here we have a dress is a string. Here we have attributes. Now, I'm going to skip over this for
now because, remember, attributes is a column
that has columns within it. So it has this structure within it but
going further down here, we have things like is
open which a big integer. Latitude is a double. Longitude is a double. Again, it's the same thing
we see in Printschema. But Printschema is designed for consumption as a human reader
who's looking at things. D-types allows us to programmatically go through each of the columns and
figure out what data type it is. Now, remember,
we talked about describe and show, so describe will give me back a dataframe. I'm going to call show
immediately on that, and that it will take a few seconds to run. So here's the output, here again, it's
wrapped so it's a little bit difficult to see but you have things like the count
the mean the standard deviation. So it doesn't make sense for
some of these columns for other ones it does we have count, mean,
standard deviation, minimum and maximum. So you can imagine a CSV
file say that we load up and we want to calculate some
simple statistics on it. Again going through our extraction
functions we can count the number of businesses there are we find out
there are a 192,609 of them. We can also do things like manipulating
the extraction of columns, so we can extract just the name column and
I'm going to call show on that. So here's business that's the name
of the data frame, select name and show that column, so that will give us
a data frame with one column in it. Now we talked a little bit
about creating a new frame, a new column rather,
using the with column operator. And here I'm going to create a new
column called, hundreds_of_reviews and I'm going to take the column
review_count treat it as a column and divide each value by 10. So, with the width column operator,
we can do a simple or a more complex operation on another
column and get some results. So here's df business,
df = business.withColumn, if we now take a look at that data frame
by looking at say take the first 4, we get something that looks like
a roll like this a little hard to see. Let's take a look at. Just the columns here. And we have hundreds of reviews. I'm going to drop that for now because I don't want to have that in
there to mess with the rest of our data, so I'm going to go ahead and drop that
using the drop command that we reviewed. Okay, let's take a look at df.filter and
here I'm going to do exactly what we did in the slides,
I'm going to filter out those those rows. I'm going to filter and return only those rows where the number
of stars is greater than or equal to 4. And here it's a little, again,
hard to see because it wraps but the stars is the second-to-last value, here we have a 4.0 of 5 .0 a 4.0 and
so on. So these if you scan down this list
you'll see that all the values of stars are indeed greater than or equal to 4 and
that shows the first 20 rows. If we want to get all of the results and
not just something that is human readable, we can use collect which will
return a list of spark rows. If we go ahead and assign that to
a variable called row lowercase row and then see what that looks like. We find we find the representation
is this spark row with fields like address and
there's the actual address itself here. Attributes, again, is that strange
structure, so attributes actually contains its own row, which is interesting so
it's somewhat recursive. And you can see more complete entries
here like categories for this particular entry in the business data frame would
have sushi bars restaurants and Japanese. So we can take a look at that row as
a dictionary and then look at the values. So here we have the values without
the names of the fields itself. So this is equipped to converting
it into a dictionary and then looking at just the values. We can also iterate through each of
the values in the row and extract it, so we can print out there's the address
there's the role of attributes down here. Here's the business ID,
there's the categories, city and so on. Here's another way of treating that as
a dictionary we can extract the keys, so we might want to use that for
programmatically analyzing our data as well and finally here's another way of
doing it using the dot Keys function. So equivalent to the previous line
remember that asDict if you print an item in the dictionary, it gives us exactly
the value the keys does as well. Now let's load up a whole bunch more
data for the next part of this notebook. So here I'm loading four additional files,
checkin, review, tip and user and you'll see how the variable that I've
chosen checkin corresponds more or less to this long name. So here I'm loading up the checkin
file the review file the tip file and the user file. I'll give you some more details
about the Yelp data set in the homework assignment for this week. So for now you can kind of
guess what's in these files, so check-ins will have some
record of people's check-ins. Review are the actual reviews
that that person gave for the business that's in their. Tip is people's tips about that business,
so it's like a very very short review. And user gives us some information about
the user ID not their real identity, but some other information about them. Okay, now that we've loaded up our data we
can go ahead and look at the schema for example, I'll show you the schema for
review because it's relatively simple. So this is a much more reasonable schema,
it's not anywhere as complex as business although we'll be spending a lot of
time with the business data frame. So here we have a schema that has the
columns from business ID down to user ID with things like cool, date, funny, review
ID, stars, text and useful in between. So, let's take a look at how we
might manipulate another file. Let's take a look at the schema for
user for example, so let's change review to user
take a look at the schema there has a few more fields,
has some interesting things here. So this is for every user, we have the
average stars that that person gave how many compliments that were cool or marked
as cool or funny or cute or hot or list. So let's say that we
wanted to find out how many users had compliment cool
values greater than 5,000. So the way we would do that is
to take that data frame user, filter it where the complement
cool field value is greater than 5,000 and then collect. So let's take a look at that and then what we can do is take
a look at the length of result. And find out that there's 79, now it turns out of course that we
could have used count here instead. So this is an alternative way of doing
exactly what I just did that is to do use collect and then look at
the length of the resulting list. I can also ask spark, so
there I looked at the list that came back. Here I'm going to take a look
at the value of result and find out that it's 79 as well. So in one case I got a list, in the other
case I just got the count, think carefully about when each of those would be
the more appropriate way of doing things. Now let's take a look at check-ins. Let's look at the schema first. So this is a very simple data frame,
has a business ID and a date. So what we might do is take a look at
what the first row looks like just to get a sense of it and we also might
want to take a look at the data types. And this should correspond to the values
that we see in the print schema as well.