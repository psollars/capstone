So let's take a look at some of this functionality
that I talked about in the lecture slides right
in this Jupyter Notebook, and I set things up so
we can follow along. Let's set up our usual
SparkSession here. So here we're doing that abstract connection
to a computing back-end, we really don't care what it is. In this particular case, I've asked for all
of the processes on my local machine by specifying local
square brackets star, given my appName
something useful. Again if we're running
just the one app, it doesn't really matter, we can only have one
SparkSession active at once. So we use getOrCreate and because we might want to use something in the RDD
so a little bit later, we'll also pull out that sparkContext object
from our SparkSession. Again, only useful if we're
doing work with RDDs. Now, let's read in some data and this is similar
to what we did in the past. Now that likely took just
about a minute to load in, there's a lot of data there and we've created five data frames, business, checkin,
review, tip, and user. Those correspond to the yelp academic dataset
files that you see listed there in the
json filenames. So let's take a look at
one of the schemas here. Let's take a look at the
schema for the tip data frame. So it's relatively simple, we have a business id,
a compliment count, the date, the text, and the user ID. Relatively
straightforward. Let's keep going a little
bit and practice some of the functions that we
learned last time, we'll take just the first row from that particular data frame. So I called it tips not
the best name for it, tips equals tip dot first
and as a dictionary. So if we take a look at
what is tips right now, we find out it is in fact
just a plain old dictionary, which means we can
iterate through that dictionary using items. So let's see what
that looks like now, this is the first row from our dataset represented as the printout from a dictionary. So here we have the business ID equals this cryptic string, compliment count for
this one is zero, you see the date there, you see the text and
you see the user ID. Again, something that's
encrypted to preserve privacy. Now let's go ahead and
create a temporary view and issue a SQL query
to see what we get. So here I'm going to
use the tip data frame, I'm going to create a table
that I'm going to be able to call or referred to in
my SQL statements as tip, and then the SQL statement
that I'm going to run is select count
star from tip. Lets go ahead and run that, and you'll see it
comes back right away. Did it actually do the work? Well, it turns out it didn't. If you look at the readings and look up things like
lazy evaluation, what happens is that Spark
determines that, okay, I will need to run
this SQL query, but so far the user has not
asked for anything explicit. You'll notice that when I
hit the next cell that runs, it will take a few seconds
for that result to come back and that's when the
actual query occurs. Why does this happen? Well, it happens for the very
good reason that there is an optimizer built into Spark. We mentioned this a few times through the course and
haven't gotten into it, it is in the readings, is called the directed acyclic graph or DAG and
that allow Spark to figure out what exactly
it has to do to satisfy the requirements that you set out in your request. So here when I say result.show, it will determine that it in
fact needs to run through the entire contents of tip
and come up with this answer. So you see where it
says result.show and the output is count1
and is 1223094, now does that match
what we would expect? Well, we know how to
count the number of lines or the number of
rows in a data frame, we use the dot count method and if we run that using Spark, we get exactly the same number. So indeed what I've
demonstrated is that SELECT COUNT star from tip, that is using the SQL interface to our data frame has resulted in the same exact results as using the built-in dot count
function in Spark. So let's go ahead and
create or replace these temp views for all of the five data frames
that we have. So they map directly onto
the data frame names that's just at the convenience and it's also a convention. So to issue a simple SQL query
like this one for example, I'm going to select
all the fields from the business table where the state equals QC and that actually refers
to a province in Canada, QC Quebec, and let's take a
look at the results and see what they look like. Well, here we go. So this
is our old friend show, show does somethings like truncating fields if
they're too long, and you have to imagine that this is wrapped around
something that's roughly twice the width
of my screen right now or three times twice
the width should do it, and you can try to study how
these things wrap around. That's why I often use the as dict functionality and walk through the dictionary
that I get back. But this shows me
just the first 20 rows of what that query looks like and if you look
carefully at the state values, all of the states are QC, so it looks like it's working. Let's go ahead and work a
little bit more on this. Now, we often count things, so that gave me the
results of select star. It's quite common to
do something like select count star from business
where state equals QC. That might be a
good query to come up with if I asked
you the question of how many businesses are there in the
province of Quebec? So you could do SELECT
COUNT star from businesses where state
equals QC and show the results and you
could tell me that there are 9,219 of them. You'll notice that column
name is count bracket one, not the best field name. Let's say I wanted to go ahead and use
that somewhere else, if I wanted to register this results table
as another table, I'd have the strange column name. So what I can do is use and we mentioned
this in the slides, we can use the AS keyword and rename that column to the count. We might want to call it count, but I'm just calling it
the count to make it absolutely clear
what's going on here. So here I've issued the
same query as I did up top, SELECT COUNT star, SELECT COUNT star, all
I've introduced here is AS the count and I get a better
column name, the count. Now we talked about triple-quoted
strings here of one. So I'm going to
select the state and the count from business, and I'm going to group by state. Now, what do you think
that's going to give us? Well, we're going to group
by state and we're going to count things and that's going
to be part of the group by, so that's an
aggregation function. Let's take a look at that
and see what happens, see if it's what we expect. So here we have for every
state in our dataset, we have a value of count. So that's the number
of businesses in our dataset from these states. Now, remember we're only
showing the first 20 rows here, there could be more. You'll notice that while
there's Arizona at 56,686, South Carolina at 1162, BAS, we're not even sure what that is, but it only has one
and you get the idea. Let's say we wanted
to change our query a little bit and order those in a descending
order by count. That is, what are the most frequent states in our data set? We could run that and we would find out that it was Arizona, not, perhaps
surprisingly, Nevada, Ontario and so on. So these appear to be sorted and those are again only
the first 20 rows. So that gives you an idea of
how we can take a SQL query, put it into a string, pass it to Spark SQL, and you'll notice that our
Spark SQL never refers explicitly to a DataFrame. That DataFrame is embedded because we registered
that temp view, we did that create or
replace temp view. So we registered that table name as corresponding
to that DataFrame. That's what allows us to use spark.sql and we're not tied to a specific
DataFrame here. Why is that important? Well, imagine we're
going to join two tables here and we use this
example in the slides. So here we're going to select
business name and tip text, from tip we're going to left join the business table on
the condition that the business ID from tip is equal to the business ID
from the business table, and then we'll take a
look at the output here. So here we're joining two tables, that's another reason why
it doesn't make sense to tie the SQL functionality
to a particular DataFrame. Here you have the name and the text and you see
the results there. You can pass through those yourself or you can walk through those by looking
at the DataFrame, say as a dictionary. Now, we talked about
sampling here. Here's an example
of how you could take that check-in table, which we saw up top has
quite a few entries in it. So we could do, just
remind ourselves, we could do checkin.count
and remind ourselves that check-in has 161,960
entries in it. If we did a table
sample of one percent, store those in result
and then counted those, we would find out
that our results have roughly one percent. So keep in mind that this
is a rough estimate, it won't be exact and
that has to do with the distributed nature of the analysis that
we're doing here. So we're having to coordinate four different
processors probably on your machine trying to cope with sampling one percent.
Interesting problem. Let's move on to looking
at user-defined functions. Here we're going to define that very straightforward
Python function where we take a
number and square it. So for example, if we
called square on 3, we would wind up with 9, square of 99, 9,801 and so on. So that's just a plain
old Python function. We have to do some importing
here before we get going with our UDFs in Spark. So the first thing we
always need to do is import UDF from pi Spark
SQL functions and then we also need to import
any types that we're going to be using
in our Spark work. So I'm going to be
using integers, so I'll have to use
integer type here, maybe also have things
like string type, array type, we saw
those last week. I'm going to define a
user-defined function called square_udf_int and I'm going to call a user-defined
function that goes like this. For every entry in our DataFrame, we're going to apply
square and we're going to have this
return an integer. The final step before I can use this in a Spark SQL string, is to register this function as a name that I can use
in my Spark SQL string. I'm discarding the output, I don't particularly
need that output. But what that register
step allows me to do is to use my square_udf_int, that is what I registered here, as a function within my SQL. In this case, I'm going to square the compliment count and I'm going to call
it something else, compliments squared
from tip and I'm going to order that by complement
squared descending. So let's see if that works. We don't have any reason to believe that this works or not, we have results always good to go back and look at
some of the data. So here we might go back and
look at compliment_count, just to see if the
original data and the squared data
look like they work. So that looks fine. Maybe we want to also
extract text so we get an idea of what this text
looks like and interestingly, all of these have
three complements. So there is an example
of how to use UDFs. The last thing I want to show you is how we
interoperate with pandas. So here we're going to
set up a query where we're going to select
the state and the count from businesses and we're
going to group that by state and an order
by count descending. So there's my query, there are my results, my pandas result, I'm
going to assign to that DataFrame and I'm going
to call two pandas on it. If I look at my result, that looks like our old friend, a pandas DataFrame and
you can manipulate it in the usual way using
your pandas commands. That's it for the demo, that should set you up for
the homework assignment.