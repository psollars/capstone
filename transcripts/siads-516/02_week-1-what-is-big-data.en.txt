Hi, welcome to SAI 516, which is big
data and scalable data processing. My name is Chris Teplovs, and I'm going to be teaching the next
four weeks of this course, so I thought we'd start off in this week
with an introduction to big data. So here's what we're
going to cover this week. We're going to talk a little bit
about what big data actually is. I'm going to introduce
a concept called MapReduce, and MapReduce is really important. It gives us the foundation for all the work that we're going to
be doing in this course. I want to talk a little bit about
distributed computing, that is, how to use a large number of commodity or cheap computing devices to
accomplish large-scale computing. I want to introduce you to Hadoop,
that's something you should know about. We're going to use Hadoop in
a very hidden way in this course, but I think you should know
a little bit about Hadoop. Then, we're going to
shift a little bit and talk about the pipeline of
map-sort-shuffle-reduce. This will all make sense, I hope,
by the time we're done this week. And finally, we're going to look at
a software package called mrjob, or mister job, I don't care how you pronounce it,
that will accomplish some of the MapReduce techniques that we're going to talk about
in theory for the first little bit. So what is big data? So big data is defined by
what's known as the three Vs. So volume, velocity, and variety. And this was from a paper about
20 years ago by Laney in which he coined these three Vs. By the way, there are additional Vs
that other people have proposed. They're pretty weak in comparison to these
three which are really important, so I want you to focus on these three Vs. So again, the three Vs are volume,
velocity, and variety. When we talk about volume, we're talking about the incredibly large
amounts of data that are now generated. And in general, one of the important
features of big data is that it's generated and
collected by machines rather than people. So for example, in the past we might
have had a very carefully designed experiment with a small number of rows or
data elements collected, say 30 or 40. This would be collected by researchers,
sometimes actually on paper. Other times on computer. Now, we're talking about large scale
data generation from things like Twitter streams, from things like particle
accelerators where people smash atoms together and generate gigabytes
of data within microseconds. So a big shift for
us recently in the last decade or two has been to this very
large volume of data. So that kind of makes sense,
big data, big volume. The other piece that's sort
of interesting to think about is the velocity with which data
is created or data arrives. So data is not only large in volume,
but comes in at incredibly fast rates. What does that allow us to do? It allows us to collect real-time data, so
we don't have to do things like sample. We don't have to do things like wait for
data to be generated. We can actually consume
data from a live stream. Finally, the third V that
Laney talks about is variety, and that's getting beyond numbers. So in the old days, we might have
thought of data as largely numerical. Now, of course, we have text,
we have audio, we have images, we have collections of
images that we call video. And we also have things like
the Internet of Things that generates a huge variety of data. So those three things together,
volume, velocity, and variety basically define big data for us. But really, what is big data? Is it something that you can
differentiate from small data? Is there a cut-off line that you can use? So does it mean terabytes? Does it mean gigabytes, too? Petabytes, exabytes, zettabytes? Do you even know what those things are? So here's a table of the multiples
of bytes that we need to look at for big data, and I think you're
familiar with things like kilobytes, megabytes, and gigabytes. Some of you may have access to terabyte
storage even on your local machine. As we move beyond terabytes,
we have things like petabytes, exabytes, zettabytes, and yottabytes. So you can see the exponents on
the values there are increasing by three decimal places each. So that's 1,000 to the fifth for
a petabyte. 1,000 to the sixth for
an exabyte, and so on. So we're talking about
incredibly large scales of data. But big data isn't just about
the number of bytes we have. It's also a way to deal with
data that is so large and complex that we can't use what I would
call traditional statistical analyses. Put another way, big data is
a different way of thinking about data. So it's not just the size, it's the whole approach that you're
going to use towards analyzing that data. And we can push on that
a little bit more and talk about the fact that this is actually
a philosophy about how analytics works and how analyses can be applied
to large data sets. So why do we care about big data? Well, we have lots of data. We also face constraints. The two biggest constraints that
we face are time and money. And we want to know how we can
gain insights from big data. So let me rephrase that a little bit and
ask what is big data good for? Well, it's good for
exploratory data analysis, and we can get more accurate pictures
of what's going on in the data by reaching into big data
compared to small sample data. It's also good for inductive statistics where we're going to
infer properties of a population. And I want to contrast that
to descriptive statistics. I want to talk about descriptive
statistics in big data for a minute right now. Descriptive statistics are not
the best things to apply to big data. When we talk about big data,
were often going to hide interesting phenomenon just by the sheer
volume of data that we have. So again,
when we're doing inductive statistics, we often make assumptions about
the underlying distributions. We often invoke things like
the central limit theorem. This doesn't make a lot of sense to do in
big data because everything is going to be significantly different
from everything else. Finally, big data's also good for
predictive analytics. So when you get to do courses on
machine learning in this program, you'll start thinking about how you
might accomplish this using big data.