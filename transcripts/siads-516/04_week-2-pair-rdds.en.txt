We talked about RDDs, Resilient Distributed Datasets,
as being a foundational layer of Spark. We're going to build on RDDs, and
talk about something called PairRDDs. PairRDDs are RDDs that
consist of key-value pairs. So this should bring you back to some
of the work that we did in MR job. So now we're thinking again
about key-value pairs. And one thing I want to drive home,
is that keys in the world of big data and Spark, are usually not unique in
contrast to python dictionaries. So when you have a python dictionary, your
keys typically are unique and in fact, if you have multiple keys that
are the same, you will overwrite values or you'll have to figure out a way to
append values, or something like that. In PairRDDs and in Spark in general, you want to have this idea that
keys are definitely not unique, I mean, they can be unique if
you have just one element. But if you think back to that word count
example of deer and bears and cars, you know that we've seen each of
those words multiple times, so the keys will exist multiple times. PairRDDs provide additional functionality
over and above what's available to RDDs. Most notably is three things, sortBy,
sortByKey, and reduceByKey, and we'll get a glimpse of
what that looks like. In PairRDDs, sortBy behaves basically
like the Python sorted function, using a key equals function. So for example,
if I gave you some word counts and asked you to sort them by the values. So that's the second element. Remember, key-value pairs are basically
a topple of a key and a value. If I had another RDD called word_count3,
and I wanted to sort those by the value, then I would say for every element in
that PairRDD that consists of a key and a value, that I would omit the second
value for my sorting function only. And let's say I wanted to do this
in a descending order, that is, I wanted to see the most
common words first. This is a sortBy a function
that would accomplish that. Another thing that is often done,
is to sort by not the value but the key, so sortByKey will do exactly that, it will
take key-value pairs, sort them by keys. And the important point here is that
the keys will be sorted alphabetically. If your keys happened to be numeric,
you will have to figure out a way to convert them into
strings that sort appropriately. Again, with the plain old sortBy,
you can specify your lambda function, sortByKey is a little bit different
because you have to pad up those numeric values to sort properly. PairRDDs also support the idea
about reducing by key. And this is more or
less what we did in MR job. ReduceByKey will take a function
that operates on the values of two elements with the same key,
and returns a new RDD. For example, if we had an RDD called RDD, probably a horrible name for
an RDD, we could call reduceByKey, and reduceByKey always takes two
values for a lambda function. That has something that's
called an accumulator, and something that calls the value. Think of the accumulator as a global
variable that's set outside now. And in fact, it's not global, but
this is the way to think about it because we're iterating through
every line in that RDD. So that lambda is going
to take two values. So for accumulator and value, what we're
going to do because we want to add things up, is we're going to take
our accumulator value, right, that running tab and add to it our value. So this takes a while to
get your head around. Here's how a word count example
might work in Spark, and we're going to walk
through this line by line. So we're going to have our input file
variable set to the RDD that's created, when we read a text file
called data/totc.txt. The first transformation
that we're going to do, is we're going to take that input file
RDD, we're going to call flatMap, and then we're going to use this
lambda function for every line. We're going to split our line. So we've seen that before,
that's nothing new. Let's take a look at this second line
here, the second transformation. That second transformation is going to
operate on the RDD that came out of flatMap, and you should take a minute to
think about what that RDD looks like. We're going to then take the map function
and save it for every line in the RDD. Now, remember our flatMap,
we didn't call collect on it. It's just giving us an RDD that
has one word per line in the RDD. What we're going to do here
in this mapping function, is we're going to take each word,
and we're going to omit a key-value pair that consists of that word and
the literal number one. Now, remember how we counted lines in the
MR job, here we're going to count words. And the way we're going to do that,
is in word_count3, we're going to take the output of our map. So if you look at that RDD, we'll see that
it has word comma 1, we're going to call reduceByKey, and we're going to use a for
accumulator and b for the current value. This is common to see a and b, or x and y, you don't see variables commonly used that
are helpful like accumulator and value. So we're going to say,
a is the accumulator, b is the value. So we're going to reduceByKey. So for every different value of word,
we're going to take our accumulator which is initialized to 0 to start with,
and add our value. In other words,
if we see the word that 1, that 1, that 1, we're going to take the value for that accumulator is going to be 0,
we're going to add 1 to it the first time through, 1 to it again,
1 again, until we get the number 3. Finally, we're going to take
that word count 3 RDD, and we're going to sort it by the value,
right? Not the key, but the value. So x subscript 1, or
x square bracket 1, and we're going to sort it in descending
order by setting ascending = false. If we then take the first ten elements. Remember, this is an action,
it's not a transformation, is going to give us a list of the top
ten words along with their counts.