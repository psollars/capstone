Now to get us ready to work with SQL commands on our
Spark DataFrames, we can create a view onto
our DataFrames and it turns out that every
Spark DataFrame can have a view created for it. Now these views can be
temporary or permanent. Permanent views require
more configuration, they use something called
Hive, you can look that up. We actually have Hive enabled
on your hosted environment, but we're going to stick
with the temporary views. They are temporary because they disappear after our
Spark session ends. Now you can recreate them in your next Spark
session and they're relatively lightweight to create. There are also concepts of global views as well
and that's important if you have multiple Spark
applications trying to access the same dataset
and you want them to, for example, share results. We're not going to get into permanent views and we're not going to get
into global views. So our views for all of the work that
we're going to be doing in Spark SQL are
temporary local views. To do that, to create one, we use a function called Create or Replace TempView and
then the name of the view. Now you can think of the name
of the view as the name of the table that you're going
to use in your SQL queries. So Spark SQL. Once we have that temp view, we can use it to issue
SQL commands via the SparkSession
object. Got that? So we're going to use
the SparkSession object and we're going to use
a function called SQL to issue SQL queries against the views that
we've just created. So how would that work? Let's say we have a
DataFrame called Tip, T-I-P, and on that DataFrame
we're going to call Create or Replace
TempView and it's conventional to call
the table the same name as the DataFrame as long as your DataFrame is
reasonably well named. So here I'm going to
call my table "tip", that's my tip is in quotation
marks and that's going to correspond to the
name of the DataFrame. It doesn't have to and you'll see some examples where
we depart from that. Then what we can
do is we can issue a SQL command against that
particular table name. So here for example, I'm going to do Select Count*, simple way to count
the number of rows in the database from tip and that's going to
be stored in result. So result is going to give
me another DataFrame. Let me spend a bit of time reviewing SQL before
we go forward. So the parts of the SQL command
that I think we need to focus on in this part of the course is the
Select statement. Yes, you can do other things, you can build up
DataFrames using SQL, but we're going to
focus on querying them. We're going to set up
the model where we have our dataframe populated
from say a JSON file, a CSV file or manually, and then we're going to issue SQL Select commands against it. In general, this is how
a Select command works. We're going to select some
columns from some tables where some condition exists
and we're going to order the results probably
by some other column. That's the essence of the Select command that
we're going to use. I want to highlight that SQL syntax is more
or less English-like. So it reads like a variation
on a spoken language. So let's say we have a "students" table
here where we have ID, name, phone, gradient
and program. In our Select statement, just thinking about
what we could select, we're going to ask the
question of, well, what columns do we
want or will be retrieved and from which
tables would you select that? Which tables contain
those columns of data? So for example, here
we could select ID and grade from students
and that reads well, select ID, grade from students. So it maps onto what we would
think of as a statement of what data we want and the results are shown at
the bottom of this slide. So we have just those two
columns, ID and grade. Now here's where a little
bit of confusion comes in. There is a Select statement
or a Select function rather that we've seen in
plain old Spark DataFrames. This is a little bit different. So we are going to be selecting columns so that's similar, but we're going to be much more often use conditions here. So we're going to tightly couple the selection criteria for
both columns and rows in SQL. Another important keyword that we're going to use is distinct. So sometimes we want to
eliminate duplicates and we want to do something with
just the unique values. The way we do that in
SQL is to use select distinct and then a column name and then from some
table or tables. So here, we have that same students table
and what I'm interested in doing here is selecting the distinct values of
program from that table. In this case, if you take a look at the values of program in the original table which
only has three rows in it, we have HCI, HCI and IAR. The results will be just the two distinct
entries, HCI and IAR. Now the other
important part of SQL for our purposes is
the "Where" clause. So here, we're going
to be reducing the output based on
specified conditions. So we're going to
constrain what is selected from our database
or from our table. So in this case, we could select name from students
where the grade is greater than or equal to
90.0 and as you can see, the results there give us just
the column that we looked for Name from students,
that's the table. If you look back at
the original table, Fred and Andrea, both have grades that are
greater than 90.0. In general, we have a broad range of common
comparison operators, these should be familiar to you. So we have equals. Notice that the equals
operator in SQL is a single equal sign which is the assignment operator
in most other languages. So it's a comparison
operator in SQL. We have not equals, greater than, greater than or equal to, less
than, less than or equal. I'm going through those really quickly because they should
be very familiar to you. There are a couple of
different operators that we also want to focus on, we have in and between and we can negate those
with the not keyword. The "In" operator
works as follows. So you can do a select, say here the field is
name from students, the table is students
where state is in some specified set
or list of values. Similarly, we can negate that in the second
example here by selecting name from students
where state not in that set. So you can see how those are complimentary to one another. Between gives us that
functionality to specify greater than or equal to and
less than or equal to in a way that
reads a little clear. So for example here, given that students table again, we could select the ID, name and phone from
students where the ID falls between
1,000 and 9,000. Yes, you could do that with other approaches such
as less than or equal to, or greater than or equal to, but it's also something that you will see in the literature. So you will see this
between operator. Because the implementation
of between is a little bit different in all the different versions and systems that we'll be using, I recommend going with the comparison operators
instead of between, but I wanted to alert you to
the fact that this exists. Now wildcards behave a
little bit differently in SQL than they do in things like plain old
regular expressions. The like keyword is
used in place of an equal sign when you
use wildcard characters. So if you're going to use wildcard characters
in your comparison, you will need to make
sure that you use the like keyword instead
of equals, for example. Now to specify a single
character wildcard, we're going to use the
underscore character. Similarly, if we're going to have a multi-character
substitution, we're going to use the
percent character. So in our example that we've been working
with, with students, we have select name from students where name
like "A percent", we'll find all of the
names that start with a capital A and have any number
of characters after them. Here's an example that I
hope will help you see how the underscore and
percent operators work. So if we selected name from students where the name is like "M_r percent" that would
find those types of names, Mark, Marion and Mork and
you'll also see how that would need to have an r in
the third position. In SQL, null means nothing. So if you have an empty cell which literally
contains nothing in it, in Python we refer
to that as none, N-O-N-E, in SQL, we
refer to that as null. So if we wanted to find
all of the entries in our table or in our view
that have nothing in them, we could use select
something like select ID and name from student
where the program is null, N-U-L-L is that special keyword. Another important
aspect of SQL for our purposes is ordering
or sorting things. So we have something like the first example
here where we do select name from students
and order that by name. We're going to specify
descending order by using the keyword D-E-S-C. The second example
here we'll sort in ascending order the
default so select name from students
order by name without any other keyword will
sort in ascending order. We can also group or
aggregate functions. So here we have a
customer's table and we're going to
select customer, and then the sum of
the order prices from orders and group
those by customer. Now, you can see the
output on the screen here. What I want to alert you
to is when you're doing a select as well as a group by. So you have a group by introduced to your select statement. The things that you
are selecting must be either explicitly named in the group by or
they have to be in aggregation function like sum or count or min or max or
something like that. If I don't specify something to group by or summary function, you'll get an error. So here for example, if I wanted to select not just customer and
sum of order price, but also ordered date from orders and group
those by customer, you see how we get into trouble. We don't have any way of
dealing with OrderDate. We haven't set sum which
wouldn't make sense either or min or max
which might make sense, but we would need to give it some way of grouping
those things. We could group by order date, and as long as that appears in the group by statement,
then we're fine. So again, two choices. If you're going to
select columns, they either have to
appear in the group by statement or they have to have a summary or aggregation function applied to them in the select statement. The aggregation
functions that we use commonly are count
for counting values, sum to add them up, AVG to find the average, max and min for maximum
and minimum respectively. So to review where we got to, in our select statement, the most important
and common bits are what we're going to
select optionally distinct. We often leave all
as the default. So select sum column from sum table where
sum condition exists. Optionally and less often, we have order by, group by, and having. So let's talk about
the where clauses. You usually don't
want all the rows. So a where clause restricts
the rows that are returned and it takes
the form of a condition, and we talked about
some of those. Here's what some of the
conditions look like. So we can have an example
where Mark is less than 40. We have a situation
where first equals John. First does not equal John. First equals loss. So that's interesting. We can actually compare columns within the
select statement. So it's not looking at first
and a literal like John. But we can look for
situations where the value of the column first is the same
as the value column last. We can also make more
complex where conditions. So here in the next example, where we have first
equals John in parentheses and
last equals Smith. Similarly, we can use the OR
operator where we have Mark is less than 40 or mark
is greater than 70. I mentioned a couple of
slides ago that we also have a having clause. So where and having take the same format of
condition phrases. Having differs
because it works on the output of an aggregation
from a group by statement. Here's what I mean by that. Let's say we have this query that says select business_id, count (dates) As
count from sample. group by business_id. So you see how I have count of dates and I've really
as status count, and we'll talk about
aliasing in a minute. That condition which you would think could normally
be done as a where inside has to be outside because our aggregation function was calculated from
that select clause. So having it wouldn't
make sense to have where inside because we
haven't calculated count yet. So anything that comes out of a summarization or group
by function where we want to put a condition
on the output of that has to go into
a having statement. Now, I introduce some
formatting changes there, and I think it's time that
we need to tackle those. So let's say that we set up a situation here
where we're going to take a DataFrame
called results, and we're going to assign
to that the output of a Spark SQL command
that reads as follows. So select count star
as the account. So I'm counting all of the
rows and I'm aliasing not as the account just gives me a nicer column name
from check in. So there's a table
called check-in where the business_id equals
some literal string. Now that becomes unwieldy
as we look through that, and you can imagine
it getting longer. Let's say I had a more
complex where statement. We can use line breaks in
Python to accomplish that. So remember if we're breaking
a line in just a plain old, any plain old Python
command or Python line, we're going to have to use
the backslash operator to indicate that this is a continuation of
the previous line. So we could introduce
backslashes and line breaks to give us that much
more readable format. So you can see now it's
select count star as the count from check-in where business_id equals some literal. Now, that's still a bit unwieldy. We can instead use something called
triple-quoted strings. Triple-quoted strings are
something you may have encountered in your other
work with Python so far. But it allows us
to not have to use those backslashes to indicate
a continuation of a line. Now these are only for
plain old text strings. So here I'm setting up a plain old text
string called query, which is going to contain select count star all the way down to the single quote at
the end of that literal. It's just a string. Why do we use triple quotes? Well, they're highly unlikely
to be found in the wild. You're not going
to often encounter data that has three
quotation marks in a row. So it's a reasonable thing
to use to surround strings. Now, once you have a string, you can use it as a query. In other words, we could set up that query string and
then we could pass that as the argument to Spark SQL to run that query
and then show our results, and I want to reiterate that the results of a Spark SQL query, any query is also a Spark DataFrame which
is why we can use show, and you also get an
idea of how you can manipulate that result set. So here's what a
simple select would look like in Spark SQL. It shouldn't look that scary. So we have a string here
that setup select star from business where business_id equals some literal value. I used a triple-quoted string just to show you what
that looks like, and then I pass that
string to spark.sql. So Spark is our
SparkSession object. It has a method called SQL. That query string you'll notice has the table name business. So I must have already registered that as
a temporary view, and then I call.show to
show me the results. I also want to call attention at this point to the use of
uppercase and lowercase. In general, SQL reserved words are
uppercase in a SQL query, and things that can
change like table names, field names, and so on
tend to be lowercase. This is just a convention. You don't have to use
uppercase like this at all. So you equivalently could be all lowercase select star from business where
business_id equals. Now the literal will have to be the correct casing inside
those quotation marks. But outside of that, you have a fair bit of leeway in terms of how you
use capitalization. The other thing I want to
mention that's very common in Spark SQL is aliasing columns. So in our result set, so here we have select
business_id AS bid, and then trim check-in
date as check-in date. So let's say we had
empty white space at the beginning or end of it, and we wanted to alias start from check-in where
business_id equals something. So that AS operator just
allows us to assign a nicer column name or one that we want to use
moving forward. But you'll see that a lot in our select column name
as something else. It allows us to do basically an in-place renaming
of the column. Now, this is a course
about big data, and sometimes we don't
want to operate on all of the data that we
have available to us. But instead, we want to take a smaller sample of that table. To do that, we use the
table sample operator, and here's an example of
what it might look like. So here I've set up a query
string which is select star. So select everything
from the table check-in and then use a
table sample of one percent. You can specify this
as percent or rows. So here I've used one percent. That is a good starting point if you want to scale
down the nature of your exploratory analyses
before you get into the full analysis on the
entire contents of that table. We talked a little
bit about having. Here's what it might
look like in a query, and here I'm trying
to get you used to what these queries look
like in Spark SQL. So I set up this triple-quoted
string where I'm selecting business_id
and then account. I can do count star. I've chosen to do
count dates as count. So I'm going to aliased dot. Instead of having count dates, I'm going to have just count
from my table called sample. I'm going to group
by business_id, and then you see how I
am looking at counts. So I have to have that as a having clause not
a where clause. So having a count
of greater than 20, and that's an example
of the output there. We can build on that
a little bit more, and let's say that I've asked you to count the dates of a business and report the results
by business_id from the highest to
the lowest count. So here what we're going
to do is set up exactly the same query select business idea and count
updates says counts, we're going to alias dot out from sample grouping by business_id and having account
of greater than 20, then ordering by count
in a descending manner.