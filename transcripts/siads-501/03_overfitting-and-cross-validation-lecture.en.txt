Welcome back. Let's
talk about overfitting, what it is and how
you can avoid it? At the end of this lecture
you should be able to explain
the following concepts; overfitting, signal versus
noise, and cross-validation, and you should be aware of
some related vocabulary, maybe not yet ready to explain it to other people regularization, and the bias-variance trade-off. Overfitting.
Data Scientist analysis begin with some dataset. A model is an abstract
description of a process. It could be used to generate
synthetic datasets, or it could be used to sort of fit pretty well to
an existing dataset. Usually, we really mean
a family of models, not just a single model, there'll be parameters
for the family of models. So each different instantiation
of the parameters of that model family is one model. For example, linear
regression with a single independent variable is a family of models
with a few parameters, the slope of the line, the intercept, and
the standard deviation Sigma for the error term. Having chosen a family of models, we'll use an inference process, or an estimation process to select the best
possible parameters, those that yield the closest fit
to the actual dataset. So the particular model
we estimate in this family would
be something like y equals three plus 2.4x plus a normally distributed error term with
the standard deviation of two. The fit between your model and the actual dataset
won't be perfect. As data scientists, we get to choose the family of models. Are we going to use ordinary least squares regression or are we going to do with
some neural network model or something else, and then we run
an estimation process that yields the
parameter estimates. Sometimes, we have choices
about the estimation process. But usually the big choice is which family of
models to start from. If we include more
parameters in the model, more degrees of freedom, we're going to be
able to get a better fit to the data set. This is nicely illustrated
in this XKCD comic. If we limit ourselves to
simple linear regression, the best fit is
an upward sloping line. Of course, the actual points don't fall exactly on that line. But we can add other parameters
or squared terms, or other transformations to get curves that are a better fit. It gets more and more ridiculous towards
the bottom where we have a whole lot of perimeters and we fit very
well our particular data. So why not go with
a family of models that has more parameters so that
we can get a better fit? That's where the idea of
overfitting comes in. The data set we have is
usually a sample from some larger population including other data points that
we're not seeing, but which were
presumably generated by the same underlying process that we're trying to
create a model off. If we fit two out of the data points in
the sample we have, like in the lower left with
the connecting lines model. If we fit too well, we're likely to capture
random variations that won't be repeated in the other data
points that are out-of-sample, things that aren't in
our original dataset that are in the population so in
that case, we're overfitting. The model will fit less well
to the out-of-sample data. We might fit data to the
out-of-sample data if we just use the simple linear regression
at the top-left. So one metaphor
for thinking about this is signal and noise. The signal is
the patterns that you see in the sample data
that will also hold true in the rest of the population and
the out-of-sample data, whereas noise is variability
that you observed in the sample data which is not predictive of what will happen
in the out-of-sample data. If you fit your model
to the noise, you will be overfitting, you'll do worse at fitting the out-of-sample data than if you had managed to
ignore the noise. Of course, the challenge is that when you are just
given a sample of data, you can't tell which parts of the observed data
are true signal, reflective of what will
happen out of sample, and which things are noise, and that's why modeling
is an art form. For concreteness, I've provided a Jupyter Notebook that
illustrates this phenomenon. It's not exactly the
same data points from the XKCD comic, but it is the same idea. So in my simulation, I've created a large population
of data points that are generated from a linear
process with some noise. So y is always going to be 5 plus 3 times x plus
some error term. The x's are uniformly
distributed from 0-10, and I've taken 1,000 samples, and the error term
is just draws from a normal distribution with mean zero standard deviation
three and again, I drew the same
number 1,000 samples. You can see a scatter plot of it. The x values are going from 0-10, and we're getting when x as four, we have 5 plus 3 times 4, so 5 plus 12 is about 17. It's exactly 17, so 17 should be our mean and then we get
some variation around that. So this is our big dataset, but now suppose we
just take a sample. So here's a sample of 20 items. You can see we get
the x values are not perfectly symmetrically
distributed here. We seem to have gotten a few
more towards the low value. We got a little gap here. That's what happens when you
draw a sample of 20 items. I suppose that this sample
was all we had, but we knew that it was
somehow coming from a larger population than
set of 1,000 points. What would be
a good model to fit this? Well, if we restrict ourselves
to a simple linear model, the best fit is that y equals
4.32 plus 3.12 times x, that's the blue line
that you're seeing here. Now, remember that
our true model was y equals 5 plus 3 times x plus
some random error. Now this 4.32 plus 3.12 times x, this blue line doesn't fit
perfectly our data sample. In fact, on average, the actual y value
is off by 2.07. So here it's very close but here, it's quite a ways off. On average it's 2.07. If we take this same blue line and compare it to all the data points that
are in our population. So if we look even out of sample, the average error is
a little bit bigger, it's 2.39. So we fit a little bit too closely to our sample
but not not too badly. Now, let's suppose
instead of taking this linear model is our family of models that
we're going to choose among. Instead, suppose we allow
ourselves to do Cubics. So cubic can look like
this or like that. This blue curve is what it actually came up
with that we have minus 0.04 times x cubed plus 0.44 times x squared plus 1.9 times x, and our intercept is at five. Now, if we look at
the residuals here, on average, they're 1.94, so we have a better fit to the sample than we had
with our linear model. But when we look at
the whole population, our average error is up to 2.55. So remember that with
the linear model, we had 2.39 as our average. With the cubic model, we have a bigger average error on the whole population but a smaller average error
on the sample. So we've overfit to the sample. We would've been better off just using the simple
linear model if we wanted to do a good job and making predictions for
the whole population. In one of your readings
they start by noticing a setting where
there's overfitting. Bunch of teams that are data hackathon and the teams are given a data sample that they could use to train a predictive model. They would then
submit their models and receive scores for how
well the model predicted outcomes both in
the public data sample and on a private sample that they never got
to actually look at. So here in this middle column, the public leaderboard rank, we see how well the teams did
on the public data samples. On the right side, you see how they did on the out-of-sample stuff on the data that they couldn't see. You can see the team
that was number one on the public leaderboard
was only number 42 on the data outside. Number two team went to 33. There are a couple teams
that did pretty well both on the public data and
on the private data. But a lot of the teams
that were doing well in the public data didn't
do on the private data. That means they really were overfitting to the public data. Modelling is an art form
because you have to bring in real-world knowledge as
assumptions that lead you to treat some things as signal
and other things as noise. You'll be learning
some specific techniques in this program like various forms
of regularization. But keep in mind that there is no purely mathematical
way to do this. To do the best job of separating
the signal from noise, you're going to have to use
domain knowledge about what could plausibly be signal. Another phrase you'll hear is the bias variance trade-off, sometimes used with
great technical rigor and other times more metaphorically
as I'll try to do here. If you use your
domain knowledge or mathematical techniques
like regularization to limit yourself
to simpler models, you'll be less sensitive to the contents of
particular datasets. That is, if you ran your estimation process
with a bunch of sample datasets you would get estimated models that are
pretty close to each other. But you'll do so at the cost
of favoring models that are systematically a little further away from the truth
that is biased. So that's the
bias-variance trade-off there that you can get
models that are going to be close to each
other even with different sample datasets
but you might do that in expensive thing a little biased towards certain outcomes which might not be
correct outcomes. So to summarize, overfitting is creating a model that fits your sample data
too well leading to poor fit to data that's
not in your sample. Which leads us to the idea
of cross-validation. Cross-validation
is a generic idea for preventing overfitting, which has been embodied in
a few specific techniques. In this intro course, we will as usual focus on the big idea. The basic idea is
that you estimate a model from sample data and then validate that model by seeing how well it fits
another sample of data. Worried about overfitting,
check for it, cross validate your model
on another sample of data. Okay, but how do you get
another sample of data? The answer is that you take your original dataset
and you split it up. You hold out some data to
use later for validation. How much data should
you hold out? Well, one extreme is that you could leave out just one item. But if that item is unusual, then your model might
be very good but still not fit the one data point. Another extreme would be
chap the dataset in half, hold out half the items
and that leaves only half the data for estimating the model which may also
lead to bad results. So a common technique, 10-fold cross-validation or more generally k-fold
cross-validation is to leave out
a tenth of the data. You estimate a model from the other 90 percent and then test the quality of the model
on the held out 10 percent. That's one folding of the data. You repeat that process for
each of the other nine folds of the data holding out a
different 10 percent each time. Then you average
your metrics for quality. That is the fit of the model
to the held-out data over the 10 folds and
that's your overall metric telling you how good
your modeling process is and this will be less prone to overfitting. If you try this 10-fold cross-validation
process lots of times, adjusting your
modeling process each time in order to improve
your quality metric, then you're effectively
using all of the data for training and you've re-introduced the risk of overfitting. So the normal process
is that you leave out some data that you only use for testing purposes at the very end to evaluate
your final model. You don't use that data at
all even in cross-validation. Then with the remaining part, you do 10-fold cross-validation using that remaining step
and that allows you to try a bunch of different models
or families of models as I've been calling
it until you get the best performance you can. At the end you uncover the test data and use it to
evaluate your final model as the last check that you haven't overfit in your exploration of all the different
model families. So I hope I have sensitized
you to the dangers of overfitting in data analysis. That's when you find a model
that fits your sample data really well but it
fits the noise as well as the signal
and that makes it fit poorly to the out-of-sample data from the rest of the population. I hope I've given you
some intuitions about a generic approach to prevent overfitting cross-validation. You hold out some data, don't use it in
estimating the model, but do use it in evaluating how good the estimated model is. If you limit yourself to
estimation processes that yield models that perform
well on the held-out data, you will avoid overfitting. I encourage you to try explaining these ideas to a friend
or perhaps another student in the class
in our slack channel. We have a few maximums that derived from the concepts
from this lecture, one beware of overfitting. That's just noise. Don't
treat it as signal. Let's hold out some
of this data as a test set to avoid overfitting. Maybe we'll formulate
that last one is an ethical commitment. Instead of and I will never do it's going to be
and I will always. So repeat after me. I will always hold out some data as a test set
to avoid overfitting. I'll close with a little joke
about overfitting. The setting is a job interview. So the interviewer says, interviewer asked me "What's
your biggest strength?" "Oh I'm an expert in
machine learning." Interesting. Interviewer
asks "What's six plus 10?" "Zero." "Nowhere near, it's 16." Okay, it's 16. Interviewer ask "Okay, what's
10 plus 20?" "It's 16." Here's a more personal story
of overfitting, one that I've heard my mother
tell a lot of times. I was four maybe five years old. This was in the old days. There used to be milk delivery to homes in
many cities in the US. Once or twice a week, a milk van drove up with
a delivery van and left a bottle or two of milk in the insulated box right
outside the door of our house. So one day our family
is out driving in the countryside and
my parents always teaching, thought they'd explained to me and my brother how farms work. They pointed the corn growing and say "That's where corn
on the cob comes from." They pointed at the
cows and say "That's where milk comes
from," and so on. A little while later just out of curiosity about how much
we kids we're taking in, they asked "So if
milk comes from cows, how do cows get the milk?" Well, that's easy I said "The
milkman brings it to them." Overfitting. I'll
see you next time.