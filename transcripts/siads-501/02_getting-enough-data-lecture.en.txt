Welcome back everybody. Today's topic is the dangers
of making inferences about a whole population
from a small sample. Now intuitively, we all understand that
it's safer to make inferences from more data
rather than less data. The uncertainty that
comes from making inferences from
small samples sometimes gets quantified as
a confidence interval or a margin of error. For example, in an opinion poll
with fewer respondents, there'll be a wider
margin of error. We're showing you
here an opinion poll. Is an online poll.
They ask people; do you approve or disapprove of your representatives in Congress? Now everybody hates
Congress in general but some people are actually pretty pleased with
their own representative. After all, that district did
elect that representative. So here, this poll, we have all 6,550 respondents, and on different dates there are different levels
of satisfaction. In December of 2018, we have a 45 percent
approval rating but there's some margin
of error because we only had 6,550 respondents. May be it was really 44.7 percent or maybe
it's 46.7 percent. We have a range error
of about two percent. Now, if we restrict our attention to just a subset
of the population, in this case, only 503
of the respondents. The sample size is 503. What we've done is
we've restricted it to only people
who've identified themselves as Lutheran,
Presbyterian or Episcopal. Here we have a slightly
different value for what percentage approved but the thing I want you to notice is that this margin
of error is wider. We have less confidence that the percentage that we
measured in this poll of 503 people is exactly the true percentage if we
had all of the Lutherans, Presbyterians and Episcopalians
answering our poll, and so we've represented
some margin of error here. We know that it's dangerous
to make inferences from not enough data but somehow
we often seem to forget that. We over-generalize
from small samples. Sometimes this is called
the law of small numbers. Now, there is no statistical law
of small numbers. It's a joke, playing on the law of large numbers
which does exist. The law of large
numbers says that as the sample size gets large, you've enough data, then the mean of the sample
will tend to be close to the mean of
the overall population. Danny Kahneman in
the book Thinking Fast, and so we've assigned
Chapter 10 of it to you. The Chapter begins with
a great illustration of the dangers of acting as if the law of small numbers
really is a law. So it turns out that they got data from all of the
counties in the United States, and they got data about
the incidence of kidney cancer. It turns out that the counties in the United States with the lowest incidence of kidney cancer are mostly
rural rather than urban. Now, you can probably
imagine why that's true if you're healthy or living out
of the country better air, maybe you're eating from
the food that you're growing, and you're getting
better nutrition, you can come up with some
interesting explanations of why this rural lifestyle would lead to a lower incidence
of kidney cancer. Okay, but what if I tell
you that the counties with the highest incidence of kidney cancer are also mostly
rural rather than urban, not the same counties of course. So now you're going to maybe come up with some explanation. "Oh, well maybe nutrition isn't so good in
the rural counties, and there's something about better water supplies and with better filtering in the cities
or something like that." None of those
explanations is right. What's really going on is that the rural counties
have fewer people. So we have a smaller
sample size and therefore more variation in our observed kidney
cancer rate even though there isn't
any difference in the true cancer rate because we have more variation
for the small counties, both the lowest and
the highest rates come from the counties with a small population, the rural counties. I've provided a Jupyter Notebook that lets us simulate what's going on and something
that you can play with to get some better
intuitions about it. Here I've assumed
that everyone has the same chance of
getting cancer, four percent, regardless of
what county you live in, and I've varied the county
sizes from 100 up to 400,000, and for each size I'm going to simulate creating 10 counties. Now, in reality, of course, counties are bigger
and cancer rates are lower but that would mean that the observed cancer rates would
all have a bunch of zeros after the decimal point
and it would be really hard to compare. So I've run the
simulation this way. Larger populations would also take longer for
the simulation to run. So I did it this way but it'll give you
the right intuitions. Now, I've done 10 counties of each one and I simulated
a bunch of people, each person has
a four percent chance of having the cancer, and you can see
what result we get. When we have a big
population of 400,000, we tend to get
observed cancer rates of about four percent which is
how we generated the data. Every person has a four percent
chance of having cancer, and you can see across
the different counties, they're all right
around four percent. So not too much variation. When the population
sizes are small however, we get a little bit
more variation. At population of 2,000, we get a 3.8 percent
or 4.2 percent, and the population is only 100, we sometimes get
something as big as seven percent, six percent, as low as one out of 100, and so we get
a lot more variation. Now, if you look across
all of these counties, which ones have
the lowest cancer rate and the highest cancer rate? They're all these small
population counties. So no difference in
the underlying cancer rates but the observed cancer rates in the small population
counties have a lot more variability and so the smallest and the largest
are all from there. Now, you can play around
with this Jupyter Notebook, and I've done it in a way
that even if you don't know the advanced packages like NumPy and
scikit-learn and Pandas, you'll still be able
to do this with just Vanilla list comprehensions. So I haven't shown you on
this screen but there's a function that I created
called "Stimulate counties". It's using just Vanilla Python
and list comprehension. So you should be able to use it just fine even if you
don't know Pandas. I did use Pandas down
here just because it has a nice way of displaying
the matrix of results, but we're otherwise not
using anything fancy here. So our big lesson today is beware of small samples which
reminds me of a joke. One day there was a fire in a waste basket in the Office
of the Dean of Sciences. A physicist, a chemist
and a statistician, they all rushed in, and the physicist immediately starts to work on how much energy would have to be removed from the fire in order to
stop the combustion, and he was writing
down the equations. The chemist is
trying to figure out which chemicals would
have to be added to the fire to prevent oxidation, given the specific
contents of the wastebasket. Meanwhile while
they're doing this, the statistician is going all around the office setting fire to all of the other wastebaskets.
What are you doing? The others demand and
the statistician replies, "Well, to solve the problem, you obviously need a
larger sample size." I'll see you next time.