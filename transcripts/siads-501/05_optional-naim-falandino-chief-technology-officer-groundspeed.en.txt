Hi everybody. I'm here
today with Naim Falandino. I'm very pleased
to have him here. A data scientist who
is a graduate of our on-campus master's program in the school of information. Naim, why don't you start by introducing yourself and telling us a little about your career? Sure. Hi everybody. So my name is Naim Falandino,
as Paul mentioned. Graduated from UMSI in 2012. Have a computer science
bachelor's degree from Michigan State that
I received in 2005. I've been a Software Engineer
now going on pushing, well, let's see, almost
20 years, I guess. A little bit less than that, and a Data Scientist
for more recent. Though I think I've always
been a Data Scientist Jayset throughout my- even my
software engineering days. These days, I am Chief
Technology Officer, a startup here in
Ann Arbor called Groundspeed Analytics
that focuses on unlocking analytics data from commercial
insurance documents. Before that, I was Chief Data Scientist and
Director of Engineering at another startup called
Deepfield which was acquired by Nokia in 2017. Prior to that, I was at Ford Motor Company as a Senior Research
Engineer Data Scientist. In the Systems Analytics
Research Group. Prior to that,
software architect, software engineering,
variety of roles. In that history of
different roles, which of those was the first one that you'd call
a data science position? In all seriousness, I
think it was, honestly, like one of my first
jobs out of school. So I was entered into
consulting actually right out of my bachelor's program.
I worked in finance. First sort of the projects
that I was put on, and this is actually what
attracted me to the firm, because I was introduced to the partner that
I was working with. He was working in
fraud detection and anti-money laundering
controls for banking. So obviously, there's
a lot of data that comes into play with trying to determine
risk associated with a given individual or
given transaction. Yes. I mean, I think
that was one of the first real taste I had of trying to put data
to work to solve problems, and I was hooked
from that point on. So from there, my next role, again, built lots of things, worked on search problems, text document problems,
time-series analysis problems, forecasting, all kinds
of stuff like that So you go back to that first job where you were acting
as a data scientist. I'm guessing it was not called
data science at that time. No. What was the first job that
was called data science? I think when I was at Ford, that was the first time that the words data science started to really be getting used. I mean, honestly, this
was early enough that even big data was a new
thing at that point. This would have
been like 2012-13, after you graduated
from our UMSI program. The first real job I
actually held where I had a data science
in my title was, actually, when I was
Chief Data Scientist at Deepfield so. Okay. I jumped right to the end there. Right. Yeah. Well, maybe we'll
get a chance to talk about what it's like in
startups. Try to see. Sure. Who do you think of as
your primary influences? Well, my dad is actually a
Control Systems Engineer. Electronic engineer, control
systems for a long time. So I'm like old school. I remember maybe in high
school and teaching me about like Kalman filters
and things like that. It was pretty cool. So I would say one of the first influences that I identified on my own,
it was in my undergrad, was Hinton and Norvig
and those guys who pioneered a lot of
what I think came to be the renaissance in
Machine Learning and AI, I guess, if you only use
the marketing term for it. The deep learning models. Deep learning models. Yes. We're going to come
back at the end to talk about what's the field and let's your advice for our students in linear maximum and
all of that venture. Let's let's start
with a project story. Okay. Tell me about some project
that turned out OK in the end, but maybe it had some
drama on the way. Well, I think most projects have some drama along the way. A lot of times. Ideally, you're having that drama upfront where you're
defining what the problem, that you're actually
trying to solve, and less so at the end, because that's usually
more painful if you're having the drama at
the end of the project. I mentioned, I've been in software engineering roles
that data [inaudible]. I might think of
myself as more of an ML engineer to some degree, at various points
throughout my career, which has me on the
delivery and turning an analysis into product or something that's
production-ready. Of course, there can
be challenges there. So one thing that
comes to mind was, I'll go to some work that I was doing at
Deepfield, actually. So Deepfield did network analysis and monitoring and like DDoS detection and
things like that. DDOs being Denial of Service? Yeah. Distributed
Denial. So basically, we worked with a really
large telecom and carrier, like ISPs and things
like that to monitor Internet traffic to end users subscribers for commercial Internet
traffic like this thing, that you might have at your home. So we're looking at really
large flows of data. I don't care about what our
marketing department said. But my recollection
is that we were monitoring something upwards of the 70 percent of the commercial Internet in North America. That's our peak when
we exited to Nokia. But we started with some really early investors that saw potential in what we're doing,
and worked with us. So they were our first places where we were trying to develop the technology and
prove out the ideas. Because the market that
we were coming into, and I know this is maybe taking the time to get to
the story but it's relevant. The competitors that we're
working against was Big Iron, which means custom hardware
silicon that sits in line in the network monitoring every packet and trying to tear it apart and
figure out what it is, which is extremely
expensive to do. We're talking tens of
millions of dollars to blanket even a small
fraction of a large network. What we did was a purely software solution
where we were doing statistical analysis of sampled packets
across the network. With enough of that,
you can put together a really comprehensive and clear picture of
what's happening. So the efficiencies
are a lot better. But when you're trying
to do anomaly detection and DDoS detection, it can be hard to get that right, because you don't
want to alert and have too many false positives, because then, it's just noise. You don't want to miss something either because that's really bad, it can have a huge
impact on the customer. So getting that sweet
spot is pretty tough. So my role really was trying to help
define how we were going to enable a level of
granularity and accuracy around the anomaly detection
that was tunable. But tunable in a way that's for the customer to
tune, to their taste. So I was working on a
variety of techniques. I gave a really quite
good one and we're using like ARMA and some
autoregressive techniques, but doing it across
a very large number of dimensions simultaneously. I was working with
another guy who has had a physics background who was doing a lot of the
principle work on a model. What we found was it worked great when
we're developing it. Then when we actually
went to go start to trial it in a very
large environments, so again, we had a
whole staging process where we didn't just ship
stuff and cross our fingers. So we're testing it.
It fell on its face. It could not handle
an actual load of data that was being sent to it in whatever our typical
customer environments. So it was the problem that the data in the
typical environment was unlike the data
you were training on or there was just too much of it and you
couldn't handle it faster? It was a scale thing. This is actually one of the
interesting things we found. We found that the variety of data that we got in
that environment was also sufficiently different that it invalidates some key
assumptions we'd made. So we were missing stuff. There are types of attacks like so-called slow and low attacks that are pretty damaging, but they don't have the
same spike characteristics of what you might
think of as a DDoS. We just couldn't identify those. Those are pretty tough
to identify in general. We had to go back to the drawing board a bit
and rethink how we were going to try to provide the level of control that we needed
for the customer. What we ended up
settling on was, well, we can't provide you a magical solution which is
what we're trying to aim for. We're trying to aim for
something that just looked at all the data and
found the anomalies. Instead, we ended up having
to put something in place where the customer had to define what their
monitoring points are that they wanted to look at and what things they cared
about trying to protect. So you'd define your
attack perimeter that you're trying to safe guard. If they said, we care
about more things, they had to put up with
more false positives and more time through the alerts? That's alright. The
idea was that we could get better over time. We had a multi-stage process where there was something
that was generating the events and
there was something else that was trying
to validate the events against tolerances
and other conditions. I think this is an interesting
example to in general. I don't know if I've
ever seen something that's gone to production
where it's the one thing, that one model works
great on its own. You often have to use
multiple techniques and multiple stages to
actually create something that's going to add value like in a market situation or to a customer for any
given use case. All right. So I want to
do in a little bit on this o no point. Yeah, sure. We had things working,
we thought we're good, we're just doing
some final testing, and we failed our
final test here. So it's a small company so people are probably playing
multiple roles but even so what was the
data scientist job in noticing or solving the problem that you
had at that point? Yeah. So it was a joint responsibility between myself and the other
people on the project to figure out we had
requirements based on what was on our companies roadmap because this was not a totally
new space we're entering. So we had certain
things that we needed to meet to actually
have a viable product. So trying to always keep
those in our forefront of our thought process
because if you inadvertently miss one of those, then the customer is just
going to be like what's this? I can't use it. So
the data scientist was really there to
make sure that working closely with the data engineers and other folks that we can actually hit the performance characteristics
that are necessary. It was a really interesting
process because it was a shared effort between the product group trying
to make sure that we could work around some
of the technical issues. It was an issue of the data group trying to
make sure that we had the stream of data and the actual infrastructure to support the analysis and then the data scientist
of course was, they're really responsible
for making sure that we were generating
high-quality signal. So it was not a total go back to
the drawing board but we had to get creative about, "Okay, what are we going to do?" In fact, what ended up happening
was we end up having to simplify what we're doing and that's not always a
bad thing actually, right? Sometimes the simple
thing that works is better than a really
complex thing that might have a lot of bells
and whistles on it. But it sounds like you also had to scale back what you
were hoping to achieve. Definitely, yeah. How does that conversation go? Is it the data scientist saying, "I'd like to be able to give you the silver bullet but I
can only give you bronze." Yeah. Or is it the product people
coming in and saying, "Oh, stop trying to optimize it. Let's ship this thing." How
did that conversation go? In this particular case, we had a really
collaborative great team. So we thankfully didn't
have the situation where one person's being
entrenched and tractable. But yeah, we did have commitments
to customers that we'd sold and so there's
a bit of a well, we said we could do this, we thought we could do this, we can't actually do this right now, maybe we
can do it later. Then we had to go back to
certain customers and well, let's scale back the
expectations a little bit, yeah, and it turns
out it was fine. But you have to again, think about how to pitch that. So the data scientist is really there especially in a startup environment where
you're trying to innovate, you're trying to do
something a little bit new. I would genuinely suggest that under promising and over delivering is really
the way that you want to go. I've seen a lot of process where people will get a bit
excited about what they think is possible and then the real-world
comes crashing in and everybody feels a
little bummed out about what the outcomes
are so you have to. That comes with experience
though, I think too, right? This is a new field so
everybody's learning. You've been through a
lot of projects now. Do you have any signature maxim that you always come back to, something that when you say it, your colleagues say, "I knew [inaudible] was
going to say that." I am notorious for telling people do you actually
have enough data for that? I can't tell you the
number of times that people have said, "Oh, we should use this suite
deep learning model or we should train our own word
vectors or embeddings." It's like, "Dude, you don't have enough data for that,
not even close." You telling me that
you'll get glove, it's trained on like
800 billion example or something crazy like that, compare that to glove being a
word embeddings tool model. Yeah, industry I
think is oftentimes still in the stage where
they're scrambling to get the data and have the
data in a usable form. People get excited about
a blog post and they wanted to play with a new
shiny toy but the reality is there's a very select
group of institutions in my opinion that actually
have the amount of data and the resources frankly
to generate that data, to use those techniques. For some of the
techniques, not all of the techniques need that many. That's right, that's
exactly right. So that comes to the other thing that I'm also
known for saying which is, this is an old saying that applies to lots of things but keep it simple stupid, right? You should not jump to the
most complicated thing. You should try to start with
what you know is attainable. You should start with what
you know and then iterate. That goes for software, I think that goes
for data science as you work your way through the exploratory phases
of the project, the initial analysis,
and the modeling. Even when you're
presenting results, right? You should have a tight
cycle where you're not going off into a dark corner for six months and then coming out and saying, "Well,
here's what I did." That usually
sometimes I think can lead to [inaudible] expectations. Sounds like you have an approach of iterative improvement. Do you have some
phrase that summarizes that when someone is going
off from the dark corner? It's a little grim but we call it going off
on a death march which is you're off
in the wilderness, working on who knows what, and maybe you come
back with something useful but more often
than not you don't. So the idea is that you want to avoid
those sort of things, you want to keep people
working together collaboratively and being really trusting of each other, right? I feel like there's another thing that pervades the tech industry
in general as people feel like they need to know everything and so it can sometimes
lead to cultures where you get looked
down on for saying, I don't know this right now or we need to figure
something out. I think as data scientist, it's extremely
important that you're the person who's first
to volunteer that, we don't know this, right? We have to go figure
it out so that we can know what to do next, right? That really does involve being an integrated part
of the team. Yeah. It takes confidence
to be able to say. Yeah. No, I don't know that. Maybe I should but I don't. Yeah. It takes self confidence
and then I think it also is a sign of a good company culture and
history at least where you've got the trust of
the management team and that it's okay to fail. You need to not have
surprises and of course, if there's limits being made, they need to be realistic but there's got to be
some trust there. So I'm going to now
the flip question, are there maxims in the
field that you hate? When you hear them
you're like, "Oh, no, don't go by that." Yes, I have some. I mean, with people I think they're
a little less experienced. Sometimes you'll
see them presenting results and
obfuscating, let's say, their support or some of the metrics that
are less favorable and anybody with
any real experience can look at the metrics and understand what's happening here. So don't try to hide behind your really great recall
if your precision is terrible or an F1 score. I've seen people try to lean
on accuracy, for example, when they've got
imbalanced data which is just a bad metric
to use in that case. So I don't like it
when people are trying to pass off results that they think are better
than they are at. Again, it should be
okay to say well, here's what we were
able to achieve and how do we rethink it. Problem formulation
is really important. So many people jump right
to the modeling, I think, and it often starts
with do I have the data structured in a
way that aligns to my problem that I'm
trying to actually solve. So that's more of a- A practice. Yeah. Not really a maxim. It's not that they're
saying correlation doesn't imply
causation and actually that you could do
some causal analysis. Yeah. This is where they're trying to make things look
better than they really are rather than just presenting. I do actually have
a good one though, you will often hear data scientists in
the industry complain about the need to go get the
data and clean the data, and I get that, that's not the fun part of
the job for a lot of people, but I think it is
part of the job. You need to be a well-rounded and able to work with
what you've got. I think that scrappiness is a virtue and so if you
can figure out how to make progress in the face of not having the data
handed to you on a silver platter, it's
a really good thing. So brush up on your
SQL skills and learn how to write some
scripts to transform things, because, again, what you'll
find in the industry is most places don't just have a really nice feature
library to lean on. I've got you. Scrappiness is a virtue. Yeah. Sure. That's another main maxim. Yeah. So in this course our
students are learning about, I'm dividing up projects into four phases that I'm calling
problem formulation, data collection and manipulation, analysis and modeling, and
then putting it into action. How much time do you usually spend if you had to allocate percentages
to those four phases? Well, so it definitely depends on the problem
you're trying to solve. I think while this
is a new field, there's been incredible amount
of work going on in it. So depending on what
your problem is, there can be sometimes a lot of best practices that
are already established, in which case maybe you
need to spend less time on your problem formulation and more time on actually
trying to improve the results for your
particular case. For most of the problems
I've personally worked on, I feel like that you always
benefit from more planning, which I think is the
problem formulation, and making sure that you are working closely
with your stakeholders to understand and make sure they understand
what is possible. You need to be very careful that when you're talking
to your stakeholders that you're actually
communicating with them because sometimes you'll find that they might be demonstrating some
magical thinking about what's possible, that machine learning is
magic and we can do whatever. That's not how it is actually. So problem formulation. I mean, if I was trying to break it down into percentages, I mean I think a
third of your time should be spent on
problem formulation. Typically, you've got to then do the exploratory analysis. This is where I think the iteration really
comes in handy. Spending some time
wallowing in the data is a good thing but you don't want to spend
all your time there. You want to probably try to
spend a little bit time, see what you can understand, and then try to see how
you can use that to address your use case. So maybe you're spending
15-20 percent of your time on the
exploratory analysis. You didn't mention anything about data collection or cleaning here. Sure. Yeah. Maybe you don't have
the knowledge yet. Yeah. As I just said, you might
need to actually do that. That actually can be a
huge chunk of the project. If we're actually
trying to be realistic about what's necessary
to go and get the data, make sure you actually have a
training set that's viable. That you understand what each of the columns really means. Yeah. I mean, again, depending on what you're
doing that you actually got potentially labeled classes
and things like that, that can be really
painful in some cases. In fact, there's whole
industries that sprung up around doing annotation of data
for Machine Learning. Yeah. There's some big companies
that I know this is. You mean to do human
annotation, figurator. Yeah, exactly. A figurator
or a scale API or whatever. Some of these companies
are like that. But yeah. I mean, a lot of places
don't have the resources to leverage somebody
like a scale of eight so it sometimes falls on the data analyst and
you've got to be creative. This I think comes down to the data preparation
if you can use heuristics to generate your
labels and things like that. So yeah. I mean, the sad reality is I
think that in a lot of cases depending on
the team structure and how much of it is falling
directly on you and what the level of maturity
of the organization is, you might really spend
30-40 percent of your time just
getting the data and making sure that it's usable, which is, again, not always
the fun part of the job. Then we've got our exploratory
analysis and remodeling. Finally, I actually want to just get a little bit
more from you because of your engineering background
about what happens in this integrating
into action phase. The data scientist's they've
done their Jupyter notebook, and you've gotten your F1 score, and then you have some
measure of accuracy, and you think it's ready, you've tried it on an evaluation set that's
different than you trained in, so you're confident
you're not over-fitting, and now you wanted to
put it into production. What usually happens
in that phase? Yeah. So this is an area that's been developing quite
rapidly actually recently, which is pretty
exciting for somebody who remembers the battle days and how painful it was sometimes to get some of these
models deployed. First off, I would say
that it actually starts at the problem formulation
and making sure that the data scientist is engaged
with the ML engineer, and working with them in terms of what data are we
actually going to have? What's the performance
characteristics that we need to attain? Is this something that's
going to be used in real-time with a
human on the wire waiting for a response
or is it batch where we're doing
this analysis weekly, nightly, whatever
the case might be? Those all imply very
different requirements in terms of all the level
of infrastructure that's required to support them. So when you think about some of the
things I've worked on, you will in parallel
be spending a fair bit of time making sure you've got the framework to
deploy the model. So the hand-off there at the end, once the results have been
presented and agreed upon, should be fairly smooth if everything went as
expected earlier. There are always
hiccups of course, so you're going to
find that well, we don't have the data
in quite the right way, and now we got to figure
out rejigger things. But it should normally be It should normally be
done in a way where you're leveraging the data that
you have available to you, that's going to be the same for production environment
and things like that. I'm hearing you say, there should be some
conversation little upstream. That's right. You say, "This is a
real time problem. We're going to have
this constraint that the answer has to come
back in a quarter of a second and that that might cause you to make some
trade-offs if I'm going to choose a simpler model
that's faster to complete." That's exactly
right. Yeah. I mean, I've personally not
worked in this area, but I've known people who do like so algorithmic advertising, I think is a really
great example of this. These are situations where I don't know if
people may not know. So when you go and
visit a website, and they're going to
serve an ad to you, there's often an auction
process that happens in milliseconds between a bunch of potential ad buyers
and the ad server. So that really is super
performance critical situation, or think about a self-driving car
application like wheels. Or things in finance. Or finance. Yeah. Exactly. Yeah, yeah. That's exactly right. So but the big lesson I'm getting there is
don't wait to the end because there may be some
additional constraints there that you didn't really realize that you ought to be taken into account earlier on. Definitely. So are there things that you
think our students should be sure to learn? So the ability to get the data yourself and
to work with the data. I mean, obviously, this
is super important. The analytical skills
are obviously central, but I think one of
the things that will make people really successful in this career is the ability
to communicate effectively. There's an old saying, I
didn't come up with this, but well, I can't
remember exactly. It's like software would be great if it wasn't for the people. But the general idea is that most technology development
is working with others. So you need to be able to be articulate about your ideas and why you think
they're important. I also think you need
to be real for yourself about what your analysis
is actually showing. All analyses are some
hint of the truth, but it's hard to
really accurately represent the full truth. So you got to be careful to not mislead even unintentionally, which I think is
the bigger concern. Most people don't want to
intentionally mislead. But it can be easy to
jump to conclusions. So communication I think
helps with that too because when you're soliciting
other people's opinions, and they're challenging
your ideas, and you're working constructively
on towards a solution, that leads to better outcomes. So think about making sure that your visualization
game is up to snuff for when you're
presenting the data, think about how you can present and be good like just a
speaker to your boss' boss. Like sometimes it
can be intimidating, but you should be able
to get good at it. Okay. When you're hiring,
what do you look for? So it depends on the
stage of the company. I think that for me right now, what I look for is and
this may sound cliche, but passion is really important. You need to want to do this work. It can be a slog at times. I think that when people
really care about the outcome and find it
enjoyable, that's super-critical. Being a team player is
obviously really important. You can be the smartest person in the world doing the
best analysis ever, but if you're insolvable, nobody's going to want
to work with you. So learn to be kind
in the workplace and defend your ideas in a way that's
palatable to others. Then, of course, the hard
skills are important. So I genuinely will say
that I like it when, this maybe my bias showing, but I like it when I see a data scientist
or somebody who's gotten a little bit more like a multidisciplinary
background. I think that the
domain is obviously, we didn't really
talk about domain, but domain is super-important for various fields
and almost any field. So having a background in
whatever industry you're going into can be really key to being able to solve
those problems well. So that thing can be
quite important too. What's going to be different in data science in five years? Well, we talked about
getting things deployed. There's some really
exciting trends right now happening with better ecosystem for basically model deployment,
validation, testing. So if you think about,
there's a project that Google has been
back and called Kubeflow, and it's basically, I call it like an ML platform for you've got your JupyterLab, you got pipelines that
you can define for like retraining and
like serving models, and then you've
got the model like actual serving component. So it's pretty cool. You can imagine a scenario where you're continuously
iterating models, deploying them in real time
without the help of anybody. Because again, if you're doing
it in the right framework, then you can just
push them out there. Then, sidecar process, you're comparing
those results against your current production model
and you're doing like A/B testing or getting to the point where you're
doing for full, like blue-green deploys were. We're swapping out models live in production to do testing. This is a harness. Yeah. It makes it, so that you
need a little bit less often engineering skill to be able to do things in production. Yeah. I think that
what we're saying is more of a push for, and it's again, depends on
the industry, but certainly, in startups, you see
more of a getting the data scientist
closer to the output. I think that's an exciting
trend that I've seen. Like it's two competing trends. One is we're demanding more that with this stuff has
to get into production, we've got all these
other constraints that data scientists aren't
really trained for, unless they also happen
to be engineers. So maybe there's a need for data scientists to pick up
more software engineering. But on the other
hand, we're getting better and better tools. We're doing that step.
So it may be that with the two to three semesters of programming that the data
scientists typically have, they'll actually be able to
control these environments. Yeah. Being a better programmer, and having a good background
in computer science, and generally,
software engineering is certainly a positive. But yeah, the tools are
really getting quite good. I think what we're seeing too is more of this being
developed as a service where you need to worry less about the
infrastructure and you can just rely on Google, ML Engine or some other tools to help you do that without
the need of a big team. So yeah, I don't know,
being balanced is good. You need to be literate about what the software aspect is, but it's also not fair to
have to know everything so. Right. Yeah. Well, any last words of
wisdom for our students? Make sure you're learning Python. Python more important
in industry right now than other frameworks.
R is great. Personally, I love R,
but I just only use Python these days because
it's what everybody uses. Understand how memory works. You got to be mindful about not blowing up your Jupyter Kernel
because you're like, "I can just load all
the stuff in there." I can't tell you how to times of somebody like,
"How's this going?" I'm like, "I know. I tried to train the
model overnight, but my kernel crashed and I'm not going to have it ready
for another couple of days." So be careful about what your
resource constraints are. Anyway those are
just minor tidbits. So thank you very much. I really appreciate
you're coming in and spending the time. I'm sure our students
who appreciated as well. Yeah. Great. That was fun being here and thanks
for your time. I hope everybody
enjoyed what I had to say at least in some way and good luck to
you in your future.