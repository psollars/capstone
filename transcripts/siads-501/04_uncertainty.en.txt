Welcome back.
Uncertainty. How sure are you about the conclusions
of your data analysis? For birthday present, I
always give the gift of uncertainty, or do I? The end of today's lecture, you should be able to appreciate the value of multiple models, multiple approaches,
when forecasting. You should make a commitment to communicate uncertainty
and you should understand the value of conveying
uncertainty in terms of concrete potential outcomes rather than statistical
abstractions. Foxes and hedgehogs, it's
a terminology coined by Philip Tetlock and has origins going back to
the Greek poet Archilochus. The fox knows many things but the hedgehog
knows one big thing. So in a more modern contexts, the hedgehog believes
in big ideas, governing principles that
really undergird everything. Think of Karl Marx and class struggle or anybody
who comes forward and says, "Elections are decided on the basis of economic interests, everything else is auxiliary." Or they say, somebody
else will say, "Elections are decided on
the basis of social issues." Some other hedgehog
will say, "No, elections are decided on
the basis of media narratives." A fox on the other hand
believes and taking multiple approaches
and combining insights, they are likely to say, "Elections are decided by all three of those things;
economic interests, social issues, media narratives, and also four other things." You're reading encourages a fox-like approach
to forecasting, which involves
three key elements. First, think in terms of
probability distributions. So express forecasts
probabilistically in terms of a range of possible outcomes or the probabilities
of those outcomes. Think of something like what's the probability that we will get three or more inches
of snow overnight? Second, change your forecasts when you get new information. So don't be afraid
to look fickle. On the other hand, you
might look forward and if you would change
your forecast by a lot, when just a little more
information comes in, then that means you
have great uncertainty about your current forecast
and you should say so. Third, look for consensus among different models
for information sources. Now, these principals aren't
just good for forecasting. They apply sometimes
in modified form for presenting the results of many different kinds of data analysis. There's really
an ethical imperative of communicating uncertainty,
people like certainty, and as an analyst,
you will look weak, if you present your results with great uncertainty but
you should do it anyway. In the reading from Chapter 6, that you'll have from
Signal in the Noise, it presents a really
compelling example. Nate Silver, the
author of the book founded the influential
political forecasting site, FiveThirtyEight
which is named after the number of US
electoral college votes. But this example is not
about election forecasting. In 1997, there was a heavy
winter snowfall in the Midwest and the best prediction
two months in advance was that, when all the winter snows melted, the spring river level
in Grand Forks, North Dakota would rise
as it does every spring, but with this big
and still at fall, it would crest at 49 feet. Now, they have 51 foot levees to keep the water from
flooding the city there, so everyone's sort
of said, "Phew, we're safe, we've got
a two foot margin here." But historically, the forecasts of exactly how high river
would get when it crested weren't that accurate. They had a margin of error
of plus or minus nine feet, meaning 95 percent of the time, the actual high watermark was within nine feet
of the prediction. So if you look at that cumulative
distribution function, that means that in 1997, there was a 35
percent chance that the levee was going
to be overflowed, that the water would go above
the 51 feet of the levee. It would've been
a good idea to take preventive measures starting
two months in advance, stack up some extra sandbags, but uncertainty of
the forecasts apparently wasn't conveyed and they had big flooding problems and
lots of damage in the city. Now, by the way
uncertain warnings of danger are always
can be problematic. If there's a 35 percent chance if something really
bad happening, the flood, and you
raise an alarm, two-thirds of the time, it's going to turn out
that the flood doesn't happen and it hurts
your credibility, and then the next year they
won't heed your warnings. In fact, if your audience
doesn't understand uncertainty, you're going to have
a problem no matter what. My advice is you should try your best to convey
uncertainty anyway, and try to educate the audience. An example of damned if you
do and damned if you don't, Nate Silver's FiveThirtyEight
site on the morning of the last US presidential
election said that, Hillary Clinton had 71 percent chance of winning the election, she lost, and FiveThirtyEight has been criticized
for getting it wrong. If Clinton had won though, the site would have
been criticized for not being confident enough. Almost all the other forecasters gave Clinton
a much higher chance of winning or just said she'd win without assigning
a probability at all. So the ethical thing is to reveal your best estimate
and tried to help your audience understand
the nature of the uncertainty. Okay. So you've resolved
that you're going to present uncertainty
rather than cover it up. How can you do that in a way that non-statisticians
will understand? The conventional way to
present uncertainty is with error bars on a bar graph
or margins of error. It turns out that most people are terrible at interpreting those. One of the optional
readings for you this week describes research that I collaborated on with a former Doctoral student
Jessica Holman. We did an online lab study
and demonstrated just how bad most people are at
interpreting error bars. But we showed that they are
much better at understanding concrete representations of alternative
plausible outcomes. We call these hypothetical
outcome plots. Each plot shows one
hypothetical plausible outcome. Rapidly switching from
one plot to the next gives you an animation of some
of the possible outcomes. The New York Times has been using this technique quite a lot, they invented it
independently and they don't call it hypothetical
outcome plots. So here's an example from
one of your readings. Every month, the US government
reports a figure for the number of jobs gained or lost in the overall US economy. There are two sources of
uncertainty in that report. One, the number that they
report is tentative. It often gets revised
as more data comes in and often it gets revised
quite substantially. The second source
of uncertainty is that you don't know whether
the month-to-month chains indicates a trend that
will be continued in the following month or weather it was just some kind of blit. So the New York Times
offers this as an animated visualization
on the left, is a hypothetical trend
in the true jobs numbers. By true numbers I mean, the final version that would be determined months later
after all the data comes in. On the right is random draws
for each month for an observed initial
jobs number which is the true number from the left
plus some random error, a draw from
a normal distribution. By rapidly going through several of these hypothetical outcomes, you can see
many possible outcomes that would come from
the same true jobs number. It might look like this or like. You can visually see that
having any one of them, which is all you'd
actually get in a particular jobs report, you can't reliably tell
what the real trend is. On the website, they animate the right side
of this graph using JavaScript to show many potential outcomes
as an animation. So in summary, we've got one ethical
commitment and a maxim. The commitment, I will always convey the level of
uncertainty warranted by my analysis even at my clients don't want
to hear about it. A maxim, we should present
this uncertainty in terms of hypothetical concrete outcomes rather than statistical
abstractions. It could be 53 feet or 44 feet
or 50 feet or 47 or 54, rather than a
statistical abstraction, it'll be 49 feet
plus or minus nine. So uncertainty. As people get older, they inevitably
start to worry that the following generation
is somehow degenerate, and not up to
the task of becoming adults and taking over
running the world. In the 1980s, young adults were denigrated as
the me generation, and Gen-Xers were
called slackers. Now, I know that video games have completely
ruined today's youth, but I wasn't sure exactly what
the bad effects were, so I asked one of my students. I said, what do you think is the biggest problem with
young people today? Is it uncertainty
or indifference? He said, "I don't
know and I don't care." I'll see you next time.