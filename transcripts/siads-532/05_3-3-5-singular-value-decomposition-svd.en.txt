There is another very famous matrix
decomposition method that is known as the Singular
Value Decomposition or SVD for short, and that instead of PCA, SVD applies to arbitrary
non-square matrices. In this case, suppose A is an arbitrary matrix that have
arrows and p dimensions, we can always rewrite the matrix A into the product
of again three matrices, U and Sigma and the
transpose of V matrix. Here, the Sigma
matrix is the n by p diagonal matrix with non-zero singular values
on the diagonals. What's very interesting about the U matrix and the
V matrix is that, the product of the U matrix and its transpose is the
identity matrix, and the product of
the V matrix and its transpose is also
an identity matrix. So singular value decomposition, you can see how similar
it looks to PCA, right. It also re-writes
the original matrix into the product of
the three matrices. The only difference is that, we no longer require
the matrix A to be square or symmetric or
positive semi-definite. In fact, A is actually an arbitrary matrix that may or may not be squared, and may or may not be symmetric. So we see that SVD is the generalization of
eigendecomposition to an arbitrary matrix. Again, to give you more
intuitive description of SVD, we can see that given a matrix A, that is the n by p matrix, we rewrite that as the
product of three matrices. The U matrix is n by n. The
Sigma matrix is n by p, and in the Sigma matrix there are only meaningful values
on the diagonal, all the other values are zero. On that diagonal, Sigma one, Sigma two to Sigma n are non-negative singular values of the original matrix A, and they are placed on
the diagonal of Sigma. If n is greater than p, then we only have p such values. These are called the
singular values of the matrix A in comparison
to eigenvalues. Then we also know
that every column of the U matrix is the so called left singular
vector of the matrix A. The third component of the
product is the transpose of the V matrix
with the size p by p. We know that every column of the V matrix or every row of the transpose
of the V matrix, is known as the right
singular vector of the original matrix A. Later, we will see that these right singular
vectors are very useful, that will help us identify
the new cleaning system. So we can see that there
are lots of connections between SVD and
eigendecomposition. In fact, there are very
interesting facts. If you look at the
SVD decomposition, A is rewritten as the
product of three matrices. In fact, we know that the U matrix is the
actual matrix of eigenvectors of the matrix
A times A transpose. The V matrix is the matrix of eigenvectors of A transpose A. That is what? A is the
covariance matrix of A. So that tells us that these eigenvectors are
orthogonal. Why is that? Because both A times A transpose, or A transpose times A are symmetric matrices.
This is great. This means that the
left singular vectors U,U and the right
singular vectors U, V are also orthogonal
to each other, and that means we can
probably also leverage them to define orthogonal
coordinate systems. We also know that the
components of the Sigma matrix, or the singular values are the non-negative
square roots of the non-zero eigenvalues of the A transpose A,
or AA transpose. So it's nice to see that SVD have so many relations to
eigendecomposition. But what's more important is that, unlike
eigendecomposition, SVD actually exists
for all real matrices, no matter whether
they're dense or sparse, no matter whether
they're square or not, and no matter whether the
matrix is normalized or not. Isn't that ideal? This is very desirable for real world data because
real world data is messy. You don't actually have
symmetric matrices, sometimes it's sparse, sometimes it's dense
and in many cases, they are not normalized. Similarly to PCA, you can also use SVD for dimension reduction. Essentially, instead of computing the exact decomposition
of the original matrix, now you can actually compute the approximation of
the decomposition. Now, suppose you have the data matrix X that has
n rows and p dimensions, you can compute approximation of x using the product
of the three matrices, U matrix, the Sigma matrix, and the transpose
of the V matrix. But here instead of p dimensions, you only need to keep the k
dimensions of the matrix U. Correspondingly, the Sigma matrix will be the k by k matrix. The V matrix, the transpose of the V matrix will be
the k by p matrix. How to get this approximation, it is very simple. Essentially, like
PCA, you just choose the largest k singular values in your single-value
decomposition, in your Sigma matrix, and then you truncate the corresponding k columns
of the original U matrix, and the k rows of the original V transpose matrix now will
give us this approximation. Based on this truncated U
matrices and V matrices, we can easily transform our original data X into the low-dimensional
representation of the data. We call that X star. You can either calculate
X star as the product of the truncated U matrix
and the Sigma matrix, or through the product of the original X matrix and
the truncated V matrix. Because either the
U matrix is n by k, and the sigma matrix is k by k, or the original matrix X is n by p and the V matrix is p by k, if you use either of
these calculation, you will get n by k matrix. So the transformed data matrix, we have n rows and data
objects but only k columns. Through that, we have
successfully reduced dimensionality of
your matrix data.