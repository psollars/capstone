You can see that there
are lots of connections between sequential
patterns and text mining. That's so far are examples
where you are using text. This is because text data are naturally represented
as sequences. They could be sequences
of words if you choose to model every
word as the item. They could also be the
sequence of characters. If you do this, the
sequential patterns in a sequence of words
or character could be interpreted as phrases, collocations, or other
expressions in natural language. For example, one could model a sentence as
a sequence of words, and try to extract n-grams
or skip-grams from the text. Though, you will see examples of n-rams such as "Ann Arbor", which is frequently used
bi-gram in natural language, and this actually
indicates the location. You can also see "hot dog". Now, you can see that
the order matters, the phrase "hot dog" does
not mean a dog that is hot, it is very frequently
used in natural language. That's why you can extract some through frequent pattern
mining from sequence data. Other expressions like "in front of" is also the
n-gram in text data. This is the very
interesting example. Do not use frequent pattern
mining you wouldn't have discovered this
combination of items. This sub sequence action
means "Txt U L8R" is the expression people
use in social media. So you can see that
by modeling text as sequences and by extracting the frequent sub
sequence from text, you can discover a lots
of interesting things. Then what about
skip-grams in text data? You can also extract skip grams, now the frequently used
in human language, such as "Not only... but also." So those are fixed expressions
in natural language. What is interesting is that, if you read lots of machine
learning papers nowadays, you can really
extract skip-grams. Now the frequently used
in enough Hayden's such as "deep * networks." Deep convolutional
networks, deep residual networks, deep neural networks. So that indicates the trend
of the research nowadays, and you can see that you
can extract this pattern. You can extract this
node h by looking at the frequent sequential
patterns from text data. In fact, that is the general procedure
that people usually use. To make use of sequential
patterns for text mining. So given a piece of text data, could be the document, could be many documents, could be a sentence, could be the customer review. So we first represent the text data as the
sequence of words. Then from the sequence, we can extract the
n-grams and skip-grams. These n-gram and skip-grams could give us the nice representation of the semantics
in this document. Once we have extracted the
n-grams and skip-grams, we use them as features. Then we can actually further
represent the state of n-grams and skip-grams as
feature vectors or as itemsets. Then we can pass this itemsets or vectors into the data many
functionalities we care about. To facilitate the classification,
clustering, ranking, recommendation or
visualization of the original text data. So the key here is to represent text data as
a sequence of words and then transform the
sequence of words into the vector of n-grams and
skip-grams as features. This is the very
commonly used procedure of machine learning for text. There are some examples of frequent patterns
for text mining. To give you the
example of how to use this procedure to
deal with text data. Again, let's look
at a few example, the sequence of words
to be or not to be. We can represent this
as the set of trigrams. That means, we
transform this sequence of words into least, into a set of trigrams. Here we have four trigrams. We can also represent this sequence as a
vector of bigrams. In this case, what we
needed to do is to find how many unique
bigrams are there in our data and use
them as dimensions. So there are four unique bigrams, that gives us the reputation of a four-dimensional vector. You assign values
to each dimension, because the bigram to be
appear twice in the sequence, you put two there and all the other three bigrams
only appeared once. So you ended up with
the feature vector 2, 1, 1 and 1. Then you can use
this feature vector, or you can use the pivots
itemset of trigrams as the input to facilitate all the functionalities
that we have learned for itemsets and vectors. So to summarize, we have introduced the
difference between sequential patterns and frequent itemsets.
What's the difference? Again, the difference comes from the order among
the category items. We have introduced
n-grams and skip-grams as special cases of
sequential patterns, and they are very powerful, they're very useful to
represent text data. We normally represent
text data as sets of vectors of n-grams
and skip-grams as features, and then facilitate downstream machinery
texts for text data.