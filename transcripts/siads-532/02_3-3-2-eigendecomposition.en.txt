So how are these patterns useful? The reason that we care about eigenvectors and
eigenvalues is because then eigenvectors and the basis of many metrics
decomposition operations. What do you mean by
matrix decomposition? In linear algebra,
matrix decomposition is also called matrix
factorization. That is basically the
operation to factorize the matrix into a product
of multiple matrices. Sometimes the products
of smaller matrices and sometimes the product of
matrices with the same size. One particular example of matrix decomposition is
known as eigendecomposition. In many contexts it is also called spectral
decomposition. So suppose A is a square matrix. Let's assume that A has
distinct eigenvalues. Then we can rewrite, we can factorize A as the product of three
different matrices. The U-matrix and
the lambda matrix, lambda is the diagonal matrix. That means, they only have non-zero values on the diagonal all the other values are zero. Then the U minus one, that is known as the
inverse of the U matrix. The inverse of a matrix, if you remember in
algebra is that, the product of the
original matrix and the inverse of matrix
equals to identity matrix. So this is what we know
about eigendecomposition. So let me give you a more
intuitive description of eigendecomposition. Pick the matrix A and then
there's square matrix. Let's assume that it has
n rows and n columns. We can rewrite this matrix as the product of three n by n
matrices of the same size. There's the U matrix and
there's the lambda matrix. You can see that
the lambda matrix is the diagonal matrix. That means there are only
meaningful values on the diagonal and all
the other values in this matrix are zero. Then there's the inverse of the U matrix that is
also the n by n matrix. So in the eigendecomposition. We're looking at
the target metrics that is a square matrix. Then lambda is a diagonal matrix. Element in this diagonal
matrix is the eigenvalue of A. So lambda is the diagonal
matrix of n eigenvalues. What do we know
about the matrix U? In fact, the matrix
U is the matrix of all the eigenvectors
corresponding to each of the eigenvalues. That is to say that every
column of the U matrix is the corresponding eigenvector of the lambda values
of the eigenvalues. So this is nice, but there are even more facts
about eigendecomposition. Although we say that eigendecomposition are
operations on square matrices. Remember that
eigendecomposition does not exist for all
square matrices. There are some other
special requirements. In mathematics, it's a fact that eigendecomposition only applies to diagonalizable matrices. You don't have to understand what the diagonaizable matrices are, but you just need to know that diagonalizable matrix will have n distinct eigenvalues. That means after the
eigendecomposition, the lambda matrix, the diagonal matrix will
have n distinct eigenvalues. So in reality,
sometimes it's hard to identify which matrix is
diagonalizable and which is not. But you can remember that eigendecomposition always applies to the symmetric matrix. That is, if your matrix is symmetric you can always
apply eigendecomposition. Sometimes there are
even more requirements of the matrix if you want the eigenvectors and eigenvalues to present particular properties. For example, if you want eigendecomposition so that the eigenvalues are non-negative, then the metrics not only
needs to be symmetric, but also needs to be so-called
positive semidefinite. So again, you don't have to go very deep into
this algebra concepts. Just remember that in some
circumstances there are special requirements
about your matrix to get robust eigendecomposition. Well, this is all good, but you still haven't told us why it is useful in reality. How can we use eigendecomposition and eigenvectors in reality? The reason that we care about eigendecomposition
and eigenvectors, is because we want to solve two particular issues of vector representation in general. As we said that vector conditions are so powerful that they can handle numerical data of different data
attributes very well. But they do have a few
problems in reality. If you want to represent
your data with vectors, you will usually find that, "Oh, they are just so many
possible dimensions." Should I add 10 more
dimensions or should I just restrict the missions to only the most important
attributes of my data. In many scenarios the
dimensions we can identify are highly
correlated to each other. Sometimes they're not
even highly correlated, they are either redundant.
What's wrong is that? In fact, if the
dimensions are highly correlated or even redundant, they will seriously affect your calculation about vector
similarity and distances. That is the major deal in
machinery in Data Mining. Let's take a look at a
very simple example. You want to represent two cities in terms
of their temperature. So you decided to
include two-dimensions, one indicates that day average of the temperature of the
cities and another indicates the night average
of the temperature and you take the measurements
both in Fahrenheit. So for city one you
have two numbers that day average temperature of city A is 77 degrees Fahrenheit, and the night average is 59. City B, the day average is
68 and night average is 50. So intuitively how much different are the two cities
in terms of temperature? You may think that
intuitively distance between the two cities is just nine
degrees Fahrenheit, right? Both the the day of average
and from the night average. However, if you use any vector similarity metric
or vector distance metric, you will see something different. For example, if you calculate the Euclidean distance
of the two vectors, you will see that the distance between the two vectors
are actually 12.7, which is not nine. What happened? The
fundamental issue is that, your two dimensions are not
orthogonal to each other. In fact the day and night temperatures are
highly correlated. So night have inflated
the distance calculation. Now let me give you a
more extreme example. Suppose now you decide to add two more dimensions into
your vector representation. Which is completely
legitimate, right? And the two dimensions
[inaudible] in are the day average
temperature and night average temperature but
measured in Celsius. Now what happened is that instead of the
two-dimensional vector, you can represent both cities with the four-dimensional vector. 77 degrees Fahrenheit,
25 degrees Celsius. Both average in the day. Fifty nine Fahrenheit at night and 15 Fahrenheit at night. City B can also be represented as the
four-dimensional vector, 68, 20, 50, and 10. Now what happened here is that, after you added two
more dimensions, you did not give any new
information to the cities. The two representations
are essentially covered in the same information.
There's nothing new. But now if you calculate the Euclidean distance
between the two vectors, the Euclidean distance
is not even 12.7, is actually even higher. So what happened is that, the dimensions of
the vectorization are not only highly correlated,
but even redundant. In this situation, you can really trust the
distance you calculate. So how will solve this problem? Intuitively, if there is a way that we can
construct the dimensions, if we can construct the
coordinate systems. Now the compact and the coordinates are
orthogonal to each other. That would be fantastic. So that will help us
tremendously when we represent our data into
vectors and matrices. Of course the problem is how. Here is why we need matrix
decomposition because this operation will
help us identify orthogonal and compact
representations of vector space.