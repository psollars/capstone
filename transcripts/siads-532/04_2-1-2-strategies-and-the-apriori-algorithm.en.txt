So frequent itemsets and association
rules are widely used in practice. In fact, to find association rules, you really need to find
frequent patterns first. How to find them, conceptually,
it's not a hard task. Essentially, you just need to scan
every transaction in your database. And then you enumerate all the possible
subsets of the items in each transaction. Then you check one by one whether their
frequency is above the minimum support. Only those that have the frequency higher
than the support are output as frequent itemsets. Once you have the support, you can then calculate the confidence
of particular associations. So this whole process can be
done in a rather simple way. You need to get the frequency of itemsets, we just need to count and
counting is very easy, right? Not really, when there are too
many possible candidates to count, counting could be hard enough. Think about that you have the database
of over 1,000 particular items. It is very common, Amazon sells 8 million products which
is way higher than 1,000, right? But even if you're dealing
with 1,000 unique items, there will be 1 million
possible to 2-itemsets. And there will be 1 billion
possible 3-itemsets. So the numbers quickly become
intractable if you want to find the frequency of
all possible itemsets. And in fact, even if you only care
about the itemsets that actually appear in your database it
is still a huge number. So you can see that the huge number
of possible patterns really make frequent pattern mining very hard. Lots of work are trying to scale up
the mining process of frequent patterns. And most of the work are more or
less utilizing the same iteration, which is known as the downward
closure property. What does that mean? The downward closure property
basically means that any subset of a frequent
itemset must also be frequent. In other words,
if one of the subset is not frequent, the superset cannot be frequent. Is that intuitive, indeed. In these three itemsets, beer,
milk and lemon is frequent, then the two itemset, beer and
milk must also be frequent. And this is because every transaction that contains three items
also contains the two items. In other words,
if the two pattern beer and milk is not frequent,
then we know that the three pattern, the superset beer, milk, and
lemon cannot be frequent. This is intuitive, but
how can we apply it to practice? The classical algorithm that applies
this intuition is known as Apriori. Also known as candidate generation and testing procedure of
frequent itemset mining. This algorithm applies
the following principle, which is known as the Apriori
pruning principle. And that is, if any itemset is infrequent, then we don't need to
check any of its superset. So how does this algorithm work? Initially, you just need to scan
the database once to get all the frequent 1-itemsets. In other words, you want to get
all the frequent single items, beer, milk and so forth. Then every time you can generate
longer patterns from shorter itemsets. The candidate itemsets you
generate are based on the frequent patterns of shorter list that
you have already discovered. Then you just test every
candidate in this candidate itemsets against the database to filter
whether they are frequent or not. And then the algorithm terminates
when there's no frequent or candidates set can be generated. Let's use the following example to
get a better understanding of how the Apriori algorithm works. Again, starting from a simple
database of five transactions. Each transaction contains
the set of items. And we are given that
the meaningful support is 2. In this case, we're using the absolute
support to make understanding easier. So we need a first scan of the database,
the goal of which is to enumerate all the one item sets or
single items and then to get their accounts and
means to get their support. We can see that the single
item beer appears four times. The item milk appears
in four transactions. The item watermelon only appears once. From this table, we can filter those
single patterns that are frequent. Which means the single patterns
where their support is higher or equal to the minimual support 2. And in this case, we have eliminated
the pattern watermelon and item y. So now we have a list of four single
items that are frequent in this database. Then based this time this list
of frequent one item sets, we enter the second
iteration of the algorithm. Basically, from the state
of one item sets, we generate a list of
candidates of two itemsets. And this is based on joining
the one item sets to each other. Based on this joining process we can
get a list of candidate 2-itemsets. We have six of them. We do not know their frequency or
their support yet. But we know that no other two itemsets
could be frequent because these two itemsets have already covered
all the frequent one itemsets. Then we scan the database for a second
time to actually count the two itemsets. And through this scanning
process we know that the itemset beer and
milk appear three times. The itemset beer and lollipop only
appeared one time so and so forth. When we check these counts
against the minimal support we can again eliminate to
two itemsets from the list. So we know that there are only
four 2-itemsets that have the support higher or
equal to the minimal support. This gives us a list of
frequent 2-itemsets. Then we continue this process. To first generate candidates of
longer patterns through self-joining the shorter patterns. In this case,
we join those frequent 2-itemsets to get all possible frequent 3-itemsets. And we can only get three of them. We know that any other 3-itemsets
cannot be frequent because they don't contain any of these frequent 2-itemset. Then once again, we scan the database to check the frequency of
the 3-itemsets ,these candidates. And we see that now only
one of them can pass our minimal support which is the pattern beer,
milk, and lemon. Now since we only have
one frequent 3-itemsets, we can no longer generate
candidates of fourr itemsets. So that means the algorithm
will terminate here. And summarizing the frequent 1-itemsets,
the frequent 2-itemsets, and frequent 3-itemsets,
we have now discovered all frequent itemsets from this simple
database of five transactions. So this is the intuitive example of
how the Apriori algorithm works. For students who want to implement them, here is the list of pseudocode of
how to implement Apriori algorithm. The input of the algorithm is
essentially a transactional database or shopping baskets. Also, there's the parameter that is
the minimal support that you care about. You need to build two data structures. Ck that contains the candidate
itemsets of size k, and Lk is the list of
frequent itemset s of size k. So you can see that for any number k,
Lk is always a subset of Ck. Now we start with L1, that is the list of all
frequent single items. We enter a for Loop where we grow
the lengths of the candidate items and frequent items by one every time. Every step inside the loop we
generate a list of candidate itemsets of one item longer from the list
of frequent itemsets of landscape. So starting from length one items
we generate candidate two itemsets. And then starting from
the two frequent itemsets, we generate candidate set
of then three patterns. We also we have generated the candidates
in Ck +1, we scan the database. For every transaction in a database, we basically increment
the count of every candidate in C as far as it appears
in that transaction. So this process can be
easily implemented using the dictionary data structure in Python. Then we know that Lk + 1 contains the candidates Ck + 1. Among those itemsets support is greater or equal to the minimumal support. And after the for loop terminates, we have computed all
the L1 to Lk minus one. And that contains all the frequent
itemsets from this one, length two to length k minus 1. And we output that as the result
of this Apriori algorithm. So the Apriori algorithm
is very intuitive. It is simple to implement but it is
powerful enough and it's used everywhere. There are still quite a few
challenges of Apriori especially when we apply
that to a huge database. The limitations are first of all, the algorithm needs multiple
scans of the database. It is not a big problem if
your database is small. But if your database contains millions or
billions of transactions, multiple scans sometimes even
mean a nightmare of computation. Secondly, through Apriori
algorithm we still have to check the frequency of the huge
number of candidates. And in reality, most of those
candidates will never become frequent. So there's the huge waste of computational
power to check all the candidates. And finally counting the frequency
of every candidate through the database scan can be tedious. It is okay if you can note
all the candidate items into memory through the dictionary
data structure. But if there are too
many candidates how to even count those
candidates is the problem. To solve this limitations
of Apriori people have proposed many follow-up algorithms. They're addressing this problem
in different perspectives. For example, some algorithms try to
reduce the passes of database scans. And some algorithms try to
shrink the number of candidates, or even better,
do not generate candidates. Some algorithms are using safety
techniques to only check part of the databases. And in that case,
they can compute approximation of the frequency of the itemsets. Some algorithms are leveraging the
distributed computing infrastructures for example, Map-Reduce to do the counting. And in fact Map-Reduce is the perfect
infrastructure to deal with counting. I recommend you to use that
as as much as possible when you are counting large databases. For students who
are interested in the further improvements of the algorithm
I refer you to the textbook. Chapter 6 and
7 of the textbook introduced lots of following up algorithms
to improve Apriori.