So principal components
are very interesting, very special patterns in your matrix data. That they define orthogonal
directions that capture most of the variance in your data. Usually there are multiple
principle components of a matrix. The first principal component would
explain the most variance of your data. And then the second principal component
would explain the most of the rest of the variances in your data. And the third one will explain most
of the rest, and so on and so forth. And the method discovers those
principal components is known as principal component analysis or PCA. Most of you may have heard
that from many contexts, because this is such the popular
data science technique. The function of principal component
analysis is essentially to find the eigenvalues and
eigenvectors of the covariance matrix. We will talk about what that is later. The eigenvectors with
the largest eigenvalues of this covariance matrix will
correspond to the directions, the dimensions that have the strongest
correlation in your data set. So the first eigenvector that
corresponds to the largest eigenvalue is the most important
direction in your data set. The second eigenvector defines the second
important direction in your data set, so so on and so forth. So, how does PCA work? First we normalize the matrix. Suppose your original data matrix
X has n rows and p dimensions. We subtract the mean of every
column from this matrix x. And then we compute
the so-called covariance matrix. Let's just name that A. The covariance matrix of your
data matrix is essentially the product of the transpose of your
matrix X and original matrix X. We call that XTX. XT is the transpose of your data matrix X. So A is the covariance matrix. If X has n rows and
p dimensions, and p columns, then X transpose X will have p rows and
p columns. So A is the p by p matrix. And what's even nicer about A is
that A is a symmetric matrix. Does that ring a bell? Because A is a symmetric matrix we can
always compute the eigendecomposition of A as we discussed earlier. And after the eigendecomposition, we can rewrite A as the product
of three matrices, the U matrix, a Lambda matrix, and
the inverse of the U matrix. In this case, because A is the p by p
matrix, then all the three matrices, U, Lambda, and
U inverse are p by p matrices. Now we can observe the U matrix that
is essentially is the collection of eigenvectors of the matrix A. We can make use of these
eigenvectors to transform the original matrix X into
the new coordinate system. And we call this transformed
matrix representation X star. That is essentially the same data,
but represented in the new coordinate system where the coordinates
are orthogonal to each other. How to do that? Essentially you just calculate
X star as the product of the original data matrix X and the U matrix that you obtain
from the eigendecomposition. Now if X is the n by p matrix and
we know that u is the p by p matrix, then the transform matrix X star
is also the n by p matrix, right? So after this transformation, now we have expressed our data with
the alternative coordinat esystem. And because the coordinates in this
new system are the eigenvectors of a symmetric matrix, we know that
they're orthogonal to each other. So this new coordinate system
has orthogonal dimensionality. Isn't that great? So now we know that through PCA, you solve one issue of vectorization
that is to make sure that the dimensions are orthogonal to each other then
what about too many dimensions? Can we also use PCA to reduce
the dimensions to only keep the most important dimensions? Yes, we can, and it is quite simple. Remember that the eigendecomposition
finds all the eigenvalues of A. Suppose there are p of them. We can essentially sort the eigenvalues
from largest to the smallest. And we can sort the eigenvectors
corresponding to these eigenvalues according to this order. So assume that we can sort
the eigenvalues of A decreasingly. We can then place the eigenvectors. We can then rearrange
the eigenvectors in the U matrix. Why do we want to do that? Remember that these eigenvectors
explains the variance in order. The first eigenvector
explains the most variance, the second eigenvector
explains the most of the rest. So in reality, we don't need all of them. Because at some point,
our eigenvectors have already explained most of the variance
in your data, we can just stop. So based on orders of the eigenvalues,
we can essentially rearrange the columns of the U matrix and the Lambda
matrix from the eigendecomposition so they are sorted in decreasing order
of the corresponding eigen values. Then we can do the very simple
operation on the U matrix. And that is, we just truncate
the top k columns of the matrix. So that gave us a new U
matrix that is p by k, right. So we only keeps the k eigenvectors that corresponds to the largest k eigenvalues. And based on this truncated U matrix, we can now transform our data
matrix into k dimensions. All what you need to do is to compute
X star as the product of the original data matrix X and the truncated matrix U,
in this case, U star. Because X, the original data matrix,
is n by p and the truncated U matrix is p by k, then the product of the two
matrices give you a n by k matrix. So after this transformation,
our data matrix has n rows corresponding to the n
data objects and only k columns corresponding to k data attributes,
instead of the original p attributes. Isn't that great? Well, I know that PCA sounds a little
bit complex in terms of mathematics. But if you want to do PCA in Python,
it is actually extremely easy./ All you need to do
is to load the sklearn package and then you load the class
of PCA decomposition. All you need to do is to use
the following two lines of code. You declare an object of the PCA class,
right. You tell the the class how many
components you want to keep, and you fit your data to the class. Then you can achieve the transformed data. The new data will have fewer
dimensions from the old data. So to summarize PCA, we know that principal components
are very special patterns of matrix data. And in fact, they're orthogonal
eigenvectors of the covariance matrix. They are very useful because
they can define new orthogonal coordinate system of your matrix. And based on this orthogonal
coordinate system, many of the vector similarity or
distance matrix are much more trustable. Through PCA,
you can transform the original vector data into low dimensional and
orthogonal vector space. And this is why PCA has been a very
useful step of feature engineering in many machine learning tasks. Is this it? In reality, shouldn't we always use PCA? And in fact,
PCA also have some of its limitations. Why? Remember that PCA relies heavily
on eigenvalues and eigenvectors. And we know that eigenvalues and eigenvectors only apply
to square matrices. Also PCA relies on eigendecomposition. And we know that stable eigendecomposition
does not apply to all square matrices. In fact, it requires that
your matrix is symmetric and sometimes positive semi-definite. And that is why we apply PCA
to the covariance matrix XTX, because we know that the covariance
matrix is symmetric and it is also positive semi-definite. But in reality, we know that the original data matrices
we deal with are almost never square. That means we almost always have
a different number of data objects than a number of attributes, right? So, is there a way that we
can do this decomposition directly on non-square matrics? If that is the case, we do not need
to compute the covariance matrix. And we can directly apply decomposition, apply dimension reduction to
our original data matrix.