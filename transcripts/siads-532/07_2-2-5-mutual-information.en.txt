Mutual Information. The reason we're interested
in it is because it is very widely used in text mining. Mutual information measures
the mutual dependency between two random
variables, x and y. It is a classical concept
information theory. It measures the amount
of information obtained about one variable x through observing
another variable y. The higher the
mutual information, the more co-related
the two variables, the more dependent
the two variables. So the equation of mutual
information, and in this case, the full mutual information
I-xy can be calculated as, again, a summation over all possible values
of the variable x, and all possible values
of the variable y. In each condition, we calculate the joint probability that the variable x takes
one particular value, and then variable y takes
another particular value x and y, times the logarithm. In the logarithm, we have the joint probability of x
taking a particular value, y taking another value, normalized by the
marginal probability that x taking the value
and y taking the value. So again, it measures
the joint probability normalized by the product of
two marginal probabilities. In other words, the
observed joint probability and the expected
joint probability, if these two variables x;y do not correlate with each other. Then we take the log, we multiply that by
the joint probability, and we sum up over
all conditions. In our scenario, because
we only care about whether one pattern appear
or not in the transaction, so the particular values of
the two variables x and y, are either 1 or zero. One meaning that the
corresponding variable x or y appears in the transaction. Zero meaning that it does not
appear in the transaction. With this set, let's take a moment to calculate
the mutual information, again, from this two way
condition in the table. The two variables
we're interested in are games and video. To calculate the full
mutual information, we need to enumerate
four possible scenarios. The transaction has
post-game and video, transactions having
game but not video, transactions have
not game but video, and the transactions that have neither of these variables. Suppose we use base-2 logarithm
for mutual information, in reality some people
use other basis, we can calculate the
mutual information of the two rebels game
and video by looking up the numbers in this two-week
condition in the table. In the first condition, we care about the joint
probability of game and video. So we look up the
number 4,000 normalized by the total number
of transactions that gives us the
joint probability 0.4. Then it times the
logarithm of 0.4, normalized by the product of
two marginal probabilities. That gives us the expected
joint probability of G and V, if they don't correlate
with each other at all. So 0.6 comes from this number, normalized by 10,000,
and there upon simplified comes from this
number, normalized by 10,000. That is how we get the
component of one condition. Then again we do
this for one sale, two sales, three
sales, four sales. We sum them up together, that gives us the
total number 0.04, which is not very large, but it is above theorem. That means the correlation
between games and videos, the two variables are positive. Similar to chi-square, full mutual information also did not give us the direction whether viewing more games indicates viewing
more or fewer videos. If we only care about one configuration out
of the four configurations, in practice, we usually do care about just
one configuration. We can compute the so called pointwise
mutual information. That is essentially the ratio of the joint probability of x, taking the value of x, y taking one particular value, normalized by the expected
joint probability, which is calculated by
the product of Px and Py, and then we take the logarithm. So you can see that the
pointwise mutual information is essentially one component of the full mutual information
that corresponds to one particular sale in the 2b condition in
the table like this. Then we can calculate the pointwise mutual information
of games and videos by looking up the
numbers that gives us the number of minus 0.17. For comparison, we also computed the pointwise mutual information
of games and not videos. Surprisingly, that gave us a much higher and
positive number, 0.42. By comparing those two numbers, we can fairly say that
there's the true correlation between buying games
but not buying videos, rather than the
correlation between buying games and buying videos. That's why people like
pointwise mutual information. Mutual information and
point-wise mutual information are widely applied in text mining when
people care about the co-occurrence and
qualification of words. Basically, the idea
is to determine the likelihood that two
words will be used together. In this case, we represent every document as
the state of items, where every word is the item. This revelation is also known as the bag of words
representation. The basic assumption is
that if two words co-occur, then their semantics or their meanings are
somewhat related. Here's the table taken from the famous church
in x study in 1990. The table conduct heat analysis
of a very large coppers, which means a collection
of documents in the associated press
with 15 million words. The authors extracted some interesting and
uninteresting associations of words to the word factor. You can see that there are several columns
that are interesting. In every row, they are evaluating the correlation
between two words. One of the words is doctor. This column fx and this column fy basically tells us the frequency of a single word in
this collection. Fx and y tells us basically
the co-occurrence, the number of times
that the two words co-occur in the same sentences. Now I-xy gives us the pointwise mutual
information of these two words. Now you can see that in the
top portion of the table, there are lots of
interesting associations. Like doctors are
associated with dentists, doctors associated
with with nurses treating and sometimes hospitals, sometimes bills I don't
know what happened. So in this cases, you can see that these words tend to co-occur with doctors, and with the high
mutual information. In the lower portion
of the table, you can see that there are
some other words like with, like a, like is. These words, although
that they even co-occur with the word
doctors more times, there are some of the other
words in the top portion. But because themselves
are very frequent, that gives them a much
lower mutual information. So by ranking the
word associations instance using a
mutual information, we can filter the
true correlations, the true related words
from the uninteresting from the co-occurrences
by chance. This is just one of
examples of applying word co-occurrence and
associations in text mining. [inaudible] there are many other interesting
applications. For example, we use word co-occurrences
for spelling check. Basically, given the word
that is spelled wrong, we can recommend the
other words that people frequently modify this
word into the query log. We can also use word co-occurence for
figuring out polsemy. That means one word
with multiple meanings. Usually, we build tools to disambiguate
different meanings of long words based
on the words that co-occur in the same
context of the target word. We can also leverage
word co- occurrence to other standard
synonyms or words, which means words
with same meanings. We use this for phrase detection,
for entity extraction, and all the basic idea
is to look at what words tend to co-occur with the target words in
the same context. We also use word
co-occurrences and associations for word clustering, for extracting concepts, for extracting topics,
for building ontology and taxonomy from very large
scale text documents.