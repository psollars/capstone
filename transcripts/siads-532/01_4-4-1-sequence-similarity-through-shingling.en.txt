2
00:00:05,900 --> 00:00:09,000
Before introducing the suitable similarity metric for
longer sequences, let's look at another application that is known as
near-duplicate detection. So in this example, you can see the search results of the study of emojis that our research group
have done recently. To our surprise that there
are lots and lots of web pages we can find
that are more or less about the same
interview that we got. So among these web pages, you can see that they
share lots of parts, but they're not
completely the same. This is known as near-duplicates. While this is in our favor
because you always want your work to hit
a wider audience, it is not the good news
for search engine companies. According to the study in 1997, more than 30 percent of web
pages are near-duplicates. The near-duplicates could
be results from mirrors, from local copies,
from page updates, those are good cases. They could also be from
spams and spider-traps. Those are bad news and they could also be due to crawling
errors of their spider. It is important for the
search engines to detect the new duplicates before you insert all the web
pages into their index. In fact, to detect
near-duplicates and to remove them from index is very
time and space consuming. Why is near-duplicate
detection so challenging? Well, you will think how
hard it is to find whether two web pages are
duplications of each other. In fact, strict duplication is easy to handle. Why is that? For those who are familiar
with programming, you can either use hashing
to detect strict duplicates. Because through hashing,
identical strings, no matter how long they are will be assigned the same hash key. Unfortunately, hashing would not work for near-duplicate
detection. If you know hashing, if you just change one character from the
original sequence, it will result in completely
different hash numbers. There are several
similar problems related to near-duplicate
detection. Not only appliable on the web, but also in problems such
as plagiarism detection where the goal is to detect near-duplicates of two documents. Sometimes two longer documents instead of strict duplicates. So what we need is the fast measurement of
sequence similarity, and ideally that should
apply to long sequences with very affordable time cost. And one solution to this has the fancy
name called Shingling. It is also proposed by Andrei
Broder in his work in 1997. The basic idea behind
shingling is to represent long sequence as a set
of overlapping ngrams. Now the concept ngrams come back. Intuitively, given the
sequence of words, you can transform this sequence
into the set of ngrams. In this case, you can
transform this sequence into a set of overlapping
trigrams "to be or" "be or not," "or not to" and "not to be." Four trigrams. What's interesting about
this ngrams is that they are overlapping with each other
in the original sentence. The reason we need this ngrams as a representation is
because they more or less, preserves the local
orders among the items. Then by flattening this sequence into a set of this
overlapping ngrams, we also find a very
effective way, a very efficient way to
deal with similarity. Why is that? Because once we can represent a long sequence
as a set of ngrams, we can apply similarity metrics
that we're familiar with, that applies for item sets to assess the similarity
of the sequence. Of course, we're talking
about the Jaccard similarity, which measures the overlap normalized by the
union of the two sets. The only difference is that in the original use case
of Jaccard distance, we're looking at similarity
of two set of items, and in this case we're
measuring the similarity between two sets of
overlapping ngrams. So let's look at one example. Suppose you have two
sequences X is "to be or not to be" and Y is
"not be or not to be." So you can see the
only difference is the swap of the first word
into a different word. Using shingling, you first find the item set
representation of X prime as a set of
overlapping ngrams. You do this for Y as well
and then you compare these two sets of
overlapping ngrams or the two sets of shingles. You can see that they share three items and they differ
in one of the items. So the Jaccard similarity of the two sets will give you the similarity of
the two sequences, that is three out of five, because the overlap of the two sets are three ngrams and the union has
five unique ngrams. Now, what if we have
a new sequence, Z, that is "be or not to not be," you can see
that the difference is you swap the order of
one of the word from Y. So how does that affect the
similarity of the sequences? Well, through shingling
you can again represent all these three sequences
into three sets of overlapping ngrams or
three sets off shingles. Then you calculate the
Jaccard similarity between X and Y and then between X and Z. You can see that in fact, the Jaccard similarity between
X prime and Y prime is three over five and
similarity between X prime and Z prime
is only two over six. So that gives us the intuitive
metric that sequence Z is actually less similar to
sequence X than sequence Y. actually less similar to
sequence X than sequence Y. So shingling is the
intuitive procedure, it is quite simple. All you need to do is to extract the overlapping ngrams
from the sequence, no matter how long the sequence is and then you calculate
the set similarity. Although it's simple, it
has lots of advantages. The first biggest advantage of shingling is that it is fast. It has the time complexity of O, m plus n instead of On times
n to compare two sequences. What does this mean? This means that the
time-cost will grow just linearly to the lengths of the sequences
instead of quadratic. So this makes it suitable, much more suitable than edit distance to compare
long sequences. Of course, because shingling relies on the Jaccard similarity, it also gets all the advantages of the Jaccard similarity. The similarity is
bounded between zero and one and you can also speed up this calculation using other
more complex algorithms such as random projection. For students who are
interested in this, I suggest you to look
at materials related to Minhash and locality
sensitive hashing. Both are algorithms
to further speed up the calculation of shingling
and Jaccard similarity. Through Minhash, you
don't even need to compare all the items in
the state of shingles. Through locality
sensitive hashing, you don't need to
compare many sequences, database in a pairwise fashion. You only need to assess
the similarities of promising of
intuitively close pairs. But shingling also
have some limitations, because it uses a set of overlapping ngrams to
represent a sequence. This set approximation may
or may not be accurate. It preserves the local orders, but it does not do so
well for global orders. Also, size of the
shingles also matter. You have to make a decision
whether you want to use trigrams or five grams
so and so forth. So in reality, this is the empirical question of how long you want
the shingles to be. There are many variations of the shingling algorithm which tries to estimate other
possible similarity functions. For instance, we
know that because we are approximating the
sequence with set, we're ignoring the
multiple occurrences of the same shingles. So there are algorithms that solve this problem by computing the weighted
Jaccard similarity. That does take into
consideration of multiple occurrences
of the same shingle. There are other
algorithms that tries to preserve the
Euclidean distance, to calculate the
Euclidean distance of the set of shingles rather
than the Jaccard similarity. You can choose from many of them in your real applications. So to summarize what
we have talked about, we've shown how to calculate
sequence similarity. We've shown different ways and we've shown that edit
distance is very powerful, that gives us accurate measure of the minimal number
of edits needed to transform one
sequence to another. We can show that any distance can be calculated through dynamic programming. Because edit distance does not work well for long sequences, it is not time efficient, we introduced shingling
that works very well for assessing the similarity
between very long sequences. All you need to do is
to break the sequence into a set of overlapping
shingles or ngrams, and then calculate the
Jaccard similarity between the two sets. I hope you also remember the
applications we have shown for sequence
similarity calculation such as spelling correction, sequence alignment and
near-duplicate detection.