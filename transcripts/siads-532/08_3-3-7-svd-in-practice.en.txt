Again, although SVD has the complex explanation
in mathematics, to use it in practice
is very simple. Again, just load the
Scikit Learn Package and find the truncated SVD class. All you need to do is
to tell the class that I need an object with only 10 or whatever you
want number of components. Then, you can fit your original
data matrix and transform that into the new data matrix
that have fewer dimensions. So we have introduced two very useful
dimension reduction and data transformation
method; PCA and SVD. Again, those two methods are
quite related to each other. In fact, you can see that PCA does eigendecomposition of the covariance matrix
X transpose X, while SVD decomposes
the original matrix X. Essentially, the
principal components you discover from PCA are the right singular vectors you discover from SVD
or the main matrix. In practice, if you
want to implement PCA, you want to use SVD to
do the implementation. The reason is, to do PCA, you need to calculate
the covariance matrix X transpose X and multiplying two big matrices may result in a
loss of precision, and instead, the solution of SVD is more stable because you do not have to do any special tricks to your original matrix. So in practice, I usually prefer to use SVD because first, it is more flexible, it handles arbitrary matrices
and original matrices, and also the solution
of SVD is more stable. As a summary of singular
value decomposition, we know that like PCA, SVD also provides the orthogonal and
low-dimensional condition for original data matrix. In fact, when we apply SVD
to document-word matrices, it has a special name, a very very fancy name called
latent semantic indexing. The reason is, the
new dimensions you discover from SVD can
be interpreted as the semantic topic or
semantic concept that is more abstract than the
original word dimensions. So SVD or latent semantic
indexing is very widely used in information retrieval and natural language
processing tasks. So with that said, the singular vectors or
the principal components are usually interpreted
as either concepts, or topics, or clusters of your
original data attributes. Although in theory,
we hope that this new coordinates corresponds to an easily interpretable concepts such as the epics movies, such as data science, or such as rapid movies. But in reality, they could
be much harder to interpret. You need to spend
an extra effort on interpreting what you really
discover from SVD and PCA. So to summarize this lecture, I hope you understand
that eigenvectors and principal components are very useful patterns
in matrix data, and this also includes
the singular vectors. I want you to understand
how PCA transforms the matrix data into orthogonal and low
dimensional vectors. Relatively, I want
you to understand how SVD can achieve similar results, and how SVD is performed on the original data
matrix instead of a square covariance matrix.