{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import markdownify\n",
    "\n",
    "\n",
    "def extract_text_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "    # Strip these tags out\n",
    "    [x.extract() for x in soup.findAll([\"style\", \"script\"])]\n",
    "\n",
    "    # Use a single line of html text for the markdown parser\n",
    "    html_content = str(soup).replace(\"\\n\", \"\")\n",
    "\n",
    "    return markdownify.markdownify(html_content, heading_style=\"ATX\")\n",
    "\n",
    "\n",
    "text = extract_text_from_html(\"./raw_syllabi/2021-12_532.html\")\n",
    "\n",
    "with open(f\"./parsed_syllabi/2021-12_532.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with fitz.open(file_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Course Syllabus for SIADS 652: Network Analysis \\n\\nCourse Overview and Prerequisites \\nThis course will introduce students to basic network analysis techniques, emphasizing developing programming skills \\nto manipulate and analyze real network data using Python. The course includes topics such as network evolution, link \\nprediction, network centrality, models of information diffusion on networks, and community structure.   \\n\\nThe prerequisites for SIADS 652 are:  \\n\\n●  SIADS 542 Supervised Learning \\n●  SIADS 505 Data Manipulation \\n●  SIADS 502 Math Methods for Data Science \\n\\nInstructors \\n\\nDaniel M Romero \\nAssociate Professor  \\nSchool of Information \\nComplex Systems \\nComputer Science \\nEmail: drom@umich.edu \\n\\nCory Bilyeu \\nLecturer \\nSchool of Information \\nEmail: cbilyeu@umich.edu \\n\\nYou Wu \\nMaster’s Student \\nSchool of Information \\nEmail: uvuuview@umich.edu \\n\\nCourse Communication Expectations \\n\\nWe will use Slack for most communication related to the class. We will monitor the channel and try to answer your \\nquestions as promptly as possible. This should not happen, but if we do not answer your question within 24 hours (not \\nincluding weekends and holidays, of course), feel free to contact us again.  We also encourage you to answer each \\nother’s questions and use this platform for discussion related to the class. While we will try our best to monitor and \\nclarify or remove wrong or misleading responses, we cannot guarantee that all statements made by students are correct. \\nPlease remember to be civil and treat everyone with kindness and respect. If you prefer not to use Slack, please feel \\nfree to email anyone in the instructional team with comments or questions.  \\n\\nHow to Get Help \\nIf you have questions concerning the degree program, encounter a technical issue with Coursera, or issues using Slack, \\nplease submit a report to the ticketing system at umsimadshelp@umich.edu. \\n\\nIf you have an issue specific to the Coursera environment, you can also begin a live chat session with Coursera \\nTechnical Support (24/7) or view Coursera troubleshooting guides. (you may be asked to log in to your Coursera \\naccount). \\n\\nFor questions regarding course content, refer to the Communications Expectations section above. \\n\\nWeekly Readings or Textbook Information \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0cEasley, David and Kleinberg, Jon. 2010. Networks, Crowds, and Markets: Reasoning About a Highly Connected \\nWorld. Cambridge University Press, USA. \\n\\nNewman, Mark. 2018. Networks. Oxford University Press, USA. \\n\\nLiben‐Nowell, David, and Kleinberg, Jon. 2007. \"The link‐prediction problem for social networks.\" Journal of the \\nAmerican Society for Information Science and Technology. 58(7): 1019-1031. \\n\\nKossinets, Gueorgi, and Duncan J. Watts. 2006. \"Empirical analysis of an evolving social network.\" Science. 311(5757 \\n): 88-90. \\n\\nRomero, Daniel M., Uzzi, Brian, and Kleinberg, Jon. 2016. \"Social networks under stress.\" In Proceedings of the 25th \\nInternational Conference on World Wide Web 9-20.  \\n\\nKempe, David, Kleinberg, Jon, and Tardos, Éva. 2003. \"Maximizing the spread of influence through a social network.\" \\nIn Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining. 137-\\n146. \\n\\nKim, D. A., Hwong, A. R., Stafford, D., Hughes, D. A., O\\'Malley, A. J., Fowler, J. H., & Christakis, N. A. 2015. Social \\nnetwork targeting to maximise population behaviour change: a cluster randomised controlled trial. The Lancet. \\n386(9989): 145-153. \\n\\nYang, Jaewon, and Jure Leskovec. 2015. \"Defining and evaluating network communities based on ground-truth.\" \\nKnowledge and Information Systems. 42(10: 181-213.  \\n\\nTo access the required textbooks, simply click on the links above to direct you to the U-M Library website. Scroll down \\nthe page and click on Available Online (some texts will have multiple online options, but it is recommended you \\nchoose Safari books online).  \\n\\nAfter you are directed to the textbook, you will see an O’Reilly pop-up window asking for you to select your \\ninstitution. U-M is not an available option, so you will need to select the option “Not Listed? Click Here”.  \\n\\nYou will be prompted to input your U-M email address (no password required).  \\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cLearning Outcomes \\n\\n●  Recognize and categorize real world networked data (social, financial, biological, transportation, information, \\netc.) and represent them using the appropriate network type (directed, weighted, signed, multigraph, bipartite, \\netc.).  \\n\\n●  Define, compute, and interpret basic network metrics such as distance, clustering, degree distribution, and \\n\\ncentrality measures.  \\n\\n●  Describe several network generative models and understand the properties of the networks they generate. \\n●  Define, compute, and implement several measures that can be used for link prediction using real world data. \\n●  Articulate and implement the dynamics and assumptions of several information diffusion models and be able to \\n\\nimplement. \\n\\n●  Articulate the influence maximization problem and basic strategies and heuristics to solve it. \\n●  Articulate the community detection problem and understand its value in several applications. \\n●  Articulate and implement several approaches for community detection. \\n\\nCourse Schedule \\n\\n●  This course begins on Jan 4th and ends on Jan 31st. \\n●  Weekly assignments and quizzes will be due on Wednesdays at 11:59 pm (Ann Arbor, Michigan time-Eastern \\nDaylight Time - EST, UTC -5). Assignments and quizzes will be due on 1/11, 1/18, 1/25, and 2/1. Note that the \\nlast deadline is after the official end of the course. If you prefer, you are welcome to submit your work earlier.  \\n\\nAssignment and Quizzes \\n\\nAssignments consist of a combination of programming and reflection questions. The goal of the assignments is to apply \\nthe theory covered in the lecture and readings and the programming covered in the NetworkX tutorials to analyze both \\nreal and synthetic networks. Assignments will be partly auto-graded and partly manually graded. Please refer to the \\ndocument Assignment Points Distribution on Coursera for the point distribution of all assignments.  \\n\\nQuizzes consist of multiple-choice and numerical question. You will have 2 attempts to answer each question and you \\nwill be told which ones you answered correctly and incorrectly after each attempt. The goal of the quizzes is to assess \\nyour understanding of network theory. While you may choose to use NetworkX to answer some of the questions, this \\nshould not be necessary for most quiz questions.  \\n\\n \\n \\n \\n \\n \\n\\x0cWeekly Office Hours via Zoom (Ann Arbor, Michigan time):  \\nYour instructor will hold weekly, synchronous office hours using the video-conferencing tool, Zoom. The schedule of office hours \\ncan be found by clicking on the Live Events link in the left-hand navigation menu. Additionally, all office hours will be recorded \\nand archived so that you can retrieve them later. Archived office hours can be found by clicking on the Resources link in the left-\\nhand navigation menu then clicking the Archived Sessions link.  \\n\\nGrading \\n\\nCourse Item \\n\\nWeek 1 \\n\\nWeek 2 \\n\\nWeek 3 \\n\\nWeek 4 \\n\\nPercentage of Final Grade \\n\\nDue \\n\\nQuiz 7% \\nAssignment 18%  \\n\\nQuiz 7% \\nAssignment 18%  \\n\\nQuiz 7% \\nAssignment 18%  \\n\\nQuiz 7% \\nAssignment 18%  \\n\\nWednesday 1/11 11:59pm \\n\\nWednesday 1/18 11:59pm \\n\\nWednesday 1/25 11:59pm \\n\\nWednesday, 2/1 11:59pm \\n\\nTotal \\n\\n100% \\n\\nNote: All assignments are required to earn credit for this course. \\n\\nLetter Grades, Course Grades, and Late Submission Policy \\n\\nQuizzes can be submitted late with a 10% daily deduction. The week 4 quiz is an exception: it can be submitted within \\n24 hours for a 10% penalty, within 24-48 hours for a 20% penalty, and then no credit will be given.  \\n\\nProgramming assignments can be submitted late with a 25% daily deduction.  \\n\\nThe grading scale for this course is as follows: \\n\\nA+ \\n\\nA \\n\\nA- \\n\\nB+ \\n\\nB \\n\\nB- \\n\\nC+ \\n\\n98% \\n\\n93% \\n\\n90% \\n\\n87% \\n\\n83% \\n\\n80% \\n\\n77% \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c                                      73% \\n\\nC \\n\\nC- \\n\\nD+ \\n\\nD \\n\\nD- \\n\\nF \\n\\n70% \\n\\n67% \\n\\n63% \\n\\n60% \\n\\n0% \\n\\nAcademic Integrity/Code of Conduct \\nRefer to the Academic and Professional Integrity section of the UMSI Student Handbook. (access to Student \\nOrientation course required). \\n\\nAccommodations \\nRefer to the Accommodations for Students with Disabilities section of the UMSI Student Handbook (access to the \\nStudent Orientation course required). Use the Student Intake Form to begin the process of working with the \\nUniversity’s Office of Services for Students with Disabilities. \\n\\nAccessibility \\nRefer to the Screen reader configuration for Jupyter Notebook Content document to learn accessibility tips for Jupyter \\nNotebooks. \\n\\nLibrary Access \\nRefer to the U-M Library’s information sheet on accessing library resources from off-campus. For more information \\nregarding library support services, please refer to the U-M Library Resources section of the UMSI Student Handbook \\n(access to the Student Orientation course required). \\n\\nStudent Mental Health \\nRefer to the University’s Resources for Stress and Mental Health website for a listing of resources for students. \\n\\nStudent Services \\nRefer to the Introduction to UMSI Student Life section of the UMSI Student Handbook (access to the Student \\nOrientation course required). \\n\\nTechnology Tips \\n\\n●  Recommended Technology \\n\\n○  This program requires Jupyter Notebook for completion of problem sets and Adobe or other PDF viewer \\n\\nfor reading articles. \\n\\n●  Working Offline \\n\\n○  While the Coursera platform has an integrated Jupyter Notebook system, you can work offline on your \\n\\nown computer by installing Python 3.5+ and the Jupyter software packages. For more details, consult the \\nJupyter Notebook FAQ. \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pdfminer.high_level import extract_text\n",
    "\n",
    "\n",
    "# def parse_pdf(file_path):\n",
    "#     text = extract_text(file_path)\n",
    "#     lines = text.split(\"\\\\\\\\n\")\n",
    "#     for i, line in enumerate(lines):\n",
    "#         stripped = line.strip()\n",
    "#         if stripped.isupper() and len(stripped) < 50:\n",
    "#             lines[i] = f\"## {stripped}\"\n",
    "#     return \"\\\\\\\\n\".join(lines)\n",
    "\n",
    "# parse_pdf(\"./raw_syllabi/2023-01_652.pdf\")\n",
    "\n",
    "# with open(f\"./parsed_syllabi/2021-12_532.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_records = [\n",
    "    (\"2021-09\", \"505\"),\n",
    "    (\"2021-10\", \"502\"),\n",
    "    (\"2021-10\", \"515\"),\n",
    "    (\"2021-11\", \"521\"), # This one is listed under week 1 and doesn't have a reference in \"resources\"\n",
    "    (\"2021-12\", \"532\"),\n",
    "    # (\"2022-01\", \"501\"), # pdf\n",
    "    (\"2022-01\", \"511\"),\n",
    "    (\"2022-02\", \"522\"),\n",
    "    (\"2022-03\", \"503\"),\n",
    "    (\"2022-04\", \"523\"),\n",
    "    (\"2022-05\", \"542\"), # This one is listed under week 1 materials\n",
    "    (\"2022-06\", \"543\"), # This one is listed under week 1 materials\n",
    "    (\"2022-06\", \"611\"),\n",
    "    (\"2022-07\", \"516\"),\n",
    "    (\"2022-08\", \"622\"),\n",
    "    (\"2022-09\", \"593\"),\n",
    "    (\"2022-11\", \"631\"),\n",
    "    # (\"2023-01\", \"652\"), # pdf\n",
    "    (\"2023-02\", \"632\"),\n",
    "    # (\"2023-02\", \"673\"), # This was just a link to a Google Doc, which I downloaded as a pdf\n",
    "    (\"2023-03\", \"642\"),\n",
    "    # (\"2023-03\", \"643\"), # This was just a link to a Google Doc, which I downloaded as a pdf\n",
    "    (\"2023-04\", \"655\"), # This one is listed under week 1 materials\n",
    "    (\"2023-05\", \"524\"),\n",
    "    (\"2023-06\", \"601\"),\n",
    "    (\"2023-06\", \"630\"),\n",
    "    # (\"2023-09\", \"696\"), # This is a pdf on in a Google Drive folder that I no longer have access to, luckily I kept a copy that I downloaded during the class\n",
    "    (\"2023-11\", \"682\"),\n",
    "    (\"2023-12\", \"688\"),\n",
    "    # (\"2024-02\", \"699\"), # This was just a link to a Google Doc, which I downloaded as a pdf\n",
    "]\n",
    "\n",
    "courses = [f\"{date}_{num}\" for date, num in course_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def extract_text(file_identifier, base_path=\"./raw_syllabi/\"):\n",
    "    html_path = os.path.join(base_path, f\"{file_identifier}.html\")\n",
    "    # pdf_path = os.path.join(base_path, f\"{file_identifier}.pdf\")\n",
    "\n",
    "    if os.path.exists(html_path):\n",
    "        return extract_text_from_html(html_path)\n",
    "    # elif os.path.exists(pdf_path):\n",
    "        # return extract_text_from_pdf(pdf_path)\n",
    "    else:\n",
    "        return f\"Course Syllabus does not exist at: {base_path}{file_identifier}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course in courses:\n",
    "    text = extract_text(course)\n",
    "\n",
    "    with open(f\"./parsed_syllabi/{course}.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25h░░░░⠂⠂⠂⠂⠂⠂⸩ ⠼ build:prettier: sill doSerial build 8\u001b[0m\u001b[K/.npm/_locks\u001b[0m\u001b[K[0m\u001b[Knpx: installed 1 in 1.444s\n",
      "parsed_syllabi/2021-09_505.md\u001b[2K\u001b[1Gparsed_syllabi/2021-09_505.md 94ms\n",
      "parsed_syllabi/2021-09_505.txt\u001b[2K\u001b[1Gparsed_syllabi/2021-10_502.md\u001b[2K\u001b[1Gparsed_syllabi/2021-10_502.md 75ms\n",
      "parsed_syllabi/2021-10_502.txt\u001b[2K\u001b[1Gparsed_syllabi/2021-10_515.md\u001b[2K\u001b[1Gparsed_syllabi/2021-10_515.md 58ms\n",
      "parsed_syllabi/2021-10_515.txt\u001b[2K\u001b[1Gparsed_syllabi/2021-11_521.md\u001b[2K\u001b[1Gparsed_syllabi/2021-11_521.md 37ms\n",
      "parsed_syllabi/2021-11_521.txt\u001b[2K\u001b[1Gparsed_syllabi/2021-12_532.md\u001b[2K\u001b[1Gparsed_syllabi/2021-12_532.md 48ms\n",
      "parsed_syllabi/2021-12_532.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-01_501.md\u001b[2K\u001b[1Gparsed_syllabi/2022-01_501.md 39ms\n",
      "parsed_syllabi/2022-01_501.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-01_511.md\u001b[2K\u001b[1Gparsed_syllabi/2022-01_511.md 41ms\n",
      "parsed_syllabi/2022-01_511.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-02_522.md\u001b[2K\u001b[1Gparsed_syllabi/2022-02_522.md 41ms\n",
      "parsed_syllabi/2022-02_522.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-03_503.md\u001b[2K\u001b[1Gparsed_syllabi/2022-03_503.md 149ms\n",
      "parsed_syllabi/2022-03_503.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-04_523.md\u001b[2K\u001b[1Gparsed_syllabi/2022-04_523.md 58ms\n",
      "parsed_syllabi/2022-04_523.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-05_542.md\u001b[2K\u001b[1Gparsed_syllabi/2022-05_542.md 85ms\n",
      "parsed_syllabi/2022-05_542.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-06_543.md\u001b[2K\u001b[1Gparsed_syllabi/2022-06_543.md 73ms\n",
      "parsed_syllabi/2022-06_543.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-06_611.md\u001b[2K\u001b[1Gparsed_syllabi/2022-06_611.md 25ms\n",
      "parsed_syllabi/2022-06_611.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-07_516.md\u001b[2K\u001b[1Gparsed_syllabi/2022-07_516.md 47ms\n",
      "parsed_syllabi/2022-07_516.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-08_622.md\u001b[2K\u001b[1Gparsed_syllabi/2022-08_622.md 37ms\n",
      "parsed_syllabi/2022-08_622.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-09_593.md\u001b[2K\u001b[1Gparsed_syllabi/2022-09_593.md 66ms\n",
      "parsed_syllabi/2022-09_593.txt\u001b[2K\u001b[1Gparsed_syllabi/2022-11_631.md\u001b[2K\u001b[1Gparsed_syllabi/2022-11_631.md 34ms\n",
      "parsed_syllabi/2022-11_631.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-01_652.md\u001b[2K\u001b[1Gparsed_syllabi/2023-01_652.md 49ms\n",
      "parsed_syllabi/2023-01_652.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-02_632.md\u001b[2K\u001b[1Gparsed_syllabi/2023-02_632.md 43ms\n",
      "parsed_syllabi/2023-02_632.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-02_673.md\u001b[2K\u001b[1G\u001b[90mparsed_syllabi/2023-02_673.md\u001b[39m 28ms (unchanged)\n",
      "parsed_syllabi/2023-02_673.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-03_642.md\u001b[2K\u001b[1Gparsed_syllabi/2023-03_642.md 26ms\n",
      "parsed_syllabi/2023-03_642.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-03_643.md\u001b[2K\u001b[1G\u001b[90mparsed_syllabi/2023-03_643.md\u001b[39m 25ms (unchanged)\n",
      "parsed_syllabi/2023-03_643.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-04_655.md\u001b[2K\u001b[1Gparsed_syllabi/2023-04_655.md 39ms\n",
      "parsed_syllabi/2023-04_655.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-05_524.md\u001b[2K\u001b[1Gparsed_syllabi/2023-05_524.md 53ms\n",
      "parsed_syllabi/2023-05_524.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-06_601.md\u001b[2K\u001b[1Gparsed_syllabi/2023-06_601.md 63ms\n",
      "parsed_syllabi/2023-06_601.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-06_630.md\u001b[2K\u001b[1Gparsed_syllabi/2023-06_630.md 41ms\n",
      "parsed_syllabi/2023-06_630.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-09_696.md\u001b[2K\u001b[1Gparsed_syllabi/2023-09_696.md 73ms\n",
      "parsed_syllabi/2023-09_696.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-11_682.md\u001b[2K\u001b[1Gparsed_syllabi/2023-11_682.md 67ms\n",
      "parsed_syllabi/2023-11_682.txt\u001b[2K\u001b[1Gparsed_syllabi/2023-12_688.md\u001b[2K\u001b[1Gparsed_syllabi/2023-12_688.md 66ms\n",
      "parsed_syllabi/2023-12_688.txt\u001b[2K\u001b[1Gparsed_syllabi/2024-02_699.md\u001b[2K\u001b[1G\u001b[90mparsed_syllabi/2024-02_699.md\u001b[39m 40ms (unchanged)\n",
      "parsed_syllabi/2024-02_699.txt\u001b[2K\u001b[1G"
     ]
    }
   ],
   "source": [
    "!npx prettier ./parsed_syllabi/ --write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./../raw_syllabi/2022-01_501.pdf',\n",
       " './../raw_syllabi/2023-01_652.pdf',\n",
       " './../raw_syllabi/2023-02_673.pdf',\n",
       " './../raw_syllabi/2023-03_643.pdf',\n",
       " './../raw_syllabi/2023-09_696.pdf',\n",
       " './../raw_syllabi/2024-02_699.pdf']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_course_syllabi = [\n",
    "    (\"2022-01\", \"501\"), # pdf\n",
    "    (\"2023-01\", \"652\"), # pdf\n",
    "    (\"2023-02\", \"673\"), # This was just a link to a Google Doc, which I downloaded as a pdf\n",
    "    (\"2023-03\", \"643\"), # This was just a link to a Google Doc, which I downloaded as a pdf\n",
    "    (\"2023-09\", \"696\"), # This is a pdf on in a Google Drive folder that I no longer have access to, luckily I kept a copy that I downloaded during the class\n",
    "    (\"2024-02\", \"699\"), # This was just a link to a Google Doc, which I downloaded as a pdf\n",
    "]\n",
    "\n",
    "pdf_syllabi = [f\"./../raw_syllabi/{date}_{num}.pdf\" for date, num in pdf_course_syllabi]\n",
    "\n",
    "pdf_syllabi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF conversion to Markdown proved to be difficult. It's not realistic to build out a script in this notebook for the 6 files that we have to convert. This Python app, \"marker\" was installed and used on these files, then they were validated for content manually before being formatted with \"prettier\". Full setup and usage instructions at: https://github.com/VikParuchuri/marker\n",
    "\n",
    "# Marker\n",
    "\n",
    "Marker converts PDF, EPUB, and MOBI to markdown.\n",
    "\n",
    "## Mac\n",
    "\n",
    "- Install system requirements from `scripts/install/brew-requirements.txt`\n",
    "- Set the tesseract data folder path\n",
    "  - Find the tesseract data folder `tessdata` with `brew list tesseract`\n",
    "  - Create a `local.env` file in the root `marker` folder with `TESSDATA_PREFIX=/path/to/tessdata` inside it\n",
    "- Install python requirements\n",
    "  - `poetry install`\n",
    "  - `poetry shell` to activate your poetry venv\n",
    "\n",
    "# Usage\n",
    "\n",
    "First, some configuration.  Note that settings can be overridden with env vars, or in a `local.env` file in the root `marker` folder.\n",
    "\n",
    "- Your torch device will be automatically detected, but you can manually set it also.  For example, `TORCH_DEVICE=cuda` or `TORCH_DEVICE=mps`. `cpu` is the default.\n",
    "  - If using GPU, set `INFERENCE_RAM` to your GPU VRAM (per GPU).  For example, if you have 16 GB of VRAM, set `INFERENCE_RAM=16`.\n",
    "  - Depending on your document types, marker's average memory usage per task can vary slightly.  You can configure `VRAM_PER_TASK` to adjust this if you notice tasks failing with GPU out of memory errors.\n",
    "- Inspect the other settings in `marker/settings.py`.  You can override any settings in the `local.env` file, or by setting environment variables.\n",
    "  - By default, the final editor model is off.  Turn it on with `ENABLE_EDITOR_MODEL=true`.\n",
    "  - By default, marker will use ocrmypdf for OCR, which is slower than base tesseract, but higher quality.  You can change this with the `OCR_ENGINE` setting.\n",
    "\n",
    "## Convert a single file\n",
    "\n",
    "Run `convert_single.py`, like this:\n",
    "\n",
    "```\n",
    "python convert_single.py /path/to/file.pdf /path/to/output.md --parallel_factor 2 --max_pages 10\n",
    "```\n",
    "\n",
    "- `--parallel_factor` is how much to increase batch size and parallel OCR workers by.  Higher numbers will take more VRAM and CPU, but process faster.  Set to 1 by default.\n",
    "- `--max_pages` is the maximum number of pages to process.  Omit this to convert the entire document.\n",
    "\n",
    "Make sure the `DEFAULT_LANG` setting is set appropriately for your document.\n",
    "\n",
    "## Convert multiple files\n",
    "\n",
    "Run `convert.py`, like this:\n",
    "\n",
    "```\n",
    "python convert.py /path/to/input/folder /path/to/output/folder --workers 10 --max 10 --metadata_file /path/to/metadata.json --min_length 10000\n",
    "```\n",
    "\n",
    "- `--workers` is the number of pdfs to convert at once.  This is set to 1 by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Parallelism will not increase beyond `INFERENCE_RAM / VRAM_PER_TASK` if you're using GPU.\n",
    "- `--max` is the maximum number of pdfs to convert.  Omit this to convert all pdfs in the folder.\n",
    "- `--metadata_file` is an optional path to a json file with metadata about the pdfs.  If you provide it, it will be used to set the language for each pdf.  If not, `DEFAULT_LANG` will be used. The format is:\n",
    "- `--min_length` is the minimum number of characters that need to be extracted from a pdf before it will be considered for processing.  If you're processing a lot of pdfs, I recommend setting this to avoid OCRing pdfs that are mostly images. (slows everything down)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
